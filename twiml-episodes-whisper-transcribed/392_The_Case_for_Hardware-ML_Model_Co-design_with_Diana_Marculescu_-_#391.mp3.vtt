WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:16.600
I'm your host, Sam Charrington.

00:16.600 --> 00:29.760
Alright everyone, I am on the line with Deanna, Marcalescu.

00:29.760 --> 00:35.240
Deanna is department chair and professor of electrical and computer engineering at the

00:35.240 --> 00:38.240
University of Texas at Austin.

00:38.240 --> 00:41.000
Deanna, welcome to the Twimal AI Podcast.

00:41.000 --> 00:42.160
Thank you for having me, Sam.

00:42.160 --> 00:43.400
I'm happy to be here.

00:43.400 --> 00:49.680
I am super excited to have you on the show, especially since just before we started recording

00:49.680 --> 00:51.960
you mentioned that you're a long time listener.

00:51.960 --> 00:57.800
And I always love talking to listeners on the show and having the opportunity to host

00:57.800 --> 00:58.800
them on the show.

00:58.800 --> 01:01.360
So thank you for that.

01:01.360 --> 01:04.160
Being a long time listener you know the routine, I'm going to ask you to share a little bit

01:04.160 --> 01:08.920
about your background and how you came to work in machine learning.

01:08.920 --> 01:09.920
Sure.

01:09.920 --> 01:11.280
I'm happy to do so.

01:11.280 --> 01:16.880
Yeah, actually I'd be listening to the podcast for quite some time, especially on my

01:16.880 --> 01:23.800
trips to conferences and more than likely currently we're not traveling that much.

01:23.800 --> 01:28.520
However, we try to do our best to stay up to date by listening to podcasts, especially

01:28.520 --> 01:29.760
in this domain.

01:29.760 --> 01:34.960
My trajectory to working in this field is a little bit less traditional, I come from

01:34.960 --> 01:37.000
the computer hardware systems area.

01:37.000 --> 01:40.800
I've worked for more than 20 years on making computer hardware systems more efficient,

01:40.800 --> 01:43.680
especially more power efficient.

01:43.680 --> 01:50.320
And the way I got to work in machine learning in particular came from the applications side.

01:50.320 --> 01:57.320
So we've used machine learning to make or to perform power management for multicore systems

01:57.320 --> 01:59.600
in the last decade or so.

01:59.600 --> 02:04.640
But more recently in the last three to four years my students and I decided to turn it

02:04.640 --> 02:09.400
around and instead of using machine learning as an application to make computer systems

02:09.400 --> 02:15.640
more efficient, instead we turned our focus to making machine learning models more efficient

02:15.640 --> 02:21.840
such that they run better on existing computer systems or trying to find ways to build computer

02:21.840 --> 02:25.520
systems that are more amenable to running machine learning models and applications.

02:25.520 --> 02:30.680
So that's where the whole idea of the code design of both the machine learning model

02:30.680 --> 02:33.640
and the computer hardware system came into play.

02:33.640 --> 02:39.680
So our first publication in the field was three years ago and ever since we continue

02:39.680 --> 02:41.320
to work in in this field.

02:41.320 --> 02:42.320
Great, great.

02:42.320 --> 02:47.080
And you recently had an opportunity to share some of what you're doing in the field at

02:47.080 --> 02:53.440
the efficient deep learning in computer vision workshop at the recent CVPR conference

02:53.440 --> 03:01.800
where you gave a keynote and working in this field and talking about issues like the

03:01.800 --> 03:10.240
efficiency of machine learning models, often questions about the edge versus cloud come

03:10.240 --> 03:11.240
up.

03:11.240 --> 03:17.000
Did your research focus on one or the other of these settings?

03:17.000 --> 03:20.720
Sure, actually both are really important.

03:20.720 --> 03:26.320
So thinking about where we are now, much of the machine learning applications actually

03:26.320 --> 03:28.680
do run in the cloud.

03:28.680 --> 03:33.320
So for both training, we train them in the cloud, much of the inference also happens in

03:33.320 --> 03:39.520
the cloud if you're trying to run a speech recognition, simple app on your phone or smart

03:39.520 --> 03:43.920
watch, if it's not connected to the network, it's not going to be able to accomplish its

03:43.920 --> 03:47.200
task because everything does happen in the cloud.

03:47.200 --> 03:52.960
On the other hand, our work tries to democratize access to machine learning as an application

03:52.960 --> 03:58.720
such that it's pushing both inference as well as training to edge devices.

03:58.720 --> 04:03.040
So this is where the idea of making machine learning models in particular neural networks

04:03.040 --> 04:09.320
more efficient and more amenable to be run on tiny devices where constraints are really

04:09.320 --> 04:10.640
important.

04:10.640 --> 04:16.080
So yeah, our focus is mostly on edge devices, although looking at the entire continuum

04:16.080 --> 04:20.280
from edge all the way to the cloud is just as important.

04:20.280 --> 04:25.280
So your keynote at the workshop is titled putting the machine back in machine learning,

04:25.280 --> 04:29.680
the case for hardware, ML model co-design.

04:29.680 --> 04:35.640
And it sounds like co-design is a key idea in your research.

04:35.640 --> 04:44.080
What it can know for me is that often we hold either one, the machine or the algorithm

04:44.080 --> 04:50.120
as fixed and try to optimize the other, you know, with the constraint of the thing that

04:50.120 --> 04:51.320
we hold as fixed.

04:51.320 --> 04:58.640
And what I'm guessing here or imagining is that you're treating both of these as things

04:58.640 --> 05:00.760
that we can kind of co-optimize together.

05:00.760 --> 05:03.520
Is that the general idea of your research?

05:03.520 --> 05:05.240
That's correct.

05:05.240 --> 05:10.800
And actually the co-design idea has been used for quite some time in embedded systems.

05:10.800 --> 05:15.440
For a long time, the idea that you want to customize a hardware that runs the embedded

05:15.440 --> 05:21.760
software has been at the forefront for the last 15 to 20 years in the field of embedded

05:21.760 --> 05:23.320
system design.

05:23.320 --> 05:24.840
So it's not a new concept.

05:24.840 --> 05:30.120
What's new is actually putting the focus on the machine in the machine learning domain.

05:30.120 --> 05:36.400
And machine has never left to be honest, the machine learning space, but we're just trying

05:36.400 --> 05:43.400
to put the focus back on the hardware because a lot of the focus that has been happening

05:43.400 --> 05:48.120
recently has been on the performance of machine learning applications, meaning accuracy or

05:48.120 --> 05:54.480
classification error, or things that actually tell us whether it does its job in the proper

05:54.480 --> 05:55.760
way.

05:55.760 --> 06:01.200
Putting things in a historical perspective, machine learning and in particular neural network

06:01.200 --> 06:07.000
research has been going on for quite some time, but it did not get at the level as we see

06:07.000 --> 06:13.680
today without the progress that we've seen in hardware design and everything we've seen

06:13.680 --> 06:17.400
from the single core to multi-core to GPU progress.

06:17.400 --> 06:24.160
So it's important to understand that the reason why machine learning applications have received

06:24.160 --> 06:28.480
so much attention and widespread usage is because of the hardware.

06:28.480 --> 06:34.520
But we're not going to be able to write that wave of dissemination without support from

06:34.520 --> 06:35.520
the hardware.

06:35.520 --> 06:43.800
So our focus is to put the machine specifics and machine hardware metrics into the design

06:43.800 --> 06:46.440
process for neural networks.

06:46.440 --> 06:50.160
And when you do that, you have one half of the problem you mentioned, right?

06:50.160 --> 06:53.240
When you think about code design, you have to add the other half of the problem.

06:53.240 --> 06:57.440
It's not just that the neural network has to be designed with a hardware in mind, but

06:57.440 --> 07:01.840
at the same time, we need to design the hardware with the application in mind in this particular

07:01.840 --> 07:03.080
case machine learning.

07:03.080 --> 07:09.200
So yes, code design is the end goal and we are, we are not quite there yet.

07:09.200 --> 07:13.440
There's quite a few researchers working in this field, but there are a few components

07:13.440 --> 07:17.000
that are needed before we get to true code design.

07:17.000 --> 07:21.320
And one is understanding what are the hardware metrics we need to look at?

07:21.320 --> 07:25.080
How do we expose them to the machine learning design process?

07:25.080 --> 07:30.280
And then how do we design neural network models that take that into account such that they

07:30.280 --> 07:36.200
fit the hardware that we might be able to have or we might be able to design for them?

07:36.200 --> 07:42.360
So it's actually a true cycle, iterative cycle of optimization.

07:42.360 --> 07:50.200
You say that we need to focus on hardware and putting the hardware back into the process.

07:50.200 --> 07:54.960
And maybe I'm asking the same question again, or maybe I'm just going to provide the

07:54.960 --> 07:59.840
answer, but my sense is that, you know, there has been a lot of focus on hardware in

07:59.840 --> 08:04.480
the fields, you know, clearly, you know, different types of accelerators.

08:04.480 --> 08:13.480
What you're saying here is we've focused on hardware for optimizing hardware for a specific

08:13.480 --> 08:14.480
class of problems.

08:14.480 --> 08:19.080
This is going to be an accelerator that works well at, you know, deep neural networks,

08:19.080 --> 08:26.720
as opposed to what I'm seeing in your research is a specific neural network architecture.

08:26.720 --> 08:31.040
We're going to kind of design the hardware for that, kind of design the neural network

08:31.040 --> 08:32.600
to work well with that hardware.

08:32.600 --> 08:35.080
Do all of this at the same time.

08:35.080 --> 08:40.480
And overall, we're going to end up with a system that, you know, performs better than

08:40.480 --> 08:46.080
hopefully the, you know, things that aren't designed, you know, so in such an integrated

08:46.080 --> 08:47.080
manner.

08:47.080 --> 08:50.840
Yes, I mean, that's exactly the point, right?

08:50.840 --> 08:56.360
So any or right, there has been a lot of hardware focus lately.

08:56.360 --> 09:00.440
Maybe it started a little bit, you know, with proxies, right?

09:00.440 --> 09:01.880
They weren't really hard on metrics.

09:01.880 --> 09:06.120
There were maybe flops, flop count, floating point operations.

09:06.120 --> 09:10.320
Maybe it was model size.

09:10.320 --> 09:13.880
Maybe it was some sort of proxy for latency.

09:13.880 --> 09:20.360
What we're trying to do is characterize or provide ways or methodologies to characterize neural

09:20.360 --> 09:26.640
networks from their latency, power, energy perspective on a given hardware.

09:26.640 --> 09:32.640
But at the same time, identify ways to build hardware that minimizes those metrics.

09:32.640 --> 09:34.040
If it makes sense, right?

09:34.040 --> 09:41.000
So the mentioning, you mentioning the talk I gave at the workshop at CVPR.

09:41.000 --> 09:47.760
So one piece that is required to make this code design process possible is the ability

09:47.760 --> 09:54.440
to characterize neural network architectures or perhaps components thereof in terms of

09:54.440 --> 10:00.400
their, the time it takes to process those things, the energy or power it takes and maybe

10:00.400 --> 10:05.720
come up with a joint metric that characterizes their energy efficiency or latency per correct

10:05.720 --> 10:07.440
inference if you want.

10:07.440 --> 10:14.200
In other words, come up with combined metrics that capture both hardware as well as accuracy

10:14.200 --> 10:15.280
in some sense.

10:15.280 --> 10:23.280
So providing that methodology for estimating power, energy and latency for these, it's

10:23.280 --> 10:28.680
actually a piece of software, right, that you write is one of the steps that allows us

10:28.680 --> 10:34.680
to identify the right configuration or architecture for the neural network because you can't optimize

10:34.680 --> 10:38.440
what you cannot model or characterize, right?

10:38.440 --> 10:40.640
And actually, it's easier.

10:40.640 --> 10:43.640
It's actually pretty easy to do that.

10:43.640 --> 10:49.960
We've done this for quite some time in the computer or hardware embedded system domain.

10:49.960 --> 10:55.360
It's much harder to have the same similar kind of model for accuracy.

10:55.360 --> 11:00.160
We can tell how accurate is a neural network until we actually train it.

11:00.160 --> 11:06.200
On the other hand, for a given architecture, we can easily tell what the power or latency

11:06.200 --> 11:10.280
will be for a given hardware platform once you characterize it, right?

11:10.280 --> 11:15.880
So it's a very low-hanging fruit that we should take advantage of.

11:15.880 --> 11:20.240
The idea there is expressed in some research that you've done called neural power.

11:20.240 --> 11:27.840
Is that the one and you're, you know, we've previously talked about, you know, characterizing

11:27.840 --> 11:34.880
neural networks in terms of, you know, things like depth and parameters and layers, and we

11:34.880 --> 11:41.400
talk about hardware in this totally different language, which is flops and power and things

11:41.400 --> 11:42.400
like that.

11:42.400 --> 11:49.320
And you're trying to kind of unify the way that we're able to talk about this.

11:49.320 --> 11:52.080
And you're doing it using a neural network.

11:52.080 --> 11:53.080
Is that right?

11:53.080 --> 11:57.720
Actually, we use machine learning for that.

11:57.720 --> 12:07.400
So we build models for latency and power that we fit using data that we collect from a

12:07.400 --> 12:11.400
lot of synthetically generating neural networks and constructs, right?

12:11.400 --> 12:18.680
Because we look at different types of convolutional layers, fully connected layers, pulling layers,

12:18.680 --> 12:23.440
things that you typically find in neural networks, deep neural networks.

12:23.440 --> 12:28.760
So we try to build a power and a latency model and, of course, also an energy model that

12:28.760 --> 12:31.080
captures all those components.

12:31.080 --> 12:36.240
And then in a composable fashion, you're able to say, once you've done this characterization

12:36.240 --> 12:42.360
and you fit the model once, you can tell just by looking at the configuration of the network,

12:42.360 --> 12:46.680
the latency and to end for an inference will be this much and the power consumption will

12:46.680 --> 12:47.680
be this much.

12:47.680 --> 12:49.920
You don't even have to run it, right?

12:49.920 --> 12:56.560
So you collect the data, you fit the model once, and then you use it over and over again.

12:56.560 --> 13:02.240
And the good thing about this is that you can use it in the optimization process when you

13:02.240 --> 13:04.400
do say architecture search.

13:04.400 --> 13:07.080
Architecture search is very time consuming.

13:07.080 --> 13:13.080
But the fact that you can characterize every single component or block of this architecture

13:13.080 --> 13:21.240
space in a zero cost of one kind of complexity, it's really appealing because it allows you

13:21.240 --> 13:24.720
to put it in this iterative optimization process.

13:24.720 --> 13:32.200
So that's where we took this expertise in characterizing hardware metrics and used it

13:32.200 --> 13:35.840
to identify neural architectures that satisfy hardware constraints.

13:35.840 --> 13:40.960
So this is the second part of the talk that I had that talked about neural architecture

13:40.960 --> 13:43.240
search using a single path approach.

13:43.240 --> 13:50.120
And inside that iterative optimization loop, we did have this kind of latency model.

13:50.120 --> 13:54.600
In that particular case, we were looking at the latency on a very specific hardware platform

13:54.600 --> 13:56.400
Pixel 1.

13:56.400 --> 14:03.560
And so is the idea that if you're applying neural architecture search without the metrics

14:03.560 --> 14:10.680
that you have available via neural power, you would have to run as long an expensive process

14:10.680 --> 14:15.560
and come up with a bunch of candidate models that after the fact you have to see if they

14:15.560 --> 14:20.040
satisfy your hardware constraints, but here you're able to integrate it in and in doing

14:20.040 --> 14:27.120
so it's kind of cut off some dead ends that your search process might need to run through.

14:27.120 --> 14:28.680
Yes, that's right.

14:28.680 --> 14:34.960
So you can prune the search process by removing candidates that do not meet maybe your latency

14:34.960 --> 14:35.960
constraints.

14:35.960 --> 14:40.920
You want to have each inference take less than a few tens of milliseconds, right?

14:40.920 --> 14:45.480
By having this characterization done once and reuse it over and over, you can prune the

14:45.480 --> 14:46.640
search process.

14:46.640 --> 14:53.240
You can also, the way we did it, we actually incorporated in our loss function a term that

14:53.240 --> 14:56.920
captures the cost in terms of latency.

14:56.920 --> 15:03.440
And by doing so, we're able to basically make the neural architecture search process hardware

15:03.440 --> 15:05.240
of aware.

15:05.240 --> 15:10.320
And there are other components to the approach that we've had because it relies on the concept

15:10.320 --> 15:15.960
of a super network that actually shares, it shares its weights across multiple different

15:15.960 --> 15:17.720
types of structures.

15:17.720 --> 15:24.240
But I mean, that's one component, but the reason why it's actually pretty powerful is because

15:24.240 --> 15:32.440
it identifies networks that satisfy these hardware constraints in this case latency while

15:32.440 --> 15:35.640
at the same time doing this in a very efficient fashion.

15:35.640 --> 15:40.720
So like you said, instead of identifying a bunch of candidates that satisfy accuracy,

15:40.720 --> 15:47.080
and then we check does it satisfy latency constraints, we're able to do this through just a single

15:47.080 --> 15:48.080
process.

15:48.080 --> 15:50.080
So it's more efficient.

15:50.080 --> 15:57.480
There's some property of the search space that tells you that if you are looking at

15:57.480 --> 16:03.840
a particular model and it doesn't meet your power constraints, some evolution of that,

16:03.840 --> 16:09.600
not to assume that you're using evolutionary models in here, won't then be better than

16:09.600 --> 16:13.240
the thing that you're looking at, like a convexity property or something like that.

16:13.240 --> 16:20.240
Is that an assumption or is that something that is just kind of demonstrably true about

16:20.240 --> 16:23.760
these search bases for some set of models?

16:23.760 --> 16:24.760
Yeah.

16:24.760 --> 16:27.760
Well, I think it's an excellent question.

16:27.760 --> 16:33.640
We just started to scratch the surface on that our particular approach is a differentiable

16:33.640 --> 16:39.560
search approach, meaning we cast the problem as a differentiable function and then you

16:39.560 --> 16:45.160
just use anything that works on that, like a typically stochastic gradient is and will

16:45.160 --> 16:50.280
make convexity assumptions, although they might not be, they might hold in reality, but

16:50.280 --> 16:53.920
it works pretty well in practice.

16:53.920 --> 16:58.000
And in our case, you're absolutely right, the efficiency of architecture search comes

16:58.000 --> 16:59.000
from two components.

16:59.000 --> 17:00.400
One is the search space.

17:00.400 --> 17:05.960
So what are the components that we actually allow to be the building blocks for our neural

17:05.960 --> 17:06.960
networks?

17:06.960 --> 17:07.960
So that's one.

17:07.960 --> 17:13.600
And the second is how do we do the search and you did allude to say evolutionary algorithms.

17:13.600 --> 17:16.480
There's also reinforcement learning that you can use.

17:16.480 --> 17:22.400
We use a differentiable approach in which all the choices that we make in terms of the

17:22.400 --> 17:29.360
architecture, although they are discrete choices, we perform a transformation that allows

17:29.360 --> 17:32.960
us to do a differentiable search on them.

17:32.960 --> 17:39.560
So apply optimization algorithms that typically are used on those kinds of objective functions.

17:39.560 --> 17:44.640
So the closest or the earliest work that is in the same kind of vein as us is darts,

17:44.640 --> 17:48.880
which does differentiable architecture search for neural networks.

17:48.880 --> 17:51.840
The difference for in our case is the search space.

17:51.840 --> 17:57.160
So the basic components that we consider is broader.

17:57.160 --> 18:01.400
But at the same time, the way we identify the different selections that take us to the

18:01.400 --> 18:10.040
final architecture is done such that we share much of the, for example, for a three by three

18:10.040 --> 18:15.720
five by five seven by seven convolutional kernel, we basically share all the weights.

18:15.720 --> 18:20.080
So the only thing we need to know for a five by five is that we use a three by three plus

18:20.080 --> 18:22.240
the outer layer that takes us to five by five.

18:22.240 --> 18:29.640
So in some sense, it's less for us to identify, you get, it puts more constraints on the

18:29.640 --> 18:34.400
search process, but it's more efficient in the kinds of things that you can't find out.

18:34.400 --> 18:43.120
So, and it is able, so our word that was published last at the ECML PKDD conference last September

18:43.120 --> 18:48.480
in Germany, and we extended it for for a journal paper this year.

18:48.480 --> 18:55.120
It allows us this more efficient search process allowed us to find a more accurate neural

18:55.120 --> 19:00.360
architecture than in the case where you would have more freedom to search.

19:00.360 --> 19:05.440
So I guess it's a fine balance between the exploitation and exploration.

19:05.440 --> 19:12.560
And we were able to find that somehow at the same time satisfying this hardware constraints

19:12.560 --> 19:14.480
in terms of latency.

19:14.480 --> 19:19.280
And interestingly, at the time when we did our comparisons, we were able to be better in

19:19.280 --> 19:21.440
terms of accuracy compared to the manuals.

19:21.440 --> 19:27.960
So to speak designs like mobile net V2, V3 that, I mean, their manual, their zero cost

19:27.960 --> 19:33.880
so to speak, but there was a lot of engineering time, a lot of hours spent by engineers behind

19:33.880 --> 19:35.200
those efforts.

19:35.200 --> 19:39.680
So I don't, I wouldn't want to call them zero time search costs, but, you know, their

19:39.680 --> 19:44.280
manual, single designs, and then, you know, we did even better in terms of the search

19:44.280 --> 19:48.360
cost and accuracy compared to other types of neural architecture approaches.

19:48.360 --> 19:53.640
So because of that, we were able to, because of the efficient search space and the way

19:53.640 --> 19:58.720
we searched, so the two components put together, we were able to push the search time to hours

19:58.720 --> 19:59.880
as opposed to days.

19:59.880 --> 20:04.320
So I think that's where the strength came in only a few epochs.

20:04.320 --> 20:07.000
We were able to find this kind of architecture.

20:07.000 --> 20:13.600
So those are the two strengths are one ability to characterize hardware metrics inside the

20:13.600 --> 20:19.760
optimization process as well as the second component, which is the more efficient search

20:19.760 --> 20:21.520
process overall.

20:21.520 --> 20:30.440
Is the resulting architecture that you are comparing to whatever your baseline is, the

20:30.440 --> 20:39.120
output of the single path network architecture search process, or do you also, you know, apply

20:39.120 --> 20:45.040
other processes to that like quantization or pruning or, you know, compression, other

20:45.040 --> 20:46.560
compression techniques?

20:46.560 --> 20:47.560
Sure.

20:47.560 --> 20:51.920
I mean, single path NASS does just that, the former.

20:51.920 --> 20:57.800
So whatever you said, it identifies the architecture, but of course, on top of that, you can perform

20:57.800 --> 21:01.400
other things like quantization, compression.

21:01.400 --> 21:05.680
There's an interesting question as to whether doing this equationally.

21:05.680 --> 21:10.680
So first, identifying the architecture and then performing compression and or quantization

21:10.680 --> 21:14.760
leads us to a better result rather than doing it together.

21:14.760 --> 21:19.920
I think there's a very interesting, there's some emerging work in that space.

21:19.920 --> 21:26.560
We're also looking at ways to identify how quantization or precision might play a role

21:26.560 --> 21:29.000
in this architecture search process.

21:29.000 --> 21:32.160
But to answer your question, you can do this.

21:32.160 --> 21:36.800
So once you identify a neural architecture, you can apply quantization.

21:36.800 --> 21:40.520
You can apply model compression or pruning, right?

21:40.520 --> 21:45.800
Those are techniques that exist, you know, going back to the presentation I had at the

21:45.800 --> 21:51.400
CFPR workshop, we did present some of the ideas that we had in that space, especially

21:51.400 --> 21:57.400
as they relate to quantization as well as to pruning, channel pruning.

21:57.400 --> 22:03.560
And so in the quantization space, the idea, I mean, there's been quite a bit of work, you

22:03.560 --> 22:09.320
know, ranging from binarized neural networks all the way to using limited precision, fixed

22:09.320 --> 22:13.920
point, limiting the number of bits for representation.

22:13.920 --> 22:19.480
Our idea was actually pretty simple, so that we published first on this in 2017 and then

22:19.480 --> 22:25.520
ever since we refined that further, we wanted to limit the number of bits representing

22:25.520 --> 22:28.600
weights to a fixed number.

22:28.600 --> 22:33.400
And it turned out that in practice, using just one or two bits is sufficient to get the

22:33.400 --> 22:38.680
best in terms of efficiency, but still not miss much in terms of accuracy.

22:38.680 --> 22:45.040
So the idea is that for each weight, we identify the, say, the combination of two powers of

22:45.040 --> 22:50.800
two that, when stochastically rounded, give you the best accuracy.

22:50.800 --> 22:58.640
So we have a few examples in the original paper where we show how using stochastic rounding

22:58.640 --> 23:02.960
actually on average will give you, you know, in theory, will give you the same accuracy

23:02.960 --> 23:07.440
as the original, unquantized neural network.

23:07.440 --> 23:12.200
So we call this light an N, so light weight neural networks.

23:12.200 --> 23:16.720
And light an NK uses K bits to represent the weights.

23:16.720 --> 23:21.760
We still maintain the activations in the original precision.

23:21.760 --> 23:27.800
And it turns out that you can get to up to two orders of magnitude, reduction in the

23:27.800 --> 23:34.680
either power, memory, storage, and still maintain the original accuracy.

23:34.680 --> 23:35.760
So this is pretty powerful.

23:35.760 --> 23:41.640
This was for Cypher 10 data set and then we extended it for ImageNet.

23:41.640 --> 23:46.440
Of course, the downside for using a quantized model is that to actually get these benefits

23:46.440 --> 23:52.280
you will need to build an accelerator that relies on replacing every multiply, accumulate,

23:52.280 --> 23:58.600
or MAC operation with these logic operators, operators that just do shifts and adds, right?

23:58.600 --> 24:02.280
And GPUs are not readily doing that.

24:02.280 --> 24:06.000
You could program CPUs to do that, but might take some effort.

24:06.000 --> 24:10.720
So I think the, what is out there right now is that if you're going to use this type of

24:10.720 --> 24:15.480
quantization, you will need to build a specialized piece of hardware to do it.

24:15.480 --> 24:20.840
So there will be true accelerators, but I think the results are pretty encouraging for

24:20.840 --> 24:26.120
us to continue to do so because it does achieve quite a bit of energy and hardware efficiency

24:26.120 --> 24:30.000
in general for not much of a loss in accuracy.

24:30.000 --> 24:33.400
From the other perspective, you could also do pruning, right?

24:33.400 --> 24:37.440
So you could consider how you want to prune the network.

24:37.440 --> 24:42.640
You don't want to lose much in the accuracy, but you want to simplify the model even further.

24:42.640 --> 24:46.760
So our work in that space, and actually that work was just published in the conference

24:46.760 --> 24:50.080
in CVPR as an oral presentation.

24:50.080 --> 24:55.440
The idea is, so the name we used is ledger.

24:55.440 --> 25:02.960
Basically is using a layer-based global ranking to identify where do we want to do pruning?

25:02.960 --> 25:05.320
So we do pruning based on importance.

25:05.320 --> 25:07.800
So we don't want to prune uniformly across the board.

25:07.800 --> 25:12.760
We want to prune only where accuracy is not going to suffer the most.

25:12.760 --> 25:17.280
And by doing so, you're able to achieve additional efficiency on the model site.

25:17.280 --> 25:20.720
And of course, that translates into hardware efficiency as well.

25:20.720 --> 25:24.880
So was the name of that paper ledger and intentional head fake to get people excited about

25:24.880 --> 25:28.360
some Bitcoin quantization approach?

25:28.360 --> 25:30.720
Hopefully.

25:30.720 --> 25:34.240
I think we need to ask my student why he chose that.

25:34.240 --> 25:36.520
I like the name.

25:36.520 --> 25:44.800
Going back to NeuroPower and this model for the metrics.

25:44.800 --> 25:48.600
You touched on the data source, but can you elaborate on that a little bit?

25:48.600 --> 25:51.560
Did you run a bunch of simulations?

25:51.560 --> 25:58.880
Did you pull power data off of actual hardware running some suite of models?

25:58.880 --> 26:01.120
That sounds very expensive and hard.

26:01.120 --> 26:03.600
How did you collect data to fit a model?

26:03.600 --> 26:04.600
Yeah.

26:04.600 --> 26:05.960
So that's an excellent question.

26:05.960 --> 26:08.680
We actually did a lot of profiling.

26:08.680 --> 26:14.440
So we started with one GPU platform and that we added more GPU platforms.

26:14.440 --> 26:18.240
And like I said, we as machine learning to build the power and latency models.

26:18.240 --> 26:21.200
So we needed a lot of data to train those models.

26:21.200 --> 26:26.160
So we relied on profiling on several GPU platforms.

26:26.160 --> 26:28.360
We profiled power.

26:28.360 --> 26:30.400
We profile latency.

26:30.400 --> 26:36.480
We profiled also memory usage, although we did not use it explicitly, except for a component

26:36.480 --> 26:39.040
in one of our models.

26:39.040 --> 26:43.880
And also at the same time, we had to profile a lot of these synthetic neural networks that

26:43.880 --> 26:48.880
exhibited different types of combinations of convolutional and fully connected and pulling

26:48.880 --> 26:49.880
layers.

26:49.880 --> 26:52.080
So we took a lot of effort.

26:52.080 --> 26:53.560
That's true.

26:53.560 --> 26:59.840
The idea though is if you are using any of those platforms, you can use data that we

26:59.840 --> 27:01.200
collected.

27:01.200 --> 27:06.760
But of course, if you want to use your own platform, you can use our methodology.

27:06.760 --> 27:09.080
So our code is available.

27:09.080 --> 27:14.920
You can use your own platform, perform the same type of profiling and then fit your own

27:14.920 --> 27:17.480
power and if latency model.

27:17.480 --> 27:24.120
So it's a methodology that allows us and others allows everyone to apply on different types

27:24.120 --> 27:25.120
of platforms.

27:25.120 --> 27:26.880
But it is data intensive.

27:26.880 --> 27:31.200
As you collect the data, though, it's just a one time kind of thing.

27:31.200 --> 27:32.200
Great.

27:32.200 --> 27:40.760
And you did that for, is it a handful of GPU architectures or, I guess, how specific

27:40.760 --> 27:47.520
is a particular model that you develop to a hardware platform?

27:47.520 --> 27:53.560
Is it kind of the architecture family or is it a specific model or generation of a platform?

27:53.560 --> 27:57.120
How what is it generalized as the hardware evolves?

27:57.120 --> 27:58.120
Right.

27:58.120 --> 28:00.200
I think that's an excellent question.

28:00.200 --> 28:04.480
So a methodology is as good as its potential use, right?

28:04.480 --> 28:07.480
So and I always get this question.

28:07.480 --> 28:11.920
So okay, neural power is good, but if I can't use it for my own platform, if I can't

28:11.920 --> 28:17.520
generalize to other types of computer hardware platforms that don't even exist, how is this

28:17.520 --> 28:20.800
going to be used or how is it going to be usable?

28:20.800 --> 28:27.400
And the answer is, I think this is where the co-design comes into the picture, right?

28:27.400 --> 28:33.360
So we've done, we've built this methodology on an existing platform.

28:33.360 --> 28:40.760
However, if you are in the business of building specialized computer hardware for machine

28:40.760 --> 28:45.160
learning applications, you have an idea, you have a blank canvas that you can start

28:45.160 --> 28:49.600
from, maybe not as blank, maybe you start from some components that you want to use.

28:49.600 --> 28:54.880
So the idea is that you can decide how you put together this architecture.

28:54.880 --> 29:00.760
And there are a lot of interesting ways to characterize a platform in terms of latency

29:00.760 --> 29:03.320
and power before it's even built, right?

29:03.320 --> 29:09.760
This is what we've done for many years before, I mean, we haven't started, you know, we

29:09.760 --> 29:14.640
don't build computer systems by building one and then measuring power in latency.

29:14.640 --> 29:18.480
We know what those numbers will be at design time, right?

29:18.480 --> 29:21.480
So the same approach can be used in this case.

29:21.480 --> 29:27.640
I really think computer hardware designers, as well as machine learning model developers

29:27.640 --> 29:33.560
have to work together because that's where the strength comes from and that's where the

29:33.560 --> 29:38.800
generalizability comes from, you're going to be able to get the expertise from both

29:38.800 --> 29:43.000
and come up with ways to do the actual true co-design.

29:43.000 --> 29:48.560
So it's a hard problem because the search space will be, you know, the cross product of

29:48.560 --> 29:53.920
the two, but we've done a lot of progress on the computer hardware side, right?

29:53.920 --> 29:59.920
So a lot of them, especially on the edge device embedded systems, this is automated quite

29:59.920 --> 30:00.920
a bit.

30:00.920 --> 30:05.160
So we can use all the expertise that exists there and then incorporate some of those things

30:05.160 --> 30:08.760
in the machine learning design process and the co-design as well.

30:08.760 --> 30:14.720
So when you combine all these things, neuropath, single path, architecture search, the light

30:14.720 --> 30:20.840
neural networks, can you talk a little bit about the results you've seen, you know,

30:20.840 --> 30:26.600
what were the baselines, what did you, were you able to demonstrate, et cetera?

30:26.600 --> 30:27.600
Right.

30:27.600 --> 30:32.840
So in terms of, I mean, our models once built the power and latency models once built,

30:32.840 --> 30:37.480
they are just zero cost or constant cost, you know, you just plug in the parameters where

30:37.480 --> 30:40.600
the architecture configuration and you get the numbers, right?

30:40.600 --> 30:44.880
So that's, you know, you do maybe a lot of work in the beginning, but then it, it amortizes

30:44.880 --> 30:47.320
across multiple uses.

30:47.320 --> 30:54.080
The neural architecture search approach basically reduced the search time by three orders of

30:54.080 --> 30:55.320
money to them more.

30:55.320 --> 30:59.720
So we took it from days all the way to ours.

30:59.720 --> 31:01.600
And there are many reasons why that was possible.

31:01.600 --> 31:04.160
One was the way the search space was defined.

31:04.160 --> 31:10.200
The second was how we did the search and of course the fact that we shared these structures,

31:10.200 --> 31:15.360
parameters across different configurations helped quite a bit as well.

31:15.360 --> 31:18.880
So three orders of magnitude in the search, meaning training time.

31:18.880 --> 31:25.200
And then in the inference efficiency, if you look at the impact of quantization or pruning,

31:25.200 --> 31:30.480
you can get maybe two orders of magnitude efficiency, especially from quantization.

31:30.480 --> 31:33.080
So that's quite significant.

31:33.080 --> 31:39.200
We're actually looking now at custom hardware that, you know, post synthesis, post layout,

31:39.200 --> 31:43.320
how much of that will translate into the actual hardware.

31:43.320 --> 31:48.520
There's still going to be at least 40 to 50% savings that still translates because the

31:48.520 --> 31:52.720
tourism I need to that I mentioned looks only at the computation.

31:52.720 --> 31:56.080
There's also quite a bit in terms of storage that we need to capture.

31:56.080 --> 32:00.000
So I think there's quite a lot of good news.

32:00.000 --> 32:05.840
So overall, on the training search side, several orders of magnitude on the inference side,

32:05.840 --> 32:09.040
a lot of efficiency from quantization.

32:09.040 --> 32:15.680
On the training search side, do you look at like ablations, you mentioned several different

32:15.680 --> 32:20.840
characteristics or several different properties that go into producing your, you know, several

32:20.840 --> 32:25.280
order of magnitude, advantages and in train time.

32:25.280 --> 32:26.280
Is that all or nothing?

32:26.280 --> 32:28.720
Is there something magical about this combination?

32:28.720 --> 32:33.800
Or, you know, have you looked at the individual tricks that you've tried to apply and, you

32:33.800 --> 32:37.720
know, there's an individual, one of those gets you 80% of the way there.

32:37.720 --> 32:40.720
How did the different techniques come together?

32:40.720 --> 32:41.720
Right.

32:41.720 --> 32:47.440
So I actually think it's the combination thereof because there are other differentiable

32:47.440 --> 32:51.960
neural architecture search approaches like Dart's inspired.

32:51.960 --> 32:58.680
Once you do that explicit representation, unless you do this super network kind of approach,

32:58.680 --> 33:01.920
you're not going to get the savings.

33:01.920 --> 33:09.080
But at the same time, the ability to consider this specific building blocks allowed us to

33:09.080 --> 33:12.160
perform this comparison with mobile net V2 and V3, right?

33:12.160 --> 33:17.640
So I guess you could design your search space differently to make other types of comparisons.

33:17.640 --> 33:19.600
The results might be different.

33:19.600 --> 33:25.440
But I think the fact that we limit the search space and we also share these parameters across

33:25.440 --> 33:30.680
configurations made it for the fast search time.

33:30.680 --> 33:38.240
And so what is ahead in this general line of research for you and your research group?

33:38.240 --> 33:39.240
Yes.

33:39.240 --> 33:44.880
So we are actually, we have not completed the co-design dream yet, right?

33:44.880 --> 33:52.320
So because we do have the power, latency characterization, we have the, you know, neural network search.

33:52.320 --> 33:58.000
We put in the hardware metrics, but do we have a push button solution that we say, okay,

33:58.000 --> 34:04.440
my task is this robotic vision application or this ARVR application, and these are my constraints

34:04.440 --> 34:05.760
in hardware.

34:05.760 --> 34:10.600
And these are my specifications for accuracy for the machine learning model.

34:10.600 --> 34:13.520
Just build for me something that does that.

34:13.520 --> 34:15.080
We did not get there yet.

34:15.080 --> 34:19.160
What I think is needed right now is to actually, even your hardware constraints are relatively

34:19.160 --> 34:27.680
coarse, right, the power latency, things like that on a typical, an actual chip CPU, for

34:27.680 --> 34:35.840
example, you can get tons of metrics and capabilities from the chip itself about, you know, that

34:35.840 --> 34:41.200
may impact the way your search operates.

34:41.200 --> 34:42.800
That's absolutely right.

34:42.800 --> 34:48.040
And, you know, everything we've done looks at one machine learning model running on this

34:48.040 --> 34:51.520
piece of hardware, and that's never the case.

34:51.520 --> 34:55.240
There are many more things, even more machine learning tasks running.

34:55.240 --> 35:01.520
What maybe one does, speech recognition, one does image recognition, one does object detection.

35:01.520 --> 35:05.520
Maybe there's some sort of multimodal fusion that happens.

35:05.520 --> 35:08.880
That's a huge space that we're looking at, right?

35:08.880 --> 35:14.760
So, I think the, I guess the goal would be to, the ability to say, okay, these are, this

35:14.760 --> 35:18.200
is what I would like to run, and it's not just one machine learning model.

35:18.200 --> 35:19.200
Maybe it's multiple ones.

35:19.200 --> 35:21.640
Maybe it's multiple modalities.

35:21.640 --> 35:26.520
Maybe it's different tasks, maybe it's the same task with different inputs.

35:26.520 --> 35:30.320
How do I run this most efficiently for this particular application?

35:30.320 --> 35:35.480
And by application, I mean something that is not a platform is, like I said, it could

35:35.480 --> 35:38.720
be robotic or an AR, VR application.

35:38.720 --> 35:45.360
So you have the ability to define or design what the platform will be, or will look like.

35:45.360 --> 35:50.760
So Truco design will happen when we're able to close the loop, right?

35:50.760 --> 35:55.920
So we're able to say, okay, this is an initial version of the hardware I think might work.

35:55.920 --> 36:00.280
Based on this, I think your network or networks should look like this.

36:00.280 --> 36:03.880
However, when I do it again, it looks like the hardware needs to be tweaked.

36:03.880 --> 36:09.280
So it's going to be an iterative continuous process until you get to the right configuration.

36:09.280 --> 36:13.520
The question then stands, well, I'm going to build this.

36:13.520 --> 36:17.840
But what if I want to run an additional task on this piece of hardware?

36:17.840 --> 36:19.400
Will I be able to do so?

36:19.400 --> 36:22.480
So one is the ability to reach this code design.

36:22.480 --> 36:25.640
But the second one is programmability and flexibility.

36:25.640 --> 36:30.000
And that's where, you know, there's a balance that you need to, I mean, do you want your

36:30.000 --> 36:35.920
system to be able to run these new applications, or maybe it's the same task, but the data

36:35.920 --> 36:37.480
set is different, right?

36:37.480 --> 36:39.080
You need to retrain it.

36:39.080 --> 36:41.000
Does that affect the way your hardware looks like?

36:41.000 --> 36:46.200
So I think these are all interesting questions that we're still looking at.

36:46.200 --> 36:53.800
And it sounds like this vein of work is primarily operating under kind of traditional

36:53.800 --> 37:01.040
GPU-like assumptions, and there are all manner of other proposed directions that one might

37:01.040 --> 37:08.040
go with hardware, you know, from kind of graph-native implementations and other things.

37:08.040 --> 37:13.920
Do you think these ideas apply similarly to some of those other types of constructs?

37:13.920 --> 37:14.920
Yes.

37:14.920 --> 37:16.400
I think that's a nice one question.

37:16.400 --> 37:21.480
I think we're pretty much, I mean, if you look at the classic machine learning research

37:21.480 --> 37:24.760
papers, they do run on typical GPU platforms, right?

37:24.760 --> 37:27.040
This is where much of the training happens.

37:27.040 --> 37:33.560
This is where inference, I mean, maybe inference can be done on, or they provide results on

37:33.560 --> 37:36.480
other types of platforms.

37:36.480 --> 37:41.560
But I really think, so one is, okay, what are the other options for us to go to in terms

37:41.560 --> 37:43.600
of the hardware that we're looking at?

37:43.600 --> 37:46.520
So that's where accelerators come into the picture.

37:46.520 --> 37:48.760
And there's quite a bit of work on that end.

37:48.760 --> 37:52.520
On the other hand, what is beyond neural networks?

37:52.520 --> 37:58.440
I mean, neural networks, I think, are, I mean, it's just deep learning has become, I don't

37:58.440 --> 38:02.920
know that it's because it's on more understandable, I don't think it's that.

38:02.920 --> 38:09.840
I think it's because it offers quite a rich field of questions that are still unanswered.

38:09.840 --> 38:16.720
But there's quite a bit of other types of machine learning models or modalities that

38:16.720 --> 38:18.960
we have to look at, right?

38:18.960 --> 38:21.080
Is there any computational impact?

38:21.080 --> 38:24.320
Of course there is on those other ones.

38:24.320 --> 38:29.000
Is there a role that hardware, computer hardware designers or developers should play in

38:29.000 --> 38:30.000
that space?

38:30.000 --> 38:31.320
Of course they do.

38:31.320 --> 38:36.040
But I think right now, much of the, if you look at 90% of the work, I think it's just

38:36.040 --> 38:41.600
deep learning, especially vision and, you know, deep neural networks, neural architectures,

38:41.600 --> 38:48.200
research, things that are really well-defined and quite a bit of work happens there.

38:48.200 --> 38:54.320
I believe there's still quite a bit of unsolved and interesting questions that have yet to

38:54.320 --> 38:59.960
be answered in other types of machine learning that perhaps have not received that much attention

38:59.960 --> 39:03.880
because they're not as amenable to be run on GPUs, right?

39:03.880 --> 39:06.960
Maybe this is why, I mean, there's a chicken and egg.

39:06.960 --> 39:14.320
Those made deep learning, you know, on the visible end of our attention, but all the others

39:14.320 --> 39:18.840
are probably not receiving the attention because they don't really run well on GPUs which

39:18.840 --> 39:19.840
all of us have access to.

39:19.840 --> 39:25.080
So I think we can elevate those kinds of machine learning applications by looking at ways

39:25.080 --> 39:30.400
to make them more efficient and increase the size and their applicability and dissemination.

39:30.400 --> 39:34.800
So I think there's quite a bit of impetus in this field.

39:34.800 --> 39:38.800
It's really hard to say, I'm not going to look in the crystal ball and say, okay, in ten

39:38.800 --> 39:43.480
years, everything on the DNN or deep learning side will be solved and then we're going to

39:43.480 --> 39:46.880
look at, I don't know, probabilistic graphical models or something.

39:46.880 --> 39:48.640
I don't know what that might be.

39:48.640 --> 39:50.640
I was going to ask, right?

39:50.640 --> 39:51.640
Yes.

39:51.640 --> 39:52.640
Right.

39:52.640 --> 39:55.880
Well, my students asked me all the time, should I still work in this because I don't

39:55.880 --> 39:56.880
know.

39:56.880 --> 40:00.040
When I graduate, we'll also be able to find a job.

40:00.040 --> 40:01.040
Definitely.

40:01.040 --> 40:05.920
I mean, this is definitely going to be a hot space for high tech.

40:05.920 --> 40:12.800
Though I think this conversation is standing out as one of a couple in the past, a couple

40:12.800 --> 40:18.840
of weeks where there was this undercurrent or suggestion of us being a peak deep learning

40:18.840 --> 40:23.120
which has not really been much of a thing in my conversations.

40:23.120 --> 40:30.320
It's a little bit, you know, that's a lot to extrapolate off of two comments.

40:30.320 --> 40:31.320
Sure.

40:31.320 --> 40:36.080
But interesting that you're saying that and so probabilistic graphical models is one

40:36.080 --> 40:43.600
of the things that you think about as a, if not a contender, a co-existing thing, are

40:43.600 --> 40:48.640
there others that come to mind as, you know, you're worth?

40:48.640 --> 40:54.480
I mean, yeah, well, closer to neural networks, I mean, you have RNNs that can benefit from

40:54.480 --> 40:57.200
much of the things that we've talked about.

40:57.200 --> 41:02.800
But I think makes DNNs and actually RNNs as well or anything that has a deep learning

41:02.800 --> 41:07.360
component in it like, you know, when you look at deep reinforcement learning, it does rely

41:07.360 --> 41:12.480
on using or encoded the state action using deep neural networks.

41:12.480 --> 41:20.240
So I think the appeal of that is that it's a very well structured way to look at a problem,

41:20.240 --> 41:21.240
right?

41:21.240 --> 41:24.440
Because you understand or we think we understand where the components are, we don't

41:24.440 --> 41:26.280
really understand how they work.

41:26.280 --> 41:28.880
But we understand what the components do.

41:28.880 --> 41:33.760
So I think it's, people are still going to stain that space quite a bit.

41:33.760 --> 41:37.080
What I think needs to happen for others, and I just mentioned probabilistic graphical models

41:37.080 --> 41:39.960
because it's really different, right?

41:39.960 --> 41:42.520
It's not as structured.

41:42.520 --> 41:49.600
And you know, when people think of AI, there's many other things that, you know, are not

41:49.600 --> 41:55.840
quantizable as how many MAC operations can you do in parallel or efficiently, right?

41:55.840 --> 41:59.280
So there's more on the decision making side.

41:59.280 --> 42:01.800
And that's what makes it a true AI, right?

42:01.800 --> 42:07.840
So I think the gap between where we are now and where we could be is quite, quite wide.

42:07.840 --> 42:14.440
I think to get to see more interesting things happen in other types of machine learning applications

42:14.440 --> 42:20.040
is a need for those things to happen more efficiently.

42:20.040 --> 42:25.800
And maybe that need is not there yet because we can still perform those tasks using

42:25.800 --> 42:27.760
existing approaches.

42:27.760 --> 42:33.880
Another way to see an impetus in that space would be to have hardware that supports it.

42:33.880 --> 42:40.280
But maybe not because of them, because of some other type of application that is really

42:40.280 --> 42:41.440
requiring that.

42:41.440 --> 42:47.800
So I think machine learning was the lucky winner of the GPU revolution.

42:47.800 --> 42:52.480
GPUs were built in mind with, you know, parallelism.

42:52.480 --> 42:56.640
And we switched from the single core to multi-core, but then multi-core were not enough and

42:56.640 --> 42:59.280
then GPU came into the picture.

42:59.280 --> 43:07.000
So can we envision a trajectory like this for other machine learning applications absolutely?

43:07.000 --> 43:08.000
I just don't know.

43:08.000 --> 43:13.440
I mean, there's many other things that people are looking at from the hardware side.

43:13.440 --> 43:18.760
I know there's quite a bit of interest in removing the memory wall bottleneck, which has

43:18.760 --> 43:21.400
been there for quite a bit of time.

43:21.400 --> 43:25.840
But to be able to do that, to do processing in memory, or to do other kinds of advances

43:25.840 --> 43:28.760
in that field, you need help from the technology.

43:28.760 --> 43:34.760
So I think maybe the progress in that space might be slowed down because we don't really

43:34.760 --> 43:39.960
have a replacement for our current technology, although it's still working quite fine so

43:39.960 --> 43:40.960
far.

43:40.960 --> 43:47.840
So talking about exponential trends, I mean, Moore's law was supposed to double performance,

43:47.840 --> 43:52.440
you name it, transistor density, pick your favorite every couple of years or every year

43:52.440 --> 43:53.440
and a half.

43:53.440 --> 43:59.360
And it stopped, but it doesn't mean we stopped in making things more, you know, better and

43:59.360 --> 44:00.440
better and more efficient.

44:00.440 --> 44:05.920
So there's always something that we can do, and I'm really hopeful that's going to be

44:05.920 --> 44:08.320
the case in machine learning as well.

44:08.320 --> 44:09.320
Awesome.

44:09.320 --> 44:10.320
Awesome.

44:10.320 --> 44:13.200
Well, Deanna, thanks so much for taking the time to chat with us.

44:13.200 --> 44:15.280
It's great to learn a bit about what you're up to.

44:15.280 --> 44:16.280
Thank you.

44:16.280 --> 44:17.280
Thank you for having me.

44:17.280 --> 44:23.320
All right, everyone, that's our show for today.

44:23.320 --> 44:29.120
For more information on today's show, visit twomolai.com slash shows.

44:29.120 --> 44:50.240
As always, thanks so much for listening and catch you next time.

