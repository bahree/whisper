WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.600
I'm your host Sam Charrington.

00:31.600 --> 00:35.960
Alright everyone, I am on the line with Danny Stoll, Danny is a research assistant at the

00:35.960 --> 00:38.520
University of Freiburg in Germany.

00:38.520 --> 00:41.600
Danny, welcome to this week in machine learning and AI.

00:41.600 --> 00:43.200
Thank you for having me.

00:43.200 --> 00:50.560
So Danny, I interview a lot of folks that are practicing machine learning professionally,

00:50.560 --> 00:56.480
as well as PhD students, but you are actually an undergraduate student working as a research

00:56.480 --> 01:00.040
assistant and you've been doing deep learning since high school.

01:00.040 --> 01:03.000
How did you get introduced to deep learning?

01:03.000 --> 01:08.000
So I guess it all started with a general interest in math for the math sake.

01:08.000 --> 01:16.320
So I learned a lot about analysis and linear algebra, stochastic, things like that.

01:16.320 --> 01:22.440
And at some point I got introduced to machine learning and deep learning via some blocks

01:22.440 --> 01:24.480
or things like that.

01:24.480 --> 01:26.360
And that really caught my attention.

01:26.360 --> 01:34.640
So I started with a famous curse offered by Andrew and G, a basic machine learning curse.

01:34.640 --> 01:40.680
And when I worked on that curse, which is an online curse, I really got hooked and I

01:40.680 --> 01:46.480
never looked back from that on and though deeper and deeper, mostly using online lectures

01:46.480 --> 01:49.600
because at the time it was still a high school student.

01:49.600 --> 01:54.080
Is that what you're studying at the University of Freiburg as well?

01:54.080 --> 02:00.680
So the program I'm in at my university is a general computer science program.

02:00.680 --> 02:07.360
So we learn all the basics of that for sure, but there are a lot of mandatory curses

02:07.360 --> 02:12.800
which cover the basics. But in the later stages of this program, you have a lot of freedom.

02:12.800 --> 02:15.520
You can choose different curses.

02:15.520 --> 02:18.840
You can hear reinforcement learning, machine learning, deep learning.

02:18.840 --> 02:21.520
You can have different seminars.

02:21.520 --> 02:24.240
You can even listen to very advanced seminars.

02:24.240 --> 02:32.120
But what I did when I started this program, I joined the reading groups with the topics

02:32.120 --> 02:37.040
related to machine learning and I joined the seminars just sitting in there and I really

02:37.040 --> 02:38.760
enjoyed that.

02:38.760 --> 02:46.120
And so the group here in, as a research assistant, automated machine learning, what's the

02:46.120 --> 02:49.120
focus of that group?

02:49.120 --> 02:57.840
So as the name says, the focus is on automatic machine learning because for once it's not

02:57.840 --> 03:02.760
everyone has experience in machine learning, it's pretty hard to do.

03:02.760 --> 03:10.160
But a lot of people would like to use machine learning or even AI for their products, for

03:10.160 --> 03:15.080
their research, people that might not even have a background in computer science.

03:15.080 --> 03:19.920
So one goal of automating machine learning is to really democratize machine learning

03:19.920 --> 03:26.000
to make it available to everyone and you do that by automating it to really remove the

03:26.000 --> 03:29.280
necessary expertise to run machine learning.

03:29.280 --> 03:34.720
And so we are going to talk a little bit about a paper you co-authored called learning

03:34.720 --> 03:37.440
to design RNA.

03:37.440 --> 03:43.040
Tell us a little bit about the origins of that paper and the problem that it sets out

03:43.040 --> 03:45.440
to address.

03:45.440 --> 03:51.440
So I guess I should start with automating why we care about RNA and what it means to design

03:51.440 --> 03:53.000
RNA.

03:53.000 --> 03:59.840
Many people or most people know RNA from their high school education and it's rural during

03:59.840 --> 04:02.840
the synthesis of proteins.

04:02.840 --> 04:09.120
But RNA can actually regulate biologic processes directly to match like proteins and there's

04:09.120 --> 04:13.880
a lot of focus on proteins because they can regulate these processes.

04:13.880 --> 04:18.560
But RNA can actually do the same thing and it has been connected to all kinds of different

04:18.560 --> 04:23.600
diseases like Parkinson's, Alzheimer's, epilepsy.

04:23.600 --> 04:28.680
And in a very recent study also autism, which was a 2019 study.

04:28.680 --> 04:37.480
So we're even still learning about where RNA is involved and it's only growing in size.

04:37.480 --> 04:43.640
So what actually matters about RNA in terms of which function it has in the body is its

04:43.640 --> 04:51.600
structure, its spatial structure, the weight holds, not the basic structure which is just

04:51.600 --> 04:55.840
a sequence of four nucleotides.

04:55.840 --> 05:03.280
But you don't really know which kind of sequence the desired way.

05:03.280 --> 05:10.040
So once you can have designed a spatial structure that you would hope has a certain function

05:10.040 --> 05:16.840
in the body, you still need to design an RNA sequence which has this structure which holds

05:16.840 --> 05:18.200
in the desired way.

05:18.200 --> 05:22.120
So you can actually have the desired function in the body and that's what RNA design tries

05:22.120 --> 05:23.360
to address.

05:23.360 --> 05:30.560
So given a high order structure design an RNA molecule, a sequence which has a structure

05:30.560 --> 05:34.120
which has the function that you want to have.

05:34.120 --> 05:40.920
So it's the function determines the structure that you need and the folding or the structure

05:40.920 --> 05:47.560
then determines the folding that's required to kind of create that structure.

05:47.560 --> 05:56.600
Yes, it's the structure determines the function it has and the basic sequence determines the

05:56.600 --> 05:59.000
structure, the higher folding.

05:59.000 --> 06:06.240
But we want to inverse that process, we want to go from function to structure to low level

06:06.240 --> 06:10.240
structure to the basic molecule itself.

06:10.240 --> 06:17.040
Great and so you kind of set out to tackle this problem.

06:17.040 --> 06:23.720
What led you to the particular approach that you ended up using here?

06:23.720 --> 06:29.240
So as we talked about in the start of this interview our group focuses mostly on automating

06:29.240 --> 06:35.360
machine learning and deep learning and one recent field there around recent approach

06:35.360 --> 06:41.120
what people are doing is called neural architecture search where you try to find the best

06:41.120 --> 06:48.680
architecture, you try to search of architectures of neural networks which perform best.

06:48.680 --> 06:55.000
And early times of neural architecture search people use reinforcement learning and basically

06:55.000 --> 07:00.360
decided for each building block or for each stage in the network what you want to use.

07:00.360 --> 07:08.440
And we saw some similarities with RNA design because there you also can choose which nucleotide

07:08.440 --> 07:15.720
at which point in the sequence you want to place and that led us to try this out and we

07:15.720 --> 07:19.400
got a proof of concept and it seems to work nicely.

07:19.400 --> 07:22.920
So from there on we went with it.

07:22.920 --> 07:29.760
Was there an existing body of research that you or kind of implementations that you were

07:29.760 --> 07:36.440
trying to automate through this neural architecture search or did you have to come up with, yeah

07:36.440 --> 07:39.680
I don't know if this question even makes sense but did you have to develop the algorithm

07:39.680 --> 07:48.560
to do the underlying kind of folding? Yeah that's actually a great question.

07:48.560 --> 07:52.880
So there has been a lot of research into RNA design because it's such an important problem

07:52.880 --> 08:00.480
and the research goes back almost 30 years and you ask about the folding itself.

08:00.480 --> 08:06.880
Folding has actually been solved for RNA and that's nice because if you compare to proteins

08:06.880 --> 08:11.920
there we are very far from solving folding. People are mostly working on folding, not designing

08:12.800 --> 08:19.120
and so we had this folding algorithm available to us. It's called the Zaka algorithm,

08:19.120 --> 08:25.200
there are other algorithms and our approach basically works with all of them but we concentrated

08:25.200 --> 08:29.520
on this one algorithm because mostly everyone else was trying that as well.

08:29.520 --> 08:36.560
As for the state of RNA engineering before our publication it was there were not many

08:37.200 --> 08:42.720
machine learning approaches but there were probably none that I know of. There was one other

08:42.720 --> 08:50.560
machine learning approach published in parallel to us but before that there was no really

08:50.560 --> 08:56.880
machine learning approach. What people usually did is they did a basic search, a local search for

08:56.880 --> 09:06.000
example to progressively improve the sequence that you have, that you want to then fold and see

09:06.000 --> 09:13.440
how well it folds and then try something else at some side of the sequence but that doesn't work

09:13.440 --> 09:21.120
very well we found because it doesn't scale well with sequence size. What we did instead is

09:21.120 --> 09:27.680
we formulated it as a reinforcement learning problem and that is in a generative way so

09:27.680 --> 09:34.480
we would put out the complete sequence that we want to fold at once and then we learned to do that

09:34.480 --> 09:41.280
very quickly and our approach scales really well with sequence length. The other machine learning

09:41.280 --> 09:51.760
approach which was published basically implements a heuristic for a local search so that's actually

09:51.760 --> 09:58.480
very different from what we have done. What's the relationship between your use of reinforcement

09:58.480 --> 10:05.360
learning and neural architecture search here? In the other days people were using reinforcement

10:05.360 --> 10:12.240
learning to do neural architecture search so reinforcement learning is the type of learning where

10:12.240 --> 10:17.600
you get a state or rather you have an agent and an environment and the environment provides a state

10:17.600 --> 10:24.320
and the agent can based on the state choose which action the agent wants to take and what people

10:24.320 --> 10:30.560
did with neural architecture search is they said okay our actions basically choose building blocks

10:30.560 --> 10:36.960
for different layers of the network say for example what do you want to choose a

10:36.960 --> 10:43.920
relu or what do you want to choose a different activation do you want a basic convolution or do you

10:43.920 --> 10:52.320
want a separable convolution things like that and we thought well that's the same for RNA design

10:52.320 --> 10:59.040
that's let's choose which nucleotides we place at different points in the sequence and so are

10:59.040 --> 11:08.560
you using neural architecture search or rather was this approach inspired by neural architecture search?

11:09.520 --> 11:16.320
So it's both so okay we already talked about how approach how basic approach was inspired by it

11:17.040 --> 11:23.920
but then we only had this basic idea and it's a very novel setting for machine learning algorithms

11:23.920 --> 11:27.920
and we didn't know what would work well which kind of architectures would we have to use

11:27.920 --> 11:33.360
what we have to use recurrent neural networks do we have to use convolutional neural networks

11:33.360 --> 11:40.160
do we need some embedding which we learn or what a basic embedding work and we said to ourselves

11:40.160 --> 11:45.360
okay we could try these things by hand but we are actually experts on doing this automatically

11:45.360 --> 11:53.840
and optimize it and then we went ahead and formulated our approach in a general way such that we

11:53.840 --> 12:00.560
could search the best architecture for it but we didn't stop there we also we also didn't know

12:00.560 --> 12:04.880
which kinds of hyper parameters we would need which how our training would need to work

12:05.600 --> 12:10.000
and finding the right hyper parameters is a very tedious and time consuming

12:11.360 --> 12:18.560
process as probably our machine learning practitioners know and our group usually tries to

12:18.560 --> 12:23.120
develop algorithms that automate this so we used our state-of-the-art algorithms or

12:23.120 --> 12:28.880
used algorithms and searched for the hyper parameters and the architectures jointly

12:29.440 --> 12:36.800
to to really optimize for that and we didn't even stop there we we thought okay but now we have

12:36.800 --> 12:42.320
to choose our environment how do we need to choose our environment what what kind of reward do we

12:42.320 --> 12:49.760
need how would our states need to look like how how do we need to choose our actions or what do

12:49.760 --> 12:55.600
our actions really correspond to all these kinds of choices of the environment and we also included

12:55.600 --> 13:01.120
these choices in our search so we searched for the best agent and the best environment jointly

13:01.120 --> 13:06.880
that's what we really did and also the training hyper parameters we we really automated our L here

13:06.880 --> 13:14.160
I mean it sounds like you kind of you know through the the kitchen sink at this problem you got

13:14.160 --> 13:20.240
a little bit of you know it sounded like multi-task learning in there reinforcement learning

13:20.240 --> 13:26.720
neural architecture search you know when you throwing so many you know different cutting-edge

13:26.720 --> 13:34.880
techniques at a problem like this you know I'm wondering what the you know are there some drawbacks

13:34.880 --> 13:40.000
or costs associated with doing that do you end up with you know something that you wouldn't want

13:40.000 --> 13:44.320
to implement because it's too complex or because there's no way for you to really understand what

13:44.320 --> 13:51.680
is doing you know or did you find that the you know the result met all the criteria that you you had

13:51.680 --> 13:56.560
for it and maybe you know that's a good place to start is what are the criteria that you you know

13:56.560 --> 14:03.280
would put in place for a solution to this problem so one basic criterion that that I tried to

14:03.280 --> 14:10.400
follow in my work always is do it simple you said we threw all these sophisticated things at it but

14:10.400 --> 14:17.680
in actuality our solution and or even our automation is extremely simple it sounds very complicated

14:17.680 --> 14:25.280
and but it but it's not it's the final result it's just a few layers it's it's it's very basic we

14:25.280 --> 14:34.000
didn't do any sophisticated techniques in there like we didn't even have learning rates get yields

14:34.000 --> 14:41.520
or we we basically only experiment with basic convolutional networks basic fully connected

14:41.520 --> 14:48.320
networks different very simple embeddings such that we could search over them efficiently and

14:48.320 --> 14:57.120
also our automation is very simple we had we formulated a 14-dimensional search space of integer

14:57.120 --> 15:03.360
parameters like how many convolutional layers do you want how many recurrent layers do you want

15:04.000 --> 15:11.280
but also continues parameters like what should the learning rate be or how should we shape the

15:11.280 --> 15:21.040
reward and we used a standard approach or our approach for hyper parameter optimization which is

15:21.040 --> 15:28.000
used in quite a lot of papers and through that at the search space and look what kind came out

15:28.000 --> 15:33.360
I guess a drawback when you do it like that is that you have computation overhead in terms of

15:34.080 --> 15:39.440
how you the big because you have to search over these hyper parameters you do that on

15:39.440 --> 15:46.320
many machines simultaneously but I think that's the better way than doing it manually and

15:47.600 --> 15:52.560
have it cost developer time because a lot of bottleneck in machine learning and deep learning is not

15:53.520 --> 15:59.600
on the computational side but on the developer time and that's also a great problem that can be

15:59.600 --> 16:04.800
solved with this automated type of parameter search and architecture search removing the need for

16:04.800 --> 16:10.320
a machine learning developers and researchers to really fiddle around the type of parameters because

16:10.320 --> 16:17.520
it is a very tedious and time consuming process and just formulate a search space run a hyper parameter

16:17.520 --> 16:24.800
optimization on top of it and see what works best and so were there any unique challenges or

16:25.600 --> 16:30.800
approaches that went into formulating that search base is it you know as simple as the things

16:30.800 --> 16:34.880
you outlined previously the number of layers and the types of layers that kind of thing or were there

16:36.960 --> 16:46.000
interesting interdependencies or you know did you have to kind of craft objective functions in

16:46.000 --> 16:55.040
an interesting way what was the kind of scope of that part of the problem so so there were some

16:55.040 --> 17:03.520
some things like the state space and the first few layers which had to which are conditional on each

17:03.520 --> 17:11.280
other we have some conditional parameters there but one other thing that that we had to come up with

17:11.280 --> 17:17.840
and it actually had a lot of impact on the final performance is the way we do the reward so what

17:17.840 --> 17:24.480
we do is we once we have designed a sequence we fold it and then we compare the folding the higher

17:24.480 --> 17:30.160
order structure with the one that we want to obtain and then we take the distance of these two

17:30.800 --> 17:38.400
but if you think about it if you're folding if your structure is 90% correct then that's not

17:38.400 --> 17:45.680
really good because you really need the structure to be exact and then we had to reshape the reward

17:45.680 --> 17:52.560
to put a lot more emphasis on if you only get 0.9% if you only get 90% right then your reward should

17:52.560 --> 18:00.720
almost be zero but if you get 0.98 correct then then the reward then there should be some reward

18:00.720 --> 18:07.440
because they're very close but we did not know how much we had to shape this reward in this

18:07.440 --> 18:15.520
regard so we formulated the reward in a way such that we had one parameter which could influence

18:15.520 --> 18:23.760
this kind of shape and then that's one part of a 14-dimensional search space with the reward

18:23.760 --> 18:29.120
function that needs to be shaped like that I imagine at one of the challenges that you run into

18:29.120 --> 18:37.280
is that you're not able to give your agent enough information as to how well it's doing unless it

18:37.280 --> 18:43.200
stumbles on to the perfect solution or something very close did you run into that challenge and

18:43.200 --> 18:50.800
how did you address it so that's a major challenge and when you when you start off with a single

18:50.800 --> 18:58.160
sequence and you run an algorithm on that you might get very close and then you get some reward

18:58.160 --> 19:06.240
yes but if it's a very complicated molecule then then it's very unlikely that you you will hit

19:06.240 --> 19:15.200
something successful and that's a major drawback of algorithms before ours but what we did is

19:15.840 --> 19:23.360
we incorporated this meta-learning on multitask learning across many molecules so we could

19:23.360 --> 19:29.440
actually transfer knowledge from for example a simple molecule where we can get good solutions

19:29.440 --> 19:36.320
quickly to more complicated ones so we can first learn the basics of the RNA folding dynamics

19:36.320 --> 19:45.760
or the inverse of that and then transfer this to some more complicated molecules so that's also

19:45.760 --> 19:51.920
where where where a very great advantage of our approach lies in because we can actually perform

19:51.920 --> 19:58.960
very well very fast on very long sequences which others cannot and if you take this component

19:58.960 --> 20:05.920
out of our approach which learns across these molecules then we do not perform as well but

20:05.920 --> 20:13.760
we would still have the state of the art so is the this transfer learning aspect is this across

20:13.760 --> 20:19.680
different training runs meaning you train up a model and then you apply it to a separate problem

20:19.680 --> 20:28.640
or you training across different problems or different molecules so it's kind of both

20:28.640 --> 20:37.360
because what we do is we do a kind of multitask learning very run on sequences on RNA molecules

20:37.360 --> 20:45.840
in sequence and in parallel and update parameters according to all of these RNA design tasks

20:46.720 --> 20:55.680
and then we use the resulting parameters to initialize our approach on a novel RNA design problem

20:55.680 --> 21:01.360
that's what we do but what we also do is we explicitly optimize the hyperparameters environment

21:01.360 --> 21:10.880
and the architecture to do this transfer well so we also optimize for being able to transfer well

21:11.760 --> 21:18.400
and how do you do that the objective that our hyperparameter and architecture search and environment

21:18.400 --> 21:29.760
search optimizes is given a training set of RNA design tasks and you can train on that how well

21:29.760 --> 21:38.960
do you on a few novel ones and we optimize this objective with our meta search and so when you

21:39.680 --> 21:46.720
kind of taking a step back and thinking about this from the the the perspective of the problem

21:46.720 --> 21:57.600
the RNA design problem what's your ultimate performance metric there and how do you how do you

21:57.600 --> 22:05.120
assess it is it are you comparing to other non-machine learning based approaches or something else

22:06.000 --> 22:14.000
we are comparing to the to non-machine learning based approaches that people proposed before us

22:14.000 --> 22:18.880
but we're also comparing it to the one machine learning based approach that I also mentioned

22:19.760 --> 22:25.520
and the kind of metric people used in the past for RNA design and which we also adapted

22:26.720 --> 22:37.040
is at each time point or in a for a given time how many RNA sequences or how many RNA design problems

22:37.040 --> 22:45.520
rather can you solve and solving means getting the structure completely right and what people

22:45.520 --> 22:51.360
usually do is they say okay there there we have a benchmark there we get 10 minutes per sequence

22:51.360 --> 22:58.000
or you have a benchmark where you get one day per sequence and then compare only the final

22:58.000 --> 23:06.320
performance but we looked at the complete trajectory of how long at each time point how many sequences

23:06.320 --> 23:13.440
have we solved and what we can see especially with our meta learning our multitask learning

23:13.440 --> 23:20.160
approach is that we solve a lot of sequences very very rapidly so for one of the benchmarks

23:20.960 --> 23:27.760
we we take some time for our parameters to load and everything but after that it takes one second

23:27.760 --> 23:34.880
and we basically beat most of the other approaches which take like five minutes to really

23:34.880 --> 23:43.280
solve softness many sequences so for one of the benchmarks we achieve state of the other results

23:43.280 --> 23:49.120
a thousand times faster than the previous state of the art and for another one we do it 60 times

23:49.120 --> 23:54.480
faster in this case of the thousand times faster you said loading your parameters is that

23:55.200 --> 24:00.880
so that's you're comparing a traditional approach that's non machine learning based with just

24:00.880 --> 24:07.440
the kind of inference against your model yeah we run the inference of our model on

24:09.120 --> 24:14.800
or the complete time it takes for our algorithm to receive the sequence to load the parameters

24:14.800 --> 24:21.120
and everything and to start predicting something so that's part of our evaluation and if some

24:21.120 --> 24:26.080
other algorithms also need some initializations then we measure that as well yes but

24:26.080 --> 24:33.600
but it's basically running the algorithms and looking at how quickly they solve a molecule or

24:33.600 --> 24:39.440
if they solve it at all I guess I'm I'm curious when you're talking about a thousand times faster is

24:39.440 --> 24:44.080
is it you know we really at that point trying to compare apples and oranges or they really

24:45.040 --> 24:53.520
comparable in the sense of a lot of your work is done in training and then you produce this model that

24:53.520 --> 25:01.920
is you know once you've got the model the time it takes to get the inference is going to be much

25:01.920 --> 25:07.280
less whereas if you have an algorithm that's doing the computation at the time of you know trying

25:07.280 --> 25:13.040
to figure out the the answer it strikes me as not being particularly comparable am I thinking

25:13.040 --> 25:17.760
about that correctly or yeah you can think about it that way but that's actually a nice thing about

25:17.760 --> 25:22.960
our approach that once you have trained it which doesn't take a lot of time in our case it took

25:22.960 --> 25:31.120
an hour or three hours and once you have that you can you can apply to novel RNA design problems

25:31.120 --> 25:37.760
which you haven't seen before and it just works that's that's definitely something that you should

25:37.760 --> 25:44.880
aim for like if you if you can pre-train your model or your algorithm beforehand and make

25:44.880 --> 25:50.640
and make use of information that you have for other problems and apply to new problems then

25:50.640 --> 25:56.240
that's a that's a very nice thing right and we also have one instantiation of our algorithm

25:56.880 --> 26:01.680
which does not do that does not transfer knowledge but just runs at the problem

26:01.680 --> 26:08.240
correctly and that also achieves state of the art performance I mean in one sense that it's saying

26:08.240 --> 26:13.360
you know a machine learning approach is where you produce a model and you can run inferencing

26:13.360 --> 26:18.880
as this model is better than an analytical approach where you're kind of doing the computations

26:18.880 --> 26:29.680
from scratch every time but you're you're saying that because the algorithm because your model

26:29.680 --> 26:36.560
generalizes it's kind of a valid comparison yes that's what I'm saying and there's also the other

26:36.560 --> 26:42.640
machine learning approach that I talked about also uses reinforcement learning but it only

26:42.640 --> 26:48.800
implements a heuristic via the reinforcement learning for a local search for these more traditional

26:48.800 --> 26:57.200
methods and um transfers knowledge for that as well but I'm not sure how well it actually does

26:57.200 --> 27:03.600
that yes but and it's and it still cannot just generate a sequence from scratch directly which

27:03.600 --> 27:12.080
which works yeah so from my point of view being able to transfer knowledge from from a set of

27:12.080 --> 27:20.240
training tasks to to novel tasks and make the performance better is a great thing but I might be

27:20.240 --> 27:26.560
biased since a lot of my work also focuses on meta learning where they try to do the same thing

27:26.560 --> 27:35.760
you try to learn to learn or you try to learn to effectively solve novel tasks if you

27:35.760 --> 27:41.760
someone's listening and they you know they hear this and they want to try it or their particular

27:41.760 --> 27:49.680
things that they need to be aware of or think about that are specifically related to maybe any

27:49.680 --> 27:55.040
interactions that all these things have with one another or they pretty straightforward to apply

27:55.040 --> 28:03.280
in conjunction with one another so because what we did is this is very simple we didn't run into

28:03.280 --> 28:10.880
really a lot of problems with using all these things in conjunction but if you want to combine

28:10.880 --> 28:16.000
very sophisticated meta learning algorithms with something like neural architecture search

28:16.000 --> 28:21.600
that's that that's a hard thing to do and that's an open research question how do you effectively

28:22.240 --> 28:27.440
meta learn to do architecture search or how do you do architecture search for meta learning

28:27.440 --> 28:34.160
that's that's what people are researching but in our case it was a very simple thing about a very

28:34.160 --> 28:41.360
simple formulation which ended up working great and that's also a strength of our approach that's

28:41.360 --> 28:46.800
very simple it's it's using hyper parameter search and architecture search and all these things

28:46.800 --> 28:52.880
but the way you formulate it it basically reduces to hyper parameter search for all of these

28:52.880 --> 28:59.760
things so we formulate a neural architecture search as a hyper parameter search and the same

28:59.760 --> 29:06.560
with the environment and that way you can very effectively search over all these choices at

29:06.560 --> 29:16.080
the same time and we talked before how we explicitly set the objective of our optimization

29:16.880 --> 29:25.280
for to the to how well at the configuration transfers so that's the thing you can do as well

29:25.280 --> 29:31.440
you can explicitly optimize for how well your meta learning optimizes or how you multitask

29:31.440 --> 29:38.720
learning transfers in the paper you talk about some ablation studies that she did can you talk a

29:38.720 --> 29:45.680
little bit about this yeah sure so in general I think it's very important to the ablation studies

29:45.680 --> 29:51.040
for your systems for your algorithms because if you do not you do not know what actually

29:51.040 --> 29:57.680
is the thing that performs well and if you do not do an ablation study then you end up doing

29:57.680 --> 30:03.440
things that don't help and it just makes your approach more complicated and more complicated for

30:03.440 --> 30:10.080
other people to use so I think in general it's very important to do an ablation study and what we did

30:10.080 --> 30:19.360
is we we ablated or we disabled the meta learning the transfer learning across molecules learning

30:19.360 --> 30:25.840
across molecules and what we observed is that while we still achieve very good results when

30:25.840 --> 30:34.400
we compare to other methods we do solve things much more slowly and also solve some molecules

30:34.400 --> 30:42.800
not at all one other thing that we disabled and saw how important it was was building a model

30:42.800 --> 30:48.960
itself like do we actually need to build a model or is it trying to just randomly predict something

30:48.960 --> 30:55.680
and as you would expect that that does not work very well yeah we also had an additional step

30:56.240 --> 31:04.560
in the reward or in the folding where once we are very close to a solution like only

31:04.560 --> 31:14.080
for sites or very small number of mistakes were made then we do some more search for it like

31:14.080 --> 31:22.560
not go through the our reinforcement learning agent again but just try some things just change

31:22.560 --> 31:30.320
some things and see how it affects the folding and if we can solve it and in early stages of our

31:30.320 --> 31:36.880
approach it was very important so we kept it in there but once we optimized for the transfer

31:36.880 --> 31:44.320
learning and everything we observed that it's important Scott less and less and only on very hard

31:44.320 --> 31:50.560
problems and very hard on a design problems it does make a difference so we ended up using that

31:51.760 --> 31:59.440
because it does help in these very hard cases but we could have disabled it for for easier benchmarks

31:59.440 --> 32:04.240
as well and that's something we learned from our ablation study that's that's very important I

32:04.240 --> 32:12.320
think to really learn what difference do your do your components make of your algorithm but what

32:12.320 --> 32:18.480
happens if you disable them are they really critical do they help at all and people do not do that

32:18.480 --> 32:26.320
enough I think did you develop any intuition for why that particular component only had the

32:26.320 --> 32:34.000
impact with the more complex molecules that's a good question I don't have a good answer I guess

32:34.800 --> 32:40.800
that that's not something we have looked too much into because only at the very late stages

32:40.800 --> 32:48.880
we found out that it's not actually important anymore so our intuition before that was like

32:49.760 --> 32:55.040
our model predicts a complete structure and it has to get everything right but our model is also

32:55.040 --> 33:03.920
very stochastic so it basically learns a distribution over molecules and you can make some mistakes

33:03.920 --> 33:11.520
there and we added this local adaptation step to really make up for this generative approach where

33:11.520 --> 33:17.280
you and allow the model to make some small mistakes that's how we thought about it but

33:17.280 --> 33:23.760
once you got a very good model and really learned how to transfer learn here we

33:24.640 --> 33:31.680
we then observed that that it did not make that many mistakes anymore and it it was more certain as

33:31.680 --> 33:38.480
well in which areas of the molecule space it wants to predict so it basically did this kind of

33:38.480 --> 33:47.120
search itself that's that's that's how I would interpret it I guess but for hard cases then for for

33:47.120 --> 33:55.200
very long molecules a very big molecules it cannot cannot do that very good still so there

33:56.000 --> 34:02.640
you still want to allow the model to do some mistakes and have it explore around good regions I guess

34:02.640 --> 34:10.240
one of the themes that comes up and pretty frequently in my interviews is this pendulum swinging

34:10.240 --> 34:15.920
between traditional model based approaches to the world where you're you know we're solving

34:15.920 --> 34:22.960
problems like this based on chemical or biological principles to more of a statistics based

34:22.960 --> 34:30.400
approach and how in a lot of research the pendulum is kind of centering again where we're trying to

34:30.400 --> 34:35.600
marry you know these model based approaches with the statistical based approaches is that

34:35.600 --> 34:42.960
something that you could see an opportunity for here so your question is whether the more

34:42.960 --> 34:48.640
traditional approaches for RNA design can be marriage with something like our approach in the

34:48.640 --> 35:01.120
future to even get better performance essentially yes so so so I see potential in in using our

35:01.120 --> 35:10.080
agent to produce a good initial guess for how the molecule should look like and then use more

35:10.080 --> 35:18.240
traditional methods to then work with this initialization because a lot of the drawbacks way

35:18.240 --> 35:24.880
of these traditional methods which are mostly search based are that it takes a lot of time to get

35:25.520 --> 35:32.480
close to a good solution so if you can already get very close to a solution right from the start

35:32.480 --> 35:42.160
like using our agent then then the drawbacks of the traditional approaches might not exist anymore

35:42.160 --> 35:49.120
and I see a lot of opportunities there thanks so much for taking the time to chat with me it was

35:49.120 --> 35:57.440
great chatting with you on about this stuff yeah it was my pleasure all right everyone that's our

35:57.440 --> 36:05.440
show for today for more information on today's show visit twomolai.com slash shows make sure you

36:05.440 --> 36:13.600
head over to twomolcon.com to learn more about the Twomolcon AI Platforms conference as always thanks

36:13.600 --> 36:43.520
so much for listening and catch you next time

