Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A few weeks ago, machine learning and AI researchers, practitioners and students from Africa and around
the world met in Cape Town for the second annual deep learning in Daba, an event that aims
to expand African participation and contribution in the field.
While I wasn't able to make it to Cape Town myself, I did have a chance to speak with some
of the event's awesome speakers and I'm excited to present to you our deep learning in Daba
series.
In this, the first episode of the series were joined by Sarah Hooker, AI resident at Google
Brain.
I had the pleasure of speaking with Sarah in the run up to the endaba about her work on
interpretability and deep neural networks.
We discussed what interpretability means and when it's important and explore some nuances
like the distinction between interpreting model decisions versus model function.
We also dig into her paper evaluating feature importance estimates and look at the relationship
between this work and interpretability approaches like Lime.
We also talk a bit about Google, in particular the relationship between Brain and the rest
of the Google AI landscape and the significance of the recently announced Google AI Lab in
a Cragana, being led by friend of the show Mustafa Sisay.
And of course, we chat a bit about the endaba as well.
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their
support of the podcast and their sponsorship of this series.
In this podcast, you heard Sarah talk about the AI residency program she's in at Google.
Well, just yesterday, they opened up applications for the 2019 program.
The Google AI residency is a one year machine learning research training program with the
goal of helping individuals become successful machine learning researchers.
The program seeks residents from a very diverse set of educational and professional backgrounds
from all over the world.
So if you think this is something that sounds interesting, you should definitely apply.
Find out more about the program at g.co slash AI residency.
And now on to the show.
All right, everyone.
I am on the line with Sarah Hooker.
Sarah is an AI resident at Google Brain.
Sarah, welcome to this week in machine learning and AI.
Hi, Sam.
I am so excited for this conversation.
Let's start with a little bit of background and you've got a somewhat non traditional
background for a Google brain researcher.
You taught yourself machine learning.
Tell me a little bit about that.
Yeah, it's interesting with these with these type of questions of how did you get into
research.
And you have these very common reference points that I shared.
I went to this school and then I did this PhD.
And a lot of my career has been driven by things which are less compactly described,
which is they've been much driven by curiosity and like moderate obsession with certain questions.
But I think probably one of the pivotal moments was I started a nonprofit four years ago.
And it was cool delta at what is cool delta analytics and it works with other nonprofits
all over the world and also teaches machine learning.
But at the time when I started it, it was my background originally academically as an
economics.
And I was very excited to just do economic modeling for nonprofits.
And then the composition of volunteers that we had because we were in the Bay Area was
kind of eclectic mix of economists and engineers and machine learning researchers.
And that was kind of a turning point for me because in some ways the tasks that we were
looking at were really exciting.
And we were working with nonprofits that were doing education programs using pre-smart
technology in Nairobi.
We also worked with fascinating problems where it was detecting illegal chainsaw activity
and rainforests using audio.
And it both gave me this idea that machine learning is just so exciting and so interesting.
But also I was working with people who were a lot better than me.
And so I think that's also was fairly critical in just my own sense of measuring my technical
progress and feeling like, wow, I really want to learn both the tools and the framework
to be able to solve some of these problems.
The second part really was I've spent two years working deploying algorithms.
So I joined an online education company called Udemy.
And there I was working on recommendation and spam detection.
And that was fascinating because a lot of it was how do we deploy models which is very
different from only focusing on developing models mainly because you're willing to give
up quite a bit of accuracy in order to deploy successfully and have a robust deployment pipeline.
So for me, kind of at the end of those two experiences and Delta has really been in parallel
this whole time, for me brain was really a question of, do I enjoy research?
Because I really enjoyed all these very domain specific questions.
And I want to see, firstly, does that curiosity translate to a research framework and is
it satisfying in the same way?
And the residency has been, in particular, doing research at brain has been a very good
way to answer that question.
So that kind of brings me to today a lot of what I've done over the last years being
doing research on interpretability and now model compression of brain.
Have you arrived at an answer in terms of how you feel about research?
And I'm curious from someone who, the perspective of someone who is new to research from an applied
background and not to mention a self-taught background like, how has that transition been
for you?
How has that experience been for you?
What do you like about research?
What do you prefer about application?
Yeah, that's an excellent question.
And it's important because I think that there's few of my colleagues who have perhaps experienced
both.
And so, although it's a sample size of one, maybe useful in some ways for other people
thinking about whether they want to do research or stay in applied, I think there's different
pros and cons.
So with my applied work, a lot of what is important about the skills and the thought process
that applied work in parts is that you can't really abstract any part of the pipeline.
So you have data preparation and your data is normally very specific to your use case.
And so often you spend a lot of time getting usable data.
And many times, for example, in the Delta projects, the cost of incremental data collection
is fairly high unless you're using a mobile app or something that automatically generates
new data.
So understanding the distribution of your data is much, much more important.
And then you also have the deployment phase and applied, which again, imposes very severe
constraints on what you can actually do because you need to justify every additional step
of complexity that you add to a model.
On the other hand, I think research is this very kind of particular way of having a discourse
by the problem.
And it's often prickly.
And I mean that in the sense that you're often using a very clearly stated set of assumptions
about how you want to navigate a given task.
But you are advancing solutions that should have a contribution, even if a time is marginal
to a large set of problems, and not just a specific problem.
But why I raised the difference earlier, you have to think about your data pipeline,
you have to think about your deployments, what applied is that research or at least the
current body of machine learning research largely abstracts those two.
So we work with three datasets.
And I'm sure most researchers listening know which ones, but evidence, sci-fi, and
image net.
And we take that as given.
And we're not too concerned about deployment.
We don't really think about this very interesting new research that is focusing on the interaction
between the model and the hardware.
But for the large part, we focus on the representation.
And so large part of our time is very much thinking about how can we learn better representations
given this data, and not really fusing too much about the details of how it translates.
This is good and bad, it really, and maybe here's, and probably, I sense in plighting
your questions also for me how abrupt the transition was.
And like, what were challenges, personally, and what did I enjoy?
I think that doing this and doing the residency is a very particular form of entering research
because you have a lot of contact with very senior established researchers in the field,
probably even more so than you would have in a PhD program.
So to a large degree, if anything, I would say the fact that I really enjoy research
and the fact that I think this way of discourse is valuable is probably also because I've
had a very exciting set of collaborations here.
And a lot of what determines how you think about a problem is also who you work with.
And so that has been very charged and very exciting.
I will say I do miss the feedback loop that comes from deploying a model because a lot
of where how we measure progress in a task and research are perhaps metrics that might
not actually might, and I say this with a note of hesitation in my voice because I think
it's an active debate, but we use metrics like accuracy or for interpretability, it's
even more complex, what metrics do we even use.
But these are metrics where you may actually have a very high, may have a successful representation
that inches up interpretable accuracy for these given data sets, but it's unclear
like if you were to actually translate that to real world tasks, whether that would be
a successful or not.
And I think I'll point there because I'm curious like whether anything I said I should elaborate
on.
And I do a few things out there.
Well, what's interesting about what you said was that you're probably at, you're doing
research at one of the places that, you know, along the spectrum of pure and applied
research is probably a lot closer to the applied side than most.
And in fact, we see all the time how research that's happening at Google Brain ends up
in products like Duplex and the Google Cloud products, I'm wondering how that fits into
the equation.
And I guess what's interesting is that you still characterize it as fairly disconnected
from application, whereas I tend to think of it as fairly close.
And I'm wondering if you can provide some context for that.
Yeah.
So I think it's much to do with incentive structures.
So brain is exciting for researchers, because there's no incentive in how we, in our iteration
cycle of ideas, that obliges us to think about product at all.
In fact, like that's what's so charged about the atmosphere is that we don't have to,
at any time, even if we don't deploy to a Google product or to any product, our research
can still be considered successful and a contribution.
The larger part of how brain, specifically, is orientated as around contributing to
the wider research discourse.
So that's what I meant by it's removed is that largely our framework is the same as academia.
We start with an idea, and then a lot of times the combination of ideas is a successful
contribution to a conference or it's open-sourcing code, so things that enrich the larger community.
That being said, I will say one way in which perhaps this is a little bit different from
what we imagine academia to be, is that because of, because it's an industry lab, there's
much more room to do empirical experiments, which I think is quite exciting, and when
I say empirical experiments, what I mean is that oftentimes that, previously, I would
say, you can see the evolution in what data sets are considered the benchmark for just
to find a certain hypothesis, or just to find a certain theoretical approach.
We kind of moved from amnesty and sufficient to now, probably most people would not consider
amnesty sufficient as evidence for your hypothesis is correct.
And now you have cypher, and I would say at places like Google, these research labs,
it is possible to do large scale experiments for data sets like ImageNet and do a lot
of empirical and important empirical work to corroborate hypothesis, or perhaps to say
that certain hypotheses that were previously held may not be correct given the data.
So that is exciting, and that's one way in which the conversation is different.
That being said, I'm sure there's aspects to a culture of academic lab that foster new
directions of thinking, that perhaps because we can iterate so quickly with experiments
here, we may not inform that in the way that we talk about these things.
That I'm not sure about, because this is the only lab I've known, but I can imagine that
would be a counterpoint in someone would say.
It does surprise me a bit to hear that you are working day-to-day with the same data sets
that everyone else is.
I would have imagined that you had some super ImageNet plus plus or cypher plus plus that
you're using, and when you're ready to publish, you dump things down for the outside world.
I can only speak for my experience, my immediate colleagues, but there's no ImageNet plus
that we just scurry around with behind the scenes.
And perhaps that's because of essentially the cost of switching in between data sets.
So this is another interesting insight.
This is a lot of the reason why we, this is my opinion, but I sense a lot of the reason
why we're using these same data sets is that often, during an idea and implementing
these data sets, when you try and translate it to perhaps a more realistic data set or
more complex, you may, it's not that you would get different results, it's just that like
the implementation and how you go about it may be fairly different.
And so there's given limited time resources and given that everyone else is articulating
progress in terms of these three data sets.
If you are a researcher, you also want to be talking in the framework of these publicly
available data sets, because research is a lot about how do you measure progress in
a task, and how do you articulate convincingly that you've made a contribution, and that
largely involves referencing prior work in the same area that has used these same data
sets.
And then you say, given that this is a commonly understood reference point, this is what
I advance is for the progress, that kind of restricts how much you can jump back and
forth.
But maybe, maybe what, maybe some researchers are using ImageNet for suspects, but I just
haven't been kept in the loop.
So in some ways, I'm thinking it's a bit of a negative critique on the state of our methods
and research in general, and perhaps even application, that we're so tied to specific
data sets.
You know, certainly having a data set as kind of a lingua franca for comparing results
and all that makes sense.
But there's also a degree to which our methods and our research results are tied to these
specific data sets, kind of the classic overfit on ImageNet.
Do you see that as well in your work?
Yes.
Sam, yeah, you're bringing up an important point.
And this is one that I often, I think, is a very active thread of discussion at brain.
I sense people, well, I sense I've heard two different perspectives on this.
There's a general consensus that, as you described, overfitting the data sets that we have.
And I would say that a few researchers would dispute that entirely or would at least acknowledge
that we've really centered a lot of attention on these three data sets.
You can even go further.
Like, both the fields that I've worked in interpretability and model compression, a lot
of the focus has been on computer vision.
In fact, I would venture that very few papers have talked about different tasks or different
architectures.
Even though there's an urgent need for interpretability beyond just computer vision models.
And I kind of said at the beginning, there's two perspectives to this.
The other perspective is that research is a very particular way of talking about a problem.
And the framework of how discussion occurs in research community has to be, is by design
fairly narrow, mainly because it's a very precise way of advancing a scientific hypothesis
and a contribution.
So it's unclear to me that if someone did deviate from this norm and showed up with a new
paper on a new data set and said, this is a huge improvement.
I think either they would have to benchmark previous methods on a new data set, which
is a large technical contribution, depending on the field.
All I would be left with doubt is to what their contribution actually is.
And so that kind of captures the dilemma that a lot of researchers feel is that they acknowledge
it, but it's unclear how to proceed.
So your particular research, or at least the research that you've spent the most time
on thus far, is around interpretability.
And you're starting to do some model compression work now, but tell us specifically about the
interpretability work that you've been publishing on.
Maybe I'll start by just introducing interpretability and how it's commonly thought of within
research.
And then that will provide some context for the work that I've been doing.
Interpreterability is a very interesting problem.
interpretability broadly, when I ask, for example, Sam, if I asked you, do we want models
to be interpretable?
What would your answer be?
Do we want models to be interpretable?
I think sometimes if it doesn't cost too much.
Excellent.
Interesting.
Yeah.
You captured one of the key misconceptions that exists in the field, which is that all models
must be interpretable.
In fact, there's like new answers there, which is what you're describing.
Yeah.
The starting point when I ask people those questions is that it's a firm yes.
So I like that you've hedged.
And I think that's because we instinctively think of interpretability as this desirable
property, kind of like fairness or bias.
And it's interesting because then the next question is, well, what do you think is an
interpretable model?
And that's generally where there's uncertainty on the part of the person who's answering.
And maybe here I'll say is that interpretability within research has really focused on this
idea for these deep neural networks, where we can't articulate in a compact way what the
function that the model learns is, can we arrive at a methodology to try and explain
the model prediction or perhaps even the model function, which means I can the model function,
like can we understand how the model maps every input to every output.
And you illustrated one of the key misconceptions is that the degree to which we want to do this
and where we really want to invest effort may of fact depend depend upon the task.
So you talked about the burden, well, the burden can be thought of in a few different
ways, but one way is this idea that for some tasks, like if we can't incorrectly explain
how the model rises the decision, the cost on human welfare may be intolerable.
Like examples of this could be, for example, healthcare, where we're using a deep neural
network to revert diagnosis.
And then the doctor has to try and explain that diagnosis to a patient.
If there's an incorrect explanation that's given to the doctor, you can understand it
has perhaps, like we do, as a society, we wouldn't be willing to tolerate that.
However, there may be other tasks where either because the actual cost to humans is seen
to be, I hate to say the words low impact, but at least there's not a significant notions
of decreasing welfare involved, or the task has enough empirical evidence where we're
just more confident in like the overall behavior of the model.
Those are tasks which are currently said we don't need to focus as much as interpretability
because we have a, we have reassurance in different ways that this model is working
the way it is.
The other key challenge about interpretability is that it's very unclear when we've actually
arrived.
And this is really the tricky part is that when do we say this model is interpretable?
We're happy, we signed off, job done.
In fact, it's both hard to measure progress on the task, but interesting enough, the
finish line may look very different for different people.
The burden we carry of delivering a satisfying interpretable explanation may be different
depending on even what our downstream task is.
Like a good example of this is if you're sitting on a plane, and your plane is just delayed,
and the pilot comes on in and says, oh, we have an arrow with one of the engines on the
plane, and then they say that, we're fixing it, we have ground staff.
So as a passenger, you're just sitting in the cheap seats, and like you actually don't
know much beyond the technical error that's being fixed.
The pilot probably knows a lot more.
They've probably been talking to the ground staff, and they have an understanding of at least
like what part of the plane and what would it influence.
And then the ground staff probably has the most technical level of conversation because
they're precisely locating a certain sensor.
And so when the ground staff tells the pilot that it's fixed, that probably involves some
level of technical detail, so the pilot's confident.
When the pilot tells the passengers, it's normally, okay, sold, we're taking off.
And so I think that in a compact way kind of describes that we may need different levels
of explanations, and that is okay.
In fact, like if the pilot were to tell you every single technical detail where the ground
staff told them, we may feel slightly overwhelmed, or at least we would feel like,
oh wow, this is really serious, because they're telling a lot of different job game that
I don't know, and I'm feeling very, very disorientated.
And a lot of interpretability is trying to understand how do we both create a meaningful
explanation, and meaningful here really means given the domain, and given who the person
is, and what they have to do with that information.
But the topic of my research has been that it also should be reliable, and by reliable,
I mean that in creating a meaningful explanation, we shouldn't communicate information that
is not an accurate reflection of what the model has learned.
And this is a delicate area because in some ways the key problem is that with deep neural
networks, we don't know the ground truth to begin with.
So it's hard to say this explanation is better than this one, because we don't know the
true explanation.
And so a lot of what I've worked on for the last year is how do we create frameworks to
measure progress on this task, even in spite of the fact that there's no ground truth?
And setting the stage for us, you pointed out the distinction between, I think you refer
to it as interpreting model function versus model decision.
But you also use the term interpretability and explainability interchangeably, or at
least that was the impression that I got.
I tend to think of like interpretability as that functional, you know, what is really
happening in the model and explainability is, you know, given the model is more or less
black boxy, how do we make some sense of what it's telling us?
Is there a distinction there for you at all, or do you use them interchangeably, but,
you know, in light of acknowledging the two different tasks?
Yeah, really what I meant when I talked about explanations is this idea that a lot of interpretability
so in interpretive research is often explanations is considered a subset.
Explanations are just trying to explain a single example.
So that's how I use that word there is that for an explanation, you have a given input
and you're trying to say for this input, why did the model arrive at this prediction?
Whereas when you're trying to understand the function as a whole, you're trying to understand
perhaps what was most important to the function of the model learned over at least a larger
subset of examples, if not the entire data set.
Most interpretability work for deep neural networks is focused on the single example explanation.
So there often it's an image and you're trying to say, why did the model arrive at this
prediction for this image?
That is being the largest part of the discourse so far within deep neural networks.
And so your work is almost at a meta level, it's, you know, there's a bunch of research
that is happening and needs to happen in interpretability, but how do we compare these
different results without really knowing what the model is doing in the first place?
Yeah, I love how you capture that very succinctly.
Yeah, that's exactly the challenge.
We now have a rich set of methods.
And the question that has to be answered is, okay, I have all these different methodologies
for arriving an explanation, which one do I use?
And so my work is focused on methods that estimate feature importance in a network, and
they estimate imperfection importance.
So the estimators that I have evaluated with my co-authors in both pieces of work that
I did over the last year have asked in this given input image, these estimators will say
this pixel will essentially arrive at a ranking of these and the most important pixels for
the model prediction.
So imagine an image of an ostrich and these estimators will arrive at a different estimate
for how important the pixel of the ostrich knows is to the prediction of the model at the
other end.
And the first piece of work that I did was it essentially started with a premise that
one definition of reliability is that if the model is not affected by a transformation
of the input, then these estimators should not be affected because if the model is unchanged,
we want these estimators to reflect the model.
And to carry out this test, we establish a very narrow ground truth where we created a
transformation to the input, which was a mean shift.
And then by construction, we ensure that the model was not affected.
And so the gradients of the model did not change, the weights of the model will unchanged,
the model was impervious to this change in the input.
But then we showed that many of these estimators changed.
What specifically do we mean by estimator here?
So estimators, I actually use them to change the way with methods.
So think of it as just these set of methods.
And I use them as estimators because I think it's useful to think of some of these tasks
as trying to, because there's no ground truth, really that model is trying to estimate
what's important.
And we're just trying to evaluate whether that estimate was good or not, whether it was
accurate or reliable.
When I think of the description you gave of the method, it calls to mind the Lyme method
of explainability, which also seeks to perturb or inject noise into the inputs and use that
to determine what parts of the input are most relevant.
Can you maybe, for those that are familiar with Lyme, talk about how the two ideas relate?
They relate, it's the same idea.
Yeah.
Okay.
That was an excellent way to anchor the conversation.
Yeah, so when I talk about methods or estimators, Lyme is a key example of an estimate that many
people are familiar with, and other examples of methods or estimators can be things like
just taking the gradient.
So you end up with a gradient heat map or it could be a guide of backdrop or this integrated
gradients, which is another fantastic example of an estimator that is trying to weigh importance
of these input pixels.
But yes, I'm really glad that you interjected there because it's one in the same.
But so Lyme is one of these estimators and it's doing noise injection, but it sounds
like you're essentially taking that same approach, but applying it again at kind of this meta
level to evaluate the estimators.
Well, maybe not quite.
So, Lyme is one of the estimators that could be evaluated, for example.
And I think perhaps if you think about it, so we have something like Lyme, we have these
other methods that are estimating importance.
And in both, in my line of research, essentially what I'm asking is all of the estimates correct
or not, Lyme is very intuitive because I think it's exciting because it gives a clear explanation
of what the model arrived at for that prediction.
But the question I ask is that how do we actually know that what is exciting to us as humans
is also a reliable reflection of the model?
And to do that, the first piece of research I described essentially takes these methods
and says, we've applied a transformation to the input images, which is a means shift.
And the model has not changed.
Do the rankings change?
And if they change, then that is considered a failure in this test case because they
should simply reflect what the model thinks is important.
And if they diverge from that, then it's unreliable.
The second research that I did was one step further, and again, this idea of how do we
measure progress on this task of reliably estimating importance?
And what we did there was we said, OK, these different methods all rank the pixels.
Let's remove the pixels that they rank as most important a fraction.
And then give those examples of these modified inputs back to the model.
And the presumption is that if the methods actually identified what was important, if they
actually rank these pixels, then it should really damage the model if you're training
it again.
And we compared it to a random removal of pixels.
So really what they were asking, if you randomly remove pixels, essentially you're randomly
assigning importance to the inputs.
And if these methods don't damage the model more than just a random removal, then in some
ways, they're less able to accurately rank than just random removal.
So that was a very interesting piece of research.
And it's kind of again, again, this idea that in the absence of any ground truth, what
we need to do in this field is kind of stay precisely these desirable properties and
come articulate frameworks in which to measure these, because then at least we haven't
established, we've articulated a way of saying we are measuring it in this sense and given
this framework, this method appears to be more accurate or reliable.
And so was the end result of these two research efforts, a kind of a ranking of these different
estimators.
And if so, what did you tend to find?
But also is ranking these estimators a one-dimensional thing or are there multiple ways that we should
be comparing these different methods?
Yeah.
And you raise an important point, so to kind of share the results, on both we found very
interesting results.
And in fact, kind of an advance, at least for me, I felt taught me new things about how
the model actually arrives at feature importance.
So I'm both, it was a little bit of a grumpy picture.
We found that in the most recent work, which is a paper called remove and retrain where
you essentially are moving the most important and retraining the model, we found that the
estimates that we consider, and we only consider a small subset.
So I'll caveat with that, they didn't perform better than a random assignment of importance.
But we did find this, which is fascinating, which is that if you ensemble, so if you take
for the same image, you take various noisy estimates, which means that you add noise to
the estimate and then do that repeatedly.
And you combine with squaring.
So these are two separate transformations applied to the estimate.
So imagine this for Lyme, like essentially you'd be taking noisy Lyme estimates, squaring.
We found that that far outperformed a random estimate of importance.
And so that's kind of interesting, aha moment, because it's not quite aha moment, because
it's not the theoretical justification yet for why that is the case.
But it suggests firstly, the importance of empirical research, which is an option you
discover on interesting things that motivate research in a new direction, but also tells
us that we, in some ways, how we're considering these estimates now, there's very exciting
things that we can do that will improve progress on a task.
And then you ask, like your second question is critical, because you say, is this one
dimension?
Absolutely.
Like, this work should be considered one dimension, and in fact, like, there's almost,
even if we were to narrow it to reliability and accuracy, which is really what this
work is trying to get at, there could be other dimensions for measuring this.
Because essentially what we've done is we've just stated, this is our way of technically
defining these properties, and given our definition, this is what we think are the best methods.
Someone else would come along and say, there's another way to define technical reliability,
and given this, this is what we find, that's the nature of this field.
And I would encourage that.
I think it's important for researchers and for the community as a whole to think about,
how do we measure, how do we ensure that we are actually delivering an explanation that
people can trust that is a reflection of the model?
I will say one more thought on that, and then I'll pause, which is that you can also
think about a whole other set of soluble properties that are on the being meaningful
to human side.
So often how that's measured now in the body of research is that user studies are conducted,
and things like, do you trust this, are asked?
And that in itself, I sense, could also benefit from a more, well, both larger scale treatments,
but also much more nuance, like, is for this task, for this person, does this make sense?
Because, inevitably, interpretability is going to evolve on a few different levels.
Like, based on our perception as a society of what do we need for us to trust, technical
implementations of deep neural networks, as well as in a job-specific way, which is that,
you know, a doctor is probably going to always want an explanation of every example.
It's always going to be at the single example level.
But as a researcher, for example, what's more important to me, I would suggest as an interpretability
tool, is that I want to get a sense of the distribution, and I want to understand, out
of the distribution of data, what data points are perhaps more, either, problematic, and
you can define problematic in a few ways.
But if I have a better way to understand what is different in a data set, that's helpful
for me in understanding, am I comfortable to deploy this model?
So all these vantage points will require different desirable properties, but I'll stop
them.
One question that I did have was in the evaluating feature importance estimates paper,
you look at, as you mentioned, a subset of these different estimator types, gradient
heat map, integrated gradients and some others, is lime a subset of one of these?
Like are you using a generic term of these that you include that lime falls under, or
did you just not look at lime particularly?
We did not look at lime, and I would be really excited to see the results online.
The constraint on these papers is always this tension between, we have limited time and
certain amount of computational resources, but partly, I think what this motivates more
than anything is open sourcing code, which is something we're doing for this paper.
And that's important because really if this is going to be a sustainable benchmark, researchers
have to be able to self-serve and also be able to take what we did and apply to that method.
But I would be really interested to see the results of this for lime, mainly because lime
imposes contiguous feature importance by that. I mean that let me frame this using a
counting example.
So gradient heat maps, for example, don't require that feature importance are connected
set of pixels.
You just take the gradient of the pre-soft mix activation for the model prediction with
respect to the input and then you rank.
But that tends to result in very diffuse attribution, meaning that importance when you rank it
doesn't concentrate exclusively on a set of connected pixels, whereas lime imposes this
constraint by its methodology that importance is restricted to the connected pixels.
Humans tend to, and I say this hesitantly because this is not a research conclusion, this
is hypothesis, but I sense humans tend to like connected pixels as more what they perceive
to be interpretable.
And so I would be excited to at least benchmark lime on another example of a method that
does assist feature importance in this way using remove and retrain, because it may have
very different results.
Or at least tell us something about how connected importance versus this very few importance
actually is related to the reliability of the method.
Interesting.
It strikes me that from that perspective, the idea that humans prefer these connected
areas versus diffuse areas, almost says that humans want explainability that makes sense
to them more than interpretability that might be more functionally accurate.
Yeah, I hesitate to say, we're just talking here, we're just throwing stuff out there.
But I will say the most, how researchers articulate this in papers is quite funny.
So this comes up, no one quite gets at this explicitly, because again, it's not something
that's exactly testable, and even in my current research, I don't benchmark any of these
connected methods, so I really hesitate to venture, but what I will talk about this frustration
with diffuse methods, because that's kind of a coherent pattern in these papers.
Researches will say that the method is perceived to be visually noisy, and that's interesting,
right?
Because that actually says nothing about whether the method is accurate or not for gradients.
It just speaks to our reluctance to, either it's hard to discern what the explanation
is saying, what's actually important, this image of going to gradients, and that might
be because as humans, we may not know what to do with the complexity, again, here I'm
using another word that is very subjective, like visual noise, the complexity of the explanation.
Whereas if something is connected and placed in a specific area, at least I sense it may
be more reassuring, but again, I think we're both throwing things out there, and that's
what makes this domain so challenging, is that a lot of it is subjective, a lot of it
is inherently like, my preference may differ in fact from your preference, and that's
why there's no clear finish line.
And did you find that the specific approach to ensembling, you mentioned squaring the estimate,
I'm not sure, well, you can elaborate on that, but the question is, did it apply equally
to all of the methods you looked at, gradient, heat map, integrated gradients, smooth gradients,
grid gradient, etc., or are there specific formulas or formulations that apply to individual
methods?
Well, this is the cool thing, is that it benefited all the estimators, all the methods
that we considered, and I apologize for, I keep on interchangeably using those words
so yeah, it really dramatically, I would say, improve the accuracy of the three estimators
that we considered, and what, what it, when I say it, like it was firstly adding noise
to the images, so for a single image, you would create a set of 15 noisy images, and then
you would arrive at 15 different predictions, and given those 15 different predictions,
you would take the estimate for the methods you're considering, and you would square it,
and then you would average those, so you left with one estimate, and again, from these
15 different images, and again, an estimate, in this case, is,
lie, example, but the actual estimate is a, like a per pixel probability, for example,
or something on a per pixel basis, right?
It's on a per pixel basis, it's often, in the case of gradients, it's not capped at a
certain, it's just the magnitude can be as important, so it's not probability, but other
methods will cap it, so it's cumulative, so the sum of all the estimates and importance
should sum to one, that's a property called completeness, but yes, so you're correct,
it's on a per pixel level, so you're averaging across, however many noisy estimates you
have to arrive at a single estimate for a given picture.
So that's your interpretability research, if you have a few more minutes, I'm curious
about some of the model compression work that you're doing, and that ties into the event
that's kind of brought us together, which is the deep learning in Dava, which is coming
up in South Africa, and you, you know, we're well beyond our typical background segment
here, but you grew up in...
I'm at a similar reaction when you said that, I'm like, oh wow, this is introducing
some context later in the game, but it's, yeah, I grew up in Africa, and mostly in Southern
Africa, so I grew up in Mozambique, South Africa, in the Soutu, Swaziland, Kenya, my family
just moved to West Africa, they just moved to Monrovia, Liberia, so I, then Dava, so
then Dava's coming up, you mentioned, but also Google is opening this AI lab in Acra,
and both are very exciting, because so much of it, I think, is quite exciting personally
for me, as a way to, well, in Dava, I would say, is very much directly doing this, and
Dava, the motivation is, let's build technical capacity, and that's the most important
determinant, I kind of mentioned this when I talked about what made this year possible,
it's contact with very experienced researchers, and just the ability to collaborate, and I
think that Dava is really in the same vein, like you're bringing these researchers from
all over the world, and all these students, and also practitioners from all over Africa
to the same place, and wow, is that exciting?
I don't need to, I'll be too exuberant about it, but I think, I recently went to Data
Science Africa in Kenya, and the energy level is insane, mainly because we, right now,
a technical talent, I would say, is very correlated with geographic location, so places like San
Francisco, New York, Paris, a handful of other geographies have a lot of, just, I'd
say, experience, and as soon as you leave one of these cities, you realize it's a clip
hanger, like the ability to have a community to learn and grow in the places where I grew
up is extremely limited, and so for students who attend these gatherings, it's just really,
they're so excited because this is a way for them to connect, many times what they've
been studying by themselves online, with people who are actually doing this and practicing
in the field or doing research, so that's why it's so exciting, I forget the second part
of your question, but maybe, yeah, maybe on points there.
Well, that provides some interesting context for your move to the Accra Office and your
work in model compression, I think that was the tie-in that I wanted you to elaborate
on.
Well, the Accra Office, so most of us, these days, is leading the Google AI Accra Office,
and he's amazing.
He's one of my, both mentors and collaborators here at Brain, and the Google Accra Office
is like any other Brain Office, so the goal is to go and have, attract the best researchers
and do research, so very much in the same vein of a lot of the things I've described today.
I think what most of us, I sense would agree with, is that this is other ideas, that by
placing in researchers in different geographies and in different environments, you have often
very novel approaches to ideas, and that is both because the resource constraints that
you find may be different, as well as because simply a lot of researchers who you work with
and your day-to-day conversations, and you may find novel directions because of that.
The other hope for this Accra Office is that having researchers there may provide important
externalities for the ecosystem as a whole, and Mr. Fah has also really championed this
master's program, which will, is also starting this year, and that's supported by Facebook
and Google, I believe, but how that relates to monocompression, it doesn't necessarily
have to, but it happens to, for my current research, which is also with Mr. Fah and Eric
and another Collaborate Trevor at Brain, and there, what I'm interested in, is this idea
that really, the starting point for this research is, why do we need such large models?
Do you know on networks that are notorious for the amount of parameters that need it,
and also the tendency for the number of parameters to grow year-over-year in the body of research?
But I think it's interesting if we impose different constraints, like, for example, if we
actually think about deployment in very resource-constrained environments, such as mobile phones, which
a lot of parts of Africa have jumped directly from neither having a laptop or mobile phone
to just jumping to a mobile phone.
So if we think about that, that imposes very different resource boundaries than what
we're currently used to, and often, like, thinking about that drives interesting ways of attacking
the same problem.
There's a great example of this, and how engineers at Google, and I believe what's that Facebook
have tackled, how do they make engineering products, which are better suited to low bandwidth
or limited connectivity environments?
And the solution was one of the solutions that we experienced and is highly visible as
this idea that there's an entirely separate internet connection that you can connect to
that will give you the experience of being in an impaired bandwidth environment.
And this was actually like a starling catalyst for a lot of engineering innovation around
these problems.
Again, like, this is my interest in model compression, I sense could have been equally
pursued in Manavue at Google Brain headquarters or in the new office in Accra.
But it's exciting to sense, like, energize, like, what I'm thinking about by also connecting
with people who are experiencing the pains of trying to deploy models.
And that's one of the most frequent questions that you get from students when you go
to teach in a place like Nairobi, is that a lot of people have an idea, have tried to
implement it, and now trying to deploy it using something like T.F. Light and now experiencing
severe pain points.
That's exciting for me at least.
My research direction is specifically on this question of how do we effectively remove
weights from a model while preserving accuracy?
There's a few different threads to how people tackle this problem.
You may have to rein me in again if we get started, so I'll pause.
I don't know if there were any immediate questions.
It sounds like from our earlier chat that you can't go into a ton of detail about your research
because it hasn't been published yet, but if you could give us an overview of kind of
the landscape that you're playing in as we wrap up, that would be great.
I'll mention probably four key directions that this problem has been thought about, and
the other defining characteristic of this field of research is that I sense it's been
underserved.
There's being very little, it's a rather new field of research.
But some of those solutions are actually very old.
I know that was an odd caveat, but I'll give you more context for what I mean.
The key approaches to this problem are that you have things like pruning, and pruning
says, let's reduce the number of weights in a network.
You either do that over the course of training by setting certain weights to zero or regularizing
the certain weights become very close to zero, or at the end of training by saying, this
is our model, and now we're going to try and arrive at a much smaller model.
The metric that's been optimized for is a normally level of sparsity given a certain degree
of accuracy.
The second approach, which has been enormously successful, and in some ways has taught
us a lot about deep neural networks in general, is quantization.
Quantization refers to this idea of you have a certain level of precision and the weight
of themselves, and you can essentially take a floating point weight, and you can reduce
the number of bits, and then you can still have a remarkable level of accuracy.
So this takes, like most often, a trained model, and then changes the representation of
the weights.
And this has huge implications for memory, so often you're able to really improve memory
of the models, the memory needed to store the models by simply changing how the weights
represented.
This is third direction, which is model distillation, so this is also interesting enough in present
as an interpretability direction, but you have this teacher model, which is your massive
deep neural network, and then you're trying to train a student network to have the same
accuracy as a teacher network with a fewer amount of parameters, and all these are quite
exciting.
The fourth, which I think is probably the most underserved, is this idea of trying to
do things that are optimized for the actual hardware.
And this is tricky, because this is a nice loop back to how we began this conversation,
but it's hard to pursue a research that is optimized to hardware, because hardware tends
to be non-standard.
And so think about a TPU, which is made by Google, is one of the first hardware that's directly
has in mind deep learning, and then a GPU made by Nvidia.
But even when you think about cell phones, it's non-standard.
And so this research, in some ways, the constraint is that you want to make it generalizable enough
but you still want to make significant inroads into how it's being deployed.
But I started this kind of framework by saying, this odd disconnect, I said, oh, in some
ways this is a very new field of research, and other ways it's not, I'm thinking specifically
of pruning.
So pruning has actually been around since the 1990s.
So pruning was the first field I mentioned where you're trying to remove weights and arrive
in a smaller model.
And there was the first paper came out in the 1990s.
It was initially called double back prop, but the current proposal in the form of optimal
brain damage, which I think is a great name for a paper.
It certainly has a ring to it, but he proposed one of the first methods.
And so some of these approaches have been around for a while.
It's just that there's a new level of attention, both because there's a sense that now the
resource constraint is firm.
There's a lot of discussion around Moore's Law and how we can't quite get the hardware
to just catch up with whatever researchers want to do.
And then, instead, we must think of interesting ways for researchers to meet the hardware.
And that's not an easy feat, because hardware and research has tended to be very siloed
in both directions.
So that's what makes this particularly, like, perhaps like a very interesting time for
a lot of the enthusiasm around the subfield.
I think we could launch into another hour long conversation about this topic.
I didn't want you to have to ring me in, so.
But we will have to find a time and place to reconvene on that one.
But Sarah, thank you so much for taking the time to chat with us.
It's been a pleasure to have you on the show and to learn about what you're doing across
all the various things that you're working on.
Thank you so much.
It was really fun chatting with you, Sam.
And oh, I think Sam had mentioned offline, which I'll repeat here for social peer pressure,
but he mentioned that he would come visit the Ghana Acro office.
So I plan to hold you to that.
Let's do it, for sure.
And you mentioned earlier, Mustafa Sisay, who is heading up that office for folks who didn't
catch it.
I interviewed him when he was at Facebook AI Research, and we will drop a link to that
show in the show notes.
But once again, Sarah, thanks so much.
Yeah, thank you.
All right, everyone.
That's our show for today.
For more information on Sarah or any of the topics covered in this show, visit twimlai.com
slash talk slash 189.
To follow the entire deep learning and double podcast series, visit twimlai.com slash
endaba 2018.
Thanks again to Google for their sponsorship of this series.
Be sure to check out the 2019 AI residency program at g.co slash AI residency.
As always, thanks so much for listening, and catch you next time.
