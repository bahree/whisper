WEBVTT

00:00.000 --> 00:11.440
All right, everyone. Welcome to another episode of the Tumel AI podcast. I am, of course,

00:11.440 --> 00:17.680
your host Sam Charrington. And today I'm joined by Al Kay Wickers. Al Kay is a research scientist

00:17.680 --> 00:22.880
at Qualcomm. Before we get going, be sure to take a moment to hit the subscribe button wherever

00:22.880 --> 00:28.480
you're listening to today's show. Al Kay, thanks for joining me and welcome to the podcast.

00:28.480 --> 00:34.400
Today we'll be digging into some of the work you and your colleagues are presenting at ICLR.

00:35.360 --> 00:40.160
And especially your work on transformer-based transform coding. But before we do that,

00:40.160 --> 00:44.240
I'd love to have you share a little bit about your background and how you came to work in

00:44.240 --> 00:49.440
machine learning. Thanks a lot, Sam. Thanks for having me. So I started out in machine learning,

00:49.440 --> 00:54.800
I think, around the time that I studied at UVA. So there was an AI course at UVA,

00:54.800 --> 01:01.200
which I joined in 2012. And a couple of co-students, who were a year higher than me,

01:02.000 --> 01:07.280
were quite business-oriented. And already during their masters, they were founding small companies.

01:07.280 --> 01:13.440
And eventually they guided me into their startup, which was called Cypher. And I believe you've

01:13.440 --> 01:17.680
actually interviewed some of the original founders. So that's Max Welling, who's a professor

01:18.480 --> 01:24.000
currently at Microsoft. And some of the other original founders are Stella Qualcomm, Taco Cohen,

01:24.000 --> 01:29.680
who has worked a lot on a group of variant networks and time in Blancofort. And so they kind of

01:30.480 --> 01:35.280
talked me into joining their company. And I guess the rest is history, because this startup was

01:35.280 --> 01:40.080
acquired after a couple of years by Qualcomm. And I've been at Qualcomm ever since, for the past four

01:40.080 --> 01:45.360
and a half years or so, having worked on concepts like reinforcement learning, for autonomous

01:45.360 --> 01:50.080
driving. And in the past two and a half to three years, mostly on neural data compression,

01:50.080 --> 01:57.920
using generative models. Well, let's dig into that topic. Talk a bit broadly about your research into

01:59.280 --> 02:04.720
the compression side of things, and we'll dig deeper. So I think neural data compression is

02:04.720 --> 02:10.800
relatively new. I think that the seminal works came out around 2016, 2017. But it's quite a

02:10.800 --> 02:15.280
beautiful application of generative modeling. Because most likelihood models, in the end, what they

02:15.280 --> 02:22.640
give you is exactly what the name advertises, a likelihood for a data point. And information theory

02:22.640 --> 02:29.280
tells us that you can then compress that data point with a certain number of bits. And what these

02:29.280 --> 02:34.800
generative models can be used for, and that's how we were applying them, is to estimate likelihood

02:34.800 --> 02:40.720
of incoming data points. So you could imagine that's images or audio or video. And using entry

02:40.720 --> 02:47.920
coding, then squeeze out any redundancy, where this likelihood model tells you exactly how many

02:47.920 --> 02:52.960
bits you're going to need in order to compress that data. So what we've done in the past couple of

02:52.960 --> 02:58.720
years is mainly do this, the trained big generative models in sort of likelihood modeling tasks.

02:58.720 --> 03:02.240
But of course, with the end goal that will eventually use these in practical setting.

03:02.880 --> 03:09.840
So whereas in typical generative models, you could imagine that big hierarchical VIEs,

03:09.840 --> 03:15.040
they train in a fully continuous way in the latent space is continuous. So a big difference in

03:15.040 --> 03:20.480
neural data compression is that eventually you have to quantize this. You have to move to integer

03:20.480 --> 03:27.040
representations instead of floating points in order to compress in a lossless way. So a large part

03:27.040 --> 03:30.800
of the work we've done in the last three years is training these big generative models with a

03:30.800 --> 03:35.840
quantized latent space. Whereas the general field of generative modeling, I would say is mostly

03:35.840 --> 03:41.040
focused on continuous latent space models. Often when we're talking about quantization, it's

03:42.400 --> 03:50.400
the focus of the conversation is around efficiency and performance. In this case, we're talking

03:50.400 --> 03:56.880
about just the fundamental nature of how you're trying to use the output of the networks for

03:56.880 --> 04:04.160
communication. The interplay between the quantization from an efficiency perspective and

04:04.160 --> 04:09.760
the way you're using it in compression. Yeah, thanks for pointing it out. It's actually a good

04:09.760 --> 04:14.560
distinction. So in our case, the only thing we need to quantize in theory as long as we don't

04:14.560 --> 04:20.400
deploy these networks is the latent space. And the reason that we compress this is that this latent

04:20.400 --> 04:25.920
variable is actually the compressed representation of the data. So as soon as you want to transmit this

04:25.920 --> 04:32.960
to let's say different hardware, the only way you can do this in a truly lossless way is if this

04:32.960 --> 04:37.600
compressed representation is quantized because of things like floating point precision, for example.

04:38.240 --> 04:44.160
Now, the quantization and efficiency angle comes into play when you think about deploying these

04:44.880 --> 04:50.080
generative codecs. So of course, when we move to device, we would likely want these models to run

04:50.080 --> 04:55.600
in fixed point instead of in floating point. But most of the research that we've done so far,

04:56.800 --> 05:01.040
leading up to academic publications, for example, has been on floating point models,

05:01.040 --> 05:06.160
where the latent space is quantized. And then as soon as we go to prototypes where we deploy these

05:06.160 --> 05:11.200
to the device, that's when we'll quantize the entire model. So the way it's activations and so on.

05:11.200 --> 05:16.880
And that's when efficiency becomes most important. So we've been talking broadly about neural compression

05:16.880 --> 05:21.760
thus far and you're working that area, but you've got a particular interest on the video side.

05:21.760 --> 05:27.120
Can you talk a little bit about how the work you've done extends to video?

05:27.120 --> 05:33.760
So video compression poses a couple of challenges that image compression, for example, does not.

05:35.520 --> 05:40.960
And that's mainly about the subjective experience. So as a human observer,

05:40.960 --> 05:45.040
you would want your video to look consistent over time and you would want the motions to be consistent

05:45.040 --> 05:52.080
and so on. And also about exploiting more redundancies. So when you work on image compression,

05:52.080 --> 05:56.960
if you want to exploit redundancies, what you'll mainly watch out for are things that are similar

05:56.960 --> 06:02.320
across the image, you know, patterns that appear in different parts of it. Whereas for video,

06:02.320 --> 06:07.520
there's also the temporal redundancy. Most of the time when you look at subsequent frames,

06:07.520 --> 06:12.080
they will actually be very close to each other, especially for background of a video, for example.

06:12.640 --> 06:17.200
And you can exploit this by making sure that any previously transmitted information,

06:17.200 --> 06:23.360
you don't encode it again, so to speak. The other part, so this subjective quality part,

06:24.160 --> 06:28.880
makes it a more challenging task and I think more interesting, therefore, than image compression.

06:30.240 --> 06:36.880
As it's very noticeable if you have a codec that produces frames that are inconsistent over time.

06:36.880 --> 06:42.080
Many years, you're just watching it and it's jerky and just visually unsettling.

06:42.080 --> 06:48.240
Yeah, exactly. Or, you know, speckle noise and those kind of things. So a lot of our research

06:48.240 --> 06:53.280
is aimed at exploiting this temporal redundancy apart from concepts that we borrow from image

06:53.280 --> 06:58.320
compression, of course. And more recently, if you've also started looking more into how to

06:58.320 --> 07:05.760
improve the perceptual quality when you're using this video codec. So that could go as far as

07:05.760 --> 07:12.080
as GAN-based compression. So when you're really hallucinating parts of the reconstructions that are not

07:12.080 --> 07:19.920
truly in the bitstream, but also things like region of interest-based coding. So for example,

07:19.920 --> 07:24.400
now that you and I are talking, the background is actually not the most important part of the video.

07:24.400 --> 07:30.160
You will want the face to be accurate and maybe text that appears in the video. So paying more

07:30.160 --> 07:34.240
attention to that and spending more bits on that is another way to improve perceptual quality.

07:34.240 --> 07:41.600
What's the relationship between the research you're doing on the neural compression side and the

07:42.480 --> 07:49.520
historical research that's that's done into compression pre-neural networks, right? We

07:51.280 --> 07:55.280
you know, figured out a lot of these same things, right? The background is not moving. We don't have

07:55.280 --> 08:06.720
to spend as much bandwidth on that. Do you are you pulling kind of insights from the evolution

08:06.720 --> 08:12.080
of that prior work or is the neural setting so different that you're coming up with new tricks

08:12.080 --> 08:19.280
all the time? I wish the latter were always true. Of course, we're boring heavily from domain

08:19.280 --> 08:25.760
expertise that's been that's been created over many years, maybe 40, 50 years even. So

08:27.200 --> 08:33.840
handcrafted codex or standard codex, they apply a lot of domain knowledge that also goes for

08:33.840 --> 08:40.240
neural codex and what you see most often in especially video coding nowadays is that certain concepts

08:40.240 --> 08:46.800
are being pulled in like this motion composition that you mentioned. Of course, it's logical to

08:46.800 --> 08:52.800
use previous information, already decoded information on the receiver end. So we're boring a lot

08:52.800 --> 09:00.480
of those concepts from traditional coding and at the same time, many of the operations used in

09:00.480 --> 09:07.280
handcrafted codex can be replaced in a sometimes more efficient way and by efficient, I mean,

09:07.280 --> 09:10.800
from a rate distortion point of view, not necessarily from a computational point of view,

09:10.800 --> 09:19.680
then what the handcrafted codex do? What are the key benchmark tasks in video compression?

09:19.680 --> 09:27.120
So there are a couple of video data sets that are commonly used for evaluation of any video codec

09:27.120 --> 09:34.320
really and these have been established by this standards community. The trick is you can't

09:34.320 --> 09:39.760
just use any video. You want it to be raw video as high quality as possible, no compression apply

09:39.760 --> 09:46.240
to it and somewhat counterintuitively because this is pretty expensive to obtain and store.

09:47.280 --> 09:51.840
What most people train on nowadays is not raw video but actually already compressed video,

09:53.280 --> 09:57.680
maybe down sampled or augmented in some way to get rid of compression artifacts.

09:58.560 --> 10:04.880
But there are a few common raw video data sets that are commonly used for neural video coding

10:04.880 --> 10:11.520
benchmarking. When you're using these raw video data sets and then you want to apply the

10:12.720 --> 10:16.080
compressions in the real world, say with mobile phone

10:18.960 --> 10:24.720
information or camera information, do you have domain adaptation issues where

10:26.160 --> 10:30.560
you have to adapt the work? Yeah, that's an interesting question.

10:30.560 --> 10:37.200
And mostly because there's no big cross domain benchmark, so it's hard to really judge the

10:37.200 --> 10:42.640
impact until you deploy. We did notice this in a few cases and we have some work addressing this

10:42.640 --> 10:48.640
exact issue. So we noticed for example, when we pre-trained our codex on some video data set,

10:48.640 --> 10:54.880
we test it on some benchmark data set, then we get a certain score. But now if we fine tune on

10:54.880 --> 11:00.560
a subset of that new domain and we test it on some whole that set, we actually see quite a big

11:00.560 --> 11:07.680
performance increase. So these neural codex, they are generalizing beyond what they're trained on

11:08.400 --> 11:12.240
and hopefully your training data set is so diverse that you can handle many cases.

11:12.800 --> 11:17.600
But there is room for domain adaptation. And one work of my colleague,

11:18.480 --> 11:23.280
this is actually aimed at exactly this issue. So the solution that they come up with

11:23.280 --> 11:31.200
is to overfit on the data to compress and then transmit the parameter update, so the model update

11:31.200 --> 11:36.400
along with the bit stream. So what you get is a sort of customized codec for every data point,

11:36.400 --> 11:40.640
where the starting point is some global model that hopefully generalizes to many data points.

11:40.640 --> 11:46.080
But by altering the model in just a tiny bit, you can gain quite a bit of rate distortion performance.

11:46.080 --> 11:55.040
And another question on the data set given that a lot of the performance improvements that

11:55.040 --> 12:02.480
you're trying to achieve are perceptual. How what's the evaluation process? How do you evaluate

12:02.480 --> 12:07.360
performance in that environment? I think the best thing you can always do in perceptual quality

12:07.360 --> 12:12.880
evaluation is user studies. So actually having people, actual people look at two videos and then

12:12.880 --> 12:19.760
judge which one of these two is better by some criteria. And we have some tools to perform

12:19.760 --> 12:25.200
user studies. Of course, user studies are quite expensive. So what we typically use until that time

12:25.200 --> 12:30.560
are proxy metrics. So there are metrics developed by Netflix, for example, called VMAF,

12:30.560 --> 12:34.960
which measures perceptual quality based on some statistics that they observed in their

12:34.960 --> 12:41.200
own testing environment. There are many perceptual metrics that we borrow from the GAN literature,

12:41.200 --> 12:46.800
such as Frichet Inception Distance. So these are not perfect. They were actually not really

12:46.800 --> 12:51.440
intended for this use case. So Frichet Inception Distance, for example, uses an Inception Network

12:51.440 --> 12:57.120
pre-trained on ImageNet to extract some of its inputs. So it's not a perfect fit to video,

12:57.120 --> 13:03.040
but it's a decent proxy to gauge image quality before you finally do that expensive user study.

13:03.040 --> 13:12.240
So one of the big things that has been happening in this space broadly, computer vision is the

13:12.240 --> 13:18.880
introduction of transformers. How has that impacted the work you're doing around compression?

13:19.840 --> 13:26.880
So there are actually a few works on transformer based compression on images and one of which

13:26.880 --> 13:32.720
and is the one that we are talking about today, of course, the work by my colleague, Janau and Taco,

13:32.720 --> 13:39.200
on transformer based video compression. And I think the main intuition behind this work is that

13:39.840 --> 13:44.560
we know that these confolutions that we're using may not be the perfect building block or basic

13:44.560 --> 13:51.840
operator. And we've seen that a lot of the work improving image codecs is aimed at improving

13:51.840 --> 13:57.520
the likelihood model. So the model that you eventually use to compress these quantized latents

13:57.520 --> 14:03.520
by modeling the distribution. And not a lot of it was focused on changing the the transform as

14:03.520 --> 14:08.720
it's called. So the operation that takes the data and then transforms it into this compressed

14:08.720 --> 14:15.120
representation. And what Janau and Janau did and found out was that you can use some of these

14:15.120 --> 14:21.120
vision transformer building blocks in order to create a better transform. And somewhat counterintuitively,

14:21.120 --> 14:24.560
because a lot of the transformer and vision transformer models are actually computationally

14:24.560 --> 14:31.440
very expensive, they could do so with less computation than with the convolutional models that we've

14:31.440 --> 14:36.880
been using up until then. So when I noticed this, there was of course quite quite a nice insight.

14:37.440 --> 14:43.920
And we've been using these vision transformer based building blocks in our transform

14:43.920 --> 14:49.760
architecture ever since. I don't think we've gone into much detail in the podcast about

14:49.760 --> 14:56.880
vision transformers, fit and swim and things like that. Can you provide an overview of how

14:57.440 --> 15:03.600
the transformer architectures being applied to vision oriented tests? I think the main intuition

15:03.600 --> 15:12.400
behind most modern vision transformers is to treat the image as a collection of patches in a

15:12.400 --> 15:18.800
sort of similar way that language transformers may treat language as a collection of

15:18.800 --> 15:26.160
sentences, words, tokens. So the typical pipeline is something along these lines. We have an image

15:26.160 --> 15:34.080
to encode, let's say. And we would extract patches from that image and bet these in a certain way.

15:34.640 --> 15:41.760
And then we just act as if these are the tokens that are being produced by language transformers as

15:41.760 --> 15:48.480
well. And we apply these transformer blocks on the resulting vision tokens. Of course, vision

15:48.480 --> 15:52.800
transformers were originally proposed for tasks like detection and classification,

15:54.000 --> 15:59.600
whereas we're using them in a compression setting. So I guess a big difference between how vision

15:59.600 --> 16:04.800
transformers were used in classification and detection and what we're doing is that eventually we

16:04.800 --> 16:10.880
have to also generate a dense reconstruction as opposed to single classification. Can you talk a

16:10.880 --> 16:17.520
little bit more about the research that you mentioned with Taco and his co-author? Like I said,

16:17.520 --> 16:23.680
the key idea behind this is to use this swing transformer, which is a type of vision transformer

16:23.680 --> 16:28.560
that is more memory efficient than the original originally proposed vision transformer.

16:29.760 --> 16:35.840
So the vision transformer, as it was proposed, it uses global self-attention and therefore

16:36.400 --> 16:42.480
it's memory usage scales quadratically with the size of the image. Now we most of the time are

16:42.480 --> 16:47.600
encoding pretty large images and video. So of course, that doesn't really work well. What swing

16:47.600 --> 16:54.320
transformers propose is to instead use local windows in which you compute local self-attention

16:54.320 --> 16:59.520
and then aggregate these windows in a actually quite similar manner to how the early convolutional

16:59.520 --> 17:05.680
architectures aggregate information. So you start off with very small windows in which you aggregate

17:05.680 --> 17:11.680
almost pixel information and then at higher levels of the hierarchy you aggregate whatever came

17:11.680 --> 17:16.880
out of those smaller windows. So many inductive biases similar to the convolutional architectures

17:17.520 --> 17:22.880
and because this attention, which is the most expensive operation, memory-wise is only computed

17:22.880 --> 17:28.480
locally, it scales linearly with the memory usage scales linearly with the size of the image.

17:29.360 --> 17:36.560
So, young you now figured this is a good fit for compression as well since we often use very large

17:36.560 --> 17:44.160
image and video and what they did was take a hyper prior architecture, which is one of the seminal

17:44.160 --> 17:51.360
image compression architectures. I think originally proposed by Google in 2018 and replace some of

17:51.360 --> 17:57.760
the operators, especially the convolution and transpose convolution by their swing transformer

17:57.760 --> 18:05.120
equivalent in order to see by making a relatively simple change, can we show that these swing

18:05.120 --> 18:09.280
transformers are better suited for extracting information than convolutions are.

18:10.480 --> 18:15.520
Was it relatively plug-and-play or were there some tricks that had to be involved to get

18:15.520 --> 18:20.800
it all to work? I mean, there always are. There always are. Some tuning required.

18:22.320 --> 18:28.720
But what's nice is that the swing codec work was open source and then this is work by

18:28.720 --> 18:36.000
Microsoft Research Asia and so starting off actually was relatively simple. But yeah, it's largely

18:36.000 --> 18:43.520
take the hyper prior architecture and modify it, tune this well and it turns out you get a similar

18:43.520 --> 18:48.640
compute architecture that is much more efficient from a rate distortion point of view. So much

18:48.640 --> 18:54.640
better performance in your head. One of the key elements of the transformer-based

18:54.640 --> 19:01.840
transform coding paper is this idea of visualizing the effective receptive fields.

19:01.840 --> 19:06.480
Can you talk a little bit about what that means and where it comes into play?

19:06.480 --> 19:12.400
So of course, knowing that you can obtain better performance using these transformer blocks is

19:12.400 --> 19:15.680
not enough. We also want to know why this is so that you could potentially make use of it.

19:16.640 --> 19:21.280
So what Young and Jeno did was visualize this effective receptive field and how you could

19:21.280 --> 19:27.440
kind of view this is asking your network for a certain feature what inputs do I need to change

19:27.440 --> 19:32.480
and in what way in order to maximize or minimize this feature. So you can just use the property

19:32.480 --> 19:37.040
that is network is fully differentiable and compute a gradient with respect to the input image.

19:37.760 --> 19:44.080
And so what they found was that for this convolutional models, if you look at their use across

19:44.080 --> 19:49.360
different compression tasks, the effective receptive field size was largely the same. So it was

19:49.360 --> 19:56.720
something like 30 by 30 pixels, influence a local feature. But if they look at the results,

19:56.720 --> 20:01.520
the same results for the swing transformer-based models, they noticed that in contrast

20:02.240 --> 20:07.440
for tasks where the model only has to look at a single image, the effective receptive field

20:07.440 --> 20:12.240
was very small, indicating that you only need a bit of local information in order to compress

20:12.240 --> 20:18.080
efficiently. But when they applied it to a task where two images needed to be compared,

20:18.080 --> 20:23.840
so a motion estimation task for this motion compensation that we talked about for video,

20:24.960 --> 20:29.600
they noticed that the effective receptive field was much larger. And this makes sense from an

20:29.600 --> 20:34.880
intuitive point of view. Because if you want to determine how across two frames a certain motion

20:34.880 --> 20:39.120
appears, you would want to look at a lot of context. You wouldn't want to look just at a very tiny

20:39.120 --> 20:47.600
area. So this kind of hints that these swing models are able to better determine how much

20:47.600 --> 20:52.000
information they should take into account from a local area in order to make their decisions.

20:52.000 --> 20:56.960
When I hear the description of the effective receptive field, it makes me think a little bit about

20:58.800 --> 21:03.360
ideas like lime where you're kind of creating some perturbations and input and you're trying to

21:03.360 --> 21:11.440
see how they flow throughout a network. Is it a similar idea? It sounds sort of similar. Yeah,

21:11.440 --> 21:17.600
I think one major application, but that really is a blast from the past, is this deep-dream style

21:19.120 --> 21:24.160
image generation where you're kind of asking your network, okay, how should I change

21:24.160 --> 21:28.080
this input in order to make it resemble a dog or a cat or some concept.

21:29.360 --> 21:32.880
But of course, similar techniques have been used in many different settings.

21:33.760 --> 21:39.680
There was also some work in this paper that looked at the kind of characterizing the latent space.

21:39.680 --> 21:45.280
Can you talk a little bit about what that showed? So, young and young, I'll perform a couple of

21:45.280 --> 21:51.440
analyses and some of which were aimed at seeing how these swings, compression models,

21:51.440 --> 21:56.720
utilize the latent space and whether they utilize it in a better way than the convolutional counterparts.

21:57.600 --> 22:04.560
And one of the ways in which you can show this is by looking at how many bits are being transmitted

22:04.560 --> 22:10.000
through every latent channel. So, the latent space has a certain number of channels and it also

22:10.000 --> 22:14.960
has some spatial dimensions. And for each channel, you can count how many bits are going to be

22:14.960 --> 22:20.640
spent in order to transmit the information in this channel. And when you order these channels by

22:20.640 --> 22:26.880
the number of bits, you can kind of construct a progressive decoding scheme. You know, you only

22:26.880 --> 22:31.120
transmit the first channel, you get some of the key information, maybe you get the shape tried,

22:31.120 --> 22:37.120
and you transmit the second channel, you get some of the details right, and so on. And it turned

22:37.120 --> 22:44.400
out that these swing models were able to distribute the information more evenly than their

22:44.400 --> 22:51.600
convolutional counterparts. So, for progressive decoding style schemes, this is potentially interesting

22:51.600 --> 22:56.640
because it means that you can opt to, for example, just transmit the first half your guaranteed

22:56.640 --> 23:03.040
to get some information across. But it's also interesting in case of imperfect channels. So,

23:03.040 --> 23:07.920
if some information is lost, let's say, could you resample these channels, or could you still

23:07.920 --> 23:13.440
decode something that looks like what you meant to decode? And so, they have some experiments

23:13.440 --> 23:19.840
in the paper as well, where they mask certain parts of the latent channels and show that the

23:19.840 --> 23:24.640
swing transformer is more robust to this than the convolutional compression models.

23:24.640 --> 23:28.960
I'm resulting in cleaner reconstructions, even if some of the information is missing.

23:28.960 --> 23:35.840
It's occurring to me that as you've been using the word channel here, I've been thinking of channels

23:35.840 --> 23:41.600
like in an image sense, like color channels and things like that. But you're not necessarily

23:41.600 --> 23:48.240
using it in that sense here. I guess it's similar if you look at input images as a tensor.

23:48.240 --> 23:53.760
Yeah, sorry. For me, whenever I think of images, I always see four-dimensional tensors,

23:53.760 --> 23:58.320
batch channel height width. And for this latent space, it's actually quite similar. There's

23:58.320 --> 24:02.960
also a batch dimension, a channel dimension. And in this case, it happens to be a large number of

24:02.960 --> 24:08.560
channels because we choose it to be. But from a tensor perspective, it's the same thing.

24:09.280 --> 24:15.280
Okay. But they don't necessarily correlate to a particular physical property, like a color

24:15.280 --> 24:24.240
or something like that. They don't know. What we hope is that these transforms, the learned encoder

24:24.240 --> 24:28.960
and decoder, that they're able to map to some space where all of these redundancies are being

24:28.960 --> 24:34.720
squeezed out. But we have no idea what that space means inherently. We can visualize it. And

24:34.720 --> 24:40.320
there actually are some visualizations in the paper as well, where you can see some correlation

24:40.320 --> 24:45.200
to the original input and some spatial patterns appearing and so on. But there are no physical

24:45.200 --> 24:50.320
properties that you can tie it to now. It sounds like it's not quite as clean as some of the early

24:50.320 --> 24:56.880
work looking at layers of CNNs and seeing the, you know, textures, you know, shapes deep and then

24:56.880 --> 25:02.080
textures and things like that. Yeah, I think for some channels, you can't trace it back to some

25:02.080 --> 25:08.160
properties. But there's no guarantee. And what we've seen is that it sometimes is highly dependent

25:08.160 --> 25:12.480
on the input image as well. So with you pass an image through it and you see some response and

25:12.480 --> 25:17.440
you think, hey, great, I found one that corresponds to grass. And then the next image goes through it.

25:17.440 --> 25:23.840
It also responds and it's not a grass image. So sadly, we can't really tie it to any one property now.

25:24.880 --> 25:33.120
To what extent does this work then kind of fit into some of the other things that you're doing?

25:33.120 --> 25:42.400
Like we alluded to some of the work on kind of intra frame for video and there's some other stuff

25:42.400 --> 25:47.360
that you've done that we haven't talked about. P frame and B frame codex and I guess the

25:47.360 --> 25:54.560
transformer base to a new foundation piece that you then can kind of plug in all the other

25:54.560 --> 25:59.760
things that you're doing for video compression around. Yeah, I would say so. I would say so.

25:59.760 --> 26:04.560
I think our team is working on on many directions simultaneously and that hopefully are complementary.

26:05.120 --> 26:10.800
So like you mentioned, we have some work on on different schemes for neural codex.

26:10.800 --> 26:17.200
And this work really is aimed at replacing some foundational building block for any codec.

26:17.200 --> 26:21.360
So what John and Jena did was not apply this just to image compression, but also did some

26:21.360 --> 26:28.640
feasibility studies that showed that it helped promise in in the video setting. And in this case,

26:28.640 --> 26:35.200
it was only aimed at how do you call it? Not streaming, but a real-time video.

26:35.760 --> 26:39.920
So in low the lay setting, it's referred to in the compression setting.

26:41.840 --> 26:47.040
But there's no reason that this could not scale to the streaming use case as well.

26:47.040 --> 26:54.160
And what are some of the future directions that you're looking at in terms of building on top of

26:54.160 --> 26:58.880
this? So I think like you mentioned scaling it to some of these other codex that that we've

26:58.880 --> 27:03.520
been building. The codex that are well suited for the streaming case, for example.

27:05.040 --> 27:09.920
Seeing whether we can reduce computational complexity further. And as you may know,

27:09.920 --> 27:15.040
we've been demoing some of our earlier codex throughout the past year. And of course,

27:15.040 --> 27:19.200
it's interesting to consider whether we can actually make a feasible prototype out of this.

27:19.200 --> 27:24.480
Something that would be runnable on the vice. On that note, what kind of results did you see

27:25.680 --> 27:35.040
from a computational perspective with this type of coding? The convolutional ones are about

27:35.040 --> 27:41.600
equally expensive in terms of max. So multiply and accumulate operations. So it depends on how

27:41.600 --> 27:47.280
you measure compute. From that point of view, swing transformer models are somewhat counterintuitively

27:47.280 --> 27:56.160
more rate distortion per Mac efficient. So you get much better performance for the same amount of

27:56.160 --> 28:03.840
Macs. But there's a small caveat. The attention operation is memory hungry. Because of the

28:03.840 --> 28:08.560
soft Mac operation, even these swing transformers, which only compute the attention in this local

28:08.560 --> 28:14.320
window, there's still a big hit to the total memory. And this is noticeable. And young and

28:14.320 --> 28:20.800
you know, also included some comparisons on things like peak memory usage, which showed that the

28:20.800 --> 28:27.600
convolutional model is still the least expensive one there. So that is one of the papers that you and

28:27.600 --> 28:35.120
your colleagues are presenting at ICOR. There are a couple of others that I'd love to hear a

28:35.120 --> 28:40.240
little bit about. One of those is the confess paper. Can you share a little bit about that paper?

28:40.240 --> 28:49.200
So they're using contrastive learning in a cross domain future setting. So what this means is,

28:49.200 --> 28:54.960
for example, you were to train a model on ImageNet and you want to use this in a setting where x

28:54.960 --> 29:01.600
rays are being used. So it really is a big jump from domain to domain. And the method data

29:01.600 --> 29:07.360
files sort of combines three steps in order to facilitate this domain shift and enable it.

29:07.360 --> 29:12.880
They do use a self-supervised pre-training in order to create a certain set of features.

29:13.680 --> 29:20.880
Then they use a feature selection scheme where they sort of train a mask that is fit for a particular

29:20.880 --> 29:27.760
target domain. And they perform fine tuning at the end. And it turns out that this three step

29:27.760 --> 29:32.320
procedure of which the self-supervised pre-training is probably the most important one.

29:32.320 --> 29:39.520
You can easily transfer from domain to domain without overfitting on the very few test samples

29:39.520 --> 29:44.160
that you may have from the new target domain. So this is especially important when you think about

29:44.160 --> 29:50.320
use cases like personalization. Let's say you've trained a big model on a huge data set for

29:50.320 --> 29:54.080
many different users. And now you want to apply this to just your use case.

29:55.360 --> 30:00.800
Then this fuchsia learning becomes all the more important, likely because your data is expensive to

30:00.800 --> 30:06.480
obtain. And how is the self-supervised, how is self-supervision built into this?

30:06.480 --> 30:12.320
So the first step of this process is mainly about learning a good representation.

30:13.120 --> 30:21.760
And so contrastive methods have generally been used in vision in order to build a certain set of

30:21.760 --> 30:30.640
features that could be used for many different tasks. So for many semi-supervised settings,

30:30.640 --> 30:36.640
for example, it turns out that self-supervised pre-training on large unlabeled data sets

30:36.640 --> 30:41.920
are a great way to kickstart the actually quite difficult semi-supervised process.

30:42.800 --> 30:48.800
So it's kind of a way to use large unlabeled data sets through self-supervision

30:49.440 --> 30:53.280
so that you don't have to go through the motion of obtaining this expensive labeled data set

30:53.280 --> 31:00.320
beforehand. Another paper we wanted to talk a little bit about is the steerable CNN's paper.

31:00.320 --> 31:02.640
What does that mean? What's a steerable CNN?

31:02.640 --> 31:08.560
I would say the best way to introduce it is to talk about these group-equivariant nets that

31:08.560 --> 31:12.960
you've probably talked about with Taka Ko and some time ago. Normal convolutional models,

31:12.960 --> 31:19.040
they're equivalent to translations. So that means you shift the input, the output of the operator

31:19.040 --> 31:26.800
shifts with it. And what group-equivariant nets are generally aiming to do is be

31:26.800 --> 31:32.320
equivariant to different symmetry groups as well. Not just translation, but for example,

31:32.320 --> 31:39.760
also reflection, flipping. I mean, you've talked to Taka about this. So steerable CNN's are one

31:39.760 --> 31:49.040
of the most general ways to accomplish this. And what my colleague Gabriela has done in this work

31:49.600 --> 31:55.600
is a theoretical analysis of the space of these steerable filters and kind of

31:55.600 --> 32:02.320
come up with the permiss duration for the space. And what this will do is, if you're interested

32:02.320 --> 32:08.960
in building a network that's equivariant to some certain symmetry group, let's say one that's

32:10.000 --> 32:16.080
solely aimed at reflections and 90 degree rotations. And what you would have to do before

32:16.080 --> 32:21.360
Gabriela's work is kind of work out an architecture and a method that could allow you to

32:21.360 --> 32:28.240
to satisfy the constraints of that symmetry group. But using your work, you sort of have a general

32:29.040 --> 32:36.000
almost automatic procedure for for deriving the steerable filters. And what's nice about it

32:36.000 --> 32:41.040
is that the code is open source. So anyone could use this and kind of kickstart

32:42.320 --> 32:46.080
building steerable CNN for their symmetry group of interest.

32:46.080 --> 32:52.640
Got it. So you have a particular type of symmetry that is expressing your problem that you want

32:52.640 --> 33:01.760
to exploit. And previously you'd have to kind of handcraft a method for taking advantage of that.

33:01.760 --> 33:11.040
And this is, it doesn't sound like it's a general mechanism, but rather a procedure that you would

33:11.040 --> 33:16.320
follow that leads to a mechanism that works for your specific use cases. Is that the right way to

33:16.320 --> 33:21.520
think about it? Yeah, I think so. I think so. Yeah, like you mentioned, most of the works on steerable

33:21.520 --> 33:30.160
CNNs so far have attacked a specific group. So for example, working on on globes, when you think

33:30.160 --> 33:35.760
about things like weather data, of course, they're not actually 2D. They operate on around the

33:35.760 --> 33:40.560
globe or on the earth. And so it's desirable to be accurate variant across that sort of space as

33:40.560 --> 33:48.400
well. And, and so what's previously been done is you kind of handcraft for this specific use case

33:49.600 --> 33:54.240
a set of steerable filters. Awesome. Well, I'd love to have you share a little bit about what you're

33:54.240 --> 34:01.200
most excited about looking forward in, in the field that that you're focused on the neuro compression

34:01.200 --> 34:08.720
and neuro video compression. What's exciting that you see down the pike? I think one excite or

34:08.720 --> 34:14.320
direction that I'm particularly excited about is the whole perceptual quality direction. I really

34:14.320 --> 34:21.280
believe that this is where neural codex shine, not just gang-based codex, but really any codec that's

34:21.280 --> 34:28.640
aimed at improving perceptual quality. But for example, with the the advent of diffusion models,

34:28.640 --> 34:32.800
a different type of diffusion probabilistic models, a different type of generative model. Again,

34:33.440 --> 34:38.320
very good at generating high perceptual quality details, but not exactly straightforward for

34:38.320 --> 34:44.000
using in the compression setting. We know that neural networks are able to generate these sort of

34:44.000 --> 34:50.160
plausible details and trying to make use of that and trying to exploit that is something that

34:50.160 --> 34:56.080
I'm particularly excited by. And at the same time, I'm also excited about bringing down the compute

34:56.080 --> 35:02.720
and actually making practical codex, things that you or I could run on a mobile phone and watch videos

35:02.720 --> 35:09.760
with, because that really moves it from the academic setting to the tangible setting.

35:11.920 --> 35:18.560
When you talk about the increasing the perceptual quality and incorporating different pieces

35:18.560 --> 35:26.720
like GANs, is it more often that you're taking elements of these different types of models and

35:26.720 --> 35:39.760
building them into one end-to-end train thing? Or are there other cases where you've got

35:39.760 --> 35:44.800
higher level components that you're bringing together into more of a system type of an approach

35:44.800 --> 35:51.440
to solving the problem? There sure are hybrid approaches. I think the most elegant way is

35:51.440 --> 35:56.960
always this end-to-end approach where you have an encoder that's a network, you have a prior model

35:56.960 --> 36:04.240
that's also a network and a decoder is a network and there are some, I think, really well-made work

36:04.240 --> 36:11.520
from the Google Perception Group, for example, on an end-to-end image codec, which I believe they call

36:11.520 --> 36:16.960
high-fidelity, generative image compression. In this case, the decoder is a conditional GAN

36:16.960 --> 36:22.480
mapping from this latent space to image space, but the encoder is a network and the

36:22.480 --> 36:28.320
priors and network as well. You can train this entire thing end-to-end by mixing a few lost functions.

36:28.320 --> 36:33.360
I think it's quite an elegant way, but it's not the only way and there definitely is something to

36:33.360 --> 36:39.760
be gained from also looking at domain expertise in particular when you think about bringing down

36:39.760 --> 36:46.640
complexity and making sure that you don't need a huge neural network in order to kind of rebuild

36:46.640 --> 36:52.480
or learn what other people have already figured out for you. Right, right, and does that? What are the,

36:52.480 --> 37:03.600
are there clear lines from, kind of, performance and efficiency perspective? In other words,

37:03.600 --> 37:12.720
like is end-to-end always more computationally intense, more efficient, or does it depend on the

37:12.720 --> 37:21.520
specific set of architecture and implementation? It's hard to predict where this will go. I mean,

37:21.520 --> 37:26.880
at the moment, most of these end-to-end architectures, they are computationally fairly expensive,

37:26.880 --> 37:33.200
because they're kind of replacing many components that make use of inductive biases,

37:33.200 --> 37:38.880
and those inductive biases, especially when you think about standard codex, they're often

37:38.880 --> 37:44.160
also aimed at bringing down the complexity, not just at obtaining the best rate distortion performance.

37:44.880 --> 37:54.240
So some innovations in more traditional methods, they may have been chosen over others,

37:54.240 --> 37:58.080
not necessarily because they are bringing out the very best rate distortion performance,

37:58.080 --> 38:03.200
but because they have good rate distortion performance, but they're also efficiently implemented

38:03.200 --> 38:10.240
on hardware. So with neural codex, especially end-to-end codex, given that it's a fairly recent field,

38:11.040 --> 38:16.320
a lot of the focus has been on making models that scale well and making use of the compute we have,

38:16.880 --> 38:20.640
and recently there's been a lot more attention towards making practical codex as well.

38:21.280 --> 38:26.320
So for example, with some of our demos in the last year, and recently, David Minnan,

38:26.320 --> 38:33.760
from the same Google perception team gave a keynote at ISIP, in which he also had a call to action.

38:33.760 --> 38:39.120
Look at models that are computationally less expensive, but still obtain good rate distortion

38:39.120 --> 38:44.480
performance. So I hope that answers your question. I think now end-to-end is still computationally

38:44.480 --> 38:51.280
fairly expensive, but we've shown that it's doable. You can do on-device decoding, but there is

38:51.280 --> 38:57.840
a change in the works, I would say. Awesome. Well, OK, it was wonderful chatting with you and

38:57.840 --> 39:05.360
learning a bit about all the things that you and your team are working on, and best of luck with

39:05.360 --> 39:22.080
your presentations at the conference. Thanks, and thanks a lot for having me.

