1
00:00:00,000 --> 00:00:13,640
All right, everyone. I am here with Stefano Soato.

2
00:00:13,640 --> 00:00:18,760
Stefano is VP of AI Application Science at AWS,

3
00:00:18,760 --> 00:00:22,440
as well as a professor of computer science at UCLA.

4
00:00:22,440 --> 00:00:25,360
Stefano, welcome to the Twomla AI podcast.

5
00:00:25,360 --> 00:00:27,360
Thanks for having me.

6
00:00:27,360 --> 00:00:29,760
I'm really looking forward to digging into this conversation.

7
00:00:29,760 --> 00:00:34,480
We'll be talking about some of your recent work on an area

8
00:00:34,480 --> 00:00:38,400
you called Graceful AI, which is very interesting.

9
00:00:38,400 --> 00:00:41,120
But before we do that, I'd love to have you share a little bit

10
00:00:41,120 --> 00:00:45,640
about your background and how you came to work in machine learning.

11
00:00:45,640 --> 00:00:48,680
Yeah, I appreciate that my background is a bit unusual.

12
00:00:48,680 --> 00:00:51,320
I grew up in Italy as you can tell from my accent.

13
00:00:51,320 --> 00:00:53,280
And I grew up studying class, studying classics,

14
00:00:53,280 --> 00:00:56,880
like history, philosophy, Latin, Greek, and so on and so forth.

15
00:00:56,880 --> 00:01:02,840
And before going to college, I entered a context

16
00:01:02,840 --> 00:01:04,880
for a summer course organized by a tiny school

17
00:01:04,880 --> 00:01:07,920
that has a class of letters and a class of science.

18
00:01:07,920 --> 00:01:10,960
And I was there to study philosophy,

19
00:01:10,960 --> 00:01:13,640
but I realized that there were problems that people were writing

20
00:01:13,640 --> 00:01:17,080
on a board that I had no clue how to solve.

21
00:01:17,080 --> 00:01:18,120
And these were math problems.

22
00:01:18,120 --> 00:01:19,120
And it really, really bugged me.

23
00:01:19,120 --> 00:01:21,920
And so I got more and more interested in that side

24
00:01:21,920 --> 00:01:24,200
and I started attending those seminars.

25
00:01:24,200 --> 00:01:26,880
And then I rolled in engineering and in 1989, I ran across

26
00:01:26,880 --> 00:01:29,520
the work of Ernst Dickman's.

27
00:01:29,520 --> 00:01:32,760
Ernst Dickman's was a pioneer of autonomous driving

28
00:01:32,760 --> 00:01:34,920
back in 1989 in Germany.

29
00:01:34,920 --> 00:01:38,840
He was the one that had autonomous vehicle going on the Autobahn

30
00:01:38,840 --> 00:01:41,960
at speeds up to 180 kilometers per hour.

31
00:01:41,960 --> 00:01:44,680
And when I joined the Caltech for my PhD,

32
00:01:44,680 --> 00:01:48,040
I was in the Department of Control and the Renial Systems.

33
00:01:48,040 --> 00:01:50,640
And that's where I got exposed to computer vision

34
00:01:50,640 --> 00:01:54,000
because the reason we don't have domestic robot helpers

35
00:01:54,000 --> 00:01:55,600
is not because we don't know how to control them,

36
00:01:55,600 --> 00:01:57,720
because we don't know how to endow them

37
00:01:57,720 --> 00:01:59,480
without a presentation of the surrounding world.

38
00:01:59,480 --> 00:02:02,200
And so I got more and more interested in that problem.

39
00:02:02,200 --> 00:02:04,160
And that was a quarter of a century ago

40
00:02:04,160 --> 00:02:06,880
and I'm still very excited and interesting about that area.

41
00:02:06,880 --> 00:02:08,880
And so that's how I got into AI.

42
00:02:08,880 --> 00:02:11,120
Nice.

43
00:02:11,120 --> 00:02:14,400
We could spend a whole different interview

44
00:02:14,400 --> 00:02:15,400
talking about that.

45
00:02:15,400 --> 00:02:17,880
I, one of my perennial favorite topics

46
00:02:17,880 --> 00:02:21,440
is the relationship between kind of control

47
00:02:21,440 --> 00:02:25,120
and machine learning and end-to-end systems

48
00:02:25,120 --> 00:02:29,720
versus more modular systems that use what we've learned

49
00:02:29,720 --> 00:02:32,200
about physics and control and other things.

50
00:02:32,200 --> 00:02:34,600
But I think that's going to be a different conversation.

51
00:02:34,600 --> 00:02:39,680
Tell us a little bit about both your role at AWS

52
00:02:39,680 --> 00:02:44,800
as well as your research interests across AWS and UCLA.

53
00:02:44,800 --> 00:02:45,640
Yeah.

54
00:02:45,640 --> 00:02:48,680
Yeah, so at AWS, I am in charge of the science

55
00:02:48,680 --> 00:02:50,480
for AI applications.

56
00:02:50,480 --> 00:02:54,080
Any applications are grouped by modalities.

57
00:02:54,080 --> 00:02:58,600
So we have vision, so images and video.

58
00:02:58,600 --> 00:03:01,000
And then we have speech, and then we have text,

59
00:03:01,000 --> 00:03:02,480
and then we have vertical.

60
00:03:02,480 --> 00:03:05,240
And verticals are domain-specific applications

61
00:03:05,240 --> 00:03:07,680
in the industrial space as well as operations

62
00:03:07,680 --> 00:03:10,920
and time series for casting and so on support.

63
00:03:10,920 --> 00:03:14,960
And we are organized as research teams,

64
00:03:14,960 --> 00:03:17,800
but we are very closely coupled with engineering teams,

65
00:03:17,800 --> 00:03:21,480
product teams and data teams.

66
00:03:21,480 --> 00:03:25,400
And we practice what we call customer-obsessed science,

67
00:03:25,400 --> 00:03:27,840
which is a different curiosity-driven science,

68
00:03:27,840 --> 00:03:30,120
which is what we do at the university.

69
00:03:30,120 --> 00:03:33,960
And so our work ends up in the hands of customers

70
00:03:33,960 --> 00:03:37,560
in a time frame that is quite fast

71
00:03:37,560 --> 00:03:40,360
for somebody who's used to academic time clock

72
00:03:40,360 --> 00:03:43,560
where you expect your impact in the world to be posthumous

73
00:03:43,560 --> 00:03:45,240
or if you're lucky during your lifetime.

74
00:03:45,240 --> 00:03:47,840
But here, there's a very quick turnaround between ideas

75
00:03:47,840 --> 00:03:50,720
being generated, services being deployed,

76
00:03:50,720 --> 00:03:54,240
and customers using them and getting benefit from it.

77
00:03:54,240 --> 00:03:56,720
So my interest is broadly speaking in AI.

78
00:03:56,720 --> 00:03:58,920
I've always been interested in autonomous system systems

79
00:03:58,920 --> 00:04:01,960
that interact intelligently with the environment,

80
00:04:01,960 --> 00:04:05,080
where intelligent is to be defined, everybody has their own.

81
00:04:05,080 --> 00:04:07,640
But to me, a dog is plenty intelligent.

82
00:04:07,640 --> 00:04:13,120
I wish we could build something that behaves like a dog.

83
00:04:13,120 --> 00:04:17,320
So that's much more difficult than doing a chess-playing program

84
00:04:17,320 --> 00:04:19,080
in a fight for survival.

85
00:04:19,080 --> 00:04:22,080
The dog will win over the chess-playing program any time.

86
00:04:22,080 --> 00:04:25,240
So you just need to unplug the power.

87
00:04:25,240 --> 00:04:30,720
And definitely, there are opportunities that are...

88
00:04:30,720 --> 00:04:34,720
The time is right for research that we've been doing

89
00:04:34,720 --> 00:04:37,360
for 10, 20 years to become useful now.

90
00:04:37,360 --> 00:04:40,400
And for me, the game changer was in 2009,

91
00:04:40,400 --> 00:04:45,280
I read a report by Cisco called Visual Network in Index,

92
00:04:45,280 --> 00:04:51,080
that pointed out that back then, 2009,

93
00:04:51,080 --> 00:04:54,120
peer-to-peer traffic was surpassed by video traffic

94
00:04:54,120 --> 00:04:55,360
on the internet.

95
00:04:55,360 --> 00:04:57,120
And they were forecasting that by 2022,

96
00:04:57,120 --> 00:04:59,240
it would be 90% of wireless traffic

97
00:04:59,240 --> 00:05:01,160
and almost the totality of internet traffic.

98
00:05:01,160 --> 00:05:02,400
And so to me, that was a game changer.

99
00:05:02,400 --> 00:05:07,280
But I thought, now we have data, we store data.

100
00:05:07,280 --> 00:05:09,880
But we don't know how to extract intelligence from data.

101
00:05:09,880 --> 00:05:11,640
And that will take more than my lifetime.

102
00:05:11,640 --> 00:05:15,040
So I was not thinking that it would be so quick.

103
00:05:15,040 --> 00:05:18,280
And then in 2014, I was involved in a project

104
00:05:18,280 --> 00:05:21,160
where there were training systems

105
00:05:21,160 --> 00:05:25,880
to detect anomalies in CT scans in medical imaging.

106
00:05:25,880 --> 00:05:28,440
And then I realized that on a task,

107
00:05:28,440 --> 00:05:30,800
where humans are not naturally evolved,

108
00:05:30,800 --> 00:05:33,160
which is to interpret not natural, but medical images.

109
00:05:33,160 --> 00:05:38,680
So where you have to train these physicians for many years,

110
00:05:38,680 --> 00:05:40,040
you could train a deep learning model

111
00:05:40,040 --> 00:05:44,640
and beat even the most experienced radiologists.

112
00:05:44,640 --> 00:05:46,360
And so that's where I realized, oh my God,

113
00:05:46,360 --> 00:05:48,200
this is happening in my lifetime.

114
00:05:48,200 --> 00:05:50,880
And so what is the right place to be part of this

115
00:05:50,880 --> 00:05:51,880
if you want to be involved?

116
00:05:51,880 --> 00:05:54,400
Well, you need to be in a place that has exposure

117
00:05:54,400 --> 00:05:58,360
to a variety of problem domains and has patience

118
00:05:58,360 --> 00:06:01,400
and has data resources to make it happen.

119
00:06:01,400 --> 00:06:03,160
And slowly, one little step at a time,

120
00:06:03,160 --> 00:06:04,720
but we've been able to make a difference

121
00:06:04,720 --> 00:06:06,720
in several of our products that our customers have been

122
00:06:06,720 --> 00:06:08,600
benefiting from.

123
00:06:08,600 --> 00:06:11,520
Yeah, I like the characterization of areas

124
00:06:11,520 --> 00:06:16,440
that we're evolved to be good at versus those that we aren't.

125
00:06:16,440 --> 00:06:19,080
You know, as a classifier for problems

126
00:06:19,080 --> 00:06:21,840
that we should seek to apply a machine learning

127
00:06:21,840 --> 00:06:23,720
for, it kind of reminds me of Andruing's

128
00:06:23,720 --> 00:06:26,360
anything that takes us more than a second.

129
00:06:26,360 --> 00:06:30,160
But it's got a maybe a more philosophical spin to it.

130
00:06:30,160 --> 00:06:31,640
Well, both the philosophical is very easy

131
00:06:31,640 --> 00:06:34,160
to underestimate how difficult some tasks are.

132
00:06:34,160 --> 00:06:37,040
One anecdote I have is that with my students

133
00:06:37,040 --> 00:06:40,280
are participated to the first two data program challenges.

134
00:06:40,280 --> 00:06:42,400
And it was very interesting to be in the audience

135
00:06:42,400 --> 00:06:44,520
and watching people who are not experts

136
00:06:44,520 --> 00:06:45,880
see this card right by.

137
00:06:45,880 --> 00:06:47,720
And these cards really look like they didn't know

138
00:06:47,720 --> 00:06:48,400
what they were doing.

139
00:06:48,400 --> 00:06:50,640
And so, you know, how can you possibly not

140
00:06:50,640 --> 00:06:52,640
see that bush and drive over it

141
00:06:52,640 --> 00:06:54,680
when anybody can see that bush, it's obvious.

142
00:06:54,680 --> 00:06:56,720
People forget that this is roughly

143
00:06:56,720 --> 00:06:59,320
the size of your brain, the size of your two hands,

144
00:06:59,320 --> 00:07:02,000
and roughly half of it processes visual information.

145
00:07:02,000 --> 00:07:04,760
So most of the real estate in your brain, even when

146
00:07:04,760 --> 00:07:07,560
you're absorbed in the most, you know, in the most

147
00:07:07,560 --> 00:07:10,120
abstracts, true thinking, most of your brain is busy trying

148
00:07:10,120 --> 00:07:12,680
to make sense of the sensory data that comes through.

149
00:07:12,680 --> 00:07:14,200
So it's really a cool and task.

150
00:07:14,200 --> 00:07:17,160
And on the contrary, some things that

151
00:07:17,160 --> 00:07:19,080
look very, very difficult to humans

152
00:07:19,080 --> 00:07:20,440
are trivial to computers.

153
00:07:20,440 --> 00:07:23,400
And so we've been able to make inroads in both.

154
00:07:23,400 --> 00:07:25,400
Some traditionally hard problems,

155
00:07:25,400 --> 00:07:27,840
like learning one from few shots.

156
00:07:27,840 --> 00:07:30,720
We have made progress, thanks to also the evolution

157
00:07:30,720 --> 00:07:32,280
of deep learning of the past five years,

158
00:07:32,280 --> 00:07:34,160
as well as the evolution of hardware, optimization

159
00:07:34,160 --> 00:07:35,800
methods, and so on.

160
00:07:35,800 --> 00:07:38,040
But also, we've been able to better understand

161
00:07:38,040 --> 00:07:40,320
the problem, you know, in a way which

162
00:07:40,320 --> 00:07:43,720
is independent of whether it's implemented

163
00:07:43,720 --> 00:07:47,320
with biological hardware or silicon hardware.

164
00:07:47,320 --> 00:07:48,880
There are some characteristics of learning

165
00:07:48,880 --> 00:07:51,240
problems that are absolutely fascinating

166
00:07:51,240 --> 00:07:53,440
that we've just started to poke into.

167
00:07:53,440 --> 00:07:56,040
And they are from the perspective of somebody

168
00:07:56,040 --> 00:07:58,240
with an academic inclination is very fascinating

169
00:07:58,240 --> 00:07:59,920
to expect to think about.

170
00:07:59,920 --> 00:08:01,040
Yeah, yeah.

171
00:08:01,040 --> 00:08:02,880
And we want to dig into one of those.

172
00:08:02,880 --> 00:08:05,040
And that is the work that you and your teams have been

173
00:08:05,040 --> 00:08:09,520
knowing around this idea of graceful AI.

174
00:08:09,520 --> 00:08:12,280
You know, what I thought was most compelling about that

175
00:08:12,280 --> 00:08:15,600
is I think we've come to appreciate that when we're

176
00:08:15,600 --> 00:08:19,200
using machine learning models and production,

177
00:08:19,200 --> 00:08:22,160
we need to constantly evolve them, constantly train them

178
00:08:22,160 --> 00:08:27,360
to counteract the effects of data drift,

179
00:08:27,360 --> 00:08:30,160
or drift in the distribution of our data.

180
00:08:30,160 --> 00:08:33,480
And your paper in this work, in general,

181
00:08:33,480 --> 00:08:36,520
kind of ask the question or prompts the question

182
00:08:36,520 --> 00:08:39,400
that, you know, are there some negative effects associated

183
00:08:39,400 --> 00:08:42,760
with constantly retraining those models, you know,

184
00:08:42,760 --> 00:08:46,080
what are they, and how do we deal with them?

185
00:08:46,080 --> 00:08:47,800
I'd like to have you tell us a little bit

186
00:08:47,800 --> 00:08:51,960
about the broader motivation that led to this body of work.

187
00:08:51,960 --> 00:08:54,080
Yeah, that's a, it's actually interesting story

188
00:08:54,080 --> 00:08:55,600
because it's certain dipitres, right?

189
00:08:55,600 --> 00:09:01,080
So, you know, one of the luxuries of being at AWS

190
00:09:01,080 --> 00:09:04,520
is that you get exposed to a media of problems

191
00:09:04,520 --> 00:09:06,200
that you don't even exist.

192
00:09:06,200 --> 00:09:08,000
And you would not dream of by just sitting

193
00:09:08,000 --> 00:09:09,880
in your office at the university.

194
00:09:09,880 --> 00:09:13,320
And so this was a case where, you know,

195
00:09:13,320 --> 00:09:17,360
the natural mode of evolution of knowledge is continuous,

196
00:09:17,360 --> 00:09:19,120
like you mentioned, so we don't have this

197
00:09:19,120 --> 00:09:21,080
arbitraris separation between training

198
00:09:21,080 --> 00:09:24,680
and inference phases, which we do have in machine learning.

199
00:09:24,680 --> 00:09:26,160
But in machine learning, we have this phase

200
00:09:26,160 --> 00:09:28,120
where we train something, we make,

201
00:09:28,120 --> 00:09:29,960
we build a model, we offer to customer,

202
00:09:29,960 --> 00:09:33,480
people use it at some point, either a better model comes along

203
00:09:33,480 --> 00:09:35,720
and certainly if you follow the academic literature,

204
00:09:35,720 --> 00:09:38,680
every single conference, there's an incremental improvement

205
00:09:38,680 --> 00:09:40,360
and you would like to harvest all these improvements

206
00:09:40,360 --> 00:09:42,520
so that can benefit customers downstream.

207
00:09:43,480 --> 00:09:45,960
But what we're gonna allow is that customers

208
00:09:45,960 --> 00:09:47,680
were not updating their models

209
00:09:47,680 --> 00:09:52,680
and we were not understanding why wouldn't you change the model

210
00:09:52,680 --> 00:09:55,440
when this one performs better.

211
00:09:55,440 --> 00:09:56,720
Now, the definition of performance

212
00:09:56,720 --> 00:09:58,680
is the one that comes from academic benchmark

213
00:09:58,680 --> 00:10:02,200
where we count the average number of errors

214
00:10:02,200 --> 00:10:04,840
and we train to minimize the proxy of that

215
00:10:04,840 --> 00:10:07,560
whether it's empirical cross-entropy or some other loss.

216
00:10:07,560 --> 00:10:09,680
So, but basically the criterion that we,

217
00:10:09,680 --> 00:10:12,400
as academic, had guessed is most relevant

218
00:10:12,400 --> 00:10:14,440
is the average number of errors.

219
00:10:14,440 --> 00:10:15,640
Turns out, what people don't care

220
00:10:15,640 --> 00:10:16,680
for the average number of errors.

221
00:10:16,680 --> 00:10:18,560
They have very specific requirements

222
00:10:18,560 --> 00:10:21,200
for what type of chords they care about

223
00:10:21,200 --> 00:10:25,960
and even specific subsets of subtypes of the data

224
00:10:25,960 --> 00:10:28,400
that particularly care about.

225
00:10:28,400 --> 00:10:33,400
And so, we realized that the proxies that we were training for

226
00:10:34,080 --> 00:10:36,240
were not the right ones for the customers.

227
00:10:36,240 --> 00:10:39,560
They were just the ones that we, by default,

228
00:10:39,560 --> 00:10:40,760
have been using for years.

229
00:10:40,760 --> 00:10:42,480
But the conversation that triggered that was important,

230
00:10:42,480 --> 00:10:45,120
it's an example of the customer obsessed science

231
00:10:45,120 --> 00:10:47,200
because we need to release the model

232
00:10:47,200 --> 00:10:50,080
that was better than everything we've done before.

233
00:10:50,080 --> 00:10:51,480
So, we were very excited.

234
00:10:51,480 --> 00:10:54,960
And then we heard the customers were not happy.

235
00:10:54,960 --> 00:10:57,200
And so, we were wondering why is that?

236
00:10:57,200 --> 00:10:59,880
Well, because there is regression, what is regression?

237
00:10:59,880 --> 00:11:03,000
Regression is when you have a new model

238
00:11:03,000 --> 00:11:04,960
and that model, even though on average,

239
00:11:04,960 --> 00:11:07,080
it performs better than the old model

240
00:11:07,080 --> 00:11:11,280
but it introduces mistakes that the previous model didn't make.

241
00:11:11,280 --> 00:11:12,120
Okay.

242
00:11:12,120 --> 00:11:14,680
And so, even if these mistakes are fewer,

243
00:11:14,680 --> 00:11:16,520
when you notice the old model makes mistake

244
00:11:16,520 --> 00:11:19,640
that the new model makes mistakes the old one did not make,

245
00:11:19,640 --> 00:11:22,600
your puzzle doesn't use it because how is this any better?

246
00:11:22,600 --> 00:11:27,600
And so, and the conversation between a scientist

247
00:11:27,720 --> 00:11:30,480
and a program manager was around the tone, well,

248
00:11:30,480 --> 00:11:33,880
of course, the new model can make a mistake

249
00:11:33,880 --> 00:11:35,600
that the old model didn't make.

250
00:11:35,600 --> 00:11:37,880
Let me explain something about machine learning to you.

251
00:11:38,880 --> 00:11:40,200
And the program manager would say,

252
00:11:40,200 --> 00:11:43,400
no, let me explain something to you about customers

253
00:11:43,400 --> 00:11:44,360
solving their own problem.

254
00:11:44,360 --> 00:11:45,520
And I was listening and I was thinking,

255
00:11:45,520 --> 00:11:47,560
well, maybe we are solving their own problem.

256
00:11:47,560 --> 00:11:51,160
So, this type of interaction with customer escalations

257
00:11:51,160 --> 00:11:52,800
is really a treasure trove for scientists

258
00:11:52,800 --> 00:11:55,160
because every time there is an obstacle,

259
00:11:55,160 --> 00:11:56,480
for an engineer, it's a frustration

260
00:11:56,480 --> 00:11:58,120
because it's a yet another problem to solve

261
00:11:58,120 --> 00:11:59,800
to get to the finish line for the scientist

262
00:11:59,800 --> 00:12:02,520
is an opportunity is because there's something to be understood.

263
00:12:02,520 --> 00:12:04,600
And here, what was there to be understood

264
00:12:04,600 --> 00:12:07,200
is that we are really not pretty much in the right proxy.

265
00:12:07,200 --> 00:12:09,600
And so, that's where we started thinking about,

266
00:12:09,600 --> 00:12:14,440
why should the average error be the right metric?

267
00:12:14,440 --> 00:12:19,840
Because some people care for having sort of even performance

268
00:12:19,840 --> 00:12:21,640
across different courts of the data set

269
00:12:21,640 --> 00:12:23,880
or some people care about having performance

270
00:12:23,880 --> 00:12:27,560
that behaves similarly in models that are deployed at the edge

271
00:12:27,560 --> 00:12:29,480
or at the cloud and some people care

272
00:12:29,480 --> 00:12:32,000
to maintain compatibility with previous models

273
00:12:32,000 --> 00:12:34,760
because they built pre-processing post-processing

274
00:12:34,760 --> 00:12:35,760
around the train model.

275
00:12:35,760 --> 00:12:38,360
And so, if you change the model suddenly you break it,

276
00:12:38,360 --> 00:12:41,640
it is months of work, it's extremely expensive.

277
00:12:41,640 --> 00:12:46,800
And so, we realized this whole universe of problems

278
00:12:46,800 --> 00:12:48,400
that arise when you're trying to train a machine

279
00:12:48,400 --> 00:12:51,520
but learning model not just to perform well

280
00:12:51,520 --> 00:12:53,760
in terms of the average probability of error,

281
00:12:53,760 --> 00:12:56,600
but that performs well relative to the context

282
00:12:56,600 --> 00:12:59,360
of pre-processing algorithm, post-processing algorithm,

283
00:12:59,360 --> 00:13:02,120
pre-existing models, models that are deployed

284
00:13:02,120 --> 00:13:04,120
on different hardware and so on and so forth.

285
00:13:04,120 --> 00:13:06,000
And so, it's fascinating because this is a problem

286
00:13:06,000 --> 00:13:08,120
that I would have never thought about

287
00:13:08,120 --> 00:13:11,080
seeing a university anywhere have not ever occurred to us

288
00:13:11,080 --> 00:13:13,320
if it wasn't for a customer coming back

289
00:13:13,320 --> 00:13:15,280
and say, I don't like this, fix it.

290
00:13:15,280 --> 00:13:19,840
So, that's kind of the essence of customer-obsessed science.

291
00:13:19,840 --> 00:13:25,160
Yeah, there's so many interesting facets to this one

292
00:13:25,160 --> 00:13:28,160
that jumps out at me is just how it's a reminder

293
00:13:28,160 --> 00:13:31,040
of how immature machine learning is.

294
00:13:31,040 --> 00:13:34,040
On the traditional engineering side,

295
00:13:34,040 --> 00:13:39,040
we've got this whole set of methodologies around testing

296
00:13:39,040 --> 00:13:42,160
and one of those types of tests is regression testing.

297
00:13:42,160 --> 00:13:43,480
We know how to do that.

298
00:13:43,480 --> 00:13:46,440
Every engineering team worth its salt runs a series

299
00:13:46,440 --> 00:13:49,160
of regression tests before they release their product

300
00:13:49,160 --> 00:13:53,960
to make sure that the product isn't taking steps backwards,

301
00:13:53,960 --> 00:13:58,000
but this is new in the context of machine learning.

302
00:13:58,000 --> 00:13:58,800
Exactly.

303
00:13:58,800 --> 00:14:00,640
And there's even some aspects of it

304
00:14:00,640 --> 00:14:03,080
which are very peculiar and specific to deep learning

305
00:14:03,080 --> 00:14:07,080
or more in general, over-complete models

306
00:14:07,080 --> 00:14:11,280
for instance, for classification, where if you take a deep learning

307
00:14:11,280 --> 00:14:13,560
model, let's say you're SNed 152 and you train it

308
00:14:13,560 --> 00:14:16,240
on a large data cell, let's say, ImageNet.

309
00:14:16,240 --> 00:14:18,560
If you repeat the experiment a hundred times,

310
00:14:18,560 --> 00:14:21,840
retrain the same exact model on the same exact data

311
00:14:21,840 --> 00:14:24,720
just from different initial conditions,

312
00:14:24,720 --> 00:14:26,920
you compare it to a hundred different models,

313
00:14:26,920 --> 00:14:29,760
but all of them have exactly the same average error,

314
00:14:29,760 --> 00:14:33,880
let's say 87.3, whatever it is.

315
00:14:33,880 --> 00:14:35,200
They all have the same average error.

316
00:14:35,200 --> 00:14:37,360
What do you go and look?

317
00:14:37,360 --> 00:14:39,960
The mistakes they make, they're completely different.

318
00:14:39,960 --> 00:14:42,040
So it's as if they were trading mistake,

319
00:14:42,040 --> 00:14:44,560
I'll get this one right, but give me this one

320
00:14:44,560 --> 00:14:45,400
and I'll get it wrong.

321
00:14:45,400 --> 00:14:48,000
So it was really eye-opening because it's way the second.

322
00:14:48,000 --> 00:14:51,160
So they all make the same number of mistakes,

323
00:14:51,160 --> 00:14:53,160
but they're different mistakes.

324
00:14:53,160 --> 00:14:55,840
And this is what we realize that very often,

325
00:14:55,840 --> 00:14:58,120
some of these criteria, for instance,

326
00:14:58,120 --> 00:15:00,400
equal error rate across different demographics,

327
00:15:00,400 --> 00:15:02,880
as well as compatibility with our models,

328
00:15:02,880 --> 00:15:06,520
are conflicting with average performance.

329
00:15:06,520 --> 00:15:09,320
But here we have a case we have an ISO error rate surface

330
00:15:09,320 --> 00:15:12,480
where all models are equivalent in terms of error rate,

331
00:15:12,480 --> 00:15:14,360
but you can move along the surface

332
00:15:14,360 --> 00:15:16,600
in high dimensional space to make sure

333
00:15:16,600 --> 00:15:18,640
that your model does not make,

334
00:15:18,640 --> 00:15:20,720
or makes as few mistakes as possible,

335
00:15:20,720 --> 00:15:22,200
that were not made at the previous model.

336
00:15:22,200 --> 00:15:25,080
So you can optimize criteria that are orthogonal

337
00:15:25,080 --> 00:15:26,560
and they're not conflicting with each other.

338
00:15:26,560 --> 00:15:28,320
So this was the first time we realized that,

339
00:15:28,320 --> 00:15:30,960
okay, this is yet another performance criteria

340
00:15:30,960 --> 00:15:33,560
that does not impinge on the existing one that we know

341
00:15:33,560 --> 00:15:36,040
and know how to optimize.

342
00:15:36,040 --> 00:15:37,840
So there are many fascinating phenomena

343
00:15:37,840 --> 00:15:40,440
that arise when you start observing the behavior

344
00:15:40,440 --> 00:15:44,760
of these networks beyond just very standard

345
00:15:44,760 --> 00:15:46,920
and what establish metrics.

346
00:15:46,920 --> 00:15:50,360
A lot of work's been going into trying to understand

347
00:15:50,360 --> 00:15:52,640
how deep learning models work

348
00:15:52,640 --> 00:15:54,960
and understand their internals.

349
00:15:54,960 --> 00:16:01,120
Have you or other previous researchers looked at this idea

350
00:16:01,120 --> 00:16:05,280
of maybe the way the error rate or not error rate,

351
00:16:05,280 --> 00:16:10,080
but the types of errors kind of cluster across this ISO error

352
00:16:10,080 --> 00:16:13,000
rate or accuracy frontier?

353
00:16:13,000 --> 00:16:15,440
Yes, quite a bit of work.

354
00:16:15,440 --> 00:16:18,520
So first of all, it kind of be ambly.

355
00:16:18,520 --> 00:16:20,880
It's fascinating how, you know, 20 years ago

356
00:16:20,880 --> 00:16:23,240
we thought that with math and analysis

357
00:16:23,240 --> 00:16:26,440
we would be informing research neuroscience.

358
00:16:26,440 --> 00:16:29,480
And now we find ourselves doing kind of artificial neuroscience

359
00:16:29,480 --> 00:16:31,680
and probing deep network the way neuroscientists

360
00:16:31,680 --> 00:16:34,880
probe neural network, which is kind of interesting

361
00:16:34,880 --> 00:16:36,400
twist of events.

362
00:16:36,400 --> 00:16:38,240
But yes, we've been looking at that at various stages.

363
00:16:38,240 --> 00:16:41,160
So one is with former postdoc of mine,

364
00:16:41,160 --> 00:16:42,400
his name is Hussein Mobahi.

365
00:16:42,400 --> 00:16:45,320
We were looking at universal adversarial perturbations

366
00:16:45,320 --> 00:16:47,680
where we realized that if you take the data

367
00:16:47,680 --> 00:16:51,800
and perturb them in a way that hits the closest decision

368
00:16:51,800 --> 00:16:54,040
boundaries so that with the smallest possible perturbation

369
00:16:54,040 --> 00:16:57,480
you change the class and you fool the network so to speak.

370
00:16:57,480 --> 00:17:00,200
All of these perturbations are aligned,

371
00:17:00,200 --> 00:17:02,560
which is very mysterious.

372
00:17:02,560 --> 00:17:05,360
And that might be in what sense?

373
00:17:05,360 --> 00:17:07,880
Say, again, aligned in what sense?

374
00:17:07,880 --> 00:17:09,680
They're aligned in the sense that their direction

375
00:17:09,680 --> 00:17:12,400
in the high dimensional space of representations

376
00:17:12,400 --> 00:17:14,400
is parallel, they're parallel to each other.

377
00:17:14,400 --> 00:17:16,960
So that you can find a single perturbations

378
00:17:16,960 --> 00:17:19,920
that apply to all the data with high probability changes

379
00:17:19,920 --> 00:17:21,960
in the class at the output.

380
00:17:21,960 --> 00:17:26,120
So that says something about the structure

381
00:17:26,120 --> 00:17:27,320
of the decision boundaries.

382
00:17:27,320 --> 00:17:28,960
There are regions of high curvature.

383
00:17:28,960 --> 00:17:31,840
And so it's very different from what we had in mind

384
00:17:31,840 --> 00:17:33,640
kind of coming from stand there,

385
00:17:33,640 --> 00:17:37,280
the dimensional classic fires, like SPM and so on.

386
00:17:37,280 --> 00:17:39,520
There was another aspect that was really puzzling to me.

387
00:17:39,520 --> 00:17:40,960
So this was when Alessandro Aquila,

388
00:17:40,960 --> 00:17:43,440
who's a scientist at AWS, now,

389
00:17:43,440 --> 00:17:45,040
the one who would still a student with a friend

390
00:17:45,040 --> 00:17:49,120
from Harvard near science, they had this conjecture

391
00:17:49,120 --> 00:17:52,800
that neural networks exhibit critical learning periods.

392
00:17:52,800 --> 00:17:54,360
Now, what is critical in a period?

393
00:17:54,360 --> 00:17:58,160
So in biological systems, either you learn a skill

394
00:17:58,160 --> 00:18:00,320
when you're young or you don't learn.

395
00:18:00,320 --> 00:18:02,920
This is why you cannot teach all-down new tricks.

396
00:18:02,920 --> 00:18:06,720
And this is why if you're born with a defect like cataract

397
00:18:06,720 --> 00:18:11,080
or with severe myopia, unless you correct it early,

398
00:18:11,080 --> 00:18:13,000
no matter how much time you have to recoup,

399
00:18:13,000 --> 00:18:14,880
you never learn, right?

400
00:18:14,880 --> 00:18:17,720
So the optical defect is fixed.

401
00:18:17,720 --> 00:18:21,280
So it's a result, but your brain has not learned correctly,

402
00:18:21,280 --> 00:18:22,960
and then you never are correct.

403
00:18:22,960 --> 00:18:26,440
So, and this is normally attributed to biology,

404
00:18:26,440 --> 00:18:28,040
to biochemistry of the brain, you know,

405
00:18:28,040 --> 00:18:31,480
you stop generating synapses and so you age.

406
00:18:31,480 --> 00:18:33,080
But neural networks don't age.

407
00:18:33,080 --> 00:18:35,360
Their connectivity is fixed at the outset, it doesn't change.

408
00:18:35,360 --> 00:18:38,200
So I told you, you know, this is, you guys are crazy,

409
00:18:38,200 --> 00:18:42,240
this is why would you ever expect the neural network

410
00:18:42,240 --> 00:18:43,840
would have a behavior like that?

411
00:18:43,840 --> 00:18:46,160
It turns out it does, and which is really puzzling

412
00:18:46,160 --> 00:18:49,120
because now you want, okay, now it cannot be biochemistry

413
00:18:49,120 --> 00:18:52,920
because an artificial neural network doesn't have any.

414
00:18:52,920 --> 00:18:54,360
There's a very resemblance to the brain,

415
00:18:54,360 --> 00:18:57,760
but really, this phenomenon must be an information phenomenon.

416
00:18:57,760 --> 00:18:59,240
And then we start to dig into say, okay,

417
00:18:59,240 --> 00:19:01,960
what is even meaning information in a deep network?

418
00:19:01,960 --> 00:19:02,800
You know, what?

419
00:19:02,800 --> 00:19:05,080
A deep network is a deterministic system.

420
00:19:05,080 --> 00:19:07,560
So as zero entropy, the weights are fixed, right?

421
00:19:07,560 --> 00:19:09,960
The input output map, one strain is a deterministic.

422
00:19:09,960 --> 00:19:12,240
So as infinite mutual information is in the input and output.

423
00:19:12,240 --> 00:19:13,920
So all of the standard concept,

424
00:19:13,920 --> 00:19:16,920
information theory, not useful, and they're not useful

425
00:19:16,920 --> 00:19:21,440
to probe the inside, the guts of the network.

426
00:19:21,440 --> 00:19:23,360
And so we spend a lot of time defining

427
00:19:23,360 --> 00:19:25,480
and measuring information quantities

428
00:19:25,480 --> 00:19:26,680
in these gigantic networks,

429
00:19:26,680 --> 00:19:28,040
in hundreds of millions of parameters

430
00:19:28,040 --> 00:19:30,040
and now even trillions.

431
00:19:30,040 --> 00:19:31,960
And what we discover, for instance,

432
00:19:31,960 --> 00:19:36,160
is that I don't know if you remember the movie,

433
00:19:36,160 --> 00:19:38,480
the eternal sunshine of the spotless mind.

434
00:19:38,480 --> 00:19:43,480
I don't know if you remember it.

435
00:19:43,480 --> 00:19:44,480
For whatever reason,

436
00:19:44,480 --> 00:19:47,080
it wants to forget, experience it to a person or a partner.

437
00:19:47,080 --> 00:19:49,080
And so it goes to a company called La Cuna

438
00:19:49,080 --> 00:19:51,760
that under, you know, there's something,

439
00:19:51,760 --> 00:19:54,240
you know, just showing some pictures of the partner,

440
00:19:54,240 --> 00:19:56,960
zap the brain to erase memory of it.

441
00:19:56,960 --> 00:19:59,240
So we thought, well, maybe we can do that with deep networks,

442
00:19:59,240 --> 00:20:00,080
right?

443
00:20:00,080 --> 00:20:03,560
We can zap the brain to forget or to erase memory

444
00:20:03,560 --> 00:20:06,560
of something that you saw in your training set.

445
00:20:06,560 --> 00:20:08,720
And it turns out that that is possible to do with deep networks

446
00:20:08,720 --> 00:20:11,360
because once you understand how information is

447
00:20:11,360 --> 00:20:15,120
defined and computed and distributed in the representation,

448
00:20:15,120 --> 00:20:18,600
then you can inject noise in very specific direction

449
00:20:18,600 --> 00:20:22,400
that will force you to erase a particular datum

450
00:20:22,400 --> 00:20:25,040
or a class or a cohort of data,

451
00:20:25,040 --> 00:20:26,160
which now is also important

452
00:20:26,160 --> 00:20:28,000
because of privacy issues and so on and so forth.

453
00:20:28,000 --> 00:20:30,640
So there's a lot of fascinating questions

454
00:20:30,640 --> 00:20:33,920
that arise when you try to understand how these

455
00:20:33,920 --> 00:20:36,720
deep networks operate and you have existence proofs

456
00:20:36,720 --> 00:20:39,880
of what is possible to do thanks to biology,

457
00:20:39,880 --> 00:20:42,200
the human visual system, the animal visual system.

458
00:20:42,200 --> 00:20:45,760
So there is definitely a lot more back and forth

459
00:20:45,760 --> 00:20:48,720
between the biological inspiration and the analysis

460
00:20:48,720 --> 00:20:52,080
than there was 20 years ago when we thought, you know,

461
00:20:52,080 --> 00:20:55,600
that maybe we'll solve the brain, you know, with analysis.

462
00:20:55,600 --> 00:20:59,440
And somebody in the 90s told me that, you know,

463
00:20:59,440 --> 00:21:01,240
if you think of science and understanding

464
00:21:01,240 --> 00:21:04,600
is a process of compression, you know, observe the astra

465
00:21:04,600 --> 00:21:07,120
and you could record their positions,

466
00:21:07,120 --> 00:21:09,600
but once you understand the laws of physics,

467
00:21:09,600 --> 00:21:11,560
you can compress them into a law.

468
00:21:11,560 --> 00:21:13,560
And maybe the brain is the most compressed possible

469
00:21:13,560 --> 00:21:14,520
representation of itself.

470
00:21:14,520 --> 00:21:16,960
There is no easier brain of representation of brain

471
00:21:16,960 --> 00:21:18,680
than the brain itself.

472
00:21:18,680 --> 00:21:20,000
And if it's true for deep networks,

473
00:21:20,000 --> 00:21:23,120
then how do we leverage, you know,

474
00:21:23,120 --> 00:21:26,400
our reductionist scientific method

475
00:21:26,400 --> 00:21:29,520
has not been successful in this particular area.

476
00:21:29,520 --> 00:21:31,400
So we need a more holistic approach

477
00:21:31,400 --> 00:21:32,760
to define a measure information.

478
00:21:32,760 --> 00:21:35,480
And then once you do that, you realize that

479
00:21:35,480 --> 00:21:40,480
despite the gigantic dimension of these spaces,

480
00:21:41,280 --> 00:21:44,520
the amount of information that they store is a tiny fraction.

481
00:21:44,520 --> 00:21:46,400
And the way in which they store it is fascinating.

482
00:21:46,400 --> 00:21:49,560
I love it because I can claim that I'm still learning

483
00:21:49,560 --> 00:21:52,280
even if I'm aging because these networks at the beginning

484
00:21:52,280 --> 00:21:55,120
accrue a lot of information, sort of they memorize.

485
00:21:55,120 --> 00:21:57,480
And then they start shedding information,

486
00:21:57,480 --> 00:21:59,000
throwing away information.

487
00:21:59,000 --> 00:22:02,040
And they do this while improving the expected error

488
00:22:02,040 --> 00:22:04,720
or the test error in the data site that you have sequestered.

489
00:22:04,720 --> 00:22:07,080
So in a sense, it seems like forgetting

490
00:22:07,080 --> 00:22:10,760
or throwing away information is a necessary part of learning,

491
00:22:10,760 --> 00:22:12,600
which when I talk to biology, they say,

492
00:22:12,600 --> 00:22:15,760
oh, of course, but I've never seen a written math.

493
00:22:15,760 --> 00:22:20,240
I've never seen a claim that is defensible based on data

494
00:22:20,240 --> 00:22:21,080
arise from that.

495
00:22:21,080 --> 00:22:22,840
So it's really, there's a lot of interplay

496
00:22:22,840 --> 00:22:25,200
between understanding biological networks

497
00:22:25,200 --> 00:22:30,200
and understanding artificial networks.

498
00:22:30,200 --> 00:22:35,160
Yeah, it seems like there would be once you've established

499
00:22:35,160 --> 00:22:40,200
that there are many, many levers that folks are using

500
00:22:40,240 --> 00:22:43,560
during the training process, the batch sizes

501
00:22:43,560 --> 00:22:46,160
and learning rates and cyclical learning rates,

502
00:22:46,160 --> 00:22:49,920
all of these things that if they're ultimately correlated

503
00:22:49,920 --> 00:22:52,880
to the rate at which the network learns

504
00:22:52,880 --> 00:22:57,880
and how things are forgotten would be even more impactful

505
00:22:58,040 --> 00:23:00,880
than was originally believed.

506
00:23:00,880 --> 00:23:02,840
Yes, that's a great point.

507
00:23:02,840 --> 00:23:04,440
One thing that we understood is the following.

508
00:23:04,440 --> 00:23:08,240
So all of these inducted biases that you mentioned,

509
00:23:08,240 --> 00:23:11,080
which could be in the class of functions with polling

510
00:23:11,080 --> 00:23:15,040
or could be in the optimization with SGD,

511
00:23:15,040 --> 00:23:16,600
with the choice of the batch size

512
00:23:16,600 --> 00:23:18,520
and the learning rate and so on and so forth,

513
00:23:18,520 --> 00:23:21,560
or they could be in explicit regularizer.

514
00:23:21,560 --> 00:23:25,240
So all of these processes are some type of regularization,

515
00:23:25,240 --> 00:23:27,520
meaning that these are added terms,

516
00:23:27,520 --> 00:23:29,680
whether explicit or implicit to your training process

517
00:23:29,680 --> 00:23:31,960
so that you don't just minimize the empirical error

518
00:23:31,960 --> 00:23:33,480
otherwise you would overfit,

519
00:23:33,480 --> 00:23:35,200
but you optimize something that hopefully

520
00:23:35,200 --> 00:23:36,200
will allow you to generalize

521
00:23:36,200 --> 00:23:39,040
and they can be computed using, for instance,

522
00:23:39,040 --> 00:23:42,000
a spec based bound and now we know how to compute

523
00:23:42,000 --> 00:23:44,640
the information terms in the spec based balance and so forth.

524
00:23:44,640 --> 00:23:47,480
What we discover, however, is that normally

525
00:23:47,480 --> 00:23:50,320
you think of regularization as a process

526
00:23:50,320 --> 00:23:54,240
that schools or regularizes your loss function.

527
00:23:54,240 --> 00:23:56,880
And this is not what's happening in deep learning.

528
00:23:56,880 --> 00:23:59,680
So there is a paper that we wrote with an internet AWS

529
00:23:59,680 --> 00:24:02,920
at NewRips two years ago, which is titled

530
00:24:04,600 --> 00:24:08,080
Time Matters in regularizing deep networks.

531
00:24:08,080 --> 00:24:09,960
And what it means is a following.

532
00:24:09,960 --> 00:24:14,160
So the intern Aditya Goldakkar did the following experiment.

533
00:24:14,160 --> 00:24:16,680
He took standard regularizers that people use,

534
00:24:16,680 --> 00:24:19,760
for instance, WADK and data augmentation.

535
00:24:19,760 --> 00:24:24,760
And then he had a few epochs of training

536
00:24:24,840 --> 00:24:28,600
without regularization and then turn on regularization.

537
00:24:28,600 --> 00:24:32,080
So asymptotically, the loss function is regularized.

538
00:24:32,080 --> 00:24:34,080
The network would not behave at all.

539
00:24:34,080 --> 00:24:36,400
Vice versa, turn on the regularizer,

540
00:24:36,400 --> 00:24:38,040
let the network converge for a few epochs,

541
00:24:38,040 --> 00:24:40,240
then turn off the regularizer.

542
00:24:40,240 --> 00:24:43,080
So the asymptotic loss is as irregular

543
00:24:43,080 --> 00:24:45,280
as if it never saw regularization.

544
00:24:45,280 --> 00:24:48,080
It only saw regularization during the initial transit.

545
00:24:48,080 --> 00:24:50,400
Yet generalization power is just as good

546
00:24:50,400 --> 00:24:52,520
as the regularized all the way.

547
00:24:52,520 --> 00:24:56,280
So it appears that regularization does not affect

548
00:24:56,280 --> 00:25:00,560
the geology and the geometry of the loss function

549
00:25:00,560 --> 00:25:01,400
at convergence.

550
00:25:01,400 --> 00:25:03,240
So the asymptotic is really not that important.

551
00:25:03,240 --> 00:25:04,960
It's all in the transient.

552
00:25:04,960 --> 00:25:08,080
Regularization affects what bottlenecks in the loss landscape

553
00:25:08,080 --> 00:25:10,200
you can manage to get into.

554
00:25:10,200 --> 00:25:11,480
And then critical and imperial,

555
00:25:11,480 --> 00:25:13,080
which is what we were talking about earlier,

556
00:25:13,080 --> 00:25:15,760
tells you that once you get into one of this bottleneck

557
00:25:15,760 --> 00:25:18,440
and onto a wide valley, and people talk about wide,

558
00:25:18,440 --> 00:25:20,720
minimum, and so on and so forth, flat, minimum,

559
00:25:20,720 --> 00:25:22,960
it's very difficult to come back from that.

560
00:25:22,960 --> 00:25:25,400
So if you enter during the initial transit,

561
00:25:25,400 --> 00:25:27,880
enter the wrong valley, which you could do

562
00:25:27,880 --> 00:25:29,880
is if you're trained with the wrong data, for instance,

563
00:25:29,880 --> 00:25:34,120
because your parents didn't realize that you were myopic

564
00:25:34,120 --> 00:25:35,640
and so they didn't put your glasses on

565
00:25:35,640 --> 00:25:37,760
until you were five, six years old.

566
00:25:37,760 --> 00:25:41,120
At that point, you cannot get back out from that valley

567
00:25:41,120 --> 00:25:44,240
and you will never learn how to see correctly, right?

568
00:25:44,240 --> 00:25:45,920
And this is for a variety of skills,

569
00:25:45,920 --> 00:25:49,920
for a variety of species from songbirds to walking

570
00:25:49,920 --> 00:25:54,440
to deep neural networks, which is puzzling and fascinating.

571
00:25:54,440 --> 00:25:56,320
The analogy that comes to mind for me,

572
00:25:56,320 --> 00:25:59,400
and I may be bluttering my undergraduate material science,

573
00:25:59,400 --> 00:26:02,280
but is one of a kneeling where you can have two materials

574
00:26:02,280 --> 00:26:07,040
that are functionally equivalent by structure,

575
00:26:07,040 --> 00:26:10,280
overall structure, but their internal structure

576
00:26:10,280 --> 00:26:12,800
is different because of the way that they've been created

577
00:26:12,800 --> 00:26:16,480
in a via heating and cooling cycles and things like that.

578
00:26:16,480 --> 00:26:19,520
And that reminds me of a point that you made in the blog post,

579
00:26:19,520 --> 00:26:23,960
which is talking about model compression,

580
00:26:23,960 --> 00:26:28,600
which is I think illustrative of this entire conversation.

581
00:26:28,600 --> 00:26:33,960
The point was that in model compression,

582
00:26:33,960 --> 00:26:36,040
we often think of it as simply trying

583
00:26:36,040 --> 00:26:40,240
to find a model with equivalent error that is smaller

584
00:26:40,240 --> 00:26:42,080
or meet some other set of constraints.

585
00:26:42,080 --> 00:26:45,120
But the architecture, what I'm taking

586
00:26:45,120 --> 00:26:49,440
to be the structure of the models matters a lot

587
00:26:49,440 --> 00:26:50,920
for the reasons that we've talked about.

588
00:26:50,920 --> 00:26:52,640
Can you elaborate a bit on that?

589
00:26:52,640 --> 00:26:55,360
Yeah, so we have this concept that we call information

590
00:26:55,360 --> 00:26:59,480
plasticity as analogous to neuronal plasticity.

591
00:26:59,480 --> 00:27:02,320
Neural plasticity is where neurons forms in absence,

592
00:27:02,320 --> 00:27:03,920
form, you know, form dendrites.

593
00:27:03,920 --> 00:27:07,200
And so they physically change configuration.

594
00:27:07,200 --> 00:27:09,160
In deep neural network, the configuration is fixed.

595
00:27:09,160 --> 00:27:12,160
However, what you notice when the network is staying

596
00:27:12,160 --> 00:27:16,480
is that some parameters don't matter at all,

597
00:27:16,480 --> 00:27:19,360
meaning that if you take that parameter and change it,

598
00:27:19,360 --> 00:27:23,400
liberally, the input-out behavior does not change.

599
00:27:23,400 --> 00:27:26,040
So you could replace that parameter with noise

600
00:27:26,040 --> 00:27:27,800
or you could replace it with zero.

601
00:27:27,800 --> 00:27:30,640
So this is called pruning.

602
00:27:30,640 --> 00:27:34,600
And observe no input-out would changes.

603
00:27:34,600 --> 00:27:37,480
Changes in the input-out would behavior.

604
00:27:37,480 --> 00:27:41,560
And so it appears, however, that during the initial transient,

605
00:27:41,560 --> 00:27:43,600
the amount of, so you could say also that this parameter

606
00:27:43,600 --> 00:27:46,240
doesn't have any information, because you could store it

607
00:27:46,240 --> 00:27:48,520
with zero bits, and you will never know the difference.

608
00:27:48,520 --> 00:27:49,840
Vice versa, because if you have a parameter,

609
00:27:49,840 --> 00:27:51,560
do you change it a little bit?

610
00:27:51,560 --> 00:27:52,880
I know the subtle behavior changes

611
00:27:52,880 --> 00:27:54,280
that has a lot of information.

612
00:27:54,280 --> 00:27:56,880
So you want to store it with a large number of bits.

613
00:27:56,880 --> 00:28:01,760
So, but it turns out that during the initial transient,

614
00:28:01,760 --> 00:28:04,680
the information counted as number of bits per weight

615
00:28:04,680 --> 00:28:07,000
moves around from the different layers, you know,

616
00:28:07,000 --> 00:28:10,360
so the crosses and goes from upper layer to lower layers

617
00:28:10,360 --> 00:28:11,440
until it settles.

618
00:28:11,440 --> 00:28:14,240
Beyond a certain point, it cannot move.

619
00:28:14,240 --> 00:28:17,000
And so beyond certain points, weights that are uninformative,

620
00:28:17,000 --> 00:28:20,480
stay uninformative, even if you change the training set.

621
00:28:20,480 --> 00:28:22,800
And so that's really the structure of the network.

622
00:28:22,800 --> 00:28:25,440
So it appears that the effective connectivity,

623
00:28:25,440 --> 00:28:27,200
which is not the physical connectivity,

624
00:28:27,200 --> 00:28:30,040
because it's, you know, that's the term in the outside.

625
00:28:30,040 --> 00:28:32,080
But the effective connectivity as measure

626
00:28:32,080 --> 00:28:34,520
by the amount of information that that connection carries,

627
00:28:34,520 --> 00:28:36,120
which we now know how to measure,

628
00:28:36,120 --> 00:28:38,360
after several years of work.

629
00:28:38,360 --> 00:28:39,720
So that is very important.

630
00:28:39,720 --> 00:28:41,960
And if you knew that at the beginning,

631
00:28:41,960 --> 00:28:43,480
okay, you could train with a smaller network.

632
00:28:43,480 --> 00:28:45,280
The problem is that to get there,

633
00:28:45,280 --> 00:28:47,400
you need to go through this very high bump

634
00:28:47,400 --> 00:28:48,400
in information in the network,

635
00:28:48,400 --> 00:28:51,280
which needs the high-dimensional space

636
00:28:51,280 --> 00:28:53,880
and needs a large number of parameters.

637
00:28:53,880 --> 00:28:56,720
And so yeah, so and there are,

638
00:28:56,720 --> 00:28:59,000
like I was suggesting, statistical physicists

639
00:28:59,000 --> 00:29:00,640
and physicists in general are very fascinating

640
00:29:00,640 --> 00:29:01,880
with some of these questions,

641
00:29:01,880 --> 00:29:05,240
because they have thought about high-dimensional systems,

642
00:29:05,240 --> 00:29:07,000
including spin glasses and so on.

643
00:29:09,000 --> 00:29:11,000
So we diverged pretty quickly

644
00:29:11,000 --> 00:29:15,080
from the specifics of the graceful AI set of papers.

645
00:29:15,080 --> 00:29:18,720
But I want to return to those and dig in deeper.

646
00:29:18,720 --> 00:29:22,080
You talked about kind of how two models

647
00:29:22,080 --> 00:29:25,200
with the same accuracy can have very different types

648
00:29:25,200 --> 00:29:29,960
of errors and how from a user perspective,

649
00:29:29,960 --> 00:29:31,880
that might be frustrating.

650
00:29:31,880 --> 00:29:33,800
They may get used to a certain type of error.

651
00:29:33,800 --> 00:29:35,960
Now the model changes and all of a sudden,

652
00:29:35,960 --> 00:29:39,080
the behavior that they're used to is different.

653
00:29:39,080 --> 00:29:44,080
And we want to, well, I guess in introducing this,

654
00:29:45,040 --> 00:29:48,680
you focus on the idea of regression

655
00:29:48,680 --> 00:29:52,680
and meaning things that were working previously

656
00:29:52,680 --> 00:29:54,560
don't break.

657
00:29:55,760 --> 00:29:59,440
But are there kind of broader types of continuity

658
00:29:59,440 --> 00:30:02,080
that a customer might come to expect?

659
00:30:02,080 --> 00:30:04,600
And have you looked at that, or to what degree

660
00:30:04,600 --> 00:30:05,920
have you looked at that?

661
00:30:05,920 --> 00:30:06,760
Yes, we have.

662
00:30:06,760 --> 00:30:08,320
In fact, when we started this project,

663
00:30:08,320 --> 00:30:12,440
we thought it was a narrow set of use cases where,

664
00:30:12,440 --> 00:30:14,800
let's say you have a photo collection.

665
00:30:14,800 --> 00:30:18,520
And now you update to the latest software,

666
00:30:18,520 --> 00:30:21,760
and which is, of course, much better than the one before,

667
00:30:21,760 --> 00:30:25,240
except now you're searching for a picture of your cousin

668
00:30:25,240 --> 00:30:27,360
that you were able to find before and now they don't

669
00:30:27,360 --> 00:30:29,160
show up in the search and what's going on.

670
00:30:29,160 --> 00:30:30,200
So that's the regression, right?

671
00:30:30,200 --> 00:30:33,600
So that you are frustrated as an individual user

672
00:30:33,600 --> 00:30:35,760
because something was working before

673
00:30:35,760 --> 00:30:38,160
and it's not working now, something broke.

674
00:30:38,160 --> 00:30:40,200
So as soon as we came up with this,

675
00:30:40,200 --> 00:30:42,720
we discovered that there's a host of other related problems.

676
00:30:42,720 --> 00:30:46,840
For instance, we didn't realize that it is exactly the same problem

677
00:30:46,840 --> 00:30:51,000
as a cross-model compatibility where you have a service

678
00:30:51,000 --> 00:30:52,560
that runs on different platforms, let's say,

679
00:30:52,560 --> 00:30:55,040
the smartphone as well as on the cloud.

680
00:30:55,040 --> 00:30:57,720
And of course, on the cloud, you prioritize performance.

681
00:30:57,720 --> 00:31:01,560
You really have no constraint on how big the model is.

682
00:31:01,560 --> 00:31:03,480
But on your phone, you don't want to keep the battery

683
00:31:03,480 --> 00:31:05,800
in no time, and so you run a smaller model.

684
00:31:05,800 --> 00:31:07,360
So what do you do?

685
00:31:07,360 --> 00:31:11,480
Do you run the best you can do with the hardware

686
00:31:11,480 --> 00:31:14,120
you have on the phone, regardless of what the big model

687
00:31:14,120 --> 00:31:15,480
on the cloud does?

688
00:31:15,480 --> 00:31:17,680
Or do you try to train the model on the phone

689
00:31:17,680 --> 00:31:21,080
in a way that resembles or mimics the model in the cloud

690
00:31:21,080 --> 00:31:23,480
in a way that can be defined?

691
00:31:23,480 --> 00:31:26,120
And if you do that, then your trade space

692
00:31:26,120 --> 00:31:30,840
is not just the parameters for a children architecture.

693
00:31:30,840 --> 00:31:32,800
You also can optimize over the architectures,

694
00:31:32,800 --> 00:31:35,440
and it becomes a hybrid search space,

695
00:31:35,440 --> 00:31:37,480
over a continuous space of weights,

696
00:31:37,480 --> 00:31:39,280
as well as the discrete space of architectures.

697
00:31:39,280 --> 00:31:42,480
And so that, we thought, was a different problem,

698
00:31:42,480 --> 00:31:45,360
but it turns out to be exactly the same problem.

699
00:31:45,360 --> 00:31:49,720
Same thing with languages, we have services that build chat

700
00:31:49,720 --> 00:31:50,640
bots.

701
00:31:50,640 --> 00:31:53,640
And so the train model sits at the center,

702
00:31:53,640 --> 00:31:56,240
but then customers build up on top of this model,

703
00:31:56,240 --> 00:32:00,400
very elaborate post-processing workflows.

704
00:32:00,400 --> 00:32:05,440
And even changes that you would think are innocuous

705
00:32:05,440 --> 00:32:06,960
can break this post-processing.

706
00:32:06,960 --> 00:32:09,920
And you really don't know a priority,

707
00:32:09,920 --> 00:32:11,440
what is and what is not doable.

708
00:32:11,440 --> 00:32:13,920
And so there is a whole space of dimensions

709
00:32:13,920 --> 00:32:16,880
along which you may want to impose either constraints

710
00:32:16,880 --> 00:32:19,880
or optimize together with the average error.

711
00:32:19,880 --> 00:32:23,280
And so for the past years, we've seen dozens of cases

712
00:32:23,280 --> 00:32:26,560
across different applications at AWS,

713
00:32:26,560 --> 00:32:29,120
where we see this problem arises.

714
00:32:29,120 --> 00:32:33,680
It's interesting that when we first published this paper,

715
00:32:33,680 --> 00:32:39,200
it was not reviewed positively, because, you know,

716
00:32:39,200 --> 00:32:41,600
well, there's no comparison with anything,

717
00:32:41,600 --> 00:32:45,640
but yes, because this is not a nobody

718
00:32:45,640 --> 00:32:47,040
has to look at this problem before.

719
00:32:47,040 --> 00:32:49,160
So it's really one of the most exciting times

720
00:32:49,160 --> 00:32:52,240
when you're not just having a new solution to an old problem,

721
00:32:52,240 --> 00:32:55,680
but you open up a new problem that one hand

722
00:32:55,680 --> 00:32:58,520
is exciting for scientists to send their teeth in.

723
00:32:58,520 --> 00:33:00,080
On the other, it can be beneficial

724
00:33:00,080 --> 00:33:03,160
because it can also help democratize the use of AI.

725
00:33:03,160 --> 00:33:06,760
Because right now, many customers are

726
00:33:06,760 --> 00:33:09,480
sitting by the sideline as new and improved models

727
00:33:09,480 --> 00:33:11,840
are coming by just because they don't want to face

728
00:33:11,840 --> 00:33:15,080
the issue of having to redo all the post-processing

729
00:33:15,080 --> 00:33:15,920
and so on and so forth.

730
00:33:15,920 --> 00:33:19,000
Let alone the cost of repressing large galleries,

731
00:33:19,000 --> 00:33:21,000
because if you have a photo collection,

732
00:33:21,000 --> 00:33:23,120
when I come in with a new model,

733
00:33:23,120 --> 00:33:25,400
you have to reprocess every single image in your gallery

734
00:33:25,400 --> 00:33:27,680
through the new model so that you can recreate an index

735
00:33:27,680 --> 00:33:29,160
to search.

736
00:33:29,160 --> 00:33:32,000
And also, if you have hundreds of thousands of images,

737
00:33:32,000 --> 00:33:33,760
that's OK, but if you have billions, OK,

738
00:33:33,760 --> 00:33:38,160
that starts becoming a little bit more complex.

739
00:33:38,160 --> 00:33:42,200
And so it turns out that both the regression problem

740
00:33:42,200 --> 00:33:46,600
and the reprocessing problem are solved in the same way,

741
00:33:46,600 --> 00:33:51,200
and that is by utilizing the existing model.

742
00:33:51,200 --> 00:33:54,200
Can you talk more about the way you approach solving the problem?

743
00:33:54,200 --> 00:33:57,960
Yeah, so the backward compatibility problem

744
00:33:57,960 --> 00:34:01,360
is fairly simple to formalize and the solutions

745
00:34:01,360 --> 00:34:04,440
that we have to pose are very simple to implement.

746
00:34:04,440 --> 00:34:07,160
And now the backward compatibility problem, the regression.

747
00:34:07,160 --> 00:34:08,320
Yes, the backward compatibility problem

748
00:34:08,320 --> 00:34:10,080
is where you have an old model.

749
00:34:10,080 --> 00:34:11,880
You replace it with a new model.

750
00:34:11,880 --> 00:34:13,880
And you would like, as in the case, for instance,

751
00:34:13,880 --> 00:34:16,040
of your photo collection, to be able to use

752
00:34:16,040 --> 00:34:17,760
the old model to search old pictures

753
00:34:17,760 --> 00:34:20,640
without having to reprocess them through the new model.

754
00:34:20,640 --> 00:34:24,560
OK, and so one way to do that is to optimize

755
00:34:24,560 --> 00:34:27,640
over a new backbone, a new model that

756
00:34:27,640 --> 00:34:31,400
has bigger, larger number of parameters,

757
00:34:31,400 --> 00:34:33,200
different architecture, completely different,

758
00:34:33,200 --> 00:34:37,400
except that model is biased to use the classifier

759
00:34:37,400 --> 00:34:38,920
that the old model used.

760
00:34:38,920 --> 00:34:41,600
So it says you force it to live in the same metric space

761
00:34:41,600 --> 00:34:43,760
where you can do clustering, where you can do search,

762
00:34:43,760 --> 00:34:44,840
and so on and so forth.

763
00:34:44,840 --> 00:34:47,880
Yeah, just to interject, is that you

764
00:34:47,880 --> 00:34:50,320
might think that, OK, my typical practice

765
00:34:50,320 --> 00:34:52,440
might be to start with my old model

766
00:34:52,440 --> 00:34:55,960
and freeze the weights of my classifier or something

767
00:34:55,960 --> 00:35:01,520
like that and just retrain and fine tune based on new data.

768
00:35:01,520 --> 00:35:05,520
Is that inadequate to assure the kind of backward compatibility

769
00:35:05,520 --> 00:35:06,760
and the errors?

770
00:35:06,760 --> 00:35:08,560
So you can do backward compatibility

771
00:35:08,560 --> 00:35:11,920
by just taking the same architecture and retraining it

772
00:35:11,920 --> 00:35:14,720
with the same classifier that you're fine.

773
00:35:14,720 --> 00:35:18,520
But if you follow the literature, every few months

774
00:35:18,520 --> 00:35:20,720
a new architecture comes by, and you

775
00:35:20,720 --> 00:35:23,160
want to harvest benefits of that.

776
00:35:23,160 --> 00:35:25,840
And so in that case, these two representations

777
00:35:25,840 --> 00:35:26,960
that are in fit to a classifier

778
00:35:26,960 --> 00:35:29,560
live in different spaces, spaces of different dimensions.

779
00:35:29,560 --> 00:35:32,320
So you cannot even compare them.

780
00:35:32,320 --> 00:35:35,840
And so, but you can, however, force the network

781
00:35:35,840 --> 00:35:38,720
to develop a representation that is

782
00:35:38,720 --> 00:35:41,120
mathematically compatible with the old classifier.

783
00:35:41,120 --> 00:35:45,280
So you can compute distances, compute angles, and so on.

784
00:35:45,280 --> 00:35:51,320
So that becomes a fairly cleanly formalized problem

785
00:35:51,320 --> 00:35:54,480
and that you can attack with standard methods

786
00:35:54,480 --> 00:35:56,000
of machine learning.

787
00:35:56,000 --> 00:35:58,000
Now, the positive congruent training,

788
00:35:58,000 --> 00:36:00,080
which is the training done in a way

789
00:36:00,080 --> 00:36:02,400
that minimizes the regression errors,

790
00:36:02,400 --> 00:36:04,240
that's a more amorphous problem.

791
00:36:04,240 --> 00:36:08,000
Because depending on what type of errors you want to avoid,

792
00:36:08,000 --> 00:36:09,640
it may take different shapes.

793
00:36:09,640 --> 00:36:12,320
And something that is still puzzling us

794
00:36:12,320 --> 00:36:20,520
is that the current best performing model for positive congruent

795
00:36:20,520 --> 00:36:23,600
training is one that does not explicitly

796
00:36:23,600 --> 00:36:27,960
enforce that you make a mistake on a certain course of data.

797
00:36:27,960 --> 00:36:33,880
But that is trained using ensembles, which don't know

798
00:36:33,880 --> 00:36:34,880
anything about each other.

799
00:36:34,880 --> 00:36:37,480
And they don't know anything about the old model.

800
00:36:37,480 --> 00:36:40,760
So we call this future proofing, because training with this

801
00:36:40,760 --> 00:36:45,520
ensures that later there will be fewer of what we call negative flips.

802
00:36:45,520 --> 00:36:51,120
Negative flips are data points for which the decision

803
00:36:51,120 --> 00:36:54,000
before it becomes wrong.

804
00:36:54,000 --> 00:36:56,880
So there are definitely very open scientific questions

805
00:36:56,880 --> 00:36:57,920
that need to be understood.

806
00:36:57,920 --> 00:37:01,600
We just put there the first seed.

807
00:37:01,600 --> 00:37:03,680
But definitely there's a lot to be understood

808
00:37:03,680 --> 00:37:08,280
that we and others are working on acting.

809
00:37:08,280 --> 00:37:10,560
And so what do these ensembles look like?

810
00:37:10,560 --> 00:37:13,280
What are the components of the ensembles?

811
00:37:13,280 --> 00:37:15,520
The ensembles could be the same architecture

812
00:37:15,520 --> 00:37:17,360
training different ways, or with different breakdown

813
00:37:17,360 --> 00:37:19,920
of the data, or complete different architectures.

814
00:37:19,920 --> 00:37:23,920
The only problem is that the ensemble is not viable in practice,

815
00:37:23,920 --> 00:37:26,760
because if at different time you want

816
00:37:26,760 --> 00:37:30,000
to run an ensemble of 10 models, your cost of interest

817
00:37:30,000 --> 00:37:31,320
will get multiplied by 10.

818
00:37:31,320 --> 00:37:34,040
And that's something that the customers will not accept

819
00:37:34,040 --> 00:37:35,400
gladly, right?

820
00:37:35,400 --> 00:37:37,120
So we really need to figure out ways

821
00:37:37,120 --> 00:37:39,680
to perform positive congruent training

822
00:37:39,680 --> 00:37:42,320
at exactly the same cost as running one model,

823
00:37:42,320 --> 00:37:43,920
not an ensemble of models.

824
00:37:43,920 --> 00:37:47,160
So it says it's a paragon of performance,

825
00:37:47,160 --> 00:37:49,400
because right now it's achieved the best performance,

826
00:37:49,400 --> 00:37:53,080
but it's not a viable target for deployment,

827
00:37:53,080 --> 00:37:55,840
because it multiplies the cost by an integer multiple,

828
00:37:55,840 --> 00:37:58,480
and that's not, you know, it's a nice target.

829
00:37:58,480 --> 00:38:01,440
So, you know, this is a problem.

830
00:38:01,440 --> 00:38:04,680
And so when you say that the ensembles can be,

831
00:38:06,200 --> 00:38:07,880
it can be, you didn't say anything,

832
00:38:07,880 --> 00:38:09,720
but that, you know, there's some,

833
00:38:09,720 --> 00:38:13,920
you made it sound like the ensemble was not constructed

834
00:38:13,920 --> 00:38:15,960
in a particular way that had this property,

835
00:38:15,960 --> 00:38:19,040
but rather the act of using ensembles

836
00:38:19,040 --> 00:38:21,720
had kind of a regularization type of effect

837
00:38:21,720 --> 00:38:25,080
that addressed PCT.

838
00:38:25,080 --> 00:38:26,560
Correct, correct.

839
00:38:26,560 --> 00:38:29,800
And one would be induced in thinking

840
00:38:29,800 --> 00:38:31,120
that because ensembles work,

841
00:38:31,120 --> 00:38:33,440
then these negative flips are points

842
00:38:33,440 --> 00:38:35,720
that are very close to the decision boundary.

843
00:38:35,720 --> 00:38:37,400
So when you train different models,

844
00:38:37,400 --> 00:38:38,560
the boundary jitters around,

845
00:38:38,560 --> 00:38:39,680
and so these points flip.

846
00:38:39,680 --> 00:38:43,760
But in fact, we discover that many of the points,

847
00:38:43,760 --> 00:38:46,320
data points that flip are actually very far

848
00:38:46,320 --> 00:38:47,360
from the decision boundary.

849
00:38:47,360 --> 00:38:50,000
They are very high confidence data points.

850
00:38:50,000 --> 00:38:51,880
And so you're making high confidence mistakes,

851
00:38:51,880 --> 00:38:53,280
which are the worst kind, right?

852
00:38:53,280 --> 00:38:56,640
Absolutely, it's not knowing that you don't know.

853
00:38:56,640 --> 00:38:57,760
It's a real problem.

854
00:38:57,760 --> 00:38:59,600
It's that don't include an effect and all that.

855
00:39:01,600 --> 00:39:05,680
And so what is the best practical approach

856
00:39:05,680 --> 00:39:07,760
to addressing this issue?

857
00:39:08,680 --> 00:39:13,680
Yeah, so right now we have a form of what we call

858
00:39:13,920 --> 00:39:15,320
focal distillation.

859
00:39:16,360 --> 00:39:18,080
Distillation is where you train a model

860
00:39:18,080 --> 00:39:19,560
and then you train another model to mimic

861
00:39:19,560 --> 00:39:21,880
the input of behavior of the first,

862
00:39:21,880 --> 00:39:23,560
but that model could be smaller or could have

863
00:39:23,560 --> 00:39:25,160
other characteristics.

864
00:39:25,160 --> 00:39:28,840
And of course, you don't want to imitate the old model

865
00:39:28,840 --> 00:39:31,480
because then you would inherit also the mistakes.

866
00:39:31,480 --> 00:39:34,000
You want to imitate the old model only

867
00:39:34,000 --> 00:39:36,000
where the old model gets it right.

868
00:39:36,000 --> 00:39:38,200
And otherwise, you want to have the freedom

869
00:39:38,200 --> 00:39:41,080
to optimize according to the new architecture,

870
00:39:41,080 --> 00:39:45,040
better data, more balanced data and whatnot.

871
00:39:45,040 --> 00:39:48,840
So is there a difference between distillation,

872
00:39:48,840 --> 00:39:51,760
broadly and student teacher type of an approach?

873
00:39:51,760 --> 00:39:54,200
Yeah, so it's a general umbrella of methods

874
00:39:54,200 --> 00:39:56,040
that go under the name of distillation

875
00:39:56,040 --> 00:39:57,240
or student teacher models.

876
00:39:57,240 --> 00:39:58,240
Yes.

877
00:39:58,240 --> 00:40:00,760
And this is a particular class of them

878
00:40:00,760 --> 00:40:03,920
where you don't just try to mimic the behavior of the model,

879
00:40:03,920 --> 00:40:05,520
but you try to mimic the behavior of the model

880
00:40:05,520 --> 00:40:09,520
restricted to the course of data that interests you.

881
00:40:09,520 --> 00:40:11,760
Now because typically, this course of data,

882
00:40:11,760 --> 00:40:15,000
including the negative flips, is a tiny minority

883
00:40:15,000 --> 00:40:16,200
of the total.

884
00:40:16,200 --> 00:40:18,720
I mean, tiny could be in the order of five, six, seven percent.

885
00:40:18,720 --> 00:40:21,600
So it is significant if you account for the fact

886
00:40:21,600 --> 00:40:25,560
that people fight very hard for a 1% performance improvement.

887
00:40:25,560 --> 00:40:27,320
And all of a sudden, you're squander 7%

888
00:40:27,320 --> 00:40:29,400
because of these negative flips.

889
00:40:29,400 --> 00:40:31,520
And so, but if you just do distillation

890
00:40:31,520 --> 00:40:34,600
on 7% of the data, that would be lost in the general loss

891
00:40:34,600 --> 00:40:35,600
function.

892
00:40:35,600 --> 00:40:37,200
So you have to do distillation in a clever way.

893
00:40:37,200 --> 00:40:40,200
That's what we call a focal distillation.

894
00:40:40,200 --> 00:40:42,440
But you know, it's, again, it's an active area of research

895
00:40:42,440 --> 00:40:44,960
and we both read about methods that are coming up

896
00:40:44,960 --> 00:40:45,960
that improve on that.

897
00:40:45,960 --> 00:40:47,640
And we also have internal efforts

898
00:40:47,640 --> 00:40:50,880
that are aimed at improving that.

899
00:40:50,880 --> 00:40:56,720
It sounds coarsely like a compositive distillation

900
00:40:56,720 --> 00:40:59,200
and like active learning where you're

901
00:40:59,200 --> 00:41:03,760
trying to identify the most important data

902
00:41:03,760 --> 00:41:05,920
to train the model on.

903
00:41:05,920 --> 00:41:07,440
Yeah, that's an important point.

904
00:41:07,440 --> 00:41:09,680
We very strongly believe in active learning.

905
00:41:09,680 --> 00:41:13,160
We haven't quite been able to make it work the way

906
00:41:13,160 --> 00:41:14,800
we would like it to work.

907
00:41:14,800 --> 00:41:17,520
But yeah, there's so many open questions.

908
00:41:17,520 --> 00:41:21,040
We could spend the rest of the afternoon on this.

909
00:41:21,040 --> 00:41:24,960
But these are all very good questions.

910
00:41:24,960 --> 00:41:32,920
And so you started with this was a problem that came up

911
00:41:32,920 --> 00:41:36,600
in the context of customer challenges.

912
00:41:36,600 --> 00:41:42,200
Have you, is the solution in front of customers

913
00:41:42,200 --> 00:41:45,560
or is this still at the research frontier

914
00:41:45,560 --> 00:41:48,360
and working its way towards practice?

915
00:41:48,360 --> 00:41:50,640
It's both so that we are still working at the frontier.

916
00:41:50,640 --> 00:41:52,680
But some of our backup compatible training

917
00:41:52,680 --> 00:41:54,840
are now in the hands of customers.

918
00:41:54,840 --> 00:41:59,040
So typically, this is how it works at AWS.

919
00:41:59,040 --> 00:42:03,400
So once you have a solution, it gets deployed to customers

920
00:42:03,400 --> 00:42:05,040
very quickly.

921
00:42:05,040 --> 00:42:07,720
And there are customers that are also engaged in the process

922
00:42:07,720 --> 00:42:09,640
so we can ensure that once we launch something,

923
00:42:09,640 --> 00:42:12,280
it has customer fighting and feedback.

924
00:42:12,280 --> 00:42:18,680
One of the things that you mentioned early on in the conversation

925
00:42:18,680 --> 00:42:23,000
is as well as in your blog post on the topic

926
00:42:23,000 --> 00:42:30,400
is pointing out this idea of the artificial, separate

927
00:42:30,400 --> 00:42:32,880
relationship between training and inference

928
00:42:32,880 --> 00:42:36,480
and kind of speaking to, you know, how the brain is kind

929
00:42:36,480 --> 00:42:42,080
of online learning, are you working in that area

930
00:42:42,080 --> 00:42:42,920
as well?

931
00:42:42,920 --> 00:42:46,040
And I'd love to kind of get your take on, you know,

932
00:42:46,040 --> 00:42:48,160
where we are as a community with regard

933
00:42:48,160 --> 00:42:54,560
to kind of, you know, breaking the artificial separation

934
00:42:54,560 --> 00:42:56,200
between training and learning.

935
00:42:56,200 --> 00:42:57,400
Yeah, that's a great question.

936
00:42:57,400 --> 00:42:59,000
Yeah, that goes under the general umbrella

937
00:42:59,000 --> 00:43:01,960
of lifelong learning or continual learning.

938
00:43:01,960 --> 00:43:04,680
And typically in the literature,

939
00:43:04,680 --> 00:43:08,080
people focus on the problem of catastrophic forgetting,

940
00:43:08,080 --> 00:43:11,000
meaning that as you train with new tasks and your model,

941
00:43:11,000 --> 00:43:12,960
you forget that are things, so how do you avoid that

942
00:43:12,960 --> 00:43:14,000
and so on and so forth?

943
00:43:14,000 --> 00:43:16,120
There are big issues of scale that goes out

944
00:43:16,120 --> 00:43:18,560
because as you train more and more tasks

945
00:43:18,560 --> 00:43:20,680
and more and more models, the question is,

946
00:43:20,680 --> 00:43:22,240
how do you fit them into an architecture?

947
00:43:22,240 --> 00:43:24,720
How do you grow organic or the architecture and so on and so forth?

948
00:43:24,720 --> 00:43:27,280
So there are a lot of very interesting questions.

949
00:43:27,280 --> 00:43:31,640
There's one piece of work that recently that gave us hope

950
00:43:31,640 --> 00:43:36,640
in this arena, which is a work actually on language.

951
00:43:36,640 --> 00:43:44,320
Language is, you know, there's always a small diatribe

952
00:43:44,320 --> 00:43:47,160
between language science team and vision science team

953
00:43:47,160 --> 00:43:51,120
because language is quote unquote easy.

954
00:43:51,120 --> 00:43:53,120
Of course, it's not easy.

955
00:43:53,120 --> 00:43:57,520
But you know, it's a domain that is very large but finite

956
00:43:57,520 --> 00:44:00,640
and the type of nuisance variability to which

957
00:44:00,640 --> 00:44:03,240
language that are subject is very small compared to images.

958
00:44:03,240 --> 00:44:07,040
So, you know, we have a sentence, you can move the words around,

959
00:44:07,040 --> 00:44:09,200
you can misspell them and so on.

960
00:44:09,200 --> 00:44:10,360
But if you have an image of a cat,

961
00:44:10,360 --> 00:44:11,520
there's image of different cats

962
00:44:11,520 --> 00:44:12,280
and the different positive,

963
00:44:12,280 --> 00:44:14,360
illumination, code color and so on and so forth.

964
00:44:14,360 --> 00:44:19,360
So it's infinitely many different embodiments of that concept.

965
00:44:20,400 --> 00:44:25,560
But in languages, one, both the data and the object of inference

966
00:44:25,560 --> 00:44:27,280
live in the same space.

967
00:44:27,280 --> 00:44:29,160
So, which is not a revision.

968
00:44:29,160 --> 00:44:31,280
In vision, you want the labeled cat,

969
00:44:31,280 --> 00:44:34,600
but in the image, there's no, in the physical scene,

970
00:44:34,600 --> 00:44:37,280
there's no labeled cat, you know, there's a pixel.

971
00:44:37,280 --> 00:44:40,360
So within this work in languages called tan,

972
00:44:40,360 --> 00:44:44,560
which is basically takes all language tasks.

973
00:44:44,560 --> 00:44:48,520
There's a variety we took the most common,

974
00:44:48,520 --> 00:44:50,880
maybe it doesn't or so.

975
00:44:50,880 --> 00:44:52,800
And you know, co-reference of the solution,

976
00:44:52,800 --> 00:44:57,960
a 90-tier recognition, semantic role labeling

977
00:44:57,960 --> 00:44:59,520
and so on and so forth.

978
00:44:59,520 --> 00:45:02,080
And Giovanni Paulini, who's a mathematician

979
00:45:02,080 --> 00:45:05,080
who joined our team a couple of years ago,

980
00:45:05,080 --> 00:45:09,440
was able to translate all of these different tasks

981
00:45:09,440 --> 00:45:12,120
into a single task which is to translate

982
00:45:12,120 --> 00:45:15,320
between different augmented languages

983
00:45:15,320 --> 00:45:17,600
where the format of the language

984
00:45:17,600 --> 00:45:19,960
embodies the task, okay?

985
00:45:19,960 --> 00:45:22,640
So by doing that, you have all these different tasks

986
00:45:22,640 --> 00:45:24,080
which you can keep training,

987
00:45:24,080 --> 00:45:26,880
but you're training one model, okay?

988
00:45:26,880 --> 00:45:29,480
And then the format of the query or the format

989
00:45:29,480 --> 00:45:31,280
of the training data specifies

990
00:45:31,280 --> 00:45:33,200
whether you're looking for name 90-tier recognition

991
00:45:33,200 --> 00:45:34,560
or some other task.

992
00:45:34,560 --> 00:45:36,360
And so in language, we're able to do that

993
00:45:36,360 --> 00:45:39,760
in a way that we haven't yet been able to do in vision.

994
00:45:41,040 --> 00:45:44,040
And still, you know, it's only very early stages

995
00:45:44,040 --> 00:45:48,840
of multi-task learning and continual learning

996
00:45:48,840 --> 00:45:51,200
because we still have the pre-training phase

997
00:45:51,200 --> 00:45:52,960
which is artificially separated

998
00:45:52,960 --> 00:45:54,400
where you train a language model

999
00:45:54,400 --> 00:45:56,640
to predict missing words and so on.

1000
00:45:56,640 --> 00:45:58,120
So it is very early.

1001
00:45:58,120 --> 00:46:01,760
I think that area of investigation will go on for a while.

1002
00:46:01,760 --> 00:46:05,680
And but definitely it's an artificial separation,

1003
00:46:05,680 --> 00:46:09,440
as you said, that we need to do away with it some point.

1004
00:46:09,440 --> 00:46:11,400
We're just not there yet.

1005
00:46:11,400 --> 00:46:14,440
And so I don't think I followed the connection

1006
00:46:14,440 --> 00:46:17,680
between the format of the language

1007
00:46:17,680 --> 00:46:20,600
and the lifelong learning aspect of

1008
00:46:21,640 --> 00:46:23,160
that we were originally speaking to.

1009
00:46:23,160 --> 00:46:24,440
Can you elaborate on that a bit?

1010
00:46:24,440 --> 00:46:29,440
Yeah, so right now, if you train for one task,

1011
00:46:29,440 --> 00:46:31,720
the type of knowledge that you acquire

1012
00:46:31,720 --> 00:46:32,920
is specific to that task

1013
00:46:32,920 --> 00:46:34,880
and you cannot transfer it to a new task

1014
00:46:34,880 --> 00:46:37,400
which you learn because if you now take that model

1015
00:46:37,400 --> 00:46:38,720
that you train on task one,

1016
00:46:38,720 --> 00:46:40,520
now you find you don't task two,

1017
00:46:40,520 --> 00:46:42,960
you've lost something with respect to task one.

1018
00:46:42,960 --> 00:46:46,080
Now you need to worry about not forgetting something

1019
00:46:46,080 --> 00:46:46,920
about task one.

1020
00:46:46,920 --> 00:46:48,280
So if you learn them seriously,

1021
00:46:48,280 --> 00:46:49,720
you'll have to face this problem

1022
00:46:49,720 --> 00:46:50,920
when you start forgetting.

1023
00:46:50,920 --> 00:46:54,160
If you learn them simultaneously,

1024
00:46:54,160 --> 00:46:57,080
it's also synergistic because now there is knowledge

1025
00:46:57,080 --> 00:46:58,600
which is shared across this task.

1026
00:46:58,600 --> 00:47:00,800
So if you have small data set on task one,

1027
00:47:00,800 --> 00:47:02,040
small data set on task two,

1028
00:47:02,040 --> 00:47:04,000
you can train on the union of the sets

1029
00:47:04,000 --> 00:47:05,960
and put it as if you have, you know,

1030
00:47:05,960 --> 00:47:09,720
Tolkien is an author and he lives in Moscow.

1031
00:47:09,720 --> 00:47:10,720
Somewhere in the model,

1032
00:47:10,720 --> 00:47:14,520
the information that Moscow is in Russia is there.

1033
00:47:14,520 --> 00:47:17,840
And so if you ask the question is Tolkien Russian,

1034
00:47:18,880 --> 00:47:20,680
you may be induced and say yes.

1035
00:47:20,680 --> 00:47:24,240
So which you wouldn't, if you had trained

1036
00:47:24,240 --> 00:47:25,360
for each task individually

1037
00:47:25,360 --> 00:47:26,800
and then you don't need to worry about

1038
00:47:26,800 --> 00:47:28,200
to get the stuff you're forgetting,

1039
00:47:28,200 --> 00:47:29,160
quite the contrary,

1040
00:47:29,160 --> 00:47:32,320
you can harvest synergistically information

1041
00:47:32,320 --> 00:47:33,320
from different tasks.

1042
00:47:34,240 --> 00:47:36,640
Got it, got it, got it.

1043
00:47:36,640 --> 00:47:38,000
Very good.

1044
00:47:38,000 --> 00:47:41,440
What else is your team excited about?

1045
00:47:41,440 --> 00:47:43,600
What else are you focusing on?

1046
00:47:43,600 --> 00:47:46,440
Or, you know, speaking broadly about,

1047
00:47:46,440 --> 00:47:49,440
speaking broadly about the field,

1048
00:47:49,440 --> 00:47:51,200
you know, what things that we haven't talked about

1049
00:47:51,200 --> 00:47:53,280
or kind of on your mind?

1050
00:47:53,280 --> 00:47:54,120
Yeah.

1051
00:47:54,120 --> 00:47:57,520
So, well, first of all, it's a very exciting time

1052
00:47:57,520 --> 00:48:00,760
because I don't think there's ever been a time,

1053
00:48:01,760 --> 00:48:03,000
certainly not in my memory,

1054
00:48:03,000 --> 00:48:07,320
but when you as a scientist have a chance

1055
00:48:07,320 --> 00:48:11,160
to do things that impact people's life, you know,

1056
00:48:11,160 --> 00:48:12,800
right here right now.

1057
00:48:12,800 --> 00:48:14,480
You know, even if you were a scientist

1058
00:48:14,480 --> 00:48:17,160
at the forefront of research through these waves

1059
00:48:17,160 --> 00:48:18,920
that went through like the semiconductor wave,

1060
00:48:18,920 --> 00:48:22,600
the communication waves, the control waves, the wireless wave,

1061
00:48:22,600 --> 00:48:26,400
you know, the time it passed between ideation

1062
00:48:26,400 --> 00:48:29,160
and realization was in the decades, right?

1063
00:48:30,200 --> 00:48:32,440
But here, you know, we have scientists to join

1064
00:48:32,440 --> 00:48:34,280
past of the PhD and six months later

1065
00:48:34,280 --> 00:48:36,560
that go this in production, you know, it's just unheard of.

1066
00:48:36,560 --> 00:48:38,080
So it's very exciting because of that

1067
00:48:38,080 --> 00:48:39,600
and there's a lot of stuff happening

1068
00:48:40,920 --> 00:48:43,960
so that the entropy is very high.

1069
00:48:45,320 --> 00:48:48,760
I think where we start seeing a lot of excitement

1070
00:48:48,760 --> 00:48:53,760
is where there is cross task and cross model learning

1071
00:48:57,160 --> 00:49:00,280
and, you know, right now we understand data

1072
00:49:00,280 --> 00:49:02,560
and now we finally also understand

1073
00:49:02,560 --> 00:49:04,840
what information is, information is in the data,

1074
00:49:04,840 --> 00:49:06,520
but it's not the data is more.

1075
00:49:06,520 --> 00:49:09,000
And now we don't quite yet what knowledge is,

1076
00:49:09,000 --> 00:49:11,320
but knowledge has something to do with information

1077
00:49:11,320 --> 00:49:13,720
and we're just beginning to shed light on that.

1078
00:49:13,720 --> 00:49:16,400
And hopefully at some point we'll be able to reason

1079
00:49:16,400 --> 00:49:19,640
not in the way in which we say we do reasoning

1080
00:49:19,640 --> 00:49:21,720
in artificial systems, but really,

1081
00:49:21,720 --> 00:49:23,400
he ways that allows us to interact naturally

1082
00:49:23,400 --> 00:49:26,280
with the environment and naturally with physical space

1083
00:49:26,280 --> 00:49:29,760
and naturally I'm with machines between different machines.

1084
00:49:31,520 --> 00:49:34,000
And so being of US again is a little bit of a luxury

1085
00:49:34,000 --> 00:49:37,840
in the sense that, you know, when we joined,

1086
00:49:37,840 --> 00:49:39,000
we had mechanical turf.

1087
00:49:39,000 --> 00:49:43,600
Mechanical turf was one of the culprits for the AI revolution.

1088
00:49:43,600 --> 00:49:46,600
And there's A to I, which blends, you know,

1089
00:49:46,600 --> 00:49:48,760
artificial systems with humans.

1090
00:49:48,760 --> 00:49:51,680
And so all the pieces are there and, you know,

1091
00:49:51,680 --> 00:49:53,400
it's up to the leadership of individuals

1092
00:49:53,400 --> 00:49:56,160
to go and find the connections and make things happen,

1093
00:49:56,160 --> 00:49:57,880
which is quite exciting.

1094
00:49:57,880 --> 00:49:59,560
Yeah, yeah.

1095
00:49:59,560 --> 00:50:02,200
Well, Stefano, thanks so much for taking the time

1096
00:50:02,200 --> 00:50:06,440
to join us and talk through some of what you're working on.

1097
00:50:06,440 --> 00:50:10,720
Very, very interesting stuff and enjoy the conversation.

1098
00:50:10,720 --> 00:50:12,000
My pleasure, I enjoy this one.

1099
00:50:12,000 --> 00:50:13,160
Thank you.

1100
00:50:13,160 --> 00:50:14,000
Thank you.

