Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Hey what's up everyone, this is Sam.
It's been nearly a month since we closed the books on our very first conference, Twimal
Con AI Platforms.
I really hope you've enjoyed the content we've shared from the event.
We went into the conference with three main goals, first to provide attendees with great
content focused on the many ways, organizations of all types are automating, accelerating
and scaling machine learning and AI.
Thanks to an incredible roster of speakers, we were able to deliver just that.
Next, we wanted to bring the extended Twimal community together and the results here
were truly amazing.
I had the great fortune of meeting and conversing with a ton of wonderful people, including
longtime Twimal listeners, former podcast guests and attendees new to the Twimalverse.
One thing that I heard universally was how open and engaged everyone was and how much
folks were enjoying the substantive meaningful conversations they were having with other
attendees.
To me, that sounds like a mission accomplished.
Finally, TwimalCon was also an opportunity to celebrate all that we've been able to
accomplish together as a community.
This includes the many innovations that our speakers are presenting on in conference
breakouts, as well as Twimal community achievements, like reaching our third year, 300th episode
and 5 million download in the last few months.
To be able to share these milestones with so many awesome friends, new and old was an
amazing experience for all of us.
I wanted to take this opportunity to thank each of you in our community for all of the
many ways that you support and engage with us.
Thank you so much.
And now on to the show.
All right, everyone.
I am on the line with Archena Venkatraman.
Archena is the John C Malone Assistant Professor of Electrical and Computer Engineering at
John Hopkins University.
Archena, welcome to the Twimal AI podcast.
Thank you, Sam, and thank you for inviting me to be here today.
I'm really excited about our conversation, and we'll jump right in by having you share
a little bit of your background.
It sounds like you had maybe more clarity than most starting from as early as the fourth
grade.
Yes, so both my parents are engineers.
My mom is a professor of electrical engineering, and my dad is a professor of mechanical engineering.
So as you can imagine, we had a very strong educational focus growing up.
And I decided fairly early on, engineering was very cool.
It was a way of solving problems, and I was very good in math and science, and I enjoyed
them.
And so around fourth grade is when I asked my mom about colleges, and where to go to
college, and what were good schools, and she said that the best school was MIT.
And I checked, and it was very close to home, which in the fourth grade was important to
me.
So I decided I would go to MIT, and so I worked through middle school, high school, learning
lots of science, and doing many extracurricular activities, and finally did end up going to
MIT for undergrad.
And I liked it so much.
I stayed there for masters and PhD.
And through that educational pathway at MIT, I started in electrical engineering.
I explored different areas, so I did research experiences in devices, in nanofabrication.
And through those research experiences and coursework, I realized I loved signal processing.
So that was the area that really gravitated towards me.
I loved the idea of transforming signals into various domains to understand different
properties of being able to estimate properties and characterize uncertainty.
And so that was my concentration in undergraduate, and I did a master's with Alan Oppenheim,
who is one of the fathers of modern day signal processing.
And we looked at different signal representations.
And I found that a very worthwhile experience and definitely helped me build fundamentals.
And at the same time, I always was missing an application to it.
And that's how I stumbled upon my PhD direction, which was loosely medical imaging.
So sort of translating these properties of informatics and data science into understanding
brain functionality through functional neuroimaging data, FMRI.
And at the same time, still exploring and learning more and applying ideas from machine learning
to these data streams and two different clinical populations.
And that's kind of how my career has evolved.
And I'm really enjoying the ride so far.
And do you have a more pointed focus currently at Johns Hopkins, or are you looking out broadly
around the intersection of these areas?
So I think my research right now is very broad.
So the most general characterization is that we are developing new machine learning tools
and frameworks and algorithms to better understand and potentially treat neurological and psychiatric
disorders.
But in the space, in the neuro space, this is a very broad spectrum.
So it ranges from basic science type questions where the goal is biomarker discovery and exploration
through the ideas of clinical translation.
So how do we provide information that might be helpful or actionable to even very far
out their explorations of trying to understand perception and being able to alter perception,
again, using different data streams and machine learning tools.
And I would say throughout the spectrum, everything is very different.
The disorders that we work on are different.
The data sets that we use are different and then our modeling approaches are different.
But they still have that flavor of kind of machine learning.
So how much can you mind from the data?
What can you understand and what can you formulate and pass on as information?
Let's maybe dig into one or two of those areas to get a sense for the way you're able
to apply machine learning.
You mentioned basic science is one of the foundational areas and you're looking at things like
biomarker discovery, how do MLM AI play into that?
So one of the kind of the fields or disciplines that I've been heavily involved with is the
branch of, I guess, computational neuroscience known as connectivity.
So at a high level, it's treating the brain as a network of interconnected parts and
not only looking at specific functionality of a region, but also considering that regions
communicate with each other in different ways and trying to build models based on those
communication patterns.
So the data that we use is called resting state fMRI data.
So unlike a conventional MRI acquisition where you would have the subject perform a certain
task in the scanner, resting state differs in that it's a passive acquisition.
So the subject is just instructed to lie there quietly, passively, and oftentimes just
fixate on a crosshair.
And what people have shown over the last decade or so is that the correlation patterns
in this kind of steady state signal, they actually reflect different functional systems
in the brain.
So they tell us about communication patterns that are relevant in terms of cognitive functionality
and biological functionality.
And so based on this data, there have been a lot of sort of work of trying to formulate
machine learning questions and they typically involve predictions.
So can we predict based on this functional connectivity data, which subjects have a neurological
disorder in which don't?
And as sort of in conjunction with that prediction, can we identify different co-activation patterns
or connectivity patterns in the brain that are sort of predictive of eventual diagnosis?
And so that is kind of the general area.
Now the work that we're doing right now is going one step beyond just a simple binary
classification, so prediction of case versus controls.
And we're trying to say, can we from this FMI data predict clinical severity of a particular
patient?
And we've the data that we've been using and our focus thus far has been on autism spectrum
disorder.
And the reason that sort of this continuous prediction task is relevant is because in
any neuropsychiatric disorder, you just have a range of different behavioral characteristics,
different symptoms, severity, different ability to respond to various interventions.
And so building these predictive models, first of all, it allows you just in a black
box sense to figure out what are strengths and weaknesses of different patients in your cohort.
And also incorporating these ideas of biomarker discovery, if you can emphasize or if you
can pick out or extract certain patterns in the brain that are relevant for understanding
clinical severity or different manifestations, it might give other researchers a clue as
to how do we develop better behavioral therapies, how do we evaluate behavioral therapies in
terms of observing these patterns and watching as they grow in fade.
And potentially, how do we develop new therapies based on other imaging techniques or other
modalities such as drug discovery or electrostimulation, et cetera?
How far along are you in this line of research?
So this, again, it's very much at the level of basic exploration.
So right now, we have developed frameworks.
So they're, again, new machine learning frameworks that couple this discovery component, which
is interpretable, which we're using a dictionary learning type framework with the predictive
modeling.
So just at a high level, a regression type framework.
And so we've been able to, unlike other methods that we've seen and other methods that
we've tried, we're able to predict severity to some extent, and we're even able to predict
multi-score severity.
So if you are looking at different quantifications of the patient, so being able to simultaneously
understand those, with that said, I mean, there's certainly a prediction error, which is
one direction we're moving in.
And I think the way to tackle this is another aspect that machine learning is very good at,
which is integrating data across different data sets.
And so now we're trying to bring in other imaging types to get a more comprehensive picture
of brain functionality or brain communication or connectivity, hopefully to improve that
moving forward.
But at the same time, our predictive models right now, they're performing at what is currently
state of the art in the field.
And at the same time, we're able to preserve that interpretability.
And so we're finding patterns and interactions that are sort of interpretable from the autism
standpoint and could eventually provide some sort of biomarker that might be meaningful.
You mentioned that the models that you're using are state of the art, are there well-established
benchmarks for these types of problems?
In this space, so in the neuro space and in the functional connectomics space, I think
one of the challenges is that we don't have very good benchmarks, both in terms of methodology
and in terms of data.
And so this is one of the reasons why the kind of neuro space and especially functional
neuro imaging is unlike other areas where AI or machine learning has made very rapid
advancement.
So if you think about computer vision and image recognition and activity recognition, there
are vast open source data sets, some millions and millions of examples, and here we are developing
machine learning frameworks that essentially have to learn from tens of examples.
And tens of examples to learn very complex functions means that sort of out of the box
algorithms tend to fail.
And so it's a lot more about building structured assumptions into the model in part to reduce
the parameter space and in part to try and guide what you think is reasonable around all
of the noise in the data itself.
I'd love to dig a little bit deeper into the models and this framework.
And when you say models in framework, are you using those interchangeably more or less
or did they have very distinct meanings for you?
No, I use them interchangeably.
Okay, and so you mentioned that the framework is, you said dictionary layered, and I
missed, I think I missed a word there, dictionary learning, dictionary learning.
And so can you elaborate on how you're using the elements of the framework to incorporate
in the operating knowledge about the relationships between the system level knowledge that you
mentioned?
So the way we are incorporating or structuring the model, it's in essence, we, so the dictionary
learning component, or you can think of it as a basis expansion component, essentially
what it assumes is that the kind of global connectivity pattern that we observe, which
in our case is an input positive semi-definite correlation matrix, it is explained or it can
be represented by a sparse collection of what we call elementary subnetworks.
So these are canonical patterns of co-activation across the brain.
You mentioned positive semi-definite connection matrix.
This is your sparse matrix of connections between these different, and what level are
you looking at?
Are these connections between neurons or larger structures in the brain?
So the correlation matrix, it's a Pearson correlation matrix.
The dimensionality is region, so region in the brain.
So if you think of the brain, you can parsulate it into different regions, and there are standard
anatomical atlases that capture specific structures, and so they try to parsulate in a biologically
meaningful fashion.
So the benefit of region parsulation is that you reduce the amount of noise that's there
if you take individual voxels, which are kind of the smallest unit of volume, very analogous
to pixels and an image.
So at the voxel level, this data is very, very noisy and very variable.
So by kind of averaging across a slightly larger region, you tend to reduce some of that
random noise.
So once you've done that parsulation, you essentially have in this FMRI data, a time course or a
signal from every region.
And so you can compute a correlation matrix where every element of the correlation matrix
is just the correlation between the time course at one region and the time course at another
region.
And so in this sort of functional connectivity, brain connectivity space, that's a standard
input that people use into modeling frameworks.
And it's kind of the the elementary unit of information that we tend to extract from
these resting state FMRI data.
Are these connectivity matrices?
Do you get them as a essentially a time series of these connectivity matrices?
So there has been a little work in looking at dynamic evolution.
The most common approach is to kind of compute a single correlation matrix across the entire
acquisition.
So the acquisition is about six minutes long.
So if you think of the sort of N by N region by region as the dimensionality of this correlation
matrix, that's the input to the model.
And so we assume that input can be represented by sort of a low-rankty composition of elementary
subnetwork.
So a subnetwork you can think of as a pattern of co-activation across the brain.
So almost like a heat map of which regions are co-activating or highly correlated with
each other in which regions are anticorrelated with each other.
And so that is kind of the structured assumption.
One of the structured assumptions on the model.
The second assumption is that what is differing on an individual level is the contribution
of those subnetworks to kind of create an entire brain connectivity or entire brain functionality.
And so because those contributions, so not only are they salient for the data representation,
so those are the features that we are using in the predictive modeling.
And so there's a coupling between the data representation and the predictive modeling.
And so in the predictive modeling, what we're trying to do is actually predict some measure
of clinical severity.
So in autism, there are kind of different batteries that you can use to quantify clinical
severity.
The most common is called the Autism Diagnostic Observation Schedule.
So it's typically administered on children.
And it's like a clinician evaluation where they kind of create settings for the child
to play and the child to describe sort of different stories and to observe different
sort of movie clips and explain what they think is happening.
And so there's kind of a standard battery that is used to come up with a level of severity
or level of deficit for autism.
And so your model is making predictions into the space of this diagnostic system.
And what does that look like, is that a kind of a one-to-ten numeric scale, or is it multi-dimensional
across different types of behaviors or expressions of the disorder?
The ADOS exam, so typically you would use the kind of the total measure that the clinician
quantifies across this behavioral paradigm and get a single number.
And it ranges from approximately zero to 30, just based on how it's designed.
There are other diagnostic criteria and they were also using and also, and they provide
a little bit different perspective of the patient.
So the second one is called the Social Responsiveness Scale.
So this is actually a parent report.
So instead of observing the child, you give a questionnaire to a parent or a caregiver
or a teacher about the behavior of the child.
And that's a different way of scoring in terms of clinical manifestation.
And then we're also using another type of behavioral paradigm that was developed by our, is commonly
used by our collaborators at the Kennedy Krieger Institute.
And so this is a, it's a behavioral paradigm that essentially measures the ability of autistic
children to perform gestures on command and also to imitate gestures.
And so one of the interesting observations of children with autism is that in addition
to the well-known social issues, there are also issues with visual and motor systems.
So visual and motor integration.
So loosely hand-eye coordination is one of them ability to imitate gestures is another.
And they, that type of deficit almost parallels the social dysfunction.
So that's a different scale that we're also trying to predict as well.
You mentioned earlier that the frameworks that you've developed achieve state-of-the-art
performance, is that relative to other machine learning approaches?
Or is this a scenario where you're comparing the result of your system to a clinician's
ability to make predictions?
I don't imagine clinicians are looking at FMRI data and trying to predict autism or
am I wrong there?
You are correct.
Clinicians do not use FMRI data to predict autism.
So in most neuropsychiatric disorders, the prediction is, well, the diagnosis is based
on behavioral information and behavioral testing.
So when I say state-of-the-art, what I'm trying to convey is just in relation to other machine
learning algorithms.
So on this data set, we have implemented a variety of algorithms from kind of basic regression
models, which are very, very early iterations of machine learning through other kind of
regression, well, sort of, so-port vector regression, random forests, which are a little
bit more current, and we've even tried end-to-end deep learning approaches.
And so in comparison to that spectrum, at least on our data set, we have found this kind
of hybrid approach where we're looking, we're sort of coupling a data representation,
as well as the predictive modeling to essentially perform the best.
Okay.
And with the advantage that you retain some degree of interpretability, which you may
sacrifice in the end-to-end deep learning space.
Yes.
And so the work that you're doing, looking at predicting autism here, this is just one
of the many disorders that you have looked at in your research, and another one is epilepsy.
Can you talk a little bit about that work?
Sure.
So the epilepsy project, it's a little distinct from what we're trying to do with autism,
and that the work with epilepsy is closer to clinical translation.
So here, our goal is to actually provide information that would be relevant to clinicians
as they're treating these patients.
And so just to kind of give you a background on the application itself, so epilepsy is
one of the most common neurological disorders.
And so in general, the first line of defense is medication, and there are a variety of
anti-aplectic drugs that have been developed.
However, it's estimated that 20 to 40% of patients don't respond to medication in the
sense that they'll continue to have seizures.
And this is really our target cohort.
And so for these patients that are called medically refractory, so they don't respond to these
epileptic drugs, there's very limited alternatives.
And it turns out the best alternative that we have right now is if we can identify, and
if we can trace the seizures to a specific region in the brain that's triggering them,
then clinicians can go in and surgically remove that part of the brain.
And currently, that's kind of state of the art in terms of care, and that will probably
have the best likelihood of the patient recovering in terms of their seizures being alleviated.
And so our focus is on this realm of seizure detection and localization.
So detecting when a seizure occurs through sort of time series measurements and also being
able to localize what is the general area of the brain and what is a specific area of
the brain that the seizures might be coming from, so that again, once we have that target,
it can be acted upon.
And so we're using a variety of machine learning tools and developing new frameworks to be
able to look at non-invasive data, so data collected from EEG and from MRI in order
to provide that localization information to clinicians.
Can you talk a little bit about some of the models that you've developed in some more
detail?
Sure.
So much of the work, especially in the last few years, has focused on EEG data.
So EEG and Zophography is probably the first type of data that's collected for patients
when they go into a hospital.
And here they will actually be admitted into an epilepsy monitoring unit and these EEGs,
so they're kind of sensors that are adhered to the scalp externally.
They're placed on the patient and then the patient is monitored over several days.
So that when they have seizures, clinicians can record that activity happening.
And so currently the state of the sort of standard of care is actually to identify seizures
both detect and localize by eye.
So essentially a clinician will view all of these signals kind of on a computer screen
and scroll through them and then try and visually identify markers of a seizure.
So when it starts and also kind of based on when it starts trying to trace where they
think it's happening, it's originating in the brain.
From the EEG meaning they're looking at which of the leads, is it as simple as which
of the leads is closest to a region that is likely to be functioning in this way?
It's very similar, it's kind of which of the leads are manifesting these particular
signatures that they've been trained to recognize.
And so as you can imagine this is very time consuming, it's prone to human error.
And so just one very kind of on the surface simple task is just developing machine learning
algorithms that can do this automated detection.
So essentially augmenting or replacing kind of what they're doing currently.
And it turns out this is a very challenging problem.
And it's challenging because EEG data is extremely noisy.
And in fact the noise in EEG data tends to overwhelm the signal in terms of the particular
seizure signatures and the particular seizure signatures look a lot like baseline EEG.
So if you or I were to look at these EEG recordings, we would not be able to tell at all what
the origin of the seizure is.
It's kind of years and years of training and sort of pattern matching and building up
models in their heads over kind of what particular features are we looking for and these features
might change from patient to patient, etc.
And so the way we're approaching it is to recognize that just at instantaneous points
in time this data is extremely noisy and we might not be able to get a good detection.
So kind of treat it as a temporal process and essentially to recognize that perhaps
the spreading of these seizure activity or these abnormal activities are just as meaningful
for identifying where and when the seizure is starting.
So if a seizure is starting in a particular area of the brain, so that'll correspond to
a certain area of EEG electrodes.
We expect that that activity will spread locally before it just jumps randomly to another
area.
And so by modeling this kind of local spreading pattern and then inferring kind of where
and when it starts from the data, we can potentially do a better job of both detection
and localization.
So the frameworks we're using here are based on probabilistic graphical models.
And so roughly these probabilistic graphical models allow you to specify sort of latent
or hidden random variables and they capture unoperable phenomenon in our case, the spreading
of the seizure activity.
And then there's observed variables which are related to the statistics of your data
or the features of your data that you're interested in.
And so sort of at a high level, these are, this is the framework that we're looking at.
And we're actually embedding some deep learning into this because the latent variables, they
give us interpretability because what we really care about is the progression of a seizure
and backtracking the onset of seizure activity.
And at the same time, we have a lot of EEG data and so we're training deep neural networks
or artificial neural networks as a complex likelihood function.
So being able to mine different patterns from the data and kind of feed that into the more
interpretable element of the model.
Interesting.
It sounds like a super challenging problem.
And one of the things that jumps out at me is we're often benchmarking or training on
ground truth data and I wonder, how do we know how accurate the physicians are, right?
And they're in the sense of, you know, they are making decisions based on these very
noisy signals, the same noisy signals that you have to deal with.
And they make a decision to take some action, maybe remove part of someone's brain.
But how well do we know if at all how right they were in that it strikes me as just very
difficult to kind of localize from EEG data to a specific part of a brain that's not
acting correctly?
So that's a fantastic question.
So I think there are many layers to that question.
So in terms of clinician accuracy, it's actually unclear how accurate they are.
So they do have a lot more information than just the EEG.
They have the patient history, patient behavior during a seizure is actually supposed to
be fairly relevant in terms of likely areas that the seizure might start.
Oftentimes they have MRI data, so a neuro radiologist can go through it and look for just
structural abnormalities.
I don't know and I haven't come across a systematic study that kind of quantifies asking a variety
of experts across a variety of institutions and then try and understand kind of coordinates
between them.
One interesting statistic is that if you look at meta reviews, long-term seizure freedom,
so postoperative seizure freedom, so if they actually go in and remove kind of a part
of the brain, it's not dramatically high, so after, so if you're looking at five years
seizure freedom rates, it's only about 50%.
Okay.
Right.
So it's, again, there's many factors that could influence the 50%, so it could be that
that particular patient develops some other type of lesion or problem and that took over,
or it could be that there was an error in the care pathway.
So maybe that patient should never have gone for surgery because it turns out there are
multiple areas of the brain that were triggering seizures and that was missed during the review,
or maybe the initial localization was incorrect and so the incorrect portion of the brain was
removed.
And so I think it's unclear which of those factors are involved and hopefully by inserting
some machine learning into that process, it's not that we would replace clinicians because
I don't think our algorithms are nearly at that point yet, but we might be able to provide
information and if we, if the algorithm identifies kind of contradictory information to what
the initial evaluation was, it'll allow the clinician to go back and really focus on
that other area and say, do I really think there's a problem there or is there something
I might have missed?
Maybe we should check it out with another type of data modality.
And are there, are you or are there folks doing multimodal models here?
It strikes me that, you know, in your description of the resources that the physicians have
access to to try to make a prediction, it's much more than just, you know, the time series
date off of an EEG leaf, there's a lot more that they're looking at.
And so it would ultimately we'd want models to be able to incorporate more of that data
as well as that happening in the research.
So that is exactly where my research is headed and so the work that we're doing in the lab.
So we've been focusing on EEG, but just because it's more readily available.
And we recently received some internal funding, a competitive internal funding award to acquire
a multimodal MRI data for some of these patients.
And so some of the work that I had done in my postdoc a while back was, well, showed that
the resting state of MRI that I had mentioned earlier for the autism project that might actually
be useful as another biomarker of seizure origin, especially in cases where there is no
obvious physical lesion that you can see in the brain.
And so by incorporating this resting state of MRI information, structural MR, we're hoping
we can get a finer grain picture.
And then again, once you have models in different modalities, you can look at concordance between
them to first identify is this a reasonable surgical candidate and also what is the potential
area where is most likely the seizure local localization area.
So we've talked about some of the basic science work you're doing, the epilepsy work as
an example of how you're trying to translate to treatment.
You also mentioned work around perception, what do you mean there?
So this is an interesting project.
It started as kind of a pet project idea of mine, but I guess we've tried to pursue it.
So the sort of at a high level, what the project is trying to do is manipulate emotional
cues in human speech.
The way this project idea came about is actually a lot of the work that I had done on autism.
So my postdoc at Yale focused almost exclusively on autism.
Again, it was more through imaging, but at the same time, I was thinking that one of
the one of the hallmark features of autism is that the patients have difficulty perceiving
social and emotional cues and that's particularly true in verbal.
So in language language domain, and so what if we could take speech and amplify emotional
cues to the point where an individual with autism could understand them readily, right?
So if it's very exaggerated, it's at least high functioning individuals don't tend to have
a problem.
It's when it's more subtle that they tend to differ in terms of their perception relative
to their peers who don't have autism.
So if we could do that amplification, and if we could do it computationally, maybe we
can use it as either way to study autism or as an assistive technology, right?
And by doing it computationally, you also have the benefit that you can start slowly undoing
it, right?
So you can think about doing a really well, so a really exaggerated emotion amplification
and then over time reducing it or over the course of an experiment reducing it and trying
sort of quantify at the level an individual can perceive it.
It turns out that no one knows how to amplify emotional cues and speech, it's an unsolved
problem.
I was going to ask that in order to be able to do this amplification based on, you know,
with a dial that adjusts to the level of requirement, we need to be able to do it at all
and I have not seen that.
I think we're making a lot of progress on the, you know, detecting emotions via, you
know, facial image data and speech data, but I've not seen much in terms of kind of
modulating steady state speech data to kind of add that kind of inflection.
Yes, it's an unsolved problem and it's actually not a commonly studied problem.
I would say emotion from speech is hard in general, so even emotion recognition from speech
is quite difficult without, without the video data.
I mean, accuracy is tend to be fairly low and then synthesis or this amplification process
is even more challenging.
So essentially, well, in order to actually implement the project idea, we have to figure
out how to manipulate emotions in speech and so that's one area that we're working
on and in order to manipulate emotions in speech, we need a data and it turns out there
are not a lot of parallel data sets, especially for English, for other languages, I think
there exists a few, but for English and by parallel, I mean to say that if you want to
learn a modulation function, you do want examples of consistent sentences and consistent
actors, right?
So you want the same person saying the same thing in different emotions to be able to
learn a modulation function.
And so with a very, very talented undergraduate student and a very talented graduate student,
we collected this data, so we hired actors from the Baltimore area to come in and read different
things with different emotions.
And we have been developing AI frameworks to try and do this emotion morphing process,
which is what we call it, this modulation process.
And I think we have some preliminary success and we're hoping to build off of it, again,
with integrating these model based frameworks and deep learning strategies and it's been
very interesting learning about this other field and this other type of data which I've
never worked with before and what the sort of tricks and subtleties are.
Have you published the data set?
So yes, so we wrote an initial paper on the data set itself and that appeared in your
speech a few months or last month, I'm sorry, and the data is available.
So it's on my lab website, there's a link where you essentially have to fill out a Google
form or will attach to Google form and then once that's done, we'll send you a download
link to actually download the data and play around with it.
Got it.
And what's the scope of the data set?
How many samples across, how many kind of neutral utterances?
So this initial data collection, it was a little small scale.
So what we focused on were very short utterances.
So single words, multi words phrases and then just a simple noun for predicate sentence
structure because of course linguistics play a role in emotion perception and I think
once you get very complicated, the number of linguistic configurations gets to be very
high.
So it's short utterances.
So there are 10 actors I believe and sort of 250 utterances in five different emotions.
So it's a total about six hours if you just sum up the audio clips.
And what's the approach to modulation?
Are you using something that you might use to synthesize speech like a wave net or something
along those lines or are you doing more traditional signal processing?
So it's a blend between the two.
So wave net is its outstanding tool, but the domain it's looking at is text to speech synthesis.
So essentially given text, it'll output speech.
And it'll output speech in this wave net voice essentially.
The problem that we're focusing on is kind of inputting speech and outputting speech.
And so we've tried a couple of different techniques.
So essentially the input speech you can decompose into kind of a pitch contour, a spectrogram
and a periodicity signal.
So it's a standard analysis pipeline.
And so what controls emotion tends to be intonation.
So in terms of perception and the signal that has a greatest role in intonation is the
pitch contour.
So how pitch varies as someone is speaking a word or a phrase.
And so we've tried, so right now we're targeting this pitch contour as kind of this low-dimensional
feature representation that can help us do this emotion manipulation.
And so we've tried a couple of different things.
We've just tried end-to-end pitch prediction.
So inputting a pitch and then for a given emotion, just outputting a pitch value for that
particular frame.
And then we're right now trying kind of a combination of a model-based approach that's
based on sort of registration or aligning signals using differential geometry or exponential
mapping.
And then that framework itself, we need to be able to predict the parameters of that
warping or that registration so that we're using some deep learning approaches to do the
prediction.
And so the combination of the two is quite interesting and we're sort of, again, trying
to go further in terms of the analysis to get a more consistent emotion warping process.
This is a great introduction to some of the work you're doing there across what sounds
like a very broad array of projects, so hats off to you.
Thank you.
I guess I'm curious, your thoughts on kind of where you see this all going.
So I think it depends on the project.
I mean, at a high level, we want to make everything work better than it currently does.
So again, in terms of the basic science, to be able to do better, more targeted predictions
with multimodal data.
So in that same vein, I have some projects, or I have another project on sort of understanding
imaging and genetic interactions.
So and this is for schizophrenia, so being able to identify genetic markers that relate
to imaging and also are predictive of diagnosis.
And so kind of in that basic science realm to again, make the models more robust, more
generalizable across data sets to extract biomarkers and potentially with these collaborations.
As additional data comes in validated on sort of validate the models and algorithms on
those so that they can be broadly disseminated as tools.
I think on the translational side, we certainly want to do a better job of localization.
So we just published probably the first method that kind of can take scalp EEG and then in
many cases just simultaneously detect and localize the seizures.
But again, our accuracy is not as high as it needs to be for clinical translation.
And so to understand what sorts of other information can we put in there, can we use more data
to get a better predictive model.
And also to again see how this performs in a prospective fashion.
And at the same time, look at multimodal MR as another non-invasive conglomeration of
modalities and improve the prediction and be able to track that and validate that the
outcome actually is reasonable.
And then on the kind of the speech front, I think we have some preliminary success in
terms of being able to do this motion.
So emotion warping, so both quantitatively sort of how well do we predict what that emotional
pitch contour looks like.
We've done some qualitative experiments where we've reconstructed the speech and used
Amazon Mechanical Turk to see to have people rate the speech utterances.
But we'd like to go farther than that.
So definitely improve our ability to create emotions and to manipulate emotions also to
be able to generalize to longer phrases and do this in a more real time fashion.
So I think that's the short term and then the long term is in epilepsy, being able
to create an automated pipeline for patients to come in and be diagnosed, sort of plan out
best therapeutic strategy and essentially improve outcomes.
And then for the speech to actually use it to study different neuropsychiatric conditions
and potentially combine them with other assistive technologies.
Great.
Well, thanks so much for taking the time to share all of that with us.
No, thank you again for inviting me and having me speak here.
All right, everyone.
That's our show for today.
To learn more about today's show, including our guests, visit twomelai.com.
If you missed twomelcon or want to share what you learned with your team, be sure to visit
twomelcon.com slash videos for more information about twomelcon video packages.
Thanks so much for listening and catch you next time.
