1
00:00:00,000 --> 00:00:19,000
All right, everyone. I am here with Alison Edinger. Alison is an assistant professor at the University of Chicago.

2
00:00:19,000 --> 00:00:36,000
Alison, welcome to our podcast. Thank you so much. Hi. Hey, I'm looking forward to digging into our conversation. We'll be talking a little bit about your research and computational linguistics and NLP and a bunch of cool things.

3
00:00:36,000 --> 00:00:45,000
But before we dig into those topics, I'd love to have you share a little bit about your background and how you came to work in the field.

4
00:00:45,000 --> 00:00:55,000
Great. Yeah. So I originally came from a background in linguistics and psychology interested in language and humans and how how language works in the brain.

5
00:00:55,000 --> 00:01:20,000
So I worked in a lab, a cognitive neuroscience lab looking at processing of language in the brain for a while. I began my PhD in linguistics with a bit of a focus on psycho linguistics, but fairly early on, I took a strong interest in the promise of computational linguistics as a way both to use methods to explore continue exploring questions about the brain.

6
00:01:20,000 --> 00:01:45,000
But also to ask interesting questions about how to apply puzzles in language to engineering applications and to design a artificial intelligence. And so this was what sent me in this direction of being what I am today, which is simultaneously someone who works on natural language processing and artificial intelligence and also someone who continues to work on modeling of cognitive processes pertaining to language.

7
00:01:45,000 --> 00:01:55,000
So I think folks who have been listening to the show for a while will recognize that is a recurring theme in our conversations.

8
00:01:55,000 --> 00:02:04,000
It comes up from time to time and I always enjoyed this kind of two way street between machine learning and neuroscience.

9
00:02:04,000 --> 00:02:18,000
Yeah, tell us a little bit about your research and how you kind of explore those relationships.

10
00:02:18,000 --> 00:02:44,000
I want to be interdisciplinary just for the sake of being interdisciplinary. You want to make sure that this is making a clear contribution to one or the other of these fields and without deluding those those contributions. And so what I have found to be one of the in terms of contributing to the artificial intelligence, something that I have found to be really a genuinely critical and very valuable source of insight from the neuroscience side has been.

11
00:02:44,000 --> 00:02:57,000
The methods that are already being applied to study the black box that is the brain right so we have a lot of questions about how the brain operates what types of competencies what types of processes and mechanisms underlie the outputs that we're seeing.

12
00:02:57,000 --> 00:03:15,000
So folks working in cognitive neuroscience and cycle linguistics have been designing methods that are controlled in various ways for a long time now to try to address these questions and we're finding now that we have very similar questions about these black box systems that are emerging in artificial intelligence.

13
00:03:15,000 --> 00:03:36,000
So while we do have slightly different questions often in terms of the way that we ask questions about the brain versus about AI, I think there are a lot of really useful things that we can draw methods and insights that we can draw from the work that we do studying humans to improve our ability to assess the competencies of AI.

14
00:03:36,000 --> 00:03:45,000
Can you give us some examples of the way we approach this from a study of the brain perspective.

15
00:03:45,000 --> 00:04:01,000
Two main examples back in 2016 I proposed that we take a method that had been used to study encoding of certain types of meaning information in particular semantic role information who did what to whom in the brain.

16
00:04:01,000 --> 00:04:20,000
This was a method that made use of simple classifiers to test whether a simple classifiers and classification tasks that are designed in a particular way with controls of the syntactic structure of the sentences to abstract away from syntax and target the meaning.

17
00:04:20,000 --> 00:04:38,000
This is used to study how and where semantic role information was encoded in the brain. And I suggested that we take this same type of controlled classification based probing approach and use this sentence sentence embedding sentence vector representations produced by encoders in an LP.

18
00:04:38,000 --> 00:04:48,000
And this this ended up being very similar to what now is quite a popular approach referred to as probing classification based probing there are a variety of ways that we talk about this.

19
00:04:48,000 --> 00:05:06,000
But a key difference in what I proposed there and a key difference in terms of what comes from the brain side is rather than just throwing a bunch of sentences and classification training items at the classifiers one of the key components of the way that this works.

20
00:05:06,000 --> 00:05:25,000
And the key difference in what we're trying to do is to make sure that we are not testing for something other than what we're trying to target and so for instance we have a controlled variety of syntactic structures to make sure that it's there isn't some confounding variable in terms of the syntax that the classifier might be picking up on instead.

21
00:05:25,000 --> 00:05:38,000
The classifier learns to do the task and we conclude from this that the relevant information was encoded in the representations what actually the classifier was just picking up on the fact that in the positive labeled items.

22
00:05:38,000 --> 00:06:02,000
The word is always the and in the negative labeled items, the class, the second word is always cat, then this is a huge confounding variable and it doesn't tell us anything and this is a challenge that we see over and over again with our evaluations, there tend to be additional variables that we didn't intend that the models pick up on that obscure our ability to evaluate what's going on.

23
00:06:02,000 --> 00:06:20,000
This idea of control of confounding variables is something that is very well established within the field of cognitive neuroscience and cycle linguistics maybe a bit less so on the AI side, but it's I think it's something that will be of significant value and it's something that I apply consistently in my own research.

24
00:06:20,000 --> 00:06:47,000
That's a really interesting notion, so on the machine learning and deep learning side, we tend to think of a lot of the challenges we encounter and understanding these models coming from the idea that these models are so great at pattern matching and they can match these patterns that are correlated to inputs as opposed to, you know, the expressing outputs that we're looking for.

25
00:06:47,000 --> 00:06:58,000
It kind of sounds like you're saying that while the brain has aspects of that too and as brain researchers, we have to do a lot of similar types of controls it's not unique to deep learning.

26
00:06:58,000 --> 00:07:12,000
Absolutely yeah that's a really nice connection there it's absolutely the case that the brain also may respond to variables that we didn't intend so often with experiments in the brain we're giving some type of stimulus we're measuring brain activity in response to the stimulus.

27
00:07:12,000 --> 00:07:35,000
We're trying to draw conclusions about how our particular manipulation affected the brain, however, if there are other variables that were influencing the way that the brain responded that we weren't taking into account and absolutely just in the same way that we can with with machine learning and with NLP we can draw conclusions that are mistaken because we didn't realize that these other variables were we're having an effect.

28
00:07:35,000 --> 00:07:56,000
Also in the example that you gave and that problem set up you were applying machine learning classifier to brain was is this signals or you know human questions responses to questions you take us a little bit deeper into the setup there.

29
00:07:56,000 --> 00:08:14,000
So I should clarify that the inspiration was a type of study that was being done it was taking taking brain recordings from humans who were viewing reading sentences and then it was using the classifier to classify on the basis of those recordings.

30
00:08:14,000 --> 00:08:26,000
What what type of sentence the people were viewing specifically what was the identity of the agent who was doing the action versus sometimes what was the identity of the patient.

31
00:08:26,000 --> 00:08:40,000
So this was a study that made quite a splash you know that some really interesting results in terms of identifying sort of how and where the brain may be encoding information about who did what to whom.

32
00:08:40,000 --> 00:08:57,000
Critical meaning information in any you know in in in in linguistic input for me what I was doing was taking sentence encodings from NLP models and treating those much like the brain recordings so you could think you know if we think of the NLP model as a brain it's producing a representation of its input.

33
00:08:57,000 --> 00:09:23,000
But this is a sentence embedding and this was being used as input to the classifier instead and so then the key question is can we successfully train a classifier to do the task if so then we're going to conclude that the critical information for doing the task was extractable from the sentence representation and so this is how we try to draw conclusions about whether this information was encoded at least in an extractable way for a classifier.

34
00:09:23,000 --> 00:09:32,000
So the idea is that the your sensor recordings like a EKG or maybe it was FMRI in the case of.

35
00:09:32,000 --> 00:09:51,000
Okay, you've got these sensor recordings ultimately they they're turned into a vector that has an abstract relationship with what's happening in the brain the embeddings have an abstract relationship with the sentences that you're looking at and you're trying to apply the same technique and see what happens.

36
00:09:51,000 --> 00:10:08,000
Yes, exactly in both cases you have a vector which represents an encoding of some kind of representation that is evoked in response to the input and we're trying to probe to see what types of information we can extract from that what seems to be encoded there.

37
00:10:08,000 --> 00:10:11,000
And what you what you learn with that.

38
00:10:11,000 --> 00:10:39,000
The case of the study that grew out of that particular inspiration and this was a couple of years ago now my focus was on trying to establish the extent to which use sentence encoders were able to systematically capture compositional information about what's going on in the sentence and I focused on two main questions really fundamental types of information in in sentence meaning the first was semantic role directly inspired by the brain study.

39
00:10:39,000 --> 00:10:55,000
It was who did what to whom and then the second one involved negation and seeing to what extent can the model to what extent do the do the representations encode systematic information about what happened and what did not happen you know if you have a.

40
00:10:55,000 --> 00:11:03,000
Bobby went to the store Bobby did not go to the store right negation tells you things about what happened and what did not happen so.

41
00:11:03,000 --> 00:11:22,000
The biggest challenge was to control for just as I've been discussing control for variables that may allow the classifier to do well without it having picked up on the actual meaning information right we don't want to have certain things that are giving clues that the classifier can pick up on.

42
00:11:22,000 --> 00:11:45,000
The classifiers are great at pattern matching if there's some other pattern in there that's going to help and do the task then it will and then we'll learn nothing about what we wanted to ask about and so basically the way that we did this was to generate synthetic data for which we could control the distributions of of all of these different types of variables the key question here is.

43
00:11:45,000 --> 00:12:12,000
Can the classifier label whether word x was the agent of word why so in a sentence like Billy went to the store did Billy do the going to the store or Billy hit Bobby did which one did the hitting you know Billy or Bobby this is the key question and you would think that this is a very simple type of information probably one of the most obvious types of information to be encoded in a sentence representation if it's capturing meaning.

44
00:12:12,000 --> 00:12:35,000
What we don't want is for the model to be able to seem like it's capturing that information when in fact it's only capturing what's likely to happen and not what happened in this current sentence so to give an example we can have a sentence the customer the waitress serve the customer or we could reverse those and have it be the customer serve the waitress the second sentence is very strange statistically it's very unlikely.

45
00:12:35,000 --> 00:13:03,000
It's still a sentence that I can say to you and you can understand an English speaker can understand even if it's an odd sentence and this is the sort of the critical way that compositionality works in language humans can understand sentences even if they are unlikely and this is the place where we may see some really fundamental divergences between models that are trained based on distributions of ideas in text versus models that systematically extract the meaning from any sentence no matter how strange or unlikely it is.

46
00:13:03,000 --> 00:13:32,000
So for this reason we controlled these types of variables such that models for instance that are only based on averaging of word representations and don't have any access to word order are only going to be at chance level performance so this this allowed us to give sort of a reality check to make sure that we're controlling for word level variables that may give clues that don't indicate that because again the waitress serve the customer the customer serve the waitress have identical word content but they mean very different things.

47
00:13:32,000 --> 00:14:01,000
These are the types of controls i'm talking about once we exerted these types of controls basically what we found was the models classifiers trained on the representations produced by the models performed only marginally above chance level performance suggesting that really this was not being captured to the extent that it looks like semantic role is being captured in these representations it's probably more an artifact of the fact that the models pick up on sort of what is statistically likely and those are types of sentences that.

48
00:14:01,000 --> 00:14:04,000
They're likely to encounter when being tested.

49
00:14:04,000 --> 00:14:19,000
Yeah we've talked about on the podcast i think you know what is a very closely related and i'm not sure if it's more specific or if it's a subset or superset but you know the idea that.

50
00:14:19,000 --> 00:14:44,000
In question answering datasets you have models that will pick up on kind of spurious correlations that aren't really inherent to answering the question and here what you're doing in some ways is a subset and that the questions you're answering is about one specific type of semantic relationship but it's also.

51
00:14:44,000 --> 00:15:04,000
As you point out kind of abstracted in a way in that the models don't get as much information and the information that they get masks a specific type of i don't know if you would call it noise or relationship with the word order in particular and so you're.

52
00:15:04,000 --> 00:15:24,000
So you are kind of taking that out of the equation for the model and and you found that the models probably are picking up on on spurious correlations yeah absolutely I can imagine a version of our test we didn't try this in particular but.

53
00:15:24,000 --> 00:15:52,000
You can imagine a version of our test that's less carefully controlled that may show very high accuracies which would you know lead us to believe that the models are great at semantic role and I think this often happens with standard semantic role labeling is this is an existing task but if we don't control for these types of of confounds then we will end up concluding that semantic role for instances solved or some version of this conclusion when in fact.

54
00:15:52,000 --> 00:16:10,000
The systematicity this sort of critical building block of of being able to understand language no matter how odd the sentences that that may well be missing if we don't try to disentangle through the model statistical competency from their ability to systematically extract the representations.

55
00:16:10,000 --> 00:16:34,000
Awesome and so that general line of research is looking at you know taking things we've learned and studying the brain and applying them to machine learning and that's a topic or related to a topic of I clear panel or debate that you're participating in that's focused on this question of how can findings about the brain improve.

56
00:16:34,000 --> 00:16:57,000
AI systems tell us a little bit about the idea with that session great yeah I think this is sort of a for a long time folks have been intrigued by the possibility that we can use inspiration from the brain to improve artificial intelligence is a very reasonable.

57
00:16:57,000 --> 00:17:13,000
The way to think about things because of course the human brain is the only existing system that actually has intelligence certainly the only existing system that does language there are certain types of tasks for artificial intelligence or other types of models can.

58
00:17:13,000 --> 00:17:29,000
It's about perform humans certainly but in terms of sort the fundamental aspects of human intelligence in particular language that I'll focus on language since this is the one that I spend the most time thinking about there's just no question human brain is the only one that can do this at the moment.

59
00:17:29,000 --> 00:17:54,000
This is not to say I typically don't go so far as to say that the human brains solution to language understanding is the only mechanistic solution that could achieve this but given that we haven't yet found an alternative that is able to you know match the human brain it seems quite reasonable that we should go ahead and certainly consider taking inspiration from the brain in terms of our actual design decisions in the way that we.

60
00:17:54,000 --> 00:18:22,000
To choose our mechanisms for for trying to master natural language processing and natural language understanding having said that in practice what we tend to find in my experience is that it is certainly easier said than done to try to take take insights from the brain and apply these to artificial intelligence in a way that will outperform the most successful more general learning mechanisms that.

61
00:18:22,000 --> 00:18:47,000
That typically you occupy the state yard in the field so it's something that people are very interested in I think and I also think that as we continue to understand more about the brain we may find more opportunities to continue to try to apply mechanisms in that way but at the moment it is challenging and we do find that.

62
00:18:47,000 --> 00:19:01,000
The best maybe we have sort of pretty high level analogies to types of things that the brain does that aren't very directly modeled after the actual mechanisms that implement those types of things so.

63
00:19:01,000 --> 00:19:24,000
The way that I have tended to find is sort of a more concrete and immediately applicable way that brain science can contribute to to improving AI is sort of more along the lines of what I've been discussing here which is using the competencies that the brain has as a standard by which we assess AI so the human ability to understand language.

64
00:19:24,000 --> 00:19:37,000
I don't think there's any question that that defines the standard for what it means to understand language for what it should mean to to achieve successful NLU and so.

65
00:19:37,000 --> 00:20:01,000
In terms of designing our evaluations I think it's absolutely critical that we look to what we look to our understanding of human capacity for language when deciding how to assess the quality of our artificial intelligence model so this is how I usually tend to pitch the immediate and essential value of brain science for AI in light of the fact that.

66
00:20:01,000 --> 00:20:11,000
Taking brain mechanisms and directly implementing them to improve AI tends to be a little bit maybe further down the road in terms of showing the type of success that we'd like to see.

67
00:20:11,000 --> 00:20:20,000
So so your money is on evaluation and assessment and not drawing architectural conclusions from the brain.

68
00:20:20,000 --> 00:20:33,000
My money is on that as something that's already happening and and important and in my opinion critical for having effective evaluations of AI it's happening now and I think it needs to happen I am.

69
00:20:33,000 --> 00:20:49,000
Absolutely not discounting the possibility that we can draw really very valuable mechanistic inspiration from the brain in terms of designing our models but it's not clear that that that were quite very yet in terms of outperforming the other approaches that we have by taking that type of approach.

70
00:20:49,000 --> 00:21:09,000
It seems to have stronger opinions and others that you know neural networks and deep learning are you know brain inspired others you know think that that's you know very loose what maybe it's correlated with how intimately you know the brain.

71
00:21:09,000 --> 00:21:30,000
Yeah yeah loosely brain inspired right I mean like very loosely inspired by basic functioning of neurons and whatnot but there are major major differences between the way that it given neural network operates in the way that it's an actual full brain operates so yeah loosely inspired I think is a reasonable characteristic very loosely inspired is a reasonable characterization.

72
00:21:30,000 --> 00:21:38,000
And ideas like spiking neural nets and things like that you don't necessarily see as getting us closer anytime soon.

73
00:21:38,000 --> 00:21:55,000
I mean there are absolutely ways that we can try to more closely simulate the functioning of a brain it's not clear to me that I mean certainly if we ended up recreating an actual brain right there's an end point of this process we recreated a brain.

74
00:21:55,000 --> 00:22:11,000
Then presume and we did it down to the smallest unit right and and we've just completely replicated all of the machinery of the brain then and functioning and all that then presumably now we would be able to do everything.

75
00:22:11,000 --> 00:22:33,000
But there's a giant giant space in terms of our capability and what's realistic and feasible at the moment between you know current neural nets and you know baby steps trying to slightly more resemble the brain versus rapidly replicating a brain and so sure at the end point of that process certainly we would you successfully.

76
00:22:33,000 --> 00:22:53,000
We managed to improve AI because we would have replicated the human brain and the human brain out performs AI but at the moment again I think when we take those steps we may in certain ways in sort of focused areas better resemble the brain but in terms of the way that we have set up tasks.

77
00:22:53,000 --> 00:23:07,000
On the engineering site at this time I suspect that making those changes would end up for practical purposes reducing performance rather than improving performance now of course we need to consider whether these improvements and reductions in performance are really.

78
00:23:07,000 --> 00:23:19,000
Being assessed in terms of measuring the right thing and in many cases probably not we already know there are many flaws with our evaluations and this is you know why I think this is an interesting critical problem at the moment in the field.

79
00:23:19,000 --> 00:23:43,000
But these are you know there's no getting around the fact that these are the benchmarks that we use at the moment and there are many other you know in many ways those performance metrics are at least informative if imperfect so yes based on the benchmarks that we have currently I suspect that those for now taking these types of baby steps to make the networks more brain like.

80
00:23:43,000 --> 00:23:52,000
Maybe way down the line could be useful but in terms of the intermediate steps I suspect it would it would be a step backward in terms of the practical.

81
00:23:52,000 --> 00:23:58,000
Performance metrics that we use to measure how what we're doing got it got it.

82
00:23:58,000 --> 00:24:24,000
You know returning to the the assessment and evaluation work it is you know some ways related to the our desire to have greater explainability interpretability understanding of what's happening inside these you know deep neural networks or you know quote unquote black box models.

83
00:24:24,000 --> 00:24:31,000
Is that a field that you think the brain study offers some insights into.

84
00:24:31,000 --> 00:24:44,000
Yeah I consider this work to be squarely within the analysis and interpretability area of NLP absolutely so I think that analysis and evaluation go hand in hand.

85
00:24:44,000 --> 00:25:12,000
Because with our evaluations we're trying to get a clear sense of how good the models are and I think that how good the models are connects quite importantly to the models competence with specific types of phenomena which often is what the analysis society is doing is trying to understand what the models have have managed to do of course that's not the only type of thing this interpretability worked as but this is the area where I have been primarily focused in trying to.

86
00:25:12,000 --> 00:25:37,000
Better capture and analyze the models competence for meaning yeah absolutely and I I focused so far mostly on this probing work but that's less recent than work that I've been doing studying pretrained language models and using more behavioral types of studies of the brain looking at brain responses to words in context.

87
00:25:37,000 --> 00:25:50,000
And adapting those for diagnostics of the competencies of pretrained language models on the basis of their ability to predict words in context and so let's dig in tell us more.

88
00:25:50,000 --> 00:26:17,000
So this is work that so so again more recently than the sentence encoders we have these pretrained language models that have of course exploded into the scene a couple of years ago and have really pushed the state of the art across most of our tasks quite substantially and the real question that then trails behind this this advance is.

89
00:26:17,000 --> 00:26:23,000
What is it that pre training is giving to these models what is it that they're learning it seems.

90
00:26:23,000 --> 00:26:44,000
The obvious interpretation is that some sort of pretty generalizable linguistic competence has been conferred on these models during the pre training process which then can transfer too many different tasks via fine tuning and so the question if you know to maybe jump in and draw the parallel I think what we kind of want is something analogous to.

91
00:26:44,000 --> 00:26:58,000
The early research and on convolutional neural nets is it oh at this layer you know we're just learning textures and at this layer we're learning shapes and at this layer we're learning colors that kind of thing I guess more recent research said well really it's all textures but.

92
00:26:58,000 --> 00:27:17,000
I think we want some kind of intuitive sense that these models are you know learning something absolutely yeah so this there is this very appealing idea that layer by layer will sort of increasingly abstract increasingly sophisticated information being represented.

93
00:27:17,000 --> 00:27:34,000
And and so the the work that I was was describing there is less focused on examining the contents of the information encoded in individual layers though there is even more recent work that you know does more of that with these more recent models.

94
00:27:34,000 --> 00:28:02,000
So this work takes a separate approach a behaviorally based approach that says all right let's just look at the models behaviors in its in the task for which it was optimized that is word prediction and say all right this is the fairest possible way that we can check what the model learned because if the model is going to show competence in something presumably it's going to be because it needed that information to optimize its objective which you know involves word prediction and so.

95
00:28:02,000 --> 00:28:18,000
The reasoning there is let's examine the model in its absolutely most natural setting let's see what its word predictions can tell us about how much it knows about the context and how much information it uses in context to make a prediction so an example that I often give is.

96
00:28:18,000 --> 00:28:26,000
And this is from another again a brain study and I borrowed it and and adapted it into a diagnostic for language models.

97
00:28:26,000 --> 00:28:39,000
He caught the pass and scored a touchdown there was nothing he loved more than a good game of blank and then we see the models competence and being able to make a prediction here that case is actually one of the easiest items in the in the data set.

98
00:28:39,000 --> 00:29:07,000
Another one that I give as an example which is a bit more opaque I saw that my mother wasn't drinking her soup then I realized that she didn't have a right and then humans can infer what's going on in this situation and can realize that okay she has the soup already so what she's missing presumably as a spoon right but this is quite a lot of inference for a model to handle and so these are sort of difficult types of prediction tasks and I drew on a number of different brain studies that.

99
00:29:07,000 --> 00:29:14,000
Forced the models to use different types of information about the context to make their word predictions so again this is.

100
00:29:14,000 --> 00:29:26,000
Drawing heavily on inspiration from the brain side adapting it in a way that allows us to ask questions in a very natural way about these models on the basis of the type of predictions that they naturally output so.

101
00:29:26,000 --> 00:29:38,000
Yeah so this was one of the the clearest finding the models did pretty well on certain things very well uncertain things probably the starkest finding was with respect to negation.

102
00:29:38,000 --> 00:29:49,000
You have sentences like a Robin is a they're very good at producing a word like bird really good at associating Robin with bird and hammer with tool and things like this but if you add a negation a Robin is not a.

103
00:29:49,000 --> 00:29:59,000
Now the appropriate continuation is just basically anything but bird but the model still prefers bird over something like tree right so.

104
00:29:59,000 --> 00:30:12,000
This is this was the so 100% of the time it fails to assign a higher probability to the true completion tree over the now false completion of bird because it just really wants to associate Robin and bird together presumably so.

105
00:30:12,000 --> 00:30:31,000
So this is the type of thing that I mean when I say we want to try to dig into how much does model really understand the meanings of words and their implications and how they affect the context versus just sort of being a really sophisticated associated pattern detecting device right.

106
00:30:31,000 --> 00:30:52,000
You mentioned in there that these ideas you pulled over from brain science like force the model to be very specific in the I guess the question that it was answering or the way that it was producing the result can you elaborate on the specific techniques or or.

107
00:30:52,000 --> 00:31:15,000
Approaches from the brain science side that you applied in this case sure yeah so in one sense one sense in which I mean that is that these items were all hand designed by psycho linguists to target specific to ask specific types of questions about how and when the brain processes certain types of information.

108
00:31:15,000 --> 00:31:37,000
And another sense in which I mean that and another sense in which I mean that is that in choosing these diagnostics I had a particular criterion which was the following there are a couple of different indices of prediction in humans one is you can give a human a fill in the blank task and just see what words they fill in.

109
00:31:37,000 --> 00:32:04,000
And then if you do this over a group of humans you can derive a measure called close probability which is just the proportion of humans that will fill in a given word in a context so if I say I like my coffee with cream and probably a decent proportion of humans will say sugar probably no humans will say socks and so we're going to have a close probability right of of maybe let's say point six for the for sugar and probably zero for socks.

110
00:32:04,000 --> 00:32:15,000
That is an example that's often used in psycho linguistics so if you have a psycho linguist in your group and maybe it won't be zero but this is you know this is mostly probably no one's going to put socks so.

111
00:32:15,000 --> 00:32:33,000
But another index of sort of the brain's predictive processing is what's known as the n400 component it's measured using EEG and it seems to index something along the lines of fit of an incoming word to context so if you have a very expected word you may see a reduction.

112
00:32:33,000 --> 00:32:59,000
In this in this component whereas if you see something that wasn't expected then you'll see sort of a normal large and 400 component and so this these two different predictive indices often align with each other in terms of indicating to us that the human was expecting a word was coming or predicted a word in that context thought that word was a good fit et cetera.

113
00:32:59,000 --> 00:33:24,000
100 seems less sophisticated sometimes the n400 seems to be missing certain types of information and my specific inspiration in using these studies was to see to because the n400 seems to be a type of component that's probably at least under some circumstances reflecting maybe a slightly shallower statistical processing rather than necessarily processing of all of the possible context information and the meaning.

114
00:33:24,000 --> 00:33:46,000
Basically I want to test whether the these elements these language models were going to pattern with the n400 and look a little more like this less sophisticated predictive brain response or whether they would look like the sort of end point of human processing when the humans have all the time they want and they're doing a fill in the blank task so basically I was pitting these two mechanisms against each other and seeing.

115
00:33:46,000 --> 00:34:01,000
And the studies that I chose were studies that specifically show a divergence between these two so the closed patterns diverge from the n400 patterns suggesting that the n400 is is missing something so the question basically was the information that's the n400 is missing.

116
00:34:01,000 --> 00:34:29,000
Is is Bert going to miss this as well that was the basic approach there and so the diagnostics were very carefully chosen to be sort of especially tricky and to test for whether it was going to be sensitive to these particular types of information and in addition they were designed in such a way to try to control the specific types of information in context that would drive the predictions that humans make in that context so in all these different ways they were controlled.

117
00:34:29,000 --> 00:34:39,000
To to help us to ask more targeted questions about what the models are doing and what were the specific results relative to your expectation with this n400.

118
00:34:39,000 --> 00:34:49,000
Yeah so the comparison to the n400 was slightly complicated and so I I tend not to try to describe the results.

119
00:34:49,000 --> 00:35:10,000
To in a way that's too anchored to the n400 but basically the models seemed the models did not resemble the n400 completely in that they did have reliable differences reliable ability to prefer good continuations to bad continuations and this was the key comparison to the n400 the n400.

120
00:35:10,000 --> 00:35:30,000
So we're talking about things like he got the pass and scored a touchdown there's nothing he enjoyed more than a good game of football is a good continuation here baseball is not a good continuation here.

121
00:35:30,000 --> 00:35:59,000
N400 shows some some favoring of baseball nonetheless in the theories that it's because baseball and football are related to each other so so in this sense but was reliably able in a majority of cases not all cases to prefer the good continuation to the bad continuation but it did not match humans in in it wasn't able to match human closed predictions very well especially in one of the diagnostics where

122
00:35:59,000 --> 00:36:17,000
where the we had role reversals like the waitress or the customer the customer served the waitress it was able to tell the difference between these but it wasn't usually able to predict the correct event so the actual sentence was the restaurant owner forgot which customer the waitress had blank.

123
00:36:17,000 --> 00:36:45,000
So served as a pretty good prediction in this case but the models were not great at matching the top human predictions there the place where the model really did resemble the n400 was in its insensitivity to negation in the fact that it continued to prefer Robin over tree even when that not the negation was in there and that is something that does resemble the patterns that were seen in the corresponding n400 study so so in some ways

124
00:36:45,000 --> 00:36:58,000
it maybe did a little better better quote unquote than the n400 but in other ways it really did show the same types of insensitivities but probably are attributable to similar types of statistical associations that it learned.

125
00:36:58,000 --> 00:37:18,000
I thought one of the directions you were going was going to talk about I guess completions that were valid completions but had some greater degree of surprise or novelty or something along those lines.

126
00:37:18,000 --> 00:37:31,000
Did did I pick up on a spurious signal there well I think you were describing the the mechanism of the n400 and the EEG's.

127
00:37:31,000 --> 00:37:58,000
Yeah well so we have basically two types of tests that were using in these diagnostics one is prediction accuracy so this is saying if we look at the models top completions is it able to match the top human completions so this is one of the measures and so in that sense there is a bit of a there are probably various completions that may be may all be valid but some are better than others and so in a sense

128
00:37:58,000 --> 00:38:13,000
this is what the close probability is being used for us to say okay well what do humans think is actually the best you know continuation here in some cases you will also ask humans for plausibility judgments and say even if this wasn't your top

129
00:38:13,000 --> 00:38:21,000
consideration your top prediction is it an okay completion but that that that wasn't really a consideration in this study.

130
00:38:21,000 --> 00:38:32,000
Another measure that we're using is there are sort of two different completions one is clearly better than the other and the question is can the model tell that one is better than the other so we're just looking at the relative

131
00:38:32,000 --> 00:38:42,000
probabilities and saying doesn't assign a higher probability to the better completion and so in that sense it doesn't matter if it chose either of those as its top completion it's just a matter of whether it

132
00:38:42,000 --> 00:38:56,000
chose the better prefers the better one over the worst one and so we're going from a couple of different angles in terms of assessing looking at the model's word predictions and probabilities over the vocabulary and seeing what

133
00:38:56,000 --> 00:39:01,000
what type of competence were we're detecting on the basis of those predictions.

134
00:39:01,000 --> 00:39:18,000
Very cool. What's next in your research? Yeah so we have I have been continuing with the composition work my student long you has recently been working on looking at two word phrase composition and

135
00:39:18,000 --> 00:39:36,000
transformers and similarly has been finding if you control for word level effects we're really not seeing very strong correspond very strong abilities of the models to align with human judgements of what the meanings of these phrases are and how they relate to one another.

136
00:39:36,000 --> 00:39:49,000
Yeah so I in general what I think is one of the most important problems in the field right now is to improve our ability to successfully and accurately evaluate how well these models are capturing meaning.

137
00:39:49,000 --> 00:40:03,000
I think that meaning is you know there may be some folks who will be less focused on meaning per se just want to focus on dancing tasks but I do think that in the end robust and flexible NOU is going to be dependent on being able to extract generalizable

138
00:40:03,000 --> 00:40:17,000
and accurate meaning information and our ability to tell how well we're doing in terms of moving toward that goal is going to hinge on our ability to evaluate this in a way that teases apart the things that we don't want and the things that we don't count as meaning understanding and the things that we do count.

139
00:40:17,000 --> 00:40:36,000
So at the moment that's that's the direction we're going we're doing a combination of things to try to target things like the ability the models to capture meaning in a compositional way using both you probing methods that look directly at the representations that the models learn using behavioral methods that look at the predictions and all more of a natural setting.

140
00:40:36,000 --> 00:40:48,000
And yeah those are the types of directions we're going to try to answer those questions awesome awesome awesome awesome thanks so much for joining us thank you very much it's been great to chat awesome thank you.

