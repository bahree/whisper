WEBVTT

00:00.000 --> 00:15.840
All right, everyone. I am here with Allison Gopnik. Allison is a professor at the University

00:15.840 --> 00:20.560
of California at Berkeley. Allison, welcome to the Twoma AI podcast.

00:20.560 --> 00:25.880
I'm very happy to be here. I'm really looking forward to our conversation. We will be focusing

00:25.880 --> 00:31.640
in on a presentation that you'll be delivering at this year's NERVS conference focused on

00:31.640 --> 00:38.040
causal learning in children and how that relates to some of the things that we're doing in AI

00:38.040 --> 00:42.760
and deep learning. Before we dive into that though, I'd love to have you share a little bit about

00:42.760 --> 00:48.920
your background and you come at things from a psychology perspective. Tell us a little bit

00:48.920 --> 00:54.280
about your background and how you came to that field. Yeah, so I actually started out my career

00:54.280 --> 00:59.160
in philosophy and I'm still an affiliate in philosophy. So I have sort of three different

00:59.160 --> 01:04.040
hats at Berkeley. I'm in the psychology department and affiliate in philosophy and then part of the

01:04.040 --> 01:10.040
the Bear, the Berkeley AI research group. And the big question that I've really wanted to answer

01:10.040 --> 01:15.720
my whole career is this, how is it that we can know so much about the world around us from so

01:15.720 --> 01:22.120
little information? All we have is the photons in the back of our retinas and the disturbances of

01:22.120 --> 01:28.600
air at our ears and yet people can figure out that the world is full of objects and people and

01:28.600 --> 01:35.400
thoughts and ideas and, you know, quirks and black holes. How is that possible? And of course,

01:35.400 --> 01:39.960
that's the big problem of epistemology and philosophy, which is where I started out. How could

01:39.960 --> 01:44.360
you possibly do that? And it's the big problem of machine learning. It's the sort of central problem

01:44.360 --> 01:51.080
of machine learning. How could we get representations from data? And in a way, the way that the big

01:51.080 --> 01:57.800
problem, I mean, a way of putting this is we seem to have these very powerful abstract structured

01:57.800 --> 02:02.440
representations of the world around us that let us make great generalizations and predictions.

02:02.440 --> 02:07.880
And yet the data that we're getting doesn't seem to is concrete and particular and doesn't seem

02:07.880 --> 02:12.040
to have those characteristics of being abstract and unstructured. So the question is how do we get

02:12.040 --> 02:19.320
there here from there? Well, very early in my career, I realized, look, the people who are doing that

02:19.320 --> 02:23.720
most effectively are actually young children. They're the ones who are going out into the world,

02:23.720 --> 02:29.240
taking what they see in here and the actions they perform and figuring out what the world is like.

02:29.240 --> 02:35.000
So if we want to answer that question, either as philosophers or as people in AI, the people we

02:35.000 --> 02:39.560
should look to, the people who are really solving that problem are young children. And basically,

02:39.560 --> 02:44.280
that's what I've been doing for my whole career is trying to figure out how is it that young children,

02:44.280 --> 02:50.520
you know, two, three, four year olds who don't aren't, you know, don't have PhDs in computer science,

02:51.480 --> 02:56.760
aren't philosophers. Nevertheless, seem to be solving these problems that have really stumped

02:57.880 --> 03:04.520
the smartest minds in philosophy and AI. And about 20 years ago, I started actually collaborating

03:04.520 --> 03:09.080
with people in both philosophy of science and in computer science to try and figure out,

03:09.080 --> 03:13.800
could we say something computationally? What kinds of representations and algorithms could the kids

03:13.800 --> 03:19.960
be using that let them learn as much as they do? And that's basically been the project ever since.

03:19.960 --> 03:24.600
And I think it's interesting that there's been, just in the past few years, there's been this,

03:24.600 --> 03:32.600
this real explosion of interest within AI in trying to look at development and look at

03:33.480 --> 03:39.880
look at children and use that as a clue to solve some of these really tough problems. And I think

03:39.880 --> 03:45.640
it's because, you know, so much of the new work has depended on the idea of designing systems that

03:45.640 --> 03:49.960
learn rather than trying to build things in in the first place. And if you're interested in systems

03:49.960 --> 03:56.040
that learn, kids are really wonderful, probably the best example we have of, of creatures that are

03:56.040 --> 04:01.480
really, really good at learning very accurately from small amounts of data. So that's kind of the

04:01.480 --> 04:08.040
big overarching, that's the big overarching picture about what I've done. Fantastic. It sounds like you

04:08.040 --> 04:13.960
think about the problem of learning in children, you know, broadly at the level of kind of these

04:13.960 --> 04:23.640
broad representations, but your talk at NURPS is focused on a particular aspect of that. And that is

04:23.640 --> 04:29.880
the way that children and for causal relationships and structure in the world. Tell us a little bit

04:29.880 --> 04:36.920
about your talk and your research in that area. Yeah. So if you're asking this question, you know,

04:36.920 --> 04:42.840
how do we get these big abstract, powerful representations of the world? One of the most

04:42.840 --> 04:47.400
important, not the only one, but certainly one of the most important kinds of ways we have of

04:47.400 --> 04:51.320
representing and thinking about the world is thinking about causality, thinking about what makes

04:51.320 --> 04:58.200
what happen. And the first line of work that I did with children was what's come to be called

04:58.200 --> 05:02.680
the theory theory. And that's the idea that children are being building everyday theories of the

05:02.680 --> 05:07.000
world that are a lot like the way that scientists build theories of the world. And that's become,

05:07.000 --> 05:13.960
you know, one of the most prevalent theories about how children solve this problem. They do something

05:13.960 --> 05:17.720
like build everyday theories of the world around them. But of course, then the question is, well,

05:17.720 --> 05:22.200
how do they do that, right? If you say that they're doing something like scientists, how do scientists

05:22.200 --> 05:28.440
build theories of the world around them? And back in the arts, there was a whole bunch of very

05:28.440 --> 05:35.480
exciting work showing that you could build causal pictures, causal representations of the world

05:35.480 --> 05:39.960
from data. And that's not the only thing scientists are doing. It's not the only thing involved

05:39.960 --> 05:44.520
in theory formation, but it's a really important thing in theory formation. Because if you have a

05:44.520 --> 05:49.480
causal model, if you have a theory, if you have a representation, then you can solve these out

05:49.480 --> 05:58.520
of distribution generalization problems. You can say, okay, well, if we, you know, if we do this

05:58.520 --> 06:03.880
to the vaccine, even if it's something we've never done before, we can predict what will happen

06:03.880 --> 06:10.600
because we have a causal model of how the vaccine works. And the advantage of thinking about

06:10.600 --> 06:15.880
causal models, so there's an example of something that lets you generalize really broadly.

06:15.880 --> 06:20.600
And it isn't just letting you generalize about a specific area like, you know, the way that

06:20.600 --> 06:25.960
having a representation of everyday physics or a representation of the visual system could help

06:25.960 --> 06:31.000
you to generalize. Because that really covers everything. It covers the way that you interact with

06:31.000 --> 06:36.120
other people. It covers the way that objects work. It covers things that you've never seen before.

06:36.120 --> 06:41.400
So if you had a way of figuring out what makes what happen. If you had a way of figuring out

06:41.400 --> 06:46.520
causal structure, you'd have a really, really powerful tool for going out into the world and

06:47.480 --> 06:51.880
going out into the world and solving new problems and making new predictions.

06:51.880 --> 06:57.640
One thing that's really important about causality, and a lot of people think of it as sort of being

06:57.640 --> 07:02.200
the thing that makes causality different from, say, just correlation of the sort that typical

07:02.200 --> 07:09.640
deep learning programs have done, is that causality lets you intervene, is the word that people use.

07:09.640 --> 07:14.760
It lets you decide what to do and think about what the consequences of that are.

07:14.760 --> 07:21.640
And causality lets you make counterfactual inferences. So it lets you, if I know that smoking

07:22.840 --> 07:28.440
causes long cancer, for instance, if I think that that's right, then I'll know that even though

07:28.920 --> 07:34.040
smoking is correlated with yellow fingers, washing your fingers isn't going to help change the

07:34.040 --> 07:40.600
cancer rate, but not smoking, getting people to not smoke is going to change the cancer rate.

07:40.600 --> 07:45.960
And it also means that I can, say, do some counterfactuals. So I can say, well, look, if we had had

07:46.600 --> 07:52.360
not anti-smoking programs earlier, we would have saved more people from cancer.

07:52.360 --> 07:57.960
So that ability to do interventions and counterfactuals is a very powerful aspect of causality

07:57.960 --> 08:05.640
and causal representations. And back in the arts, people like Clark Gleymor and Peter Spirties

08:05.640 --> 08:12.760
at CMU, and notably, probably most famously, Judea Pearl in UCLA, started developing these

08:12.760 --> 08:20.360
formal models for how those kind of causal representations could work computationally.

08:20.360 --> 08:25.880
So causal-based, graphical probabilistic graphical models were the kinds of representations

08:25.880 --> 08:31.000
that they had. And back in the arts, we started trying to see our children doing something like

08:31.000 --> 08:36.280
inferring causal-based nets from data. And amazingly, much to everyone's surprise,

08:36.280 --> 08:40.360
it turns out that even if you're looking at two, three, and four-year-olds, they're really good at

08:40.360 --> 08:48.440
doing that. You can give them a pattern of data of conditional probabilities, and they'll pull out

08:48.440 --> 08:54.360
the right causal consequences from that data. So they seem to be doing something that looks like

08:54.360 --> 09:04.840
a very effective causal inference. Going back to the theory theory, what were the alternatives

09:04.840 --> 09:12.120
to the theory theory before that theory? And how did you demonstrate that the theory theory was

09:14.200 --> 09:22.200
you know, was predictive and had merit? Yeah, right. So that's a great question. And in fact,

09:22.200 --> 09:29.480
I think people in ML and AI should be very familiar with what the alternatives work,

09:29.480 --> 09:34.440
because they're the alternatives that we have in ML2. So one alternative, again, going back to

09:34.440 --> 09:40.040
really play-do in Aristotle, is look, it just looks as if we have all this abstract structure

09:40.040 --> 09:44.840
and representations. Really, it's just you've looked at a whole bunch of data and you pulled out

09:44.840 --> 09:49.800
the statistics of the data, and that's letting you make predictions. And that was one thing that

09:49.800 --> 09:55.000
the sort of what's sometimes called the empiricist option was, okay, maybe it's not that children

09:55.000 --> 10:01.320
have these abstract representations. Maybe they're just just following the data. And it just looks

10:01.320 --> 10:05.160
like they have the abstract representation, because they have a whole lot of data and a whole lot of

10:05.160 --> 10:11.720
observations. And then the other option, which again is very active in AI now, is well, look,

10:11.720 --> 10:16.200
maybe they aren't actually learning the representations. Maybe they're just built in. So

10:16.200 --> 10:23.080
work that's being done in AI now suggests that you might have built-in

10:24.760 --> 10:29.720
constraints, inductive constraints that are like assumptions about how the world works,

10:29.720 --> 10:33.080
assumptions about how physics work, assumptions about how people work. And if you just build those

10:33.080 --> 10:38.040
in in the first place, then you can help to solve the problem. So those were the two options that

10:38.040 --> 10:44.280
were and still are on the table. And I think those of us who actually look at young children learning,

10:44.280 --> 10:48.760
you know, there are probably elements of both of those that are right, but it doesn't look like

10:48.760 --> 10:54.040
either of those is what's happening, because from the time we can test, and this is where the great

10:54.840 --> 11:01.160
methodological and experimental advances in developmental psychology kick in, even very little babies

11:01.160 --> 11:06.840
already seem to have these abstract, powerful representations of the world. That's very different

11:06.840 --> 11:13.960
from what a previous generation thought about about babies. But at the same time, even three

11:13.960 --> 11:18.600
and four year olds, this is work that we did back in the eighties, are really changing what they

11:18.600 --> 11:24.760
think about the world based on their experience. So they seem to be have abstract representations

11:24.760 --> 11:30.200
and be learning and changing those representations from the time they're very little. And theory of

11:30.200 --> 11:34.520
mind, for example, which is something that people are thinking about in AI as well, that ability to

11:34.520 --> 11:39.000
understand what's going on in someone else's mind, that was the work that we did back in the eighties,

11:39.000 --> 11:45.640
that I think really clearly showed that children have this succession of every day of every day

11:45.640 --> 11:50.680
theories in the world. And then in the more recent work about causal inference, what we could show

11:50.680 --> 11:55.080
is not just that they, you know, will have an abstract theory and then another one based on the

11:55.080 --> 11:59.960
data, but we can actually say something about what those look like, what the representations look

11:59.960 --> 12:06.440
like, and how the representations change. Does the theory theory imply that children are

12:06.440 --> 12:12.760
taking an active role and kind of recognizing that a theory is a theory and testing the bounds

12:12.760 --> 12:20.920
of that theory and experimentation? Yeah, exactly. So that's the thing that is the new work that we're

12:20.920 --> 12:26.440
really doing now. So we showed over an extended period that children could could do this, they could

12:26.440 --> 12:32.440
get statistics, they could infer causal, infer causal structure. And you might wonder how on earth

12:32.440 --> 12:38.200
could you do this with like two year olds, right? I mean, if you asked most grownups, what does

12:38.200 --> 12:43.240
conditional, this pattern of conditional dependency is indicate a causal chain or a common effect,

12:43.240 --> 12:48.600
they would be not know what you were talking about. And the way we did it is we have a little

12:48.600 --> 12:52.760
little machines, one of them is called the blanket detector that you'll see in my talk,

12:52.760 --> 12:57.080
the little box that lights up when you put things on it plays music sometimes and doesn't,

12:57.080 --> 13:02.120
other times. So it's a new causal system. And the kids challenge is to figure out how it works,

13:02.120 --> 13:07.080
figure out which things are blankets. Well, blankets will make it go, which things are blankets,

13:07.080 --> 13:13.240
and then they have to make it go themselves. So without actually asking about causal structure,

13:13.240 --> 13:17.880
we can see what kinds of inferences they're making and we can control what kind of data we get

13:17.880 --> 13:21.960
from. So we can give them different patterns of data and then we can see, then we can see what

13:21.960 --> 13:27.800
they do, we can see what kinds of inferences they make. And as I said, to a remarkable degree,

13:27.800 --> 13:33.720
the kids are making the right kinds of inferences. If they were little Bayesian hypothesis testers,

13:34.520 --> 13:39.320
again, to get back to the theory theory, give them two hypotheses and they're picking the ones with

13:39.320 --> 13:45.320
the best posterior probability. But the big question is, so that that's really impressive. But then

13:45.320 --> 13:49.240
we still have this question about how are they doing that? What's happening that's letting

13:49.240 --> 13:58.040
them solve that problem? Because of course, the big issue with Bayesian reasoning and with probabilistic

13:58.040 --> 14:02.760
generative models in general is if they're interesting at all, the search space is enormous.

14:02.760 --> 14:08.520
So if you think about even a Bayesnet with four or five causes, you very quickly have a very,

14:08.520 --> 14:14.280
very big space of possibilities. And the question is, how do you limit that? How do you search through

14:14.280 --> 14:19.000
that space? How do you solve that problem? An idea that we've had and a lot of people in AI have had

14:19.000 --> 14:24.120
now is that active learning, to get back to your question, active learning experimentation,

14:24.120 --> 14:32.200
that's the way that scientists solve that problem. They are not just stuck in your main frame

14:32.200 --> 14:38.200
with data pouring over you. You can actually decide which kinds of data you want,

14:38.200 --> 14:43.640
depending on what kind of hypothesis you're testing. And for causal work, again, because of this

14:43.640 --> 14:50.280
intervention quality of causation, you can specify pretty clearly, here's the kind of experiment

14:50.280 --> 14:56.680
you should do. Here's what you should do. Here's how you should wiggle X to see if Y works.

14:56.680 --> 15:02.760
And if you think about little kids, I mean, like that's their entire life, right? I just,

15:02.760 --> 15:10.920
I just, it's a kind of nice, it's a kind of nice convergence. I just spent a, a, a sabbatical at

15:10.920 --> 15:16.440
Mela in Montreal, where, yeah, she was Benjiro and his colleagues are doing fantastic work just

15:16.440 --> 15:21.320
about this question about causal inference and causal models. And at the same time, I was

15:21.320 --> 15:28.360
visiting my one-year-old grandson. And if you watch my one-year-old grandson, basically all he

15:28.360 --> 15:34.120
does is do experiments. I mean, you know, he, he will occasionally eat if his mom gives him some food

15:34.120 --> 15:39.320
and, and other than that, what he's doing constantly is doing experiments. And what all the rest

15:39.320 --> 15:45.080
of us are doing is trying to keep him from killing himself by, by doing experiments. And, you know,

15:45.080 --> 15:49.160
it's funny, we just sort of take for granted. Oh, okay, look, I'm looking at this one-year-old and

15:49.160 --> 15:53.640
look what he does. He takes the spoon and then he bangs it on the pot and then he turns the pot over

15:53.640 --> 15:59.480
and then he sees if he can stick the spoon in the light socket and so on and so forth, we just take

15:59.480 --> 16:05.400
that for granted. But why would they be doing that, right? I mean, that's a lot of, a lot of physical

16:05.400 --> 16:11.240
energy going on to just go out and try things in the world. But if you think of them as being

16:11.240 --> 16:16.520
these causal inference, active causal inference engines, that's exactly, that's exactly what they

16:16.520 --> 16:21.000
should be doing. And what we're doing now is we're in collaboration with some people at Mela and

16:21.000 --> 16:29.560
at Berkeley and actually at Google Leap Mind as well. What we've done is to set up these environments

16:29.560 --> 16:37.240
in which you can find out about causal structure by doing experiments. We have our kind of virtual

16:37.240 --> 16:43.160
version of our blanket detector. And what we can do is see what do kids do when you just let them

16:43.160 --> 16:49.720
loose in this kind of environment. And how does that compare to various kinds of causal learning

16:49.720 --> 16:54.920
algorithms you might have? And if, you know, if you think about it like a classic RL algorithm,

16:54.920 --> 16:59.000
for example, is not going to do the kinds of things that are the best experiments because the

16:59.000 --> 17:03.640
classic RL algorithm is just going to try and find the outcome and maximize it. And what you

17:03.640 --> 17:09.400
need for experiments is to try lots of different things, change what you do based on what happened

17:09.400 --> 17:17.000
before. So, but some of the things, again, like my colleagues, Deepak Prathak and Polkad

17:17.000 --> 17:22.040
Agrawal and others coming out of Berkeley have these curiosity-based algorithms that seem to be

17:22.040 --> 17:28.040
closer to what the kids are doing. And I think if you kind of combine the curiosity-based idea,

17:28.040 --> 17:33.080
the idea that what you're trying to do is get a system that will make predictions and frustrate

17:33.080 --> 17:41.720
them and explore, and the causal models idea, that could be a very powerful mechanism for solving

17:42.360 --> 17:48.200
some of these search problems. So, what degree does your work begin to

17:50.440 --> 17:56.600
kind of articulate, explore the structure of the or the complexity of causal relationships

17:56.600 --> 18:02.920
that children are able to deal with at various ages? And what does that tell us about our

18:02.920 --> 18:13.160
ML models? Yeah, so the first pass, both with Basinets and with our work, was just pretty simple

18:13.160 --> 18:17.560
causal relationships. So, you know, here's one variable and another variable and does variable

18:17.560 --> 18:25.080
X cause variable Y. And you can see the experiments you do to try and find that out. And then what

18:25.080 --> 18:31.640
what Pearl and Glamour and colleagues had done was to look at more complicated things like,

18:31.640 --> 18:35.560
is it a causal chain or is it a common effect structure or a common cause structure?

18:36.680 --> 18:43.080
But starting in the sort of in the late arts, we started collaborating with people like the

18:43.080 --> 18:47.880
cognitive scientist Tom Griffiths, who's at Princeton, Chris Lucas, who was a student of mine,

18:47.880 --> 18:54.760
who was at is now in Edinburgh, to try and see if we could also make inferences about more abstract

18:54.760 --> 19:00.680
features of causal systems. So, for instance, could I infer not just, is this blanket detector,

19:00.680 --> 19:06.760
did this block make the detector go or not? But is the detector deterministic or stochastic? Or

19:06.760 --> 19:13.160
does the detector work with a conjunctive logic? You need two things to make it go in combination,

19:13.160 --> 19:19.400
or does it have a disjunctive logic? Each cause is separate. And what we've shown is that kids are

19:19.400 --> 19:25.560
quite good at, even again three and four year olds, are quite good at making inferences about

19:25.560 --> 19:31.320
those more abstract features of the system as well. And to get back to machine learning, you know,

19:31.320 --> 19:35.640
the more abstract your representations are, the more powerful your generalizations are going to be.

19:35.640 --> 19:39.880
So, the kids seem to be quite good at even making those more abstract inferences. And something

19:39.880 --> 19:44.280
that's really interesting is that the kids are actually better at doing that than adults are. So,

19:44.280 --> 19:51.320
well, here's the thing, and this is another point about children and childhood in general.

19:51.880 --> 19:59.000
So, if you give the adults a structure that's really common, that they have a strong prior

19:59.000 --> 20:03.960
for, then they're good at making inferences. But how about if it's something that's kind of weird

20:03.960 --> 20:08.520
and unusual? So, it's an abstract feature that isn't as obvious, and you don't have a stronger

20:08.520 --> 20:14.200
prior form. When you do that, the kids are actually better than the adults. And in a sense, the kids

20:15.320 --> 20:21.080
lack of knowledge, lack of previous knowledge is really an advantage when you want to explore

20:21.080 --> 20:27.480
the space, explore the space more widely. And I think that leads to another big point that I've

20:27.480 --> 20:32.200
made that I'll be making in my talk, which is that actually just being, it gets, gets back to my

20:32.200 --> 20:39.080
one-year-old, just being a kid, just the fact that you have this period of childhood before adulthood,

20:39.640 --> 20:45.240
that in and of itself may be something that humans use to solve this problem. And the argument

20:45.240 --> 20:50.280
that I've made is, again, think about that search problem. One of the reasons the search problem

20:50.280 --> 20:54.440
is so challenging is because there are always these explore exploit trade-offs, right? So,

20:55.080 --> 20:59.400
one of the things that we learned at the very beginning of computer science was that explore,

20:59.400 --> 21:07.000
exploit trade-offs are a bear, and there isn't any simple or optimal way of resolving them.

21:07.000 --> 21:11.880
But one idea that people often have is, okay, when you look at the actual algorithms that are

21:11.880 --> 21:16.280
trying to deal with explore exploit tensions, is start out exploring. And in particular,

21:16.280 --> 21:23.640
start out with these very wide, high temperature, bouncy, noisy kinds of searches that get through

21:23.640 --> 21:30.520
a whole bunch of the space. And then, once you've done that, narrow in and exploit. And the idea is

21:30.520 --> 21:36.120
that that keeps you from, you know, getting stuck in local optima, it keeps you from, it keeps you from

21:36.120 --> 21:42.440
settling on a particular option too quickly. And what I've argued is you could think about human

21:42.440 --> 21:48.040
life history as they call it a biology, human development, as being evolution's way of, so one of

21:48.040 --> 21:55.160
the ideas is often a simulated annealing kind of idea. So you start out looking really widely,

21:55.160 --> 22:00.760
and then you really, with a really high temperature, and then you cool off. And my slogan is that

22:00.760 --> 22:07.560
childhood is evolution's way of solving explore exploit tension and doing simulated annealing.

22:07.560 --> 22:13.640
So if you sort of say, who looks like they're bouncy and random and noisy and trying lots of

22:13.640 --> 22:19.080
things that are not very effective versus who looks like they're narrowing in, using a lot of their

22:19.080 --> 22:24.360
prior knowledge to do something effectively, you'll see that first, you know, that's what a one-year-old

22:24.360 --> 22:30.920
looks like, and as opposed to what an adult looks like. So part of the idea is that if we actually

22:30.920 --> 22:37.880
built in a developmental sequence, we sort of let AIs have be children for a while, that might,

22:37.880 --> 22:43.880
we might also get some clues about how to learn more effectively. And this kind of trade-off that

22:43.880 --> 22:49.960
you see where the children need a lot of care, they need a lot of people around them looking after

22:49.960 --> 22:54.920
them, but that gives them this chance to go out and explore. That might be a trade-off that's

22:54.920 --> 23:02.200
relevant for AIs as well. And I should say in some of the other things that I'm going to talk about,

23:02.200 --> 23:05.880
what we've, so one thing that we've been doing is looking at how children are using

23:05.880 --> 23:12.200
experimentation and exploration to try and solve these problems. So one thought that

23:12.920 --> 23:21.400
that brings to mind for me is in some ways could you say that the natural machine learning

23:22.200 --> 23:27.880
cycle kind of resembles this childhood adulthood in a sense that training is kind of like childhood

23:27.880 --> 23:33.560
and inference once the model is kind of built and fixed and that's kind of adulthood.

23:33.560 --> 23:38.360
Something like that I think is right. Yeah, I think the idea is that you have a period of learning

23:38.360 --> 23:44.200
and then you have a period of actually using what you've learned to go out and make inferences.

23:45.240 --> 23:49.000
But the kind of learning that the children are doing seems to be much more

23:50.040 --> 23:56.760
wide-ranging than the kind of learning that a typical machine learning system is doing.

23:56.760 --> 24:00.760
And something that I think is really interesting is when you actually look at the practicalities

24:00.760 --> 24:05.800
and talk to people about, well, how do you solve these exploit problems? A kneeling shows up

24:05.800 --> 24:10.360
again and again and often in multiple cycles where you'll, you know, do heat things up,

24:10.360 --> 24:15.000
cool things down, heat things up, cool things down. But there isn't a kind of general theory

24:15.000 --> 24:20.360
about how to do that as far as I can tell or about why that works. And again, looking at

24:20.360 --> 24:26.040
kids might give us some clues about, you know, how does a kneeling work in the wild when you see it

24:26.040 --> 24:31.320
in children? One of the things that I think is really cool about this idea is if you think about

24:31.320 --> 24:39.240
exploit trade-offs, things that look like bugs from the exploit perspective might actually be

24:39.240 --> 24:43.640
features from the explore perspective. So, for instance, having a system that's noisy that has

24:43.640 --> 24:50.840
a lot of variability is not good if what you want is to make inferences and act effectively,

24:50.840 --> 24:56.440
but it is good if what you want is to be able to learn as much as possible. So, many of these

24:56.440 --> 25:01.960
things about kids that have seemed like defects, like the fact that they're curious all the time,

25:01.960 --> 25:07.880
that they're kind of unpredictable, that they are variable, that they're noisy, those might

25:07.880 --> 25:12.520
actually be advantages from the perspective of learning. And I think we don't have a good

25:12.520 --> 25:16.440
theoretical account of how that all works and thinking about the kids could help.

25:16.440 --> 25:21.800
What would you say are the closest ways that we're approximating that in the machine learning world?

25:22.760 --> 25:32.600
People like Joshua Benjo and his colleagues and others are trying to, now, and I think this

25:32.600 --> 25:37.240
is going to be the way for the future, is to try and design hybrid systems that have some of the

25:37.240 --> 25:42.680
power of the, that can use some of the power of machine learning, but then can also have some of

25:42.680 --> 25:51.480
the structure and generalization of a causal system. So, Joshua has these flow nets that are

25:51.480 --> 25:58.920
trying to do that, so they're trying to add a layer of, a layer of further structure on top of

26:00.600 --> 26:05.320
a kind of classic machine learning system. And RL is interesting from this perspective too,

26:05.320 --> 26:10.760
because you can argue that, and people have argued that reinforcement learning in the psychological

26:10.760 --> 26:17.560
sense is like the most primitive form of causal learning. As opposed to just picking out correlations,

26:17.560 --> 26:21.400
what happens in reinforcement learning is that you do make an intervention and you see what the

26:21.400 --> 26:26.360
outcomes are. And it's important that you're actively going out and making interventions and

26:26.360 --> 26:34.440
seeing outcomes. But typically in RL, what you're, the outcome of that isn't a model as much as

26:34.440 --> 26:39.400
just the fact that you're more likely to make those inferences or those policies later on.

26:39.400 --> 26:45.480
So something like model based RL ends up looking a lot like causal inference, it ends up looking

26:45.480 --> 26:50.520
a lot like causal structure, and that might be a relevant, that feels like that's a relevant

26:52.200 --> 26:57.480
outcome. But I think it's important that the objective functions for a system like that

26:57.480 --> 27:02.200
would have to be things like information gain or like knowledge or like curiosity,

27:02.200 --> 27:08.920
rather than being things like how well you're scoring on, on some mention. And there's

27:08.920 --> 27:13.240
really elegant work in our lives and others that show, for instance, that if you look at the kids

27:13.240 --> 27:19.720
playing around, they seem to be acting in a way that will get them information. That information

27:19.720 --> 27:23.960
gain seems to be sort of an objective function that describes what it is that they do.

27:23.960 --> 27:28.440
What's an example of that? Well, there's beautiful work by my colleague Celeste Kid,

27:28.440 --> 27:34.200
and she looked at really young babies, you know, 10-month-olds. And what they did was they showed

27:34.200 --> 27:40.120
the babies different sequences of events that had different amounts of information in the

27:40.120 --> 27:44.920
technical information of theoretic sense. And what they discovered was that there was this kind

27:44.920 --> 27:49.960
of sweet spot, and they just measured how long the babies looked at each of these events,

27:49.960 --> 27:55.880
and they discovered there was this sort of sweet spot if something was too random, too far removed

27:55.880 --> 28:01.240
from where you were now. The babies wouldn't look, but they also wouldn't look at things that

28:01.240 --> 28:05.480
didn't give them very much new information. There was this kind of sweet spot of just where the

28:05.480 --> 28:11.720
information gain was going to really help you to make progress, and the babies looked the most

28:11.720 --> 28:17.400
at those events. And we've been thinking about that, too. One of the problems with using

28:18.120 --> 28:23.240
just using information gain, this is something that comes up in a lot of these curiosity-based

28:23.800 --> 28:29.880
RL kind of algorithms, is what's called, you know, the TV problem. The problem is if you just

28:29.880 --> 28:35.880
use technical information gain, then if you just put someone in front of a random TV with

28:35.880 --> 28:39.800
a random static at it, you're getting lots of information in the information through

28:39.800 --> 28:42.680
theoretic point, but you're not getting anything that's actually going to be very useful.

28:43.240 --> 28:48.760
So I think one of the real interesting frontiers is this balance between

28:49.400 --> 28:54.360
noise and structure, right? So the kids are noisy, but they're not just completely noisy. They're not

28:54.360 --> 28:59.000
just acting like random agents. They're doing things that make sense, given the kinds of problems

28:59.000 --> 29:02.920
they're trying to solve, the kinds of causal structures that they're trying to infer,

29:02.920 --> 29:08.920
and how you get that balance between introducing noisiness and variability, and then also having

29:09.880 --> 29:13.960
interventions and experiments that are relevant to the problems you're trying to solve.

29:13.960 --> 29:18.280
That's something the kids seem to be really remarkably good at doing, and we don't quite know how to

29:18.280 --> 29:26.040
characterize that computationally. I had cut you off to ask a question earlier, and you about to

29:26.040 --> 29:33.960
mention some additional points from your talk over those. Yeah, so what we've been doing now,

29:33.960 --> 29:38.120
so one thing that we've been doing is looking at how children are using active learning to

29:38.920 --> 29:43.640
figure out the causal structure of the world in these online environments. Another thing that we're

29:43.640 --> 29:48.280
doing is trying to see if, I mentioned that we've done work showing that children could get these

29:48.280 --> 29:54.360
more abstract, what are some of us called, over hypotheses about causal structure, but one thing

29:54.360 --> 30:00.680
that we're doing now is trying to see if kids can do things like actually decide what the right

30:00.680 --> 30:06.920
causal variables are. So a big problem, so it's easy to say, okay, look, I'll tell you, you know,

30:06.920 --> 30:11.000
here's variable X, and here's variable Y, and then you can see whether they're dependent, and see

30:11.000 --> 30:15.080
if there's a causal relationship between them, but how do you decide which variables to look at in

30:15.080 --> 30:20.840
the first place? And this is a big problem for ML as well, where it turns out that, you know, in the

30:20.840 --> 30:25.560
classic adversarial examples for something like ImageNet, it turns out, well, wait a minute,

30:25.560 --> 30:30.280
no, the system is actually not even dividing up the world in the right way. It's paying attention

30:30.280 --> 30:35.480
to, you know, fine details of the texture instead of paying attention to the objects,

30:36.600 --> 30:40.680
and it might look as if it's making the right kinds of inferences, but it isn't really,

30:40.680 --> 30:46.040
because it just hasn't divided up the world in a way that makes sense from the perspective of

30:46.040 --> 30:52.840
different variables. You know, if you're a scientist, there's all sorts of classic examples like,

30:52.840 --> 30:57.960
you know, it turns out that if you're looking at the relationship between cholesterol and heart

30:57.960 --> 31:02.760
disease, oh, there's actually two different kinds of cholesterol. One of them makes heart disease

31:02.760 --> 31:07.640
more likely, one of them doesn't. So if you just didn't, if you didn't have the right measures,

31:07.640 --> 31:11.160
if you just looked at cholesterol overall, you wouldn't see the relationship, and then it turns

31:11.160 --> 31:17.400
out that, then it turns out that you can. And it turns out that kids are actually very good.

31:17.400 --> 31:22.200
We have some really beautiful experiments where kids are actually good at picking out, oh, okay,

31:22.200 --> 31:28.280
this is the variable, this is the object, or this is the property that is the one that I should

31:28.280 --> 31:33.880
be looking at for purposes of trying to do causal inference. So they're simultaneous. Yeah,

31:34.440 --> 31:40.840
so this is, this is my brilliant graduate student, Marielka, do. So what we do is we set up a

31:40.840 --> 31:47.160
situation where the kids have to figure out their little Shelley, the turtle, wants to grow

31:47.160 --> 31:52.040
cactuses. And he likes some cactuses. He likes the ones with round things on them, but not the

31:52.040 --> 31:57.240
spiky ones. And we're trying to decide how can we help him grow his cactuses? And there's two

31:57.240 --> 32:01.960
different things we can do. We could put the seeds in different colored flower pots, or we could

32:01.960 --> 32:06.840
have different colored water incants that are water and seeds. And we're trying to figure out

32:06.840 --> 32:11.480
how do we make sure that Shelley gets the cactuses that he does? So we've set up, here's these two

32:11.480 --> 32:17.640
potential variables. It could be the water incants, it could be the pots. And what we can show is that

32:17.640 --> 32:22.840
if we show children that the water incants make a difference and the pots don't, and now we give

32:22.840 --> 32:28.120
them a new case, this is a new cactus, a new water incant, a new pot, they'll say, okay, the water

32:28.120 --> 32:31.800
incant is the thing, I hope I got that right, the water incant is the thing I should be paying

32:31.800 --> 32:36.520
attention to. I should be playing with that water incant. I should be changing it. We should

32:36.520 --> 32:41.480
be doing things to that water incant, because I've already figured out that the pot really isn't

32:41.480 --> 32:46.920
relevant to these differences. So in philosophy, they talk about this as causality being

32:46.920 --> 32:51.160
difference-making. The causal thing is the thing that makes a difference. And the kids already

32:51.160 --> 32:55.880
seem to be saying, okay, which things in my world are the things that make a difference? Which are the

32:55.880 --> 33:01.080
things that I can control and change and make a difference? And again, if you think about an AI

33:01.080 --> 33:07.640
system, if it was paying attention to, think about, you know, RL, part of the problem with RL is

33:07.640 --> 33:14.840
it's just paying attention to everything in the image, right? And if it could say, oh no, these

33:14.840 --> 33:19.800
are the things that I should be wearing. These are the things that, you know, think about like the

33:19.800 --> 33:26.440
classic example where you see a robot being trained with RL. And it's doing all this ridiculous

33:26.440 --> 33:32.440
stupid stuff that isn't going to make any impact at all before it kind of stumbles on the thing

33:32.440 --> 33:37.960
that might work. If you could start out having that robot say, oh, okay, I know these are the things

33:37.960 --> 33:42.440
that are going to make a difference and these things aren't. It would be much better all. And if it

33:42.440 --> 33:55.000
could learn that, that would be even better. Does your research venture into the role of biases learned

33:55.000 --> 34:02.840
by children in the process of their exploration? Yeah, one of the things that we've, one of the things

34:02.840 --> 34:08.440
that I mentioned before was about the fact that children aren't just using these causal inferences

34:08.440 --> 34:15.080
to make draw conclusions about, you know, blanket machines. They're using them a lot to draw

34:15.080 --> 34:21.800
inferences about other people. And actually, something that we haven't published yet were just in

34:21.800 --> 34:28.040
the middle of doing it. For example, suppose kids see that there's a bunch of kids who are playing

34:28.040 --> 34:36.840
together and kids who have, you know, a particular kind of funny glasses are being welcomed into

34:36.840 --> 34:42.760
the group and kids who have a funny hat are being shunned, for instance. So they just would be seeing

34:42.760 --> 34:48.600
those, seeing those patterns. Are they going to conclude that there's a difference between the

34:48.600 --> 34:52.360
people who have the glasses and the hats? Even if, I mean, that's a nice example of variable

34:52.360 --> 34:57.320
selection, right? You know, kids start out not thinking that this difference is going to be important.

34:57.320 --> 35:01.320
But if they see that people are being treated differently, they might very well end up

35:01.320 --> 35:06.360
concluding. And in fact, we have some evidence that they do conclude that, that those people are,

35:06.360 --> 35:12.120
those, those are different groups. Those are people who should be treated differently, for example.

35:12.840 --> 35:18.040
So I think it's quite plausible that part of what's happening is that kids are paying attention

35:18.040 --> 35:24.520
to these social differences. And then they end up having various kinds of biases as a result.

35:24.520 --> 35:29.880
And it's an interesting question about would, you know, it seems plausible that an AI might very

35:29.880 --> 35:36.280
well reproduce that. And we might be worried about how we could counter how we could counter that

35:36.280 --> 35:43.560
both for the children and for the AI's. So what are the main takeaways that you want to leave

35:43.560 --> 35:49.000
the folks that are hearing your talk at the causal inference and machine learning workshop?

35:49.000 --> 35:53.880
So I think there's two. One of them is, this may be a little preaching to the choir,

35:53.880 --> 35:59.160
is that causal inference is really important. It's a really powerful, it's a really powerful

35:59.160 --> 36:05.320
technique causal representations give us lots of advantages. And, and we sort of made some progress

36:05.320 --> 36:10.440
computationally on causal inferences and representations. And that, that's a really,

36:10.440 --> 36:15.160
that that's exactly the, the technique we'd need to solve some of the limitations of our current

36:15.160 --> 36:20.280
systems. But then in a way, the even more important idea is that looking at little kids,

36:20.280 --> 36:25.880
which is not something that typically people in AI have done that feels like, oh, wait a minute,

36:25.880 --> 36:28.680
you know, the world of people who are sitting in little chairs and playing with three

36:28.680 --> 36:33.400
year olds is completely different from the world of computer scientists. And I think it's kind

36:33.400 --> 36:38.200
of wonderful that that computer scientists have realized, oh, wait a minute, you know,

36:38.200 --> 36:42.120
these little kids who we weren't paying any attention to, we thought they were just, you know, kind of,

36:42.840 --> 36:49.880
I don't know, um, uh, mushy stuff that wasn't like what we do in AI. That those are,

36:49.880 --> 36:56.120
that those are really, uh, you know, those are those kids might really have the clue to designing

36:56.120 --> 37:02.120
new systems. I think that's the really big point that I want to make. And, and I think back and forth,

37:02.120 --> 37:07.240
by looking at kids who are such great learners, we can figure out how to make more effective AI.

37:07.240 --> 37:11.640
But also, by looking at AI, we can figure out what's going on in those kids brains that makes

37:11.640 --> 37:16.840
them such effective learners. I think that's a really, really promising, exciting, uh,

37:17.880 --> 37:21.080
line of research and one that we're starting to do when I hope we'll continue to do.

37:21.960 --> 37:26.200
That's awesome. So, AI researchers go play with your kids. Exactly. Yeah, I mean, I think,

37:27.000 --> 37:30.520
and, you know, I think this might be an example as well where,

37:31.560 --> 37:35.720
you know, the diversity issues are really relevant, right? So, you know, I think part of the

37:35.720 --> 37:40.360
reason to be frank, why kids haven't played a bigger role in AI is because kids are kind of girl

37:40.360 --> 37:47.400
stuff, right? Um, uh, you know, they're, they're things that people who are off raising families are

37:47.400 --> 37:52.840
paying a lot of, of paying a lot of attention to. And the people who were doing AI and the people

37:52.840 --> 37:57.080
who are raising the families haven't necessarily been the same people. And I think it's a kind of

37:57.080 --> 38:02.040
tribute to the way that we have a much wider, more diverse group of people being involved in AI,

38:02.040 --> 38:08.280
including more women, more people who are raising families, that that's a nice example where that

38:08.280 --> 38:13.320
actually turns out to contribute to something that's really basic to the science. Yeah, yeah.

38:13.880 --> 38:19.080
Awesome. Awesome. Well, Allison, thanks so much for joining us and sharing a bit about your talk and

38:19.080 --> 38:33.720
what you've been up to on the research front. That's great. Thanks for having me.

