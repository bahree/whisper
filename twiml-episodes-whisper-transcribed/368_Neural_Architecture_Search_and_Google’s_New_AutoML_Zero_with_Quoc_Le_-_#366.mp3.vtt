WEBVTT

00:00.000 --> 00:12.640
Welcome to the Twimal AI podcast. I'm your host, Sam Charrington.

00:14.640 --> 00:20.080
All right, everyone. I'm here with Fuck Lay. Fuck is a research scientist at Google.

00:20.080 --> 00:29.360
Quack, welcome to the Twimal AI podcast. Hi, everyone. It's great to have you on the show. I've

00:29.360 --> 00:36.160
followed your research for your work for quite some time. And I'm looking forward to digging into

00:36.160 --> 00:42.320
some of the new things that you're working on. But before we do that, I'd love to have you share

00:42.320 --> 00:46.560
a little bit about your background and how you got started working in machine learning.

00:46.560 --> 00:55.680
Okay, so I was born in Vietnam and I did my undergrad in Australia.

00:58.000 --> 01:06.160
And in my second year of my undergrad, I started a summer project, doing machine learning

01:06.160 --> 01:12.640
with Alex Mola, back in Australia. And back then, I was a player that I would call

01:12.640 --> 01:23.360
Toronto Methods. And then I did my PhD at Stanford on a lot of deep learning back in the day

01:23.360 --> 01:41.440
when deep learning was very cool. And that's around 2007. And around 2011, I did a summer

01:41.440 --> 01:50.800
internship at Google. And that was when Google Brain Project was kind of founded. So when I was there,

01:52.000 --> 01:59.360
that was Andrew and Jack Dean and Greg Coraro was there. And I was the intern. So we started

01:59.360 --> 02:07.840
up quite small. That's all right. Yeah. And then I did some of the, you know, scaling up

02:07.840 --> 02:17.840
neural networks with the Google Brain Fox. And then, you know, and then after two years,

02:17.840 --> 02:27.600
I did some work on machine translation with Ilya and Oryo Vinyal. He's now at it mine.

02:28.720 --> 02:35.200
Ilya is now at OpenAI. And we developed some of the N2N translation methods. And

02:35.200 --> 02:45.040
and then around 2016, I started looking into more like, you know, auto-email neural architecture

02:45.040 --> 02:52.160
search. Yeah. And more recently, I looked into more like together with auto-email. I also

02:52.160 --> 02:59.920
looked into semi-subvised learning and so on. Awesome. Awesome. Now you mentioned early on doing

02:59.920 --> 03:06.880
work with Alex Mola. Was he, was this before he was at Carnegie Mellon or was he visiting in

03:06.880 --> 03:17.200
Australia? He was a professor in Australia. Yeah. Yeah. Yeah. I went to a university in

03:17.200 --> 03:26.400
small air in the capital city of Australia. And he was, yeah, camera and he was a professor there

03:26.400 --> 03:35.520
doing research. So I thought, you know, I had, I have been long very interested in AI and machine learning.

03:37.840 --> 03:41.920
And I took, before that I took a class in data mining and so on. And I thought, you know,

03:41.920 --> 03:47.840
it's a little bit boring, but machine learning and the ability to actually learn is actually

03:47.840 --> 03:58.160
super fascinating. So I contacted him and he, he, he, he was doing like auto methods.

04:00.400 --> 04:06.480
Machine learning and and we, we worked together for maybe a few years. Yeah.

04:07.440 --> 04:16.640
Before he went to, he went to America and then CMU and Amazon. Okay. Okay.

04:16.640 --> 04:24.720
Yeah. So a lot of your recent work has been focused on this idea of, you know,

04:24.720 --> 04:30.720
automating machine learning and neural architecture search to allow machines to find the best

04:30.720 --> 04:34.480
deep learning architectures and like, you know, talk a little bit about how you

04:36.160 --> 04:43.120
arrived at working in that area and what some of the motivations were for getting started

04:43.120 --> 04:51.680
digging into that problem. Yeah. Yeah. So I've been long interested in this idea of self-improvement.

04:52.560 --> 05:01.520
Machine should be self-improving itself, machine learning, right? And even, and when I started

05:01.520 --> 05:09.200
doing kernel methods with Alex, I always asked him, you know, how the, the code, the kernel bandwidth

05:09.200 --> 05:17.520
and so on or how some of the hyper parameters in kernel methods decided and, you know, apparently,

05:17.520 --> 05:24.720
they decided by using things like cross validation and so on. And then when I work on kernel methods,

05:24.720 --> 05:30.480
sorry, neural networks, my hope is to make the hyper parameters go away.

05:30.480 --> 05:39.280
Right. But that's how it's the opposite. So if now if you look at a, a convolution neural networks,

05:39.280 --> 05:46.880
it's, it has a lot of hyper parameters, right? Like how many, how many layers you want it to be

05:47.360 --> 05:53.360
and how many channels you want it to be and what are some of the size of hyper parameters since

05:53.360 --> 06:01.120
are like kernel widths and so on. And so, not all the training parameters. Yeah, all the, yeah,

06:01.120 --> 06:08.800
and learning, right? And as researchers develop more and more techniques for neural networks,

06:08.800 --> 06:15.520
there's more decisions that you have to make that feel like, you see, like a problem that

06:15.520 --> 06:27.440
you know, can be helped by a little bit of automation. So in, so I, I, I observe a lot of my

06:27.440 --> 06:32.720
colleagues at Google when they design neural networks and I asked them about the principles of

06:32.720 --> 06:38.000
designing neural networks and, you know, you started out having some really solid principles.

06:39.360 --> 06:45.360
Like you add skip connection so that gradient can flow through the network and so on.

06:45.360 --> 06:51.040
But as you tune the network harder and harder, you no longer have the principle. It's a,

06:51.040 --> 06:57.200
it's around, you know, trial and error, right? You, you try this a little bit and it seems a little

06:57.200 --> 07:05.120
bit better. So you try, you try that more. So I think that that is something that may be ready

07:05.120 --> 07:12.240
for automation. So even during my grad school, I already talked about trying this, but I thought,

07:12.240 --> 07:18.800
you know, maybe we didn't have enough compute because training in that already takes, took me days.

07:20.400 --> 07:26.800
So when I saw that neural, you can train neural networks in 30 minutes or something like that,

07:26.800 --> 07:33.120
you know, from on CIFA, I thought, oh, maybe this is the right time to try this. So that's when I

07:33.120 --> 07:42.640
started doing this neural architecture search in 2016. It's interesting that, you know, even with all

07:42.640 --> 07:48.800
of the compute resources of Google, you had to wait until the time was compressed enough in order

07:48.800 --> 07:57.520
to be able to tackle the problem. Yeah, yeah. Third cell that to get really good results, you want

07:57.520 --> 08:07.280
the networks to be really big and that will take a long time to train. Yeah, and it's funny coming

08:07.280 --> 08:12.400
from me that we have so much resources at Google, but training neural networks is still taking a long

08:12.400 --> 08:23.360
time. Yeah. And so maybe talk about the first steps in that area, did you jump right into

08:23.360 --> 08:32.240
the neural architecture search or was that the, you know, an end stage or a end result of this work?

08:34.320 --> 08:46.240
Oh, well, you know, I work on some of the related ideas on and off since 2012, like Blender

08:46.240 --> 08:51.440
with how to do better hyperparameter tuning for new networks. And none of that is really published

08:51.440 --> 08:57.440
because I didn't have good results and, you know, I didn't have enough compute and so on. So,

08:58.560 --> 09:01.280
so I tried it on and off.

09:06.880 --> 09:14.160
Over the time, you know, every year I would set out some time to try this idea for a few months

09:14.160 --> 09:22.480
and, you know, and it didn't work very well because lack of compute and so on. And then around 2016,

09:22.480 --> 09:30.640
I met Beretsov, who is my colleague now at Google, and he's very talented. So we say, oh,

09:30.640 --> 09:44.560
let's let's try this idea of using like a reinforcement learning to generate a network, like a

09:44.560 --> 09:55.760
little layer in a network for, for a SIFA model. SIFA model is already at that time, you could say

09:55.760 --> 10:01.920
that, you know, train enough a few depends on how, where you want it to be, but, you know, from

10:01.920 --> 10:09.440
30 minutes to a few hours. And that seems like about the right amount of time to get this going.

10:09.440 --> 10:16.240
And my prediction is that you have to train maybe either between from 1000 to 10,000 models. And

10:18.480 --> 10:22.640
I did a backup, our envelope calculation and I thought, oh, this might be the right time to do it.

10:22.640 --> 10:28.560
But, you know, I tried this, some of these related ideas in, you know, much before that.

10:28.560 --> 10:34.000
So you're doing a SIFAR, which is an image recognition for object detection and images.

10:36.640 --> 10:44.240
And so you're doing, you've got this, when you say you're doing,

10:44.240 --> 10:56.400
when you say you're doing tens of thousands of models, is that part of the optimization process

10:56.400 --> 11:01.760
that you're describing here? You're expecting that you need to do 10,000 in order to optimize the

11:01.760 --> 11:10.000
hyper parameters. Yeah. So, so in this process, you have a controller, which is also a machine learning

11:10.000 --> 11:24.000
model. And every time it makes an update, that's basically, it has to, it has to try training one

11:24.000 --> 11:30.880
model to conversion, one SIFAR model, two conversions. And it will take the signal from the

11:30.880 --> 11:38.720
conversions of the, the SIFAR model, you know, maybe the SIFAR model will get 70%. So that 70%

11:38.720 --> 11:45.280
would be used as a signal to make one update for them controller, one update, right? So,

11:46.000 --> 11:51.920
typically machine learning models take a long time to train. So it requires, you know, 10,000

11:51.920 --> 11:56.560
of updates. So that's basically the number of models that we had to try.

11:57.520 --> 12:04.240
Were your initial attempts at this doing, you know, how do you distinguish between like,

12:04.240 --> 12:12.320
doing your hyper parameter optimization and kind of architecture search, because there is a varying

12:13.440 --> 12:19.680
degree of complexity in trying to come up with these new architectures. Yeah, and your more

12:19.680 --> 12:27.280
recent work on this is like using evolutionary algorithms and a like to do this. Can you maybe talk

12:27.280 --> 12:35.920
through kind of the progression of complexity that you went through? Yeah, so the first project that

12:35.920 --> 12:48.800
we did was architecture search for like a SIFAR model. And that's already was already very

12:48.800 --> 12:56.560
expensive back when we did it. And we didn't choose hyper parameters. You only, in other words,

12:56.560 --> 13:05.520
we didn't choose hyper parameters like learning rate or way decay or dropout, which is focused on

13:05.520 --> 13:14.000
architectural hyper parameters, basically, you know, number players. What kind of player do you use

13:14.000 --> 13:21.520
in what stage in the network? And that's already took us like almost a week for every time we try

13:21.520 --> 13:30.240
this, it takes like a week on a few hundred GPUs. So after that, we moved to ImageNet. And because

13:30.240 --> 13:38.400
ImageNet, the network is bigger, we decided to use this idea called transfer learning, which is

13:38.400 --> 13:46.880
basically find a module that works well on SIFAR 10 and transfer to ImageNet, because searching

13:46.880 --> 13:55.360
directly on ImageNet would be very difficult. This is very expensive. Now in parallel with that,

13:55.360 --> 14:02.160
we also try a lot of methods in not only in reinforcement learning and but also in evolution.

14:04.240 --> 14:10.560
And we also observe the evolution that does as well or even better than reinforcement learning.

14:10.560 --> 14:21.680
So we slowly adopt more, sorry, evolutionary methods. And then, so some of the first

14:21.680 --> 14:26.320
of any intuition for why evolution works better than reinforcement learning?

14:28.080 --> 14:37.760
Oh, I see. So evolution is very flexible and very easy to implement, right? For example,

14:37.760 --> 14:45.520
in evolution, you just need to decide mutation and cross-over mutation meaning you have a network

14:45.520 --> 14:52.080
and then it just changes a little bit and across-over meaning take two networks and make them.

14:52.640 --> 15:00.960
So implementing evolution is actually quite easy. Now in contrast, reinforcement learning,

15:00.960 --> 15:07.200
because we're not experts in reinforcement learning. So we have, we try a lot of reinforcement

15:07.200 --> 15:13.520
learning methods like we started out with reinforced and then we tried something like PPO and so on

15:13.520 --> 15:21.600
and TIPO recently. And they're good, but they also require a fair bit about tuning to get working

15:21.600 --> 15:30.720
very well. So on the other hand, evolution seems to be very flexible in terms of implementation.

15:30.720 --> 15:38.000
And it's also wanting that evolution does quite well is that it's easy to diversify

15:39.840 --> 15:47.920
the models. So you can just try to, in reinforcement learning, once it happens, it will zoom in

15:47.920 --> 15:57.520
into a particular area, area in the surface and optimize the particular model. Whereas in evolution,

15:57.520 --> 16:04.560
it will diversify the population over time. It's easy to control the diversification process

16:04.560 --> 16:17.840
to get better models. So we use more evolution methods now and then feed forward.

16:19.840 --> 16:24.720
Okay, so in the second project, we already found one network that was kind of stay-of-the-art

16:24.720 --> 16:36.080
in computer fusion on power or slightly better on the on the stay-of-the-art of image net.

16:36.080 --> 16:41.600
So that was super exciting because we didn't think that it was possible. And then

16:44.080 --> 16:50.640
after that, we realized that this transfer learning has a problem that you know, you transfer the

16:50.640 --> 16:58.160
cell. And maybe the type of things that you want on cipher and the kind of cells that you want

16:58.720 --> 17:05.840
for image net is quite different. So we started searching on image net directly. Basically,

17:05.840 --> 17:11.440
you search a model on image net directly, but that became super expensive. You know, our

17:11.440 --> 17:18.160
back-up envelope calculation will take a few months for this to finish. So we realized that maybe

17:18.160 --> 17:24.480
one idea that we can have is search on small scale. Search a smaller model on cipher. Let's say,

17:25.280 --> 17:32.880
is that of searching the biggest model possible that you could find? Search for a small model,

17:32.880 --> 17:40.080
like that train only like five epochs in, you know, eight hours or something like that, right?

17:40.080 --> 17:47.600
So that's small. And after that, after we found a good model, we figure out a way to scale it up to

17:47.600 --> 17:57.200
be a size. So basically, make it new with larger image or make it deeper or make it wider.

17:58.400 --> 18:05.120
Is that scaling up in a learning way or scaling it up? Yeah, scaling up in a learning way.

18:05.120 --> 18:10.720
Okay. We're scaling up in a learning way. So the second stage of scaling up, basically,

18:10.720 --> 18:18.480
what we did was, you know, develop like a, we learned the way that we should be scaling up.

18:21.120 --> 18:27.200
And it looks like it works very well and that became something called efficient. Now it's been

18:27.200 --> 18:36.320
used quite a bit in various places at Google and in academia. And the smallest network that we

18:36.320 --> 18:43.920
found turns out to be super helpful for mobile devices. So people, because the network, small network

18:43.920 --> 18:51.280
seemed to be quite fast for mobile devices. That became something like a mobile network V3.

18:53.600 --> 19:01.120
Yeah, at Google. And after that, we say, you know,

19:01.120 --> 19:09.280
okay, now that we can get stay of the art on image net, but the problem is that a lot of

19:09.280 --> 19:16.640
building blocks that we used are very much, you know, building blocks that pre-describe or

19:18.240 --> 19:24.720
pre-describe by human experts. Let's say we have to make use of the rail law layer design,

19:24.720 --> 19:32.320
but design by human experts or we made use of a convolutional layer design by human experts

19:32.320 --> 19:39.920
or batch norm layer design by human experts. So we say, can we, can we design everything from

19:39.920 --> 19:48.320
scratch, basically assume that we know a non-pile library, like non-pile is this basically just,

19:48.320 --> 19:54.640
you know, matrix vector multiplication and bunch of non-linearity. Can you use a non-pile library

19:54.640 --> 20:02.320
to evolve the concept of machine learning? And that became something like auto-email 0.

20:03.280 --> 20:08.640
Which basically, yeah, that's auto-email 0. And that's basically the thought process behind it.

20:09.680 --> 20:16.080
In auto-email 0, 0, we didn't get stay of the art yet. But what's exciting about it is,

20:16.080 --> 20:26.640
it generate a program from just matrix vector multiplication and to do machine learning,

20:28.000 --> 20:33.360
which is super exciting. And I hope that using this method we can discover

20:34.560 --> 20:40.640
fundamental new fundamental new building blocks for machine learning.

20:40.640 --> 20:48.880
Yeah, folks haven't, if anyone listening hasn't taken a look at any of the blog posts or the paper

20:48.880 --> 20:56.320
for auto-email 0, really interesting. There's one particular diagram that I've seen a couple

20:56.320 --> 21:03.760
different versions of it, but it kind of walks through the process that this algorithm takes to learn

21:03.760 --> 21:13.600
a model and shows the various steps that it introduces and as well as the program that it outputs.

21:14.560 --> 21:18.000
And it's super interesting, you know, talk a little bit about this idea of,

21:19.360 --> 21:25.360
you know, having this model work by evolving a program. Where did that come from?

21:25.360 --> 21:34.880
So, go ahead, sorry. I was just going to say, you know, a lot of what we're doing is all software,

21:34.880 --> 21:40.080
it's all programs, but this in particular is like arithmetic arithmetic operations that

21:41.520 --> 21:49.040
in a very kind of simple way define all the steps that are used to evolve these algorithms.

21:49.040 --> 22:03.920
Yes, yes. So, if you think about what computer, sorry, machine learning experts are doing now

22:03.920 --> 22:08.400
are they? Is that basically they look at a computer program, you know, they use TensorFlow, right?

22:09.120 --> 22:14.480
They look at TensorFlow or PyTorch and they have a bunch of players and then they figure out

22:14.480 --> 22:22.720
to develop to write a computer program to write a new program to do machine learning.

22:22.720 --> 22:29.600
Let's say you want to do forecasting or something like that, right? Basically, you look at

22:30.560 --> 22:37.680
how people use LSTM and then you put together a computer program to do forecasting.

22:37.680 --> 22:46.560
Now, the act of writing that program is still now not learned, right? So, basically, it's basically

22:46.560 --> 22:52.720
human expert knowledge and changing that program can affect the quality of the model greatly.

22:53.520 --> 23:02.000
Now, stepping back a little bit is that that program, the auto and the program you just put

23:02.000 --> 23:09.040
assume a lot of knowledge about machine learning, right? The fact that the concept of gradient, you know,

23:09.040 --> 23:17.120
by propagation is assumed during this process, right? Because the different automated differentiation

23:17.120 --> 23:24.800
is built in into TensorFlow and PyTorch. So, we said that maybe what we can do is

23:24.800 --> 23:35.120
to step back a little bit from PyTorch and TensorFlow and start it from NumPy and using NumPy,

23:35.120 --> 23:45.200
can you put together a small computer program that can do, you know, SIFA classification or something

23:45.200 --> 23:54.800
like that? Yeah, and in the setup that we have, we have three functions. So, one function is

23:56.160 --> 24:05.120
setup, meaning that, you know, it's like a DNA that you start with, right? And then there's a

24:05.120 --> 24:12.240
predict function, meaning that whatever you have learned, you have to use it to force survival,

24:12.240 --> 24:18.480
right? So, you have to make, you've given a given situation view in, you have to make some

24:18.480 --> 24:25.840
prediction. And then the third function is the learn function. You know, it's, it has to learn

24:25.840 --> 24:32.240
so that the predict function is better over time. So, there's only this template have only three

24:32.240 --> 24:42.720
functions, setup, predict, and learn. And AutoML0 has to fill in the rest of the program,

24:43.440 --> 24:48.640
the rest of the instruction within these building blocks. Right, it's starting with those

24:48.640 --> 24:54.800
three functions being totally empty. Yeah, it started out with these three functions totally empty.

24:54.800 --> 25:01.520
So, at the beginning, it will start it, you know, amazing, right? At the beginning, it will do

25:01.520 --> 25:08.080
nothing. So, most of the programs will be garbage. So, you have no signal at all.

25:09.440 --> 25:16.080
You have no signal at all. So, by some random block, right? It will find some kind of

25:18.240 --> 25:27.360
dot product linear kind of layer that somehow does more than better than random. Just slightly a

25:27.360 --> 25:33.120
little bit, sorry, better than random, right? That's just basically the predict function does

25:33.120 --> 25:41.200
a little bit better than random. And then it will basically slowly put together one more,

25:41.840 --> 25:47.360
one more layer to become like a neural net. And then it will invent the concept of gradient.

25:47.360 --> 25:54.240
So, over time, it will come, it started from a very small program that is like do linear to

25:54.240 --> 26:02.000
go through many, many steps to eventually do a neural network. Yeah, and I referenced this

26:02.000 --> 26:08.240
diagram. If I'm understanding the diagram correctly here, identifying all these points where

26:09.120 --> 26:17.840
the algorithm evolves these techniques that, you know, humans do now. Like it eventually figures out

26:17.840 --> 26:25.760
how to do SGD. It eventually figures out like Rayloo and other things and it figures out techniques

26:25.760 --> 26:29.840
like gradient normalization and stuff. Am I reading that correctly that this is all stuff that

26:30.880 --> 26:37.120
has figured out? Yeah, yeah, yeah. So, basically, if you put it in the whole sequence,

26:37.840 --> 26:44.080
you know, the first step it will find like something like linear model, it find logic clipping,

26:44.080 --> 26:56.240
it will find learning rate, and it will find Rayloo, you know, and then normalizing the input norm,

26:56.240 --> 27:05.760
the gradient, and then, you know, doing having malinear interaction, things like that. So, things

27:05.760 --> 27:14.080
that, you know, like over the time in the last maybe 30 years of neural networks evolution.

27:14.800 --> 27:20.720
Just so that I understand there's no, there's no kind of priors, there's no like

27:22.320 --> 27:28.960
recipe book or techniques that are given to the model, the algorithm, and all it's figuring out

27:28.960 --> 27:40.240
all of this from nothing. Yeah. So, so the caveat is that AutoML 0 has access to 64 functions

27:42.080 --> 27:49.120
from Lampai. Okay, that's there's some bias here. So, this 64 function from Lampai,

27:50.960 --> 27:57.040
because the way that linear algebra works is very in favor of neural nets, right? So it will

27:57.040 --> 28:02.800
tend to develop things like neural nets, because linear algebra. But that's the only thing.

28:02.800 --> 28:06.720
There's no product there for it to use, eventually it's going to try and use it on some stuff.

28:08.480 --> 28:13.040
But it would be hard for it to find something like trees, because, you know,

28:13.680 --> 28:17.760
Nampai doesn't have the functions that kind of suitable for trees.

28:19.600 --> 28:24.880
But it's because Nampai, you can argue that Nampai is like a library that's very suitable for

28:24.880 --> 28:29.280
neural nets, right? So, it will evolve things that eventually look like a neural net.

28:31.600 --> 28:36.160
Now, what's surprising is that it did the whole process of developing models that started from

28:36.160 --> 28:44.240
linear and then put in development learning rate and normalizing the gradient and things like that.

28:44.240 --> 28:50.720
It looks very much like the evolution, our own evolution process of developing machine learning

28:50.720 --> 29:03.120
models. And so do you have you, have you, has this allowed you to see future techniques that we

29:03.120 --> 29:13.280
may learn to apply? Yes, but we haven't found anything extremely novel in the sense that like we

29:13.280 --> 29:19.280
never, we haven't seen it before. But we haven't found something that we haven't looked into

29:19.280 --> 29:29.520
more closely. So, in particular, it found this bilinear layer that normally, if you do a neural net,

29:29.520 --> 29:39.360
you would take X, multiply by some W, and then you apply some nonlinearity. Now, what we found

29:39.360 --> 29:54.560
during AutoML 0 is that it prefer X, WX. So, and then apply some nonlinearity. So, that basically,

29:54.560 --> 30:03.120
what they say is some kind of malinear interaction, right? And apparently this concept has been developed

30:03.120 --> 30:14.240
by other colleagues at Google. But we never, we didn't know that before project. So, we are in the

30:14.240 --> 30:22.000
process of trying out this layer on larger problems. But it looks like that layer is actually quite

30:22.000 --> 30:30.240
promising. Interesting. Yeah. So, the other thing is the concept of gradient normalization. So,

30:30.240 --> 30:36.960
basically you take the gradient and then you normalize it before you make the update. So, this is not

30:36.960 --> 30:40.640
not something new. So, people have done this before, but it's not very popular.

30:43.520 --> 30:54.000
And I think maybe one thing is that we are also trying this on bigger networks now ourselves.

30:54.000 --> 31:02.880
But if you can think about this process, it will aid the process of discovering either new idea

31:02.880 --> 31:08.720
or discovering older ideas, but we actually overlook because we have so many ideas in

31:08.720 --> 31:17.520
machine learning that we tend to overlook them. So, but some of the recent data is look promising

31:17.520 --> 31:24.080
or some of the ideas that it found. Do you think there's an opportunity to use a technique similar

31:24.080 --> 31:32.160
to what you did earlier with CFAR, where you apply AutoML 0 in a small way and then scale it up

31:32.160 --> 31:40.240
to bigger problems or networks? Yeah, I still we still thinking about how to do it effectively,

31:40.240 --> 31:47.920
but basically, basically you're right. So, the problems that we did in AutoML 0 is like a,

31:47.920 --> 31:55.840
it's not even CFAR, it's a Dow scale CFAR. It's a small version of CFAR. So, and then you know,

31:55.840 --> 32:09.360
the concept like gradient normalization or things like, you know, malinia interaction is something

32:09.360 --> 32:14.160
that it found and then we can take some of these layers and transfer into big problems. Now,

32:14.160 --> 32:20.560
unfortunately, the problem in CFAR, it's so downscale that you we cannot present it like an image.

32:20.560 --> 32:25.840
So, it is only 1D. And in 1D, you cannot find things like convolution on neural networks.

32:26.480 --> 32:34.080
So, that's a that's a limitation, but in the next step, we try to make it have like make it see an

32:34.080 --> 32:42.800
image rather than just a 1D image. Now, that's that's one aspect, which is basically how to scale

32:42.800 --> 32:49.040
this more effectively, which we did it before. The other thing that we did is to zoom in into a

32:49.040 --> 32:54.560
particular aspect of the neural network and can you do better. So, related to this is the

32:55.680 --> 33:02.720
paper that we published, you know, a couple days ago, on evolving a better activation and

33:02.720 --> 33:12.080
normalization layer. So, basically, in a neural network, people use this layer called BASHNOM

33:12.080 --> 33:19.760
and RELU a lot, right? This is in Dresnet. If you use Dresnet, you have BASHNOM and RELU

33:19.760 --> 33:25.680
and then there's a skip connection, right? And we say, let's use this into a single layer and

33:25.680 --> 33:34.240
search for a new mathematical operation to replace this BASHNOM and RELU. So, we accept the rest

33:34.240 --> 33:43.360
of the network, but we search for a mathematical operation from to to find a new layer. And it

33:43.360 --> 33:51.520
seems like to find a very good layer as a replacement and it works better than BASHNOM and RELU.

33:51.520 --> 33:58.880
And is the motivation there, primarily, network performance or is it computational or

34:00.000 --> 34:09.360
what is driving you to focus on those particular layers? So, BASHNOM and RELU, one thing the BASHNOM

34:09.360 --> 34:15.360
RELU is a good thing about it is it allows you to train with very big batch size, right? But when

34:15.360 --> 34:21.040
it's very small batch size, it doesn't work very well. So, it's a layer that Google would like,

34:21.040 --> 34:28.400
but most data scientists would not like because you don't have a big computer to train with a

34:28.400 --> 34:34.480
big batch. So, there's a replacement called Group NOM and RELU, which is very good, but it works

34:34.480 --> 34:44.400
very well on small batch size, but on the big batch size it's still a little bit worse than BASHNOM.

34:44.400 --> 34:55.520
So, it's still confusing to many people what layers to use, even at Google. So, developing a new layer

34:55.520 --> 35:01.360
is that, first of all, BASHNOM RELU is if it can be a good replacement for BASHNOM and RELU,

35:01.360 --> 35:07.280
that means the layer can be used by both internal researchers and external researchers.

35:07.280 --> 35:13.440
Well, and the second thing is BASHNOM RELU helps training, it stabilizes the training a lot,

35:13.440 --> 35:22.240
it speed up the training. So, we use it a lot in our work, so we really want to improve on that

35:22.240 --> 35:29.120
aspect. It plays a huge role. There's a paper where they just say that you just train only the

35:29.120 --> 35:37.920
BASHNOM layers and don't train the convolutional nets and you still can get a good performance.

35:37.920 --> 35:44.400
You know, it's not great, but it's good performance. It means that these layers play a very good

35:44.400 --> 35:52.480
important role. And is that paper? Talking about training those layers, only those layers from scratch

35:52.480 --> 35:59.760
or fine-tuning only those layers? Training those from scratch. Yeah. And so,

36:00.720 --> 36:06.480
is the idea that you're applying techniques like what you've done at AutoML Zero to finding

36:06.480 --> 36:17.840
these new layers or is that a totally separate approach? Oh, it's highly related. And I would say,

36:17.840 --> 36:26.560
you know, it's like an application of this idea of AutoML Zero. Yeah. Yeah.

36:27.520 --> 36:32.720
Cool. So, you've also been working on semi-supervised or self-supervised

36:34.720 --> 36:42.160
learning recently. Yes. Can you describe some of that work and how it relates to this stuff?

36:42.160 --> 36:48.960
Yeah. Sure. So, I've been working on this automated machine learning and

36:50.160 --> 36:58.240
automated architecture design and so on. And I gave talks about this new development and a lot

36:58.240 --> 37:02.320
of people came to me and complained. They say, you know, you automate the design of the neural networks,

37:02.320 --> 37:12.960
but I have more data than you. So, I bid you. So, I say, oh, that's a good point. So, I thought about,

37:13.600 --> 37:19.840
okay, automate machine learning is cool, but can you automate the labeling process?

37:21.200 --> 37:27.040
Can you automate labeling, right? Because most people would prefer to have more data.

37:27.040 --> 37:33.280
Because more data is very important, right? I don't mean that architecture is not important,

37:33.280 --> 37:41.040
but having more data is also very important. So, the question is, can you automate the process

37:41.040 --> 37:48.560
of labeling data? So, today, you don't have anything. Today, you basically get some

37:48.560 --> 37:53.120
unlabeled data and you give it to some human experts and then they label the data for you, annotate

37:53.120 --> 37:56.960
the data for you. So, you get techniques like active learning that can help you

37:56.960 --> 38:01.760
out the best labels, the best data to label, which helps. That's right. Yeah, active learning

38:01.760 --> 38:08.160
will speed up that process by selectively choose the example to annotate the data.

38:09.120 --> 38:18.240
Now, so, one idea that we had with the concept of pseudo labels, so fake labels,

38:18.240 --> 38:26.800
can you take your model and generate and evaluate on the unlabeled set?

38:28.160 --> 38:33.360
And now you have weekly label data. Our observation is this, right? Like, can you take your own model?

38:34.560 --> 38:39.200
Generate labels on a new set of unlabeled data and then put it back,

38:39.920 --> 38:45.440
assuming that they are correct labels and train the new model on that. Well, actually,

38:45.440 --> 38:52.880
I tried this idea many, many years ago too and it didn't work. And the problem is that if you

38:53.600 --> 39:01.840
take the model and generate the new label data, the new weekly label data, some of them are

39:01.840 --> 39:07.200
accurate, you know, a three would get a three, like that label of three, but sometimes a three

39:07.200 --> 39:14.960
would get a label of five. And this error would propagate, propagate into the new training and it would hurt

39:16.080 --> 39:20.000
the new training and the new training would not get better result than the old training.

39:21.440 --> 39:24.160
Right? Because it's the confirmation bias going on.

39:26.400 --> 39:32.800
Now, you, so I did not know any way to fix that problem.

39:32.800 --> 39:43.680
So, so I thought the concept of self labeling is a, is too good to be true. But recently,

39:43.680 --> 39:51.760
we realized that there's a way to overcome this process is when you train the new training,

39:51.760 --> 39:59.760
you inject a lot of noise into the new student. So that's, so you have a teacher that generate

39:59.760 --> 40:07.760
labels on a label data. You have a combined set of true label and weekly label data

40:09.040 --> 40:14.720
or pseudo label data and you train new students on this new combined set. When you train the

40:14.720 --> 40:23.360
student, make sure that you insert a lot of noise. So the a lot of noise in the student will,

40:23.360 --> 40:31.600
we still don't know this, how this happened yet. But the noise in the students seems to have

40:31.600 --> 40:38.960
this process that will overcome the confirmation bias. Probably because it will make the student

40:38.960 --> 40:44.560
more robust, right? Because it has, it has new, not trust the labels all that much. Exactly.

40:45.280 --> 40:49.840
Right? So it learned not to trust the label that much because it has to cope with so much noise.

40:49.840 --> 41:00.320
And amazingly, it actually outperformed the teacher. So the noise that we use is basically things like,

41:00.320 --> 41:08.720
you know, drop out and drop certain parts of the model, data augmentation, and super aggressively

41:08.720 --> 41:15.680
doing this data augmentation and noise. And eventually it would do better than the student.

41:15.680 --> 41:22.720
And you can just iterate the process, right? Once you have a better student, you label new data

41:22.720 --> 41:27.840
and then you put back. So we keep doing this and it seems to work very well.

41:28.720 --> 41:34.080
And how what's your performance metric or your benchmark?

41:34.080 --> 41:50.480
Yeah. So we worked on this data set called ImageNet. So I think when we work on this data set,

41:50.480 --> 41:58.080
the say of the art was like 82% and then using architecture search, we've pushed it into 85%,

41:58.080 --> 42:08.880
85.4% or something like that. And then using this auto label process, you get to 88.4. So

42:08.880 --> 42:20.960
3% improvement. So and keep in mind that 1% improvement on ImageNet at that high range is very

42:20.960 --> 42:28.480
difficult. And I'm talking about top one accuracy. Okay. And so is this, are you,

42:30.160 --> 42:38.320
is the setup here that you are you starting with your standard kind of 70% training and 30%

42:39.600 --> 42:48.400
test ratio or does that matter in this? Oh, so you're having your student

42:48.400 --> 43:00.320
label that 30% like did these ratios come into play into this? Okay. So we just follow the

43:00.320 --> 43:06.960
conventional ImageNet setup. So ImageNet has already had a split of maybe 1.2 million

43:06.960 --> 43:15.520
example of training and 100,000 for validation. And for other label data, you would use a different

43:15.520 --> 43:22.320
compass. So at Google, we have this data set called JFT that has about 300 million images.

43:24.240 --> 43:35.280
And we are operating other data beyond ImageNet that you've labeled using a model trained on ImageNet

43:36.160 --> 43:41.120
that you don't have any labels for. I mean, meaning that you don't have any labels for your

43:41.120 --> 43:46.240
external data set. You're just labeling it based on ImageNet. Some of those are going to be wrong.

43:46.240 --> 43:53.520
So you introduce the noise and it all seems to. Yeah. Yeah. So basically, yeah. So here's something

43:53.520 --> 43:58.800
that we find really surprising. There's one experiment in the paper that people did not check.

43:58.800 --> 44:04.800
But it's super exciting. Interesting is that we can propagate back images that so, you know,

44:04.800 --> 44:12.720
images has a 1,000 categories, right? Like, you know, some flowers, some dogs, and some cats,

44:12.720 --> 44:19.520
and so on. Now, we propagate back images that don't have, that don't look like any,

44:20.240 --> 44:26.320
like any categories in 1,000 category. So it could be like some kind of very strange animal.

44:27.200 --> 44:32.400
And the model which is just like us is the ImageNet. Yeah. It doesn't belong to any,

44:32.400 --> 44:42.000
yeah, any categories. And it still helps. Just by saying that, you know, this image is not any of

44:42.000 --> 44:47.360
these categories. And this put a lot of low, low percentage, low probability on a lot of these

44:47.360 --> 44:53.920
categories from propagate back is still okay. And that consistent with the consistent maybe with

44:53.920 --> 45:02.160
the idea that the images are even if they're not, you know, helping turn your classifier layers

45:02.160 --> 45:09.040
at the end, they're helping the network learn textures and, you know, low level features better.

45:10.000 --> 45:13.760
Yeah. So I think what really happened is that it just tried to learn

45:15.120 --> 45:22.640
information about natural images, right? And the fact that, basically, you give it a label.

45:22.640 --> 45:26.960
So the teacher give it a label. And then when you do data augmentation, you ship the image

45:26.960 --> 45:35.040
a little bit. Right. And then you say that the label is the same. So, you know, the probability table

45:35.040 --> 45:42.320
looks very similar. So the model has to work really hard to stay consistent in the production.

45:42.320 --> 45:50.240
So that's, and natural image in general is just very similar in similar ways, right? So it learns

45:50.240 --> 45:57.200
to be consistent in terms of labeling for new images. And maybe that's the reason why it's

45:57.200 --> 46:03.040
very helpful. Even though the images might not have anything to do with your other set.

46:04.320 --> 46:07.120
And you know, I'm curious in kind of articulating that

46:08.480 --> 46:14.320
intuition for what's happening is that based on, you know, all of your experience working with

46:14.320 --> 46:20.080
these kind of networks. So did you perform specific experiments to try to understand, you know,

46:20.080 --> 46:23.920
what the effect might be and, you know, what were those experiments?

46:25.600 --> 46:31.440
Oh, okay. So network introspection or any kinds of things. I'm just curious, you know,

46:31.440 --> 46:36.560
one of the kinds of things you've done, you know, to deepen your understanding of why this is working.

46:37.280 --> 46:43.040
Oh, you mean this particular experiment or in general?

46:43.040 --> 46:50.560
This particular experiment. Okay, so we try to lower the threshold, right? So when you take

46:50.560 --> 46:58.720
the your model and then label a new, a compass of 300 million images, we feel that we have a

46:58.720 --> 47:03.840
chance to feel that our low confidence images, right? So things that, you know, have very low

47:03.840 --> 47:13.280
prediction, low probability of having to be in any way of the class. So we can keep, you know,

47:13.280 --> 47:20.880
only 10,000 images or we can keep 30, sorry, 1 million images or 30 million images or,

47:20.880 --> 47:27.520
you know, 300 million images or anywhere in between, right? So we vary the threshold.

47:27.520 --> 47:38.720
And it seems like actually the threshold can be quite really low, right? So it could be, you know,

47:38.720 --> 47:44.480
in the 130 million or something like that, it's still okay. And then we visualize the image that

47:44.480 --> 47:53.920
are actually very low accuracy and a low confidence. And then we visualize them and see what they

47:53.920 --> 47:59.120
look like. And they don't, apparently, they don't look like anything like on image net. So that's what

47:59.120 --> 48:06.400
we found why the fact that they're helpful is very surprising. But why they are helpful, we still

48:06.400 --> 48:14.320
don't know. So is this a hypothesis? We don't, we don't know. Yeah. Yeah. Yeah. I, which is basically

48:14.320 --> 48:21.920
our current work is trying to analyze why it's helpful. Okay. And what do you expect your

48:21.920 --> 48:32.640
direction to be in analyzing that? We're probably going to look into the hidden state of the

48:32.640 --> 48:37.280
neural network. And you see, we see, you know, with these low confidence,

48:41.040 --> 48:46.960
where is it, is it trying to make the hidden state more consistent to each other,

48:46.960 --> 48:52.000
which is basically the same phenomenon that a lot of people do in contrastive learning in

48:52.000 --> 48:57.280
self-supervised learning is that they also have two images of data augmented image. And they're

48:57.280 --> 49:03.520
trying to make sure that the prediction is the same. And if you have two images of unrelated

49:03.520 --> 49:09.840
images, you make sure the prediction is different. Right? So I think this is what happened in here.

49:09.840 --> 49:14.800
So we're going to visualize some of the hidden state of the neural network with and without

49:14.800 --> 49:21.040
these low-confident example and see, you know, what happened during training. Okay.

49:21.040 --> 49:25.360
That's that's one direction that we think that we will be doing. Okay. Okay. Yeah.

49:27.120 --> 49:33.840
Cool. You were also an author on Mina. Yes. Were you involved in that? Can you talk a little

49:33.840 --> 49:41.280
bit about that work? Yeah. So many years ago, I worked on something called sequence learning,

49:41.280 --> 49:49.440
and to end neural networks through NLP. And that's still for translation. And I spent like two

49:49.440 --> 49:54.960
years after that trying to be like a chatbot to chat with me, because I always fascinate like,

49:54.960 --> 49:59.760
can you have an agent that can talk to me, you know, in the intelligence? I've been all your emails

49:59.760 --> 50:06.240
and you know, there's a down mic here or were you trying to, were you trying to have a communication

50:06.240 --> 50:10.480
with the chatbot or were you trying to replace yourself, quote unquote, with the chat by having

50:10.480 --> 50:18.720
other people be able to talk to you? Both. Both. You know, just talking to computers is fascinating.

50:18.720 --> 50:27.760
Yeah. Yeah. And so I failed. And then we, I, why, and then we ended up meeting this person

50:27.760 --> 50:35.520
called at Google and a very great engineer at Google called Daniel. And he, he said, how about

50:35.520 --> 50:42.160
let's work together to make this better. And we did a lot of work on scaling up some of the models

50:42.160 --> 50:50.400
that we built. We collected a huge amount of social media data, you know, people talking on the

50:50.400 --> 50:56.560
internet. And then we train a huge model, like a model that I, I, you know, maybe a hundred times

50:56.560 --> 51:07.120
because of what I train back in the day. And it can do multi-tone conversations. It started doing

51:07.120 --> 51:14.720
multi-tone conversation very well. And one of the magic moments that I really think that

51:14.720 --> 51:26.080
truly magic is that it actually invented like a joke. It invented a joke. So people talk, it's, it's, it

51:26.080 --> 51:38.960
made this pun that, you know, horses go to Harvard. So cows go to Harvard, cows go to Harvard. And horses

51:38.960 --> 51:47.440
go to Harvard. But it's very fascinating. I remember, it's been a while since I looked at that one,

51:47.440 --> 51:54.320
but I remember it was the, you know, when you describe it as inventing this joke, it wasn't anywhere

51:54.320 --> 52:00.320
in the training data that you could find, right? Yeah. The only instance of mentioning

52:00.320 --> 52:08.800
Hayward is, doesn't have any, anywhere, there's only one instance of mentioning the word Hayward in the

52:08.800 --> 52:15.600
training data. And we look at that context, and it has nothing that looks like what we are like

52:15.600 --> 52:20.400
horses go to Harvard at all. And so what do you think was happening there? How did that work?

52:21.360 --> 52:27.120
I guess unlike what we see in kind of conventional language models, like even the big ones,

52:27.120 --> 52:32.320
they're picking up stuff that they've seen before generally, right? Okay. So I can tell you my

52:32.320 --> 52:38.560
version. My version is a following. My version is a following. We still don't know, right? That's

52:38.560 --> 52:46.960
what I say. It's a magic moment. I think, first of all, a lot of social media jokes about puns.

52:46.960 --> 53:00.320
Right? A lot of, you know, like we find puns kind of funny. And so, and so, and a punny. And so

53:02.000 --> 53:09.040
in the training data, we train with my pair at coding, meaning that we take a world and we break it

53:09.040 --> 53:18.000
down into disease. So it's, Hayward is not a world, it's right, two words, Hayward, and Harvard,

53:18.000 --> 53:24.000
and etc. So it must have learned the concept of puns. And you wouldn't understand that, you know,

53:24.000 --> 53:30.880
Harvard and Hayward somehow is kind of related. So it made up this pun. So two things, learning

53:30.880 --> 53:39.600
the concept of puns and learning how to put together like a new concept. And yeah, it does a lot

53:39.600 --> 53:46.240
of new things like it make a joke. Like it makes jokes about why chicken crossed the road,

53:46.240 --> 53:51.440
some things like that. I don't remember the particular jokes I can find it and send it to you.

53:53.200 --> 54:00.160
It makes all kind of new jokes that we never found in the training data. And it's truly

54:00.160 --> 54:08.080
fascinating. And so where do you, you know, that line of work, where do you see that going?

54:09.360 --> 54:15.360
Well, what were the key, you know, was this, was this kind of simply, you know, quote unquote,

54:16.400 --> 54:24.640
an instance of scaling up the training or where there are some, you know, new techniques or

54:24.640 --> 54:32.640
novel things that were developed, you know, in the model that you can see applying elsewhere.

54:33.760 --> 54:42.240
I see. Well, first of all, it basically tells a lot of people that scaling up is very

54:42.240 --> 54:52.240
effective. Why to be, you know, NLP models, right? Like I spent two years failing doing that

54:52.240 --> 54:57.280
chatbot. And then suddenly someone came along and found a solution. And the solution is kind of

54:57.280 --> 55:05.440
simple, which is basically just make the model a lot larger. We also found a lot of new interesting

55:05.440 --> 55:11.120
insights is that during the process of building the bar, we have a lot of difficulty how to measure

55:11.120 --> 55:15.920
the performance of the bar. So basically, we, we, one thing that we want to measure is how human

55:15.920 --> 55:22.560
like it is, right? We want to be able to have a conversation like a human like. So we, to measure

55:22.560 --> 55:26.800
human life, we always want to ask human to look at a conversation that we have with about because

55:26.800 --> 55:33.680
we built many models of the box, right? Right. Many, many models of the box. And every model,

55:33.680 --> 55:37.600
we have to look at a used human expert to look at the conversations and say, hey, how many human

55:37.600 --> 55:46.400
like this is? So during the process, what in that we observe is that as the complexity of the model,

55:46.400 --> 55:51.200
so, you know, as you train the model better, so the objective function is called the complexity,

55:51.200 --> 55:57.120
is like a very logo objective. This is basically how, how well you predict the next world, right?

55:57.840 --> 56:03.760
And we've noticed that this objective function correlates very well with human,

56:03.760 --> 56:13.920
a judgment of human likeness of the bar. And, and then we did a real plot of perplexity,

56:14.720 --> 56:18.560
which is the local objective function, you know, tricked me in the next world. And,

56:20.080 --> 56:27.200
human likeness, and we saw like a, a strong correlation. So that's basically another contribution

56:27.200 --> 56:32.000
for NLP is that actually this local objective function, which is just predicting the next work,

56:32.000 --> 56:38.320
next work, if you do a good job at it, turns out it's also a more global objective function,

56:38.320 --> 56:47.120
which is human likeness, how, how I can behave like a human. So local mimicking is global mimicking.

56:48.720 --> 57:00.480
What's interesting about that is that, you know, perplexity being kind of predicting the next work,

57:00.480 --> 57:07.280
what, what makes jokes and puns work is that the next word is a surprise compared to what you saw.

57:07.280 --> 57:12.720
So how do you get something that's good at jokes, but also is optimizing on perplexity?

57:16.320 --> 57:22.960
Yeah, that's, that's the reason why I say it's very simple, it's surprising that low, so

57:22.960 --> 57:29.840
these are, let me try to say it again. So there must be a low, a global objective function that we

57:29.840 --> 57:37.840
are optimized, we like making fun, so that we can get engagement, right, making fun, saying something

57:37.840 --> 57:44.480
meaningful, right, that's, that's global, right, but something that's not just optimizing the next

57:44.480 --> 57:50.000
world. But what I'm just claiming is that local objective function is that actually just predicting

57:50.000 --> 58:00.880
the next world is highly common, global, that we're going to relate to the global. And I mean,

58:02.240 --> 58:08.080
I guess you could argue that your what perplexity doing is doing in the case of the joke is

58:08.080 --> 58:13.280
predicting surprise, it's not like flying to the surprise, what is predicting the surprise based

58:13.280 --> 58:21.040
on? It's predicting the surprise. Yeah, you could say that. Yeah, yeah. Huh, interesting. Cool.

58:23.040 --> 58:29.200
Well, thank you so much, Quark, for taking the time to share with us what you're up to and

58:31.280 --> 58:37.200
and kind of walk us through these recent works here is really, really interesting stuff.

58:37.200 --> 58:45.440
Oh, thank you so much. It's a pleasure to talk to you. Same, same. Awesome. Awesome. Thank you.

58:47.760 --> 58:53.200
All right, everyone. That's our show for today. To learn more about today's guest or the topics

58:53.200 --> 58:59.600
mentioned in this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast,

58:59.600 --> 59:05.600
please subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for

59:05.600 --> 59:07.600
listening and catch you next time.

