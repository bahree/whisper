WEBVTT

00:00.000 --> 00:15.440
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:15.440 --> 00:27.120
Alright everyone, I am on the line with Sammy Koyejo. Sammy is an assistant professor at

00:27.120 --> 00:33.280
the Department of Computer Science in the University of Illinois, just a couple of hours away from

00:33.280 --> 00:38.240
where we are here in St. Louis, Sammy. Welcome to the Tumel AI Podcast. Thank you. It's

00:38.240 --> 00:42.160
a pleasure to be here. It is great to have you on the show and I'm looking forward to learning

00:42.160 --> 00:48.640
a bit more about your research. Let's start up. Absolutely. Let's start up by having you share a bit

00:48.640 --> 00:57.360
about how you came to work in ML and AI. Sure. So a reasonable place to start would be

00:59.040 --> 01:06.000
Brown grad school where I was interested in working on physical air communication systems.

01:07.200 --> 01:11.520
As time went on, I got more and more interested in intelligence,

01:11.520 --> 01:18.480
bit onto those systems and I started working on an area known as cognitive radios. Over time,

01:18.480 --> 01:23.680
I spent more and more time thinking about the intelligence piece and maybe less on the communication

01:23.680 --> 01:29.520
piece and eventually I started working on machine learning as my main area of research. So by the

01:29.520 --> 01:35.920
time I finished my PhD, I switched advisors and worked in a different area and specifically,

01:35.920 --> 01:44.080
I was looking at probabilistic models, Bayesian inference and related topics. My thesis focused on

01:44.080 --> 01:49.680
what is known as constrained or regularized Bayesian inference. It's a style of probabilistic

01:49.680 --> 01:55.120
inference where you add part of your prior structure is built into the inference algorithm and not

01:55.120 --> 02:03.200
just as part of the prior specification. So it's an interesting thread. From there, close to

02:03.200 --> 02:10.960
my PhD, I started to work with cognitive neuroscientists and so I got interested in cognitive

02:10.960 --> 02:19.280
neuroscience and I ended up spending a couple of years at Stanford building machine learning

02:19.280 --> 02:25.760
methods and machine learning models for cognitive science applications, neuroimaging applications

02:25.760 --> 02:33.120
and related ideas. About four years ago I started here at Illinois and I think of my research

02:33.120 --> 02:41.760
broadly as adaptive and robust machine learning. My research is quite broad, so I work in a bunch

02:41.760 --> 02:48.080
of different areas. I still work in cognitive neuroscience and neuroimaging a bit and building

02:48.080 --> 02:54.000
machine learning tools for those applications. But in addition, in my core machine learning work,

02:54.000 --> 02:59.920
I think about scalable machine learning, fault tolerance and machine learning and a variety of

02:59.920 --> 03:08.240
topics related to how to construct machine learning systems that make good predictions for

03:09.280 --> 03:14.800
various kinds of evaluation metrics and how that interacts with human decision making.

03:14.800 --> 03:22.080
So sort of a variety of research threads but hopefully coming together in a coherent set of ideas.

03:22.080 --> 03:27.440
Nice. Now I'm so kind of intrigued by this idea of a cognitive radio. How far did you get into

03:27.440 --> 03:35.040
that topic? Is that an actual thing? Yeah, yeah, it's an actual thing. I haven't followed

03:35.040 --> 03:41.200
the development over the past few years but it came about at some point where essentially

03:42.160 --> 03:50.560
ran out of spectrum in the US and in many developed countries. And so in order to get more

03:50.560 --> 03:58.960
bandwidth, those idea to make use of spectrum that ends up being sort of dead spots. So in various

03:58.960 --> 04:05.200
cities, different channels say broadcast on certain radio frequencies but leave some others.

04:05.200 --> 04:12.560
And so one way to construct a radio that works in these systems is to actually sort of hop between

04:12.560 --> 04:17.200
different frequencies by detecting when a frequency is in use and when is not in use and

04:17.200 --> 04:22.400
you sort of hop in between and sort of communicate for a short time. And this way you can make

04:22.400 --> 04:29.200
better use of the spectrum and actually make use of spectrum that otherwise would you could not use

04:29.200 --> 04:35.440
for any other purpose. So it's an interesting idea. I believe that there are systems built out.

04:35.440 --> 04:42.000
I don't know the extent to which there are popular implementations of these things because

04:42.000 --> 04:48.960
again, I haven't followed this idea. But it was there in response to I think quite difficult

04:48.960 --> 04:56.880
problem in the communication space of running out of wireless spectrum. Okay, interesting. Interesting.

04:56.880 --> 05:04.800
So you mentioned a couple of key areas in your research. One focused on metrics and metric

05:04.800 --> 05:12.080
elicitation, the other focus on robust distributed learning. And let's start with the first of those.

05:12.080 --> 05:17.360
When you're thinking about metric elicitation, tell us a little bit more about the problem that you

05:17.360 --> 05:24.080
are trying to solve there. Yeah, yeah, happy to. So it's a bit of a history. So it's probably worth

05:24.080 --> 05:30.880
taking us half a step back and giving you some context. Sure thing. So the way I like to think about

05:30.880 --> 05:37.280
the problem is roughly when you build a machine learning system and you're sitting using the

05:37.280 --> 05:42.800
machine learning system for decision making, many real-world decision making tasks are actually

05:42.800 --> 05:48.640
quite complex and they involve trade-offs between different factors. And the issue is that many of

05:48.640 --> 05:54.320
our default machine learning metrics don't account for these trade-offs in decision making.

05:54.320 --> 06:02.080
And so at some point around, it's not been a few years, but I think around 2014 or so,

06:03.280 --> 06:09.120
I got interested with, at the time, my postdoc advisor, Pradeep Ravi Kumar,

06:10.000 --> 06:17.120
in thinking about how to construct machine learning models that could optimize complex metrics.

06:17.120 --> 06:21.760
So think of, in fact, there are lots of great sort of common examples. So

06:21.760 --> 06:28.560
and information retrieval, popularly, people use what is known as the F measure. So this is some

06:28.560 --> 06:33.680
ratio of precision and recall. It's commonly used for prediction in these settings because

06:33.680 --> 06:39.200
this thought to be a good measure of performance. But up until some of our early work,

06:39.920 --> 06:44.080
there wasn't a good understanding of how one built a machine learning system that was specifically

06:44.080 --> 06:50.160
good at optimizing F measure. So before that, you build your system to be good at optimizing accuracy,

06:50.160 --> 06:55.040
and which again, we're fairly good at doing or maybe some weighted accuracy. But there wasn't

06:55.040 --> 07:00.720
a good sense of what to do if I changed the measure into something more complex like F measure.

07:01.440 --> 07:06.560
So we started this series of papers over a few years where we got better and better understanding

07:07.040 --> 07:12.640
how to construct good learning models for, again, what we call complex metrics. What is sometimes

07:12.640 --> 07:18.560
called non-decomposable metrics. And they're called non-decomposable metrics because I can't write

07:18.560 --> 07:25.680
down the metric as an average. And once you break this averaging possibility, lots of standard tools

07:25.680 --> 07:32.560
that we like to use, like say, graded descent in a sort of straightforward way, or tools that

07:33.360 --> 07:39.760
we like to use in terms of analysis start to break down. And so it becomes somewhat of a more

07:39.760 --> 07:46.160
challenging problem to solve. And so like I said, we had a series of papers where we tried to

07:46.160 --> 07:50.720
understand these metrics a lot more and come up with good methods for optimizing them. And I'd give

07:50.720 --> 07:56.560
these talks and really excited about early work on this showing, hey, give me your really complex

07:56.560 --> 08:02.640
evaluation measure. I can tell you how to optimize it. And I should say that we had a characterization

08:02.640 --> 08:07.040
that was quite general. And so it could adapt to different notions of what good measures are.

08:07.600 --> 08:13.520
How complex are you able to get, often when I think about metrics in the way you're describing them,

08:13.520 --> 08:20.560
you've got your metrics that the data scientist or machine learning engineer is trying to build

08:20.560 --> 08:26.720
a system to, and then maybe there are metrics that aren't the one they're using to train their

08:26.720 --> 08:33.920
models, but they're still kind of in their domain like your F-score. But then on the other end

08:33.920 --> 08:38.320
of the spectrum, there are the actual metrics that business people care about which don't look like

08:38.320 --> 08:45.120
either of those, or often don't look like either of those. Do your research get to that other end

08:45.120 --> 08:52.960
of the spectrum? Yes and no. So I can give you some sense of the scope of where we can say some

08:52.960 --> 08:59.840
useful things. Okay. So the first few papers, I should take a step back and mention that for

08:59.840 --> 09:05.920
classification problems, the primary statistic that one uses to measure whether you have a good

09:05.920 --> 09:11.440
model or not is something related to what is known as the confusion matrix. So the confusion matrix

09:11.440 --> 09:17.600
is essentially measuring for every kind of prediction and for every ground truth label. How often

09:17.600 --> 09:22.240
in a setting, but a certain label do you make a certain prediction? So I look at the average times

09:22.240 --> 09:27.440
I say, predict one when the ground truth is one, and this gives me some number. It's a confusion

09:27.440 --> 09:32.400
matrix for one one, or I can do this for one two. How often do I predict two when the ground truth

09:32.400 --> 09:36.720
is one, or how often do I predict three when the ground truth is one, and all combinations of this

09:36.720 --> 09:41.760
for say a multi-class classification problem. So with K classes, end up with a K squared confusion

09:41.760 --> 09:50.640
matrix for every pair of ground truth and prediction. So initially we worked on linear combinations

09:50.640 --> 09:57.040
of the entries as a confusion matrix. So you could imagine, for instance, that predicting three

09:57.040 --> 10:01.280
when the ground truth is one is more expensive than predicting two when the ground truth is one.

10:01.280 --> 10:05.840
And ideally you want to predict one when the ground truth is one. We're kind of familiar with

10:05.840 --> 10:10.640
this kind of scenario when we, you know, in its simplest form, like false positive and false

10:10.640 --> 10:17.040
negatives and some applications. Exactly those. Yes. I went to the more general multi-class case,

10:17.040 --> 10:20.480
but I think the binary case is enough for illustrative purposes. So in the binary case, it's true

10:20.480 --> 10:26.000
positives, true negatives, false positives, false negatives. Thanks for pushing me that direction.

10:26.000 --> 10:31.760
So initially we worked through, say, linear weighted combinations of this. Eventually we got

10:31.760 --> 10:38.960
two ratios of linear things, which captures things like f measure. They can write metrics like

10:38.960 --> 10:44.960
that as ratios of linear things. Now we're at the point where we can pretty much do sort of any

10:44.960 --> 10:50.800
function of the confusion matrix and we can come up with what is known as a consistent estimator.

10:50.800 --> 10:57.120
So some estimator that we know will have good large sample properties. So some learning algorithm

10:57.120 --> 11:04.400
where I can come up with a learner or optimization process that I know will have good behavior

11:04.400 --> 11:09.680
in terms of optimizing for some arbitrary function of confusion matrix. So how far does this go to

11:09.680 --> 11:15.760
reward metrics? We think it goes reasonably far, but it's clearly once you get to real world

11:15.760 --> 11:20.400
settings, many of the things you care about don't always, I'm not always captured by the confusion

11:20.400 --> 11:25.760
matrix and or cannot be reduced in that sort of simple way. Though many are, so many things

11:25.760 --> 11:31.200
are just weighted forms of some weight attacks different kinds of mistakes. So as long as you

11:31.200 --> 11:36.480
sort of roughly in a setting where the thing you care about is the ticks of mistakes and waiting

11:36.480 --> 11:42.480
on those or even in survivatory functions of those, they're pretty good algorithms now that

11:42.480 --> 11:49.200
based on work that I and others have worked on to build to build good algorithms for optimizing

11:49.200 --> 11:54.880
these. So like I said, or as I was going to say, I would give these talks being excited by

11:55.680 --> 12:01.760
this line of algorithms and saying, hey, we can do in any metric you like and you get the feedback,

12:01.760 --> 12:05.600
well, now you've made it clear that there are lots of different ways to measure what performance is.

12:05.600 --> 12:11.120
Well, which of these should I use? So it's quite unclear. Now, once it's clear that there are many ways

12:11.120 --> 12:18.560
of measuring performance, it becomes trickier to think of or to pick which one is best suited to

12:18.560 --> 12:26.320
a certain setting. And so the idea of metricalistation, which is where we ended up with, is trying to

12:26.320 --> 12:33.280
turn the problem on its head. I'm trying to ask, can I elicit good metrics by interaction with

12:33.280 --> 12:38.720
experts or with users or with panels of experts? So are there strategies that can come up with,

12:38.720 --> 12:46.160
that can interact with an expert to figure out what measure is closest is a close approximation

12:46.160 --> 12:52.400
to how they're determining trade-offs or value of different kinds of predictions. And so the idea

12:52.400 --> 12:58.960
is that if we can do this well, then you can optimize for that metric. And more importantly,

12:58.960 --> 13:04.640
this metric is transportable. So I could change the class of models I'm optimizing. I could change the

13:04.640 --> 13:10.640
data distribution. I could change the setting in some ways. But as long as it's capturing the expert

13:10.640 --> 13:16.400
trade-offs, then this is a good way of measuring good performance. And so this is something that I can

13:16.400 --> 13:25.440
use as I change the settings. So the example I like to give is a simplest example I think that

13:25.440 --> 13:34.240
maybe illustrates the idea of elistation is in a healthcare setting where say a doctor would

13:34.240 --> 13:42.240
try to is interested in constructing an automated health decision-making system. And so the doctor

13:42.240 --> 13:48.960
is an expert. They have some notion of how expensive say for a certain set of measurements,

13:48.960 --> 13:58.080
how expensive it is to say misdiagnosis, so misdiagnosis when there's actually sort of some disease

13:58.080 --> 14:05.040
there or overdiagnose somehow. So like predict the say that someone has some disorder when they

14:05.040 --> 14:12.080
actually don't. And so this the actual trade-off depends on the costs of treatment and maybe

14:12.080 --> 14:17.840
sort of potential side effects, sort of all these other considerations. But if you think about

14:17.840 --> 14:22.640
this as a decision-making problem, you can imagine that you could sort of compress everything down to

14:22.640 --> 14:26.880
there's some cost to making a prediction in one direction, some other cost making a prediction

14:26.880 --> 14:35.120
in the other end. Often going from this intuition idea to a concrete trade-off function is hard.

14:35.120 --> 14:42.400
If it was easy then one could sort of then construct models to directly try to optimize the doctor's

14:42.400 --> 14:47.600
trade-off function. But it's hard to do in real settings. And so what the idea of metricalistation

14:47.600 --> 14:54.560
is is to come up with a strategy to interact with say the doctor is an expert. And based on this

14:54.560 --> 15:01.120
interaction, actually pinpoint the right trade-offs that should correspond with their preferences.

15:01.120 --> 15:05.280
And then you can optimize those preferences directly and if you construct things at a downstream

15:05.920 --> 15:15.040
models. Other examples we have considered say things like ranking models. We haven't worked on

15:15.040 --> 15:19.360
this. We don't have results for this yet, but in the pipeline are things like say ranking models

15:19.360 --> 15:25.600
if you're building recommender system. Imagine that your users have different preferences

15:26.160 --> 15:31.600
and the order in which they want to see things. So you could imagine then constructing a

15:31.600 --> 15:37.680
listation procedure that did a series of say AB tests with your users and tried to pinpoint

15:37.680 --> 15:44.000
for the user population what the best approximation to the best sort of ranking cost function should

15:44.000 --> 15:51.280
be for their setting. Is the process always akin to AB testing? You mentioned that in the

15:51.280 --> 15:56.560
scenario of users. It is the same thing applied to doctors. You know, what do you prefer choice,

15:56.560 --> 16:02.400
say choice B? So there are many ways you could imagine developing a problem like this.

16:03.200 --> 16:10.240
We chose to go with the pairwise preference approach. The motivation after talking to experts

16:10.240 --> 16:16.720
in the area was that with pairwise preferences, it's sort of much more likely that we can get

16:18.240 --> 16:25.600
users to easily give us answers to the comparisons than if we tried other ways of

16:25.600 --> 16:31.520
interacting with experts or with users. Because somehow pairwise preferences are easier

16:32.720 --> 16:38.880
for experts to give feedback on than other kinds of ways of querying.

16:38.880 --> 16:45.520
Okay. So I should mention this is work led by my student, Gauru Shirendiani,

16:46.640 --> 16:53.040
here at Illinois and in collaboration with colleagues, Rutameta and sort of other folks at Illinois.

16:53.040 --> 17:00.080
So this is work that again a few years, at least the listation piece has been a couple years

17:01.040 --> 17:06.960
old. And again, it's trying to answer this question of what metric should I optimize in order to

17:06.960 --> 17:11.200
get my machine learning system to do sort of the thing in the real world that I wanted to do.

17:12.080 --> 17:16.160
So again, the thing in the real world is rarely optimize accuracy. It's typically something

17:16.160 --> 17:22.560
much more subtle, something much more complex. And we're starting to get initial answers for

17:22.560 --> 17:27.360
sort of reasonable algorithms for trying to answer this question. I also should mention,

17:27.360 --> 17:33.360
so the pain point in trying to construct this listation procedures is how many queries

17:33.360 --> 17:39.840
are going to ask the expert because in principle, in theory, if you had had infinite queries,

17:39.840 --> 17:46.480
you could elicit anything. But yeah. So the idea is in as few queries as possible get to

17:47.280 --> 17:57.040
formulation of your metric that is as accurate as possible or matches as closely to what the expert

17:57.040 --> 18:02.560
would do you define it as kind of produces a classifier that most closely matches what the expert

18:02.560 --> 18:07.360
would predict. Like, how do you tie it back into the metric of the classifier?

18:08.400 --> 18:12.080
I think you're talking about evaluation, which is tricky in these problems.

18:14.080 --> 18:19.280
But the target, the conceptual target is to like get the best approximation to the

18:19.280 --> 18:26.000
trade-off function that the expert is using. The practical evaluation is if I sort of change the

18:26.000 --> 18:32.240
classification setting or change distribution in some way, I should be able to get the same sort

18:32.240 --> 18:38.240
of outcomes as ideal outcomes from the model as what the expert would pick as ideal outcomes

18:38.240 --> 18:44.560
from the model. So it should replicate the expert's predictions, but again, this idea of

18:44.560 --> 18:49.760
transportability. So we should be able to do this in a variety of settings. I should mention it's

18:49.760 --> 18:55.200
quite close. Metricalistation is not that far from ideas like inverse reinforcement learning.

18:56.000 --> 19:00.800
So again, there's a whole literature primarily in the reinforcement learning world,

19:00.800 --> 19:04.800
whereas a lot of focus on learning good reward functions, and sometimes by learning this

19:04.800 --> 19:10.880
good reward functions by interaction with humans. In some sense, we're solving a easier version of

19:10.880 --> 19:17.040
the problem than what the RL folks are trying to solve. We take advantage of a lot of additional

19:17.040 --> 19:22.080
structure that comes from the classification version of the problem, which is most of what we've

19:22.080 --> 19:27.760
focused on so far. So we can get sort of much stronger results, much better algorithms than often,

19:27.760 --> 19:31.200
what could get and does it much more general reinforcement learning setting, because you don't

19:31.200 --> 19:38.080
have to think about sequentiality of sort of in the same way that an RL setting would have to

19:38.080 --> 19:45.280
reason about. One issue also I should mention is an investor reinforcement learning in particular,

19:45.280 --> 19:50.800
there's not always a focus on actually getting the reward function right. So to your point earlier,

19:50.800 --> 19:55.680
often the focus is on replicating behavior, not necessarily getting your reward function right.

19:55.680 --> 20:01.120
So in this say driving setting, I want to be able to drive the same way that the human drove

20:02.800 --> 20:07.680
where the human is the expert. So get me the reward function that does this the best in the setting.

20:07.680 --> 20:14.640
Not necessarily in this in these settings, sometimes there is not a focus on say what happens if I

20:14.640 --> 20:24.320
change the environment a little bit. And so now the reward function has sort of so tuned to the

20:24.320 --> 20:29.040
original setting that it doesn't work as well in the new setting. So one difference is that we're

20:29.040 --> 20:35.440
very focused on again, this idea of transportability. So we're the focus is still on learning or

20:35.440 --> 20:41.200
eliciting metrics such that they are agnostic to things like data distribution and the specific

20:41.200 --> 20:47.680
learner they're using and sort of other kinds of important, but things that want to abstract

20:47.680 --> 20:52.000
the way because we want it to have these trade-off functions that you can then apply in sort of

20:52.000 --> 20:57.600
general settings. So learning a simple setting potentially apply in a more complex setting is

20:57.600 --> 21:04.000
maybe one way to think about it. And so how do you get to that level of generalizability? Is it

21:04.000 --> 21:09.680
in the you know your selection of data that you're training on or does it have to do with the

21:10.320 --> 21:15.680
questions or sequence of questions that you're asking the pairs that you present or they're like

21:15.680 --> 21:23.840
you know black art techniques like prop out or things like that. Now it's actually in fact for

21:24.880 --> 21:34.240
the binary classification setting with linear trade-offs in the confusion matrix. It essentially

21:34.240 --> 21:39.680
balls down to a very simple binary search. So actually many settings that it balls down to

21:39.680 --> 21:45.600
almost trivial sort of textbook algorithms. And all the work is in characterizing sort of how

21:45.600 --> 21:52.640
do I want to define the feasible metric space and how does one reason about how to search efficiently

21:52.640 --> 22:00.080
into space. And so often once you do that work the final step of the algorithm to elicit in many

22:00.080 --> 22:05.680
settings is actually much more straightforward than you might imagine. So the thing that enables

22:05.680 --> 22:17.360
transportability is so far we've mostly focused on settings where the metric of interest

22:17.360 --> 22:24.320
is some function of the confusion matrix elements. And what's interesting to note is that sort of

22:24.320 --> 22:31.760
trade-offs in confusion matrix elements don't depend on sort of how good you are in classifying.

22:31.760 --> 22:36.320
They just trade-offs between different kinds of errors. So those kinds of functions are

22:36.320 --> 22:41.840
agnostic really to sort of if you're able to estimate them well enough they're agnostic to

22:41.840 --> 22:48.160
things like data distribution function class things like that. So for instance specifically again

22:48.160 --> 22:54.960
in the doctor example if you're interested in the cost of misdiagnosis versus sort of overdiagnosis

22:54.960 --> 22:59.200
of missing versus not missing a diagnosis. If you can think about this as a binary classification

22:59.200 --> 23:05.520
problem but just weights between false positives and false negatives. The what matters is getting

23:05.520 --> 23:11.920
those weights right and the actual value of the false positive and false negative doesn't matter

23:11.920 --> 23:16.000
as much so you can have a learner that's much better at getting low false positive false negatives

23:16.000 --> 23:20.640
and not to learn that's much worse at getting low false positive false negatives. So this

23:20.640 --> 23:25.760
differences would be say differences in using a linear model versus say maybe using a deep learning

23:25.760 --> 23:30.960
model in these two settings. So they would have different confusion matrix trade-offs but as

23:30.960 --> 23:36.880
long as you get the trade-offs right the actual values are not that important. So again we've

23:36.880 --> 23:43.840
focused mostly on settings where that property is mostly true. In classification this is most

23:43.840 --> 23:48.880
often the case if you're focusing on functions of the confusion matrix so it comes up sort of

23:48.880 --> 23:53.680
naturally based on the problem definition that we're interested in. And are there any

23:53.680 --> 24:01.360
properties that arise that relate the you know for example the number of pairs that you have

24:01.360 --> 24:07.360
to present to the dimensionality of your confusion matrix or something like that? Absolutely yeah so

24:07.360 --> 24:15.760
it roughly grows about linearly with the size of the confusion matrix. So to get roughly the

24:17.120 --> 24:22.960
so the conceptual with theoretical claim is to get a certain error accuracy the number of queries

24:22.960 --> 24:29.440
that you need scales roughly linearly with the sort of size of the confusion matrix. At least for

24:29.440 --> 24:37.680
I should say that this is true for linear and ratio of linear things if you're doing more complicated

24:37.680 --> 24:43.840
function classes other terms that to show up. So for linear things it mostly scales linearly

24:43.840 --> 24:49.280
with the size of the confusion matrix which is again sort of number of classes squared.

24:49.280 --> 24:56.000
For ratio you sort of have an extra factor of two there but again order wise it's mostly linear.

24:56.800 --> 25:05.280
If you go to say polynomial functions or something more complicated then it scales roughly that

25:05.280 --> 25:11.520
earlier of size of confusion matrix plus or times some term that depends on order to polynomial.

25:11.520 --> 25:20.480
So roughly that order. So you pay some cost for more and more complex types of score functions.

25:21.360 --> 25:27.120
There's some other discussion which we've been trying to reason through about how how complex is

25:27.120 --> 25:32.320
your sort of score function space need to be to capture human preferences appropriately. I think

25:32.320 --> 25:39.760
that's an important question that we have an answer then and I think maybe not that many folks in

25:39.760 --> 25:47.360
the field have maybe thought about very carefully. In fact we've been working a bit on actually

25:47.360 --> 25:54.560
reducing the complexity from even linear because some of the say psychology literature suggests that

25:54.560 --> 26:01.520
we mostly focus on sort of a few features as opposed to say arbitrary trade-offs between things.

26:01.520 --> 26:08.880
And so potentially the space of metrics is even lower dimensional than say linear in some large

26:08.880 --> 26:16.400
confusion matrix space. Again there's some interplay between human computer interactions sort of

26:16.400 --> 26:23.280
psychology, a bit of algorithms, a bit of machine learning. So it's an interesting set of problems

26:23.280 --> 26:27.760
and an interesting space for us to work in because it's sort of quite unique within machine learning

26:27.760 --> 26:32.400
to have all of these problems come together. But we think it's an important set of problems

26:32.400 --> 26:39.120
because we think it addresses core problems particularly in practice when folks are trying

26:39.120 --> 26:44.960
design systems and they have either a specific rough notion of what good systems should look like

26:44.960 --> 26:50.240
but accuracy is not cutting it or they're interested in some downstream measure that might

26:50.240 --> 26:55.200
involve say interaction with users and again potentially accuracy isn't getting them the results

26:55.200 --> 27:01.520
that they want. One area that we started to look at that is quite exciting as an application area.

27:01.520 --> 27:06.400
Again it's early days but I thought I should mention this. It's thinking about elicitation

27:06.400 --> 27:13.840
in the fairness space. So in machine learning fairness it's very clear that different measures

27:13.840 --> 27:20.240
of fairness end up with different notions of trade-offs between how you treat different

27:20.240 --> 27:26.000
say subgroups. I'm thinking primarily about say statistical group fairness in this case but

27:26.000 --> 27:33.040
similar ideas hold for other notions of fairness as well. And so one could imagine and Sarah's

27:33.040 --> 27:38.560
sort of first steps on this and there are also a couple of papers on this idea of coming up with

27:39.360 --> 27:47.200
elicitation procedures that can build context-specific notions of what metrics or

27:47.200 --> 27:53.200
statistics you should be trying to normalize across groups in order to achieve a fairness goal

27:53.200 --> 27:58.480
in a certain setting. So that's an application area where thinking very carefully about exactly

27:58.480 --> 28:05.840
what you're measuring is interesting and potentially quite important to get the results that one

28:05.840 --> 28:13.680
would want. I think it's still not absolutely clear to me and either the medical or the fairness

28:13.680 --> 28:24.720
scenario what these pairs concretely look like. In the case of I'm even having trouble like

28:24.720 --> 28:29.840
formulating the question concretely in the medical case. But I can imagine that there's

28:29.840 --> 28:35.680
a degenerate case where you're asking the physician would you rather spend like you're taking

28:35.680 --> 28:43.120
pokes at the function? Would you rather do this a thousand times or this one time or something

28:43.120 --> 28:49.360
like that but I'm getting a sense that that's not exactly it. You can show that if your metric is

28:49.360 --> 28:53.840
a function of certain quantities there are only things that matter our differences in those

28:53.840 --> 29:01.280
quantities. So in terms of pair-right comparisons for the confusion matrix setting you might imagine

29:01.280 --> 29:06.080
comparing confusion matrices which is not something that's easy to do by the way. And so part of the

29:06.080 --> 29:13.600
work is coming up with ways to translate those comparisons and two comparisons that say a medical

29:13.600 --> 29:19.280
expert could do. So the variety of techniques that we've started to work with to try to solve this

29:19.280 --> 29:27.520
last mile task. So you can imagine for instance showing where two different classifiers that have

29:27.520 --> 29:33.440
different confusion matrices is sort of the outputs where they differ in terms of their predictions

29:33.440 --> 29:39.520
or a variety of ways of working on sort of interpreting train models. So once you have a way to

29:41.120 --> 29:47.440
so we have good ways by the way of translating confusion matrices back to classifiers this

29:47.440 --> 29:53.600
ties very closely to earlier work I mentioned an optimizing arbitrary metric. So we have very good

29:53.600 --> 29:59.520
understanding now of how confusion matrices relate to models. So we can sort of go back and forth

29:59.520 --> 30:06.800
very easily. So once you have this then you can convert comparisons of confusion matrices which

30:06.800 --> 30:13.360
is what matters in terms of the trade-off into comparisons of models. So sort of model that

30:13.360 --> 30:19.040
achieves confusion A versus model achieves confusion B. And what the expert needs to be able to do

30:19.040 --> 30:27.040
is tell us their preference between model A versus model B some sequence of times. And we choose

30:27.040 --> 30:33.520
the sequence of comparisons in such a way that after sort of after a few queries we can pinpoint

30:34.800 --> 30:40.640
the trade-offs that best capture how they're weighing different kinds of errors in the confusion

30:40.640 --> 30:49.040
matrix for instance. So is is this model comparison formulation A way of looking at this or is

30:49.040 --> 30:54.240
is kind of fundamental to what you've described around metric elicitation always based on this

30:54.240 --> 31:05.280
model comparison. The way that we have built up the approach the fundamental piece is to summarize

31:05.280 --> 31:10.080
is roughly being able to compare confusion matrices which will bolt down to comparing models.

31:11.040 --> 31:18.560
To take us that back though again it's sort of whatever you're using as the sort of the parameters

31:18.560 --> 31:25.680
of your cost function. So the quantities in your cost function only differences in those

31:25.680 --> 31:31.520
quantities will show up as differences in the measure. So for instance if in addition to

31:32.240 --> 31:37.440
confusion matrix entities you really care about smoothness of the function that becomes a third

31:37.440 --> 31:44.480
thing that you add sort of a new parameter in the set of things that you would be comparing.

31:44.480 --> 31:51.040
And so you'd get say two classifiers that differed in confusion matrix or confusion matrices

31:51.040 --> 31:57.200
that they achieved and also maybe had different smoothness. And you would then tell sort of you

31:57.200 --> 32:03.200
would be you would be asked to give a preference between the two. So it's comparing the fundamental

32:03.200 --> 32:09.040
thing is being able to compare whatever quantities determine the metric. So however you define

32:09.040 --> 32:13.600
a metric whatever quantities determine a metric you need some procedure that allows the expert to

32:13.600 --> 32:18.560
compare those two things. In the classification space which is a space that we've studied by

32:18.560 --> 32:25.360
for the most. The natural entities are confusion matrices. And so you need a way to compare

32:25.360 --> 32:30.880
confusion matrices which we do by sort of providing back to models. I should say we have

32:31.840 --> 32:40.000
started a new line of work thinking about how to maybe do this how to select samples intelligently

32:40.000 --> 32:47.040
so you can imagine instead of comparing models using say whether predictions differ the most

32:47.600 --> 32:55.120
you could imagine the algorithm also selects a specific sample it says if you pick this model

32:55.120 --> 32:59.440
make this kind of prediction if you get picked this other model make decide a kind of prediction

32:59.440 --> 33:07.280
and using that as a way to get feedback. It's still early days on that line so it's hard to say

33:07.280 --> 33:14.640
sort of very clearly what is doable and what works well. Right now I'd say the work that is

33:14.640 --> 33:21.200
most mature is focusing on comparing confusion matrices translating this into comparing models.

33:22.160 --> 33:26.640
And then using that as a way to pinpoint preferences for the expert decision maker.

33:27.600 --> 33:32.480
And then we're going to talk about a totally different experience. Yeah. Yeah.

33:32.480 --> 33:42.960
A bit of extra time on this but so another line of work which we've been making I think

33:42.960 --> 33:51.680
quite interesting progress on is a question of robust distributed learning. So this is work

33:52.480 --> 34:00.160
led by my student Song Sier and collaborator here to annoy Indy Gupta who is a professor in the

34:00.160 --> 34:10.880
system side at Illinois. So the setup is that we're interested I'm laughing because sort of like

34:10.880 --> 34:20.640
you said how different it is but so the setup of the problem is that for various reasons

34:20.640 --> 34:25.760
particularly so the scalability in privacy there's a lot more interest in training machine learning

34:25.760 --> 34:31.520
models in a distributed way. So scalability is being able to use sort of lots of machines at the

34:31.520 --> 34:36.720
same time and potentially just getting more throughput running through much more data per second.

34:37.600 --> 34:43.360
So you can imagine this in data centers where sort of each machine has some amount of computing

34:43.360 --> 34:49.280
power. The idea is if I run lots of these machines at the same time on a stack of data I can

34:49.280 --> 34:56.400
and I do things appropriately I can I can sort of train my model much faster. You could also imagine

34:57.680 --> 35:04.880
in fact one of the I think interesting use cases of this is in sometimes called sort of

35:04.880 --> 35:13.360
internet of things or edge networks where say you're interested in training machine learning

35:13.360 --> 35:20.560
models partially on your edge device. So good example is something like a cell phone. You want

35:20.560 --> 35:25.600
to do somewhat processing on your cell phone and the idea is that if I do this appropriately I

35:25.600 --> 35:30.560
can avoid sharing data directly with the centralized server so I don't have to transfer data.

35:31.280 --> 35:36.720
This might win in terms of communication and if I do some extra work and I also get a win in

35:36.720 --> 35:44.320
terms of privacy so I can actually protect the user's data from some easy snooping but still get

35:44.320 --> 35:49.760
the benefits of training a big machine learning model across lots of devices. So that's the set

35:49.760 --> 35:55.680
of the general setup of again distributed machine learning in general. So one unique problem that

35:55.680 --> 36:02.960
shows up in distributed machine learning is that once you distribute your machine learning process

36:02.960 --> 36:09.040
you've made the system much more vulnerable to failures of various kinds and potentially to

36:09.680 --> 36:16.560
explicit adversarial attacks. So failures if you have 10 computers and so any one of them could

36:16.560 --> 36:22.800
potentially fail at some point. You could have communication issues so just now some network

36:22.800 --> 36:28.400
thing fails and in between within an optimization loop or between a training loop and so because of

36:28.400 --> 36:34.160
that if you're modeling and your optimization process is not robust you could imagine

36:35.120 --> 36:40.720
potentially breaking the whole training process. The worst case version of this and this comes

36:40.720 --> 36:49.040
from the system literature is known as Byzantine attacks. This is the idea that you want to protect

36:49.040 --> 36:56.080
your overall system against the worst case setting where an attacker takes over some sub-setting

36:56.080 --> 37:01.760
machines and does whatever they want in those sub-set machines. So they could for instance

37:01.760 --> 37:06.320
try to poison data on those machines or try to send wrong information back to the rest of the

37:06.320 --> 37:14.320
system or whatever else. And in Byzantine machine learning or Byzantine robustness the focus

37:14.320 --> 37:20.480
is on typically the idea is the attacker is doing this as a way to break the system. So if they

37:20.480 --> 37:27.280
can send the right wrong information if you like they can get the model to converge to whatever

37:27.280 --> 37:35.440
they like and sort of get arbitrarily bad behavior in your system. And so what you what we're

37:35.440 --> 37:42.320
interested in is our strategies one for just better distributed machine learning as normally as

37:42.320 --> 37:47.360
our initial target and just coming up with better optimization strategies both for standard

37:47.360 --> 37:52.800
distributed learning and also this idea of what is generally called federated machine learning.

37:53.360 --> 37:58.560
This idea of again training machine learning systems distributed way without sharing

37:59.680 --> 38:08.320
say gradients or sort of sharing information at every setting. You said without sharing gradients or

38:08.320 --> 38:19.360
the idea in federated machine learning very close to what is sometimes called local SGD is that each

38:19.360 --> 38:25.440
device runs several steps of gradient descent on their local data. And instead of sharing

38:25.440 --> 38:30.720
gradients at every step as you would do in a standard distributed setting they would share model

38:30.720 --> 38:37.120
parameters after a certain amount of training on the device. So again if you do this plus a few

38:37.120 --> 38:44.400
extra steps you can get privacy you can get much lower communication overhead. If you allow for

38:44.400 --> 38:49.200
machines to come in and out then you get something close to say what the Google system a federated

38:49.200 --> 38:55.440
learning system does where they can train say next word prediction models on your cell phone without

38:55.440 --> 39:00.960
actually sort of transporting your text data all the way to Google server so you can get privacy

39:00.960 --> 39:05.840
you can get some robustness but you can still get sort of reasonable performance hopefully close to

39:05.840 --> 39:10.880
what you would get if all the data was in the same place. There are a variety of strategies but the

39:10.880 --> 39:17.600
rough ideas is again targeting this distributed optimization in a way that hopefully replicates

39:17.600 --> 39:24.880
something close to centralized optimization. Most of our work has focused on the setting where there

39:24.880 --> 39:31.680
is a server somewhere and the optimization or the learning setup is that the workers communicate

39:31.680 --> 39:39.680
with the server every few rounds so either again using gradients or using models if it's either

39:39.680 --> 39:47.760
federated as standard distributed settings. And so there's a simple strategy actually that was quite

39:47.760 --> 39:53.120
popular when people started getting interested in robustness in distributed learning systems.

39:53.120 --> 40:03.680
So the idea was well mostly federated averaging which is the standard method for

40:05.120 --> 40:11.840
sort of federated machine learning or even standard distributed learning. Most of the methods work

40:11.840 --> 40:17.120
by averaging the gradients at the centralized server so the workers do whatever they do for a few

40:17.120 --> 40:22.880
steps one or a case steps they send some information back to the server server averages it and that

40:22.880 --> 40:30.160
becomes the information it gets sent across. And so the idea initially was well we know how to do

40:30.160 --> 40:37.040
robust great robust averaging. So if the potential failure point is this average of lots of

40:37.040 --> 40:42.240
different to the model parameters across devices and there's the potential for some of these

40:43.440 --> 40:51.280
model parameters to be incorrect or explicit attacks then we could do robust averaging and if you

40:51.280 --> 40:59.120
do robust averaging then you avoid the possibility that one of these devices can lead your model in

40:59.120 --> 41:04.080
sort of the wrong direction so that over steps you know have this what is again known as business

41:04.080 --> 41:11.280
behavior so get you to arbitrarily have a bad estimate or a bad model parameter by sort of

41:11.280 --> 41:19.120
optimization failure. So a lot of the early work in this area focused on trying to come up with or

41:19.120 --> 41:26.720
use robust ways of computing averages. So they're placed say the mean with the median which is

41:26.720 --> 41:32.320
known to be robust to sort of lots of outliers and other more sophisticated schemes that is a

41:32.320 --> 41:37.840
trimmed mean approach way throw away the sort of largest and smallest elements in your average.

41:37.840 --> 41:43.760
A few other more sophisticated this crumb which is quite popular as a way to do this sort of

41:43.760 --> 41:52.800
robust average. What we showed last year is an interesting behavior which I think was not obvious

41:54.160 --> 41:58.480
when we're I think folks would first think about this problem. So it turns out that

42:00.000 --> 42:10.880
you can construct a sequence of sort of bad model updates such that the mean remains close

42:10.880 --> 42:19.760
but the model parameter diverges over optimization steps. And the issue is that sort of the mean being

42:19.760 --> 42:27.200
close is not the same as sort of the optimization direction for lack of better term going in the right

42:27.200 --> 42:32.400
direction. So I'll try to explain this in the sort of standard distributed learning case. I think

42:32.400 --> 42:38.160
this is where it's maybe clear to see. So in the standard distributed learning case all the workers

42:38.160 --> 42:44.000
compute gradients under local data they send the gradients to the centralized server centralized server

42:44.000 --> 42:49.120
computes the average of the gradients and sends this back out to the workers and this average of

42:49.120 --> 42:56.160
the gradients is what is used for sort of the next step of gradient descent. So again the original

42:56.160 --> 43:02.240
papers try to just compute this average gradient in a robust way to avoid failures. So again if some

43:02.240 --> 43:07.920
steps of the workers were sending wearing information as long as use the robust average the mean

43:07.920 --> 43:14.320
would be close to the sort of original mean even if there were a few failures. But it turns out

43:14.320 --> 43:19.840
that if I'm running gradient descent I construct gradient updates such that the means are close

43:20.640 --> 43:24.720
but the direction of the sense is actually if you like even opposite from the direction that

43:24.720 --> 43:29.280
it should be going. So I can get the model to do really anything I want while keeping the

43:29.280 --> 43:37.520
means close at every step. Is the idea that you're accumulating small distances in the same

43:37.520 --> 43:42.240
in a deliberate direction over time and thus you're throwing your mean off or is it more nuanced

43:42.240 --> 43:47.920
than that? It's close. It boils down to the difference between sort of distance and angle.

43:49.280 --> 43:55.600
So what really matters for good gradient descent self-management is to be going in the right direction.

43:55.600 --> 44:02.000
So the way you construct the attack is you keep the distance close where you get the direction to

44:02.000 --> 44:07.680
just be a little bit off and you do this and accumulate this sort of a little bit off direction

44:07.680 --> 44:12.960
over steps. And so again you can get the model to do really whatever you want in this distributed

44:12.960 --> 44:21.440
setting. So does paper I believe it's in UAI last year where we show this? Yes, UAI 2019 called

44:21.440 --> 44:27.920
fall of empires breaking Byzantine tolerant SGD by inner product manipulation where we essentially

44:27.920 --> 44:34.640
break all of the existing methods for try to do robust distributed learning by computing robust

44:34.640 --> 44:42.160
averaging. Does the paper demonstrate that in a scenario that is real worldish to some degree

44:42.160 --> 44:49.520
that the attacker has enough information to actually execute the attack? That's a great question.

44:49.520 --> 44:58.160
So the setup in a lot of security work and very definitely into Byzantine world is that you try

44:58.160 --> 45:03.680
to protect against the worst case with the hope that if you get the worst case then you sort of

45:03.680 --> 45:10.160
you get easier cases for free including for instance benign cases so things just fail and turn off.

45:10.800 --> 45:18.640
So the focus intentionally is not on what is easily rep upcubal in real world settings. It's on

45:18.640 --> 45:24.160
if the attacker had full knowledge of everything and could do whatever they want, what could they do?

45:25.040 --> 45:29.200
And can I come up with a procedure that's robust to the thing that they could do? And it is that

45:29.200 --> 45:34.400
if you're robust in a setting then you get easier settings for free and it's typically the way a

45:34.400 --> 45:39.040
lot of security folks think about so security design is can I be secure against the worst case

45:39.040 --> 45:46.960
behavior? You could argue I think reasonably that sometimes it's a bit of overkill but again

45:46.960 --> 45:51.840
the idea is if you get this you get easier cases for free. And luckily in machine learning

45:51.840 --> 45:56.880
there are distributed machine learning there's lots of interesting things we can say and actually

45:56.880 --> 46:01.760
importantly you don't lose that much in terms of sort of overall training performance. So we have

46:01.760 --> 46:08.560
both theory and lots of experiments showing that if you do reasonable things you still get reasonable

46:08.560 --> 46:14.240
training performance not that far from what you'd get in standard settings. In fact I'd say more

46:14.240 --> 46:19.600
specifically if there are no failures you get something very close to training in a standard way.

46:20.160 --> 46:25.200
If there are failures you can show that many of the failures would break the standard training system.

46:26.480 --> 46:32.480
You can still train and get good results though you converge slower than if you were in a sort

46:32.480 --> 46:38.400
of completely benign setting with no failures. So you pay some cost but you pay a cost that sort of

46:38.400 --> 46:44.240
allows you to actually get results compared to settings where again if there was an attack you

46:44.240 --> 46:50.160
would just be completely vulnerable. So what's the nature of the approach? So one of the approaches

46:50.160 --> 46:57.920
that we found very effective is a bit of a twist on the problem but I like it because I think it's

46:57.920 --> 47:05.280
I think it's clever. So the idea is to use a validation set. So what we do is we actually

47:05.280 --> 47:12.080
we assume that the centralized server has access to an additional data set that's separate from

47:12.080 --> 47:18.480
the data sets trained on that the workers are training on. And so what we do is in every step

47:19.520 --> 47:25.680
use your centralized data set to check whether the gradients that you're getting are helping you

47:25.680 --> 47:30.640
optimize better or not. So I think pushing the model in a direction that sort of minimizing

47:30.640 --> 47:36.400
validation error essentially. So it feels like cheating but maybe I can argue that it's not

47:36.400 --> 47:43.680
it's ever ways. So one so you're paying some cost you're doing a bit of extra work on the centralized

47:43.680 --> 47:48.400
server. So before the centralized server all it had to do was compute an average or maybe a sort of

47:48.400 --> 47:53.680
smart average. Now the centralized server is doing this extra work of checking whether you're

47:53.680 --> 48:01.600
making some progress or not based on the gradient that you got. We can show that you can do a

48:01.600 --> 48:10.720
sort of highly stochastic check. So in particular you can construct a checking procedure

48:11.360 --> 48:15.520
that roughly boils down to taking one sample and checking whether this one sample

48:16.080 --> 48:21.520
slightly improves and and loss. And if that happens that's good enough to be able to check

48:21.520 --> 48:26.320
whether things are making progress and this is good enough to give you the protection that you

48:26.320 --> 48:32.560
need. So somehow the claim is that in terms of computational cost the overhead you're going to

48:32.560 --> 48:40.080
pay is low. In practice the win is quite large so this kind of approach is robust to some of the

48:40.080 --> 48:46.080
attacks I talked about where none of the robust averaging methods are. So all the robust averaging

48:46.080 --> 48:51.120
methods are susceptible to again getting the distance right so making the distance close but

48:51.120 --> 48:56.080
getting the angle completely wrong so going in the wrong direction. Whereas this method that checks

48:56.880 --> 49:02.560
whether you're actually making progress stochastically. So again just using a few samples can do

49:02.560 --> 49:08.960
this very efficiently and is able to give you protection against this sort of kind of worst case

49:08.960 --> 49:18.720
attack. So this is a paper we presented at ICML last year which we call Zeno Rebus synchronous

49:18.720 --> 49:26.080
SGD with an arbitrary number of Byzantine workers. Another nice property of the method

49:26.880 --> 49:33.120
is that previous work gave you protection to up to half of the machines potentially being

49:33.120 --> 49:41.840
corrupted. In this setup you can show that we have protection for much much higher

49:41.840 --> 49:50.480
potential fraction of corrupted workers. So really you need roughly maybe one good worker in

49:50.480 --> 49:54.880
your system to actually make progress. Again of course if there's only one good worker you pay

49:54.880 --> 50:01.280
some cost so everything will be slower. But another nice thing is if there's corruption but it's

50:01.280 --> 50:07.200
low magnitude you can actually be okay so you can actually use corrupted gradients to still make progress.

50:07.200 --> 50:14.720
Again as long as you can imagine an incorrect gradient being computed that's sort of a little bit

50:14.720 --> 50:21.680
off from the true gradient but not completely off sort of benign failure setting. We can still make

50:21.680 --> 50:28.640
progress using that where sometimes the standard method can have trouble with it. So again this

50:28.640 --> 50:34.240
line of work I think is quite exciting because it's sort of coming up with good ways for training

50:34.240 --> 50:40.640
large scale distributed systems with robustness built in. Along the lines of kind of the assuming

50:40.640 --> 50:48.080
the worst case security scenario is there are you or anyone else working on some kind of model where

50:48.800 --> 50:54.560
the workers and the centralized server kind of are in cahoots to determine if either of them is

50:55.440 --> 51:00.000
is corrupted because the centralized server is kind of a weak point in your previous example.

51:00.000 --> 51:05.360
I think that's a very important point. So this is assuming sort of centralized server that can be

51:05.360 --> 51:12.480
trusted essentially. There is a bit of work and not fully trusting the centralized server. I'm

51:12.480 --> 51:17.280
familiar to have to look up to get the exact references on paper is that where there's sort of

51:17.280 --> 51:23.920
two layers of checks or checks in both directions. I think that that entire direction is extremely

51:23.920 --> 51:29.840
interesting. So there's a bit of work thinking about that. Though again it's a bit sort of somewhat

51:29.840 --> 51:38.240
early days in that line of research. Okay. Cool. Yeah. So we've been thinking about this like I said

51:38.240 --> 51:42.400
for standard distributed learning settings where you're pushing gradients around. We're thinking

51:42.400 --> 51:47.600
about this for federate learning settings where you're pushing model parameters around. Combining

51:47.600 --> 51:54.720
this with just trying to scale up distributed learning to be faster doing adaptive learning rates

51:54.720 --> 52:00.160
and sort of other methods to get distributed learning to convert faster. So again this whole

52:00.160 --> 52:08.240
echo system of scaling up distributed learning and combining this with robustness because of

52:08.800 --> 52:13.680
special failure modes that show up when you train distributed learning systems.

52:13.680 --> 52:18.880
Very interesting stuff. I'm starting to think about how we're going to combine all this into a single

52:18.880 --> 52:29.680
title more. Yeah. Yeah. Well usually just sticking in and I also have to do it when I write up

52:29.680 --> 52:37.280
documents for some reason. And I didn't get to talk about any of my sort of cognitive near imaging

52:37.280 --> 52:46.560
work. So if I made your job a little bit easier. Yeah. So some of the threads. I mean of course

52:46.560 --> 52:53.040
clearly those sort of basic tools. Go back and forth. I should say and I don't think I emphasize

52:53.040 --> 52:59.440
this enough. I've been really lucky to have excellent colleagues and truly amazing students

52:59.440 --> 53:04.480
to work with here. And a lot of those sort of great ideas come from these years and from the students.

53:04.480 --> 53:08.880
So I mean sometimes I say that my job is to just get out of their way and sort of get them to

53:08.880 --> 53:17.440
do great things. So yeah. Sometimes that leads to a bit of sort of breath and ideas. Things that

53:17.440 --> 53:22.720
are somewhat constant. There's some I think the core mathematical tools are roughly. There's

53:22.720 --> 53:29.040
some new things that come on but a lot of the basics are shared. I think there's some nice

53:29.040 --> 53:34.800
cross-talk between for instance I didn't mention this but because we're thinking about robustness now

53:34.800 --> 53:39.920
as in general we've been thinking about robustness and other settings. So in the standard learning

53:39.920 --> 53:46.000
settings we think about robustness and loss functions, robustness and listation. So there's again

53:46.000 --> 53:52.640
some cross-talk between ideas that come from this. And the other way we're thinking about distributed

53:52.640 --> 53:58.240
learning when you have again complex settings, complex losses, interacting, complex prediction

53:58.240 --> 54:04.160
problems. So how does one change a distributed learning problem to account for the complex setting?

54:05.200 --> 54:11.360
So I think there is some cross-talk that comes between both these. And again share tools but

54:11.360 --> 54:15.120
I think it's fair to say that they're quite distributed, not pun intended.

54:15.120 --> 54:19.200
Well Semy, thanks so much for taking some time to walk us through what you're up to.

54:19.200 --> 54:26.640
Yeah, it was a pleasure. Thank you for time and I was glad to have some time to finally chat

54:26.640 --> 54:30.000
with you. I think we tried to set this up for you sometimes. Yeah, absolutely.

54:30.000 --> 54:34.560
Absolutely. We had a chance to go through this. It was a pleasure. Absolutely. Same. Thank you.

54:38.560 --> 54:43.600
All right everyone that's our show for today. For more information on today's show

54:43.600 --> 54:59.520
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

