1
00:00:00,000 --> 00:00:10,560
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I'm your host, Sam

2
00:00:10,560 --> 00:00:16,800
Charrington. And today I'm joined by Erwin Bello. Erwin was formerly a research scientist at

3
00:00:16,800 --> 00:00:22,800
Google Brain and is now on the founding team at a stealth AI startup. Before we get into today's

4
00:00:22,800 --> 00:00:27,760
conversation, please be sure to take a moment to head over to Apple Podcasts or your listening

5
00:00:27,760 --> 00:00:33,120
platform of choice. And if you enjoy the show, please leave us a five star rating and review.

6
00:00:33,120 --> 00:00:38,640
Erwin, welcome to the show. Thank you. Thanks for having me. I'm looking forward to our conversation.

7
00:00:38,640 --> 00:00:44,240
We're going to be digging into your research on sparse expert models as well as some of the

8
00:00:44,240 --> 00:00:48,880
trends you're following in large language models. But before we do that, I'd love to have you

9
00:00:48,880 --> 00:00:52,880
share a little bit about your background and how you came to work in machine learning.

10
00:00:52,880 --> 00:01:02,080
Yes, so I have a background in applied math and stats. I studied both in France. Then came to

11
00:01:02,080 --> 00:01:09,680
Stanford as a great student, mostly statistics and computer science. And during that time,

12
00:01:09,680 --> 00:01:16,880
studying, you know, I slowly got into AI and mostly deep learning, starting with computer vision,

13
00:01:16,880 --> 00:01:23,440
also doing some natural language processing. And I thought that that line of focus was really

14
00:01:23,440 --> 00:01:29,440
interesting. And so after graduating, I went to Google Brain to keep working on deep learning.

15
00:01:31,280 --> 00:01:35,760
I spent, you know, roughly five years there, mostly doing deep learning research,

16
00:01:36,320 --> 00:01:42,080
some product stuff as well, worked on a bunch of, you know, on a wide range of projects,

17
00:01:42,080 --> 00:01:48,640
like starting with a combinatorial optimization and ranking, also worked on some auto-ML,

18
00:01:48,640 --> 00:01:55,360
where you use machine learning to learn machine learning itself. And most recently, I worked on

19
00:01:55,360 --> 00:02:02,480
computer vision and also large scale models with a focus on this new exciting class of architectures

20
00:02:02,480 --> 00:02:09,040
called sparse expert models. Nice. That is a great segue. Why don't we jump into that topic?

21
00:02:09,040 --> 00:02:13,360
I guess we should start at the top. What is a sparse expert model?

22
00:02:13,360 --> 00:02:19,840
Yeah, so perhaps the easiest way to define them is by contrast to regular dance models.

23
00:02:20,400 --> 00:02:26,240
So regular dance models apply the same parameters to all inputs. And by contrast,

24
00:02:26,240 --> 00:02:30,880
sparse expert networks dynamically select which parameters to use for each input.

25
00:02:31,680 --> 00:02:37,360
So that means that part of your neural networks, like parts of your neural network are activated

26
00:02:37,360 --> 00:02:44,480
on a pair example basis. You know, in practice, if you have a batch of examples,

27
00:02:44,480 --> 00:02:49,120
all of the networks, parts will be activated, but for a single example,

28
00:02:50,400 --> 00:02:55,600
that single example is only going to see part of the network parameters. And so this allows

29
00:02:55,600 --> 00:03:01,120
to increase model capacity in the sense that you're working with much more parameters

30
00:03:01,120 --> 00:03:10,880
without increasing computation of flops. Because each input and in that context input can refer

31
00:03:10,880 --> 00:03:18,560
to either sentence or a token. Each input only interacts with the same number of parameters and

32
00:03:18,560 --> 00:03:23,200
therefore has the same amount of computation applied to it.

33
00:03:23,200 --> 00:03:30,880
And the case that you're looking at here with these sparse expert models, are you specifically

34
00:03:30,880 --> 00:03:39,360
targeting extreme levels of sparsity in the data? Or, no, okay, no. So usually other examples of

35
00:03:39,360 --> 00:03:44,080
sparsity can refer to the datasets. For example, you know, in reinforcement learning,

36
00:03:44,960 --> 00:03:52,240
people will talk about sparse rewards. Sparse expert models like the sparsity is in the model.

37
00:03:52,240 --> 00:04:02,320
And the reason the way you can, the reason why it's called spars is because they're equivalent to

38
00:04:02,320 --> 00:04:10,720
having very large dense models where parts of the model weights are zeroed out. And so if you look

39
00:04:10,720 --> 00:04:18,160
at your metrics multiplies, at your matrices in your model, having a set of experts is the same

40
00:04:18,160 --> 00:04:25,120
as having a very large matrices with a lot of zeros. And so this kind of sparse.

41
00:04:25,120 --> 00:04:34,880
Sorry, experts in this case refers to dynamically swapping out parameters based on an error.

42
00:04:34,880 --> 00:04:41,040
In this case, like it's sort of an independently learned neural network that has unique weights.

43
00:04:41,040 --> 00:04:47,600
And so typically you'll have a bunch of them, like let's say you're working with a hundred experts.

44
00:04:47,600 --> 00:04:55,840
And an input comes in. You have a router network that decides where to send that output, that input,

45
00:04:55,840 --> 00:05:01,840
you know, you might send it to only one expert or three experts. You reach two experts.

46
00:05:01,840 --> 00:05:09,840
You know, so the experts take the input, computing output, and then at the end of your layer,

47
00:05:10,960 --> 00:05:17,280
the expert outputs get sort of average with expert probabilities as well.

48
00:05:18,480 --> 00:05:23,440
On top of that, when you train these architectures, you have a low-spanning

49
00:05:23,440 --> 00:05:32,160
thing objective to make sure that all the experts are, you know, used roughly uniformly because

50
00:05:32,160 --> 00:05:37,120
you don't want to be in a situation where, you know, a lot of your experts are

51
00:05:37,120 --> 00:05:44,400
unused during training because that's bad for utilization. But yeah, at the very high level,

52
00:05:44,400 --> 00:05:48,880
this is equivalent to having a very large dense matrix with a lot of zeros.

53
00:05:48,880 --> 00:06:02,640
Is having some large number of independently trained experts, I guess the way I originally heard

54
00:06:02,640 --> 00:06:09,600
you describe this or what I thought I heard was not necessarily these independently trained experts

55
00:06:09,600 --> 00:06:20,560
or networks, but swapping out parameters at a particular layer of a single network based on

56
00:06:20,560 --> 00:06:25,920
the input. Are those kind of functionally equivalent? Yeah, that's correct, yeah.

57
00:06:26,800 --> 00:06:31,840
Yeah. So, you know, typically you'd have a dense layer, you have an input,

58
00:06:31,840 --> 00:06:39,200
it's multiply in the, you know, in the easiest case, it's multiply by matrix and that matrix has weights

59
00:06:39,200 --> 00:06:47,440
and it corresponds to a single expert, right? In the case of mixture of experts, now you have

60
00:06:47,440 --> 00:06:55,120
a lot of experts, you know, it can go up to hundreds, even more. And you selects, each input is

61
00:06:55,120 --> 00:07:01,360
routed with a network that is also trained. Yeah, everything is trained, you know, jointly, but, you know,

62
00:07:01,360 --> 00:07:09,440
that router sends an input to an expert and that expert does the computation. And the other experts

63
00:07:09,440 --> 00:07:16,000
do not get to see that input, so that's why it's pass. Yeah, this allows, you know, the networks to

64
00:07:16,000 --> 00:07:22,560
like vastly expand the number of parameters because instead of having, you know, one expert in the

65
00:07:22,560 --> 00:07:27,680
dense of, in the case of a dense network, now you have, let's say, a hundred. And so these

66
00:07:27,680 --> 00:07:32,880
these layers have, you know, a hundred times more parameters. These parameters are not sort of

67
00:07:32,880 --> 00:07:39,200
functionally equivalent to dense parameters because they're sparse and so it's not the same as

68
00:07:39,200 --> 00:07:46,480
just, you know, a hundred X in your model size, but it does bring a lot of benefits at pre-training

69
00:07:46,480 --> 00:07:54,880
and now it's when we're tuning on the stream applications as well. And are each of the experts

70
00:07:55,760 --> 00:08:03,360
also typically deep networks, or did they tend to be shallower networks then?

71
00:08:03,360 --> 00:08:17,280
Okay, so it's usually, you know, usually your entire deep neural net is pretty deep, so let's say, you know, 50 layers. But each

72
00:08:17,280 --> 00:08:24,800
expert layer is usually, you know, a two layer neural net actually. Okay, you know, you can do it one or two

73
00:08:24,800 --> 00:08:31,280
like we do two because, you know, paper, it's tied to our choice of activation functions, but those

74
00:08:31,280 --> 00:08:39,200
are pretty shallow. And we find that this is kind of the best, this gives the best results. Another

75
00:08:39,200 --> 00:08:48,400
thing we found in that paper is that you don't want to use expert layers everywhere because that

76
00:08:48,400 --> 00:08:57,360
leads to a bad sort of quality to latency tradeoff. So, you know, the name of the game in a lot of

77
00:08:57,360 --> 00:09:03,920
these researchers to get the most performance at a given, you know, training cost or in

78
00:09:03,920 --> 00:09:10,640
French budget, right? And so, when you do sort of, when you optimize your architecture, what you do

79
00:09:10,640 --> 00:09:17,360
is like you're trying to get like, no, good accuracy, but maintaining the network speed. And so, for

80
00:09:17,360 --> 00:09:24,400
that we find that, you know, having expert layers, every four blocks of layers is a good, is a good

81
00:09:24,400 --> 00:09:32,720
choice. And that's what we went with in that paper. Okay. Can you talk a little bit about the

82
00:09:34,240 --> 00:09:41,200
kind of the scalability of the method? How well does it scale? And what did you have to do to get

83
00:09:41,200 --> 00:09:49,360
it to scale? So, just zooming out a little bit. With mixture of expert models, now you have

84
00:09:49,360 --> 00:09:56,880
two dimensions that are different. So, you have the number of formulas versus the flops or the

85
00:09:56,880 --> 00:10:02,080
amount of computation. Usually, they're the same. For dense models, they're the same because more

86
00:10:02,080 --> 00:10:08,080
primers means more computations as well. With expert models, if you scale the number of experts,

87
00:10:08,720 --> 00:10:14,720
you're not scaling the number of computation that is applied, for example. And so, this adds

88
00:10:14,720 --> 00:10:20,960
like an additional dimension that, you know, makes the problem a little bit more difficult to study.

89
00:10:21,520 --> 00:10:27,120
But so, yeah, actually, scaling mixture of expert models was kind of an solved problem

90
00:10:27,760 --> 00:10:34,480
that motivated our research on the subject. And so, one thing that people noticed was that

91
00:10:34,480 --> 00:10:40,960
they were able to scale the number of experts, to the number of primers. But when they were

92
00:10:40,960 --> 00:10:46,320
scaling the flops, or like just the amount of computation applied throughout the model,

93
00:10:48,240 --> 00:10:54,480
training these models would usually lead to instabilities. So, until recently, these models were

94
00:10:54,480 --> 00:11:03,440
quite unreliable at scale, at computation scale, flops scale. And so, you know, one contribution of

95
00:11:03,440 --> 00:11:11,760
our recent work with my co-workers at Google Brain was to figure out how do we make this large scale

96
00:11:11,760 --> 00:11:19,200
mixture of experts train stably, right? So, you know, we did a large scale study of

97
00:11:19,200 --> 00:11:26,000
quality versus stability trade-offs. One thing we found is that usually the techniques to improve

98
00:11:26,000 --> 00:11:31,760
stability, health accuracy or performance quite a bit. And so, that's, you know, that's not super

99
00:11:31,760 --> 00:11:42,160
satisfying. We ended up sort of repurposing an auxiliary loss called the Z loss. So, that loss

100
00:11:42,160 --> 00:11:49,920
is usually applied to the final sort of layer logits. Now, we applied it in the router of the

101
00:11:49,920 --> 00:11:56,080
experts. So, again, the router is the small neural network that takes inputs and decides where to

102
00:11:56,080 --> 00:12:02,640
which experts to send them. And so, one thing we found was that when we apply an auxiliary loss

103
00:12:02,640 --> 00:12:09,760
on the router to sort of make the probably distribution over which experts to send each input,

104
00:12:09,760 --> 00:12:15,040
we make that distribution more, you know, more smooth. We found that this helped a lot with

105
00:12:15,040 --> 00:12:23,760
stability. So, that's one of the contribution of the paper. Another, you know,

106
00:12:23,760 --> 00:12:30,400
a line of work was that you want to design these models and scale them while taking

107
00:12:30,960 --> 00:12:39,200
your hardware into considerations. So, that process of routing inputs to experts, you know,

108
00:12:40,000 --> 00:12:47,760
requires your accelerators to communicate between them. So, you know, let's say in our case,

109
00:12:47,760 --> 00:12:56,480
we were working with TPUs. And so, you know, one simple way to see it is like, let's say, you know,

110
00:12:56,480 --> 00:13:03,600
you have one expert per TPU, right? So, depending on where you want to send an input,

111
00:13:03,600 --> 00:13:07,920
you're going to have to send that token to a different TPU. And so, there are communication costs

112
00:13:07,920 --> 00:13:16,400
associated with that process. And so, that's kind of, yeah, a requirement to think ahead and think

113
00:13:16,400 --> 00:13:23,840
about, you know, the trade-off of communication versus computation to set some of the model dimensions

114
00:13:24,800 --> 00:13:30,240
optimally. And so, that's one thing we explored a bit that we explained in the paper.

115
00:13:30,960 --> 00:13:41,600
And did you set those dimensions empirically or did you develop some formulation for

116
00:13:41,600 --> 00:13:47,840
how you might optimize that? A little bit of both. I would say, it's mostly empirical, but

117
00:13:48,880 --> 00:13:57,200
you know, in what direction to go usually. So, the process is usually, you know, you'll train

118
00:13:57,200 --> 00:14:03,440
a network and you'll both look at its, you know, its loss or accuracy. And at the same time,

119
00:14:03,440 --> 00:14:11,440
you'll profile the network. And so, you'll see, okay, how that much time is spent on, you know,

120
00:14:11,440 --> 00:14:16,960
communication in that layer, that much time is spent on computation in that layer. And as you're

121
00:14:16,960 --> 00:14:24,160
run more and more experiments, you start getting a sort of a mental model for how to, you know,

122
00:14:24,160 --> 00:14:30,960
improve performance or improve latency without holding the other. So, that was kind of an iterative

123
00:14:30,960 --> 00:14:39,360
process mostly. Going back to your previous point about the introduction of ZLOS,

124
00:14:40,080 --> 00:14:44,240
you know, was it clear in the process of designing the architecture that, you know,

125
00:14:44,240 --> 00:14:49,360
that was what you needed to do? Or how did you get to, oh, ZLOS is going to fix this.

126
00:14:51,040 --> 00:14:56,080
Well, you know, the credit goes to one of my collaborators, Sparratt, who figured out, but,

127
00:14:56,080 --> 00:15:05,840
no, so, you know, we started by exploring sort of all the classic techniques to, to improve

128
00:15:05,840 --> 00:15:13,040
stability. You know, usually it's like you inject some noise so that if the model goes

129
00:15:13,040 --> 00:15:19,040
unstable, it's used to walking with noise. And so, maybe it can recover from that instability.

130
00:15:19,040 --> 00:15:27,200
Another technique is that you clip activations so that the model does an output, you know,

131
00:15:27,200 --> 00:15:34,400
activations that are out of its usual range. But these techniques, you know, we found that they

132
00:15:34,400 --> 00:15:42,000
held model quality a lot. So, that's not satisfying. I think we thought about the ZLOS in the context of

133
00:15:42,000 --> 00:15:53,440
around of errors and numerical precision. So, you know, one direction that a lot of recent

134
00:15:53,440 --> 00:16:01,120
large tail modeling work is exploring is to reduce, like, you know, to use lower precision

135
00:16:01,120 --> 00:16:07,520
format, format, exactly quantization. Because this enables more efficient communication

136
00:16:07,520 --> 00:16:15,040
cost between processes, between processors and our memories, also faster computation,

137
00:16:16,160 --> 00:16:22,880
and, you know, just also less memory requirement for storing distances and activations.

138
00:16:24,320 --> 00:16:30,080
The issue with lower precision formats or quantization is that it comes at the expense of larger

139
00:16:30,080 --> 00:16:37,120
round of errors, right, because depending on which precision format you have, you know, sort of a minimum

140
00:16:38,560 --> 00:16:47,520
distance between two consecutive floating points. And so, yeah, if you use lower precision,

141
00:16:47,520 --> 00:16:54,320
it's faster, but larger round of errors. And in certain cases, this round of errors can lead to

142
00:16:54,320 --> 00:16:58,800
instability that you can't recover from. This is when, you know, the training loss

143
00:16:58,800 --> 00:17:06,320
all of a sudden explodes, and you don't recover from from that point in the weight space once

144
00:17:06,320 --> 00:17:14,000
you fridge that. And so, what the ZLOS does is that, so it's a little penalty that you add on top

145
00:17:14,000 --> 00:17:22,800
of the logits of the router to encourage these logits to be small in value. And because they're

146
00:17:22,800 --> 00:17:31,680
smaller, they're more accurately modeled in floating point format. And so, what we found is that,

147
00:17:31,680 --> 00:17:38,960
so first of all, whenever you use logits, you want to use sort of a FP32 format,

148
00:17:38,960 --> 00:17:45,360
like kind of a high precision format, because these logits are like exponentiated. And so,

149
00:17:45,360 --> 00:17:52,400
if there's a small round of error in the logits, there's going to be a much larger round of error

150
00:17:52,400 --> 00:17:59,200
in the exponentiated logit, which are the probabilities. But we found that, so, you know,

151
00:17:59,200 --> 00:18:03,200
that's a trick that, you know, a lot of people know, and most of these architectures are using,

152
00:18:03,200 --> 00:18:10,960
it's like, you use float 16 everywhere, or B float 16 everywhere. But in your softmax functions,

153
00:18:10,960 --> 00:18:18,640
or when you have a logit, you use, you cast your inputs to float 32. But that wasn't enough

154
00:18:18,640 --> 00:18:24,240
for the router. And so, the idea was like, yeah, let's, you know, let's have this additional loss

155
00:18:24,240 --> 00:18:29,840
to make sure that these logits are even smaller, so that they can be more accurately modeled.

156
00:18:29,840 --> 00:18:38,000
To be clear, were you also using quantization in your process, or was it more

157
00:18:39,760 --> 00:18:46,080
quantization in general introduces noise, and z-loss is used to fight, you know,

158
00:18:46,080 --> 00:18:51,600
quantization noise may be at a work with our noise. Our architectures, you know, we use B float 16.

159
00:18:53,040 --> 00:19:00,240
She's a precision format with 16 bits, which is slightly different from, well, actually quite

160
00:19:00,240 --> 00:19:08,080
different from FB 16. But, you know, B float 16 is supported on TPUs, and it has sort of a larger

161
00:19:08,080 --> 00:19:15,280
range of numbers it can, it can represent, but at the cost of also larger round of errors compared

162
00:19:15,280 --> 00:19:23,280
to FB 16. And so, you know, we already knew of this trick of always using, you know, full

163
00:19:23,280 --> 00:19:30,560
precision or FB 32, when you're used, when you're in logits or before applying softmax or

164
00:19:30,560 --> 00:19:37,360
exponential, exponential, exponential. So, we already kind of did that by default. You know,

165
00:19:37,360 --> 00:19:41,600
we're not the ones that figured out that trick out. I think we explained it pretty well. We,

166
00:19:41,600 --> 00:19:47,520
know, we go in depth in that paper, you know, trying to solve as a guide for people who want to

167
00:19:47,520 --> 00:19:54,960
design large scale models and sparse models in general. But we didn't invent that. What we

168
00:19:54,960 --> 00:20:00,960
figured out though was that this wasn't enough in the router. So, remember that the router,

169
00:20:01,920 --> 00:20:07,520
you know, decides which, where to, which experts to send each tokens to. In practice, you know,

170
00:20:07,520 --> 00:20:13,520
the router outputs are probably distribution over all of the experts. So, you know, this is

171
00:20:13,520 --> 00:20:19,760
implemented by a softmax, right? Like, you take, you have a scalar for, you know, or like a

172
00:20:20,720 --> 00:20:29,120
log probability for where to send the expert, your input token, and then you take a softmax of that.

173
00:20:30,320 --> 00:20:36,400
And so, we thought, you know, if FB 32 is not enough, what can we do? Right? Like,

174
00:20:36,400 --> 00:20:43,040
one thing you can do is to push these logits to be smarter, so that run of errors, how to even less.

175
00:20:43,760 --> 00:20:51,120
What did you find experimentally overall with the method? What, what, what data sets did you

176
00:20:53,040 --> 00:21:01,680
benchmark it on and what kind of results did you see? Yes. So, we pre-trained on, mostly on C4,

177
00:21:01,680 --> 00:21:08,400
like CommonCrawl, you know, which is a few hundred billions or even more depending on how you filter

178
00:21:09,040 --> 00:21:17,200
tokens. But so, one of the issues that we also solved in that paper was that people were having

179
00:21:17,200 --> 00:21:23,280
sort of uncertain quality when fine-tuning this mix of expert models. So, usually the way you

180
00:21:23,280 --> 00:21:29,920
get set up the art and this NLP task is by pre-training large language models for quite some time,

181
00:21:29,920 --> 00:21:35,680
like, you know, hundreds of billions of tokens, and then fine-tuning on the task of interest.

182
00:21:37,120 --> 00:21:42,400
And so, sparse models were showing a lot of promise in the pre-training phase where you

183
00:21:42,400 --> 00:21:50,080
you would see, you know, speed ups around 4 to 7x, you know, so that would mean, like, you know,

184
00:21:50,080 --> 00:21:56,320
instead of training a dense model for like 10 days, I can get the same results with a sparse

185
00:21:56,320 --> 00:22:02,720
model for two days, trained for two days, but only at pre-training. When you would move to that second

186
00:22:02,720 --> 00:22:09,280
phase of fine-tuning, yeah, like the sort of improvement of the sparse model over the dense model

187
00:22:09,280 --> 00:22:13,760
would kind of like vanish, you know, and this was, you know, that was kind of a big issue, right?

188
00:22:13,760 --> 00:22:19,280
So, there were two big issues. Meaning, pre-training took just as long as with a dense model or

189
00:22:19,280 --> 00:22:23,840
pre-training took way longer and undid all the gains on, I'm sorry, fine-tuning.

190
00:22:23,840 --> 00:22:30,320
So fine-tuning undid all the gains. Okay. Usually fine-tuning is faster because the datasets are

191
00:22:30,320 --> 00:22:37,600
faster, so you were less about sort of the duration of fine-tuning, but fine-tuning undid all the gains.

192
00:22:38,400 --> 00:22:42,800
Okay, so that was the second issue we addressed in that work, the first one being that

193
00:22:44,000 --> 00:22:48,960
these models didn't scale reliably because of instability. And so, you know, we

194
00:22:48,960 --> 00:22:56,320
we run a lot of experiments on fine-tuning and we identified a rather counter-intuitive

195
00:22:56,880 --> 00:23:04,800
property, which is that the way you fine-tune sparse models is quite different than the way you

196
00:23:04,800 --> 00:23:11,680
want to fine-tune dense models. And so, if you just take the same hyper-primers that you usually

197
00:23:11,680 --> 00:23:16,720
take for dense models, those hyper-primers are pretty bad for sparse models and actually

198
00:23:16,720 --> 00:23:20,800
using them for fine-tuning will undo all the gains that you had from pre-chain.

199
00:23:22,080 --> 00:23:26,400
And so, we had to, you know, we come up with sort of hyper-primer recommendations

200
00:23:27,360 --> 00:23:33,440
in the paper. Usually, you want to increase the noise at a high level, you want to increase the

201
00:23:33,440 --> 00:23:41,840
noise when fine-tuning sparse expert models. We believe that this is because sparse models are

202
00:23:41,840 --> 00:23:47,680
more of a prone to other feeling, right? With sparse models, you have a much larger

203
00:23:48,720 --> 00:23:54,000
modeling capacity because you have way more parameters. And so, there's a risk of other

204
00:23:54,000 --> 00:24:01,760
feeling on these, you know, smaller fine-tuning datasets. Another thing that, you know, is a

205
00:24:01,760 --> 00:24:07,280
potential hypothesis, you know, there's no way to, no one has really verified that yet, but,

206
00:24:07,280 --> 00:24:12,880
you know, I think that's kind of an ectotic evidence from a couple of papers that this is the case,

207
00:24:13,520 --> 00:24:21,680
is that there is, there is a range of parameters versus computation that is optimal for sparse

208
00:24:21,680 --> 00:24:27,440
models and you don't want to deviate too much from that range. So, you know, as I said earlier,

209
00:24:27,440 --> 00:24:32,960
now, the amount of compute and the number of parameters are not necessarily related

210
00:24:32,960 --> 00:24:37,440
anymore because when you increase the number of experts, you increase parameters, but you don't

211
00:24:37,440 --> 00:24:43,280
increase computation. You know, and a bunch of papers have tried to, you know, yeah, let's use

212
00:24:43,280 --> 00:24:48,320
a thousand experts, you know. And so, you have a Nance model, like a sparse model with

213
00:24:49,040 --> 00:24:56,240
trillion of parameters, but relatively much fewer computations. And so, I think there's some

214
00:24:56,240 --> 00:25:00,320
evidence that this is a bad scenario and that you want to keep in the more, you don't want to stay

215
00:25:00,320 --> 00:25:06,640
in the more reasonable ratio of parameters versus compute. And yet, there's this like, you know,

216
00:25:08,000 --> 00:25:16,960
maybe somewhat instantiated theory that parameters correspond to knowledge and computation correspond

217
00:25:16,960 --> 00:25:24,560
to, corresponds to intelligence, you know, for whatever nibblest definitions of knowledge and

218
00:25:24,560 --> 00:25:30,080
intelligence, you want to work with. But I think this kind of makes sense, right? It's like more

219
00:25:30,080 --> 00:25:36,720
parameters, like parameters can encode knowledge and computation referred to, yeah, how much

220
00:25:37,280 --> 00:25:43,120
computation, how much can you sort of modify an input. So, that kind of makes sense to me. And

221
00:25:43,120 --> 00:25:47,680
this is also what's supported by some of these experimental results. What

222
00:25:47,680 --> 00:25:56,320
tasks did you, were all of the tasks that you were looking at in the paper NLP tasks? Yes.

223
00:25:57,360 --> 00:26:04,960
And is the method applicable to other types of tasks? Yeah, it's actually been applied

224
00:26:04,960 --> 00:26:12,480
to vision kind of concurrently. Like the first user of mixed-up export models was for NLP,

225
00:26:12,480 --> 00:26:21,520
but yeah, it's been done in vision since. So, this is really like a modeling or like a new class

226
00:26:21,520 --> 00:26:30,880
of architectures. You can apply to different modalities, different tasks. I think, you know,

227
00:26:30,880 --> 00:26:39,920
maybe NLP and maybe textual domains, I think intuitively, might make more sense. Because

228
00:26:39,920 --> 00:26:50,320
tokens are kind of sparse, you know, in the NLP. Whereas vision, I feel like, is a bit more

229
00:26:50,320 --> 00:27:01,600
continuous. And do you see these types of models as being something that, you know, it's a tool

230
00:27:01,600 --> 00:27:06,880
on the shelf or in the tool bag and under a certain set of conditions, you know, it might be the

231
00:27:06,880 --> 00:27:14,480
tool that you reach for, or do you see it as more of a, you know, a step in evolution. And this

232
00:27:14,480 --> 00:27:19,120
is what we're going to be doing as a standard approach at some point in time.

233
00:27:20,800 --> 00:27:26,240
So, I think our paper tries to take it from, you know, the format to the letter.

234
00:27:26,800 --> 00:27:31,840
Okay. So, this is definitely, you know, like an additional tool, not two chefs in the

235
00:27:31,840 --> 00:27:38,560
deep learning toolbox per se. But the results are comparing enough that this is something you

236
00:27:38,560 --> 00:27:45,840
seriously want to consider. Again, as I said, there are quite some technical details in

237
00:27:45,840 --> 00:27:54,560
getting them to work reliably. Also, especially making them fast on your hardware and software

238
00:27:54,560 --> 00:28:03,120
stack. So, you know, as I explained before, like depending on some of the, you know, under communication

239
00:28:03,120 --> 00:28:09,360
costs on your hardware, you might want to set this mixture of experts differently.

240
00:28:10,720 --> 00:28:16,480
Then there's also a question at inference, which is that, you know, if you do inference for a

241
00:28:16,480 --> 00:28:22,640
single sort of input, it's very inefficient because you have all these other experts that aren't

242
00:28:22,640 --> 00:28:28,320
used at the same time. And so, I would mostly use them for, like, batch inference when you know

243
00:28:28,320 --> 00:28:34,640
that all of the experts are going to be used. That's one. Another thing that makes inference a little

244
00:28:34,640 --> 00:28:42,720
bit more complicated is that, you know, with a dense model, if you have a very small batch size,

245
00:28:42,720 --> 00:28:48,960
you know, it can fit on few accelerators. But now, sparse models may have much more, you know,

246
00:28:48,960 --> 00:28:55,840
way more parameters. And so, you need a large number of GPUs or TPUs also at inference.

247
00:28:56,480 --> 00:29:02,320
So, and again, that may only be worth it if you know, if you have high throughput, like a lot of

248
00:29:02,320 --> 00:29:07,920
queries per second, and you can batch things together. But I would say there's definitely something

249
00:29:07,920 --> 00:29:16,560
to consider seriously. It just takes a little bit more technical expertise. And, you know,

250
00:29:16,560 --> 00:29:23,120
there are also a lot of other things that I think are very exciting in the space at the moment

251
00:29:23,120 --> 00:29:28,800
that people should also consider. That kind of complimentary, you know, like, mix of experts,

252
00:29:28,800 --> 00:29:34,480
I really see as like this new class of architectures that I really think, I think, you know,

253
00:29:34,480 --> 00:29:38,000
I think in 10 years it's going to be like, what are people who are really applying, like,

254
00:29:38,800 --> 00:29:41,920
all the same parameters to all inputs, like that's kind of insane, right?

255
00:29:41,920 --> 00:29:50,160
Well, I wanted to go down on the idea of new class of architectures. One question that was

256
00:29:50,160 --> 00:29:56,560
emerging for me is, in some ways, it sounds like a technique. And in other ways, it sounds like

257
00:29:57,280 --> 00:30:03,040
an architecture, meaning, you know, there's part of me that wants to, hey, can we just take,

258
00:30:03,520 --> 00:30:09,200
you know, off the shelf burt or something and apply mixture of experts at individual layers?

259
00:30:09,200 --> 00:30:15,840
You know, verse, but you've also articulated that there are, you know, that there are constraints

260
00:30:15,840 --> 00:30:21,360
around the number of consecutive layers that you'd want to have, for example,

261
00:30:21,360 --> 00:30:28,320
to suggest that, you know, it's not ever going to be a plug-and-play kind of thing. Can you talk

262
00:30:28,320 --> 00:30:32,480
a little bit about that? I think it's kind of this, you know, if you think about belt and GPT,

263
00:30:32,480 --> 00:30:38,320
you know, these architectures have also been optimized, right? Like, the way that the model

264
00:30:38,320 --> 00:30:42,640
dimensions are set, or like, the normalization, the activation functions, like, people have

265
00:30:42,640 --> 00:30:48,320
optimized these over the years. And the fact that we recommend, you know, hey, maybe apply

266
00:30:49,520 --> 00:30:54,400
the sparse layer every four layers, that's just an optimization that we figured out and that people

267
00:30:54,400 --> 00:31:00,960
can apply directly, or like some of our recommendations on how to set up some of the export-related

268
00:31:00,960 --> 00:31:05,840
dimensions based on the hardware. There's also something that we figured out that people can

269
00:31:05,840 --> 00:31:12,320
apply directly. I would say it's harder to, you know, understand and to work with then

270
00:31:13,120 --> 00:31:18,240
dance models, but, you know, for some of these large-scale runs that, you know, are costing

271
00:31:18,240 --> 00:31:23,760
in the millions of dollars, this is something that, you know, it's worth figuring out.

272
00:31:23,760 --> 00:31:31,280
Yeah, and maybe another way to ask my question is, you know, if you, if, you know, knowing what you know

273
00:31:31,280 --> 00:31:40,080
and having this tool available, you wanted to create a kind of, you know, best-in-class,

274
00:31:40,080 --> 00:31:47,360
large-scale language model, would you, like, start with, you know, a bird or a GPT or something,

275
00:31:47,360 --> 00:31:54,320
and try to apply mixture of experts to it, or would you start with, you know, what you know

276
00:31:54,320 --> 00:31:59,760
about mixture of experts, and try to build off of that to get to a language model. Does that

277
00:31:59,760 --> 00:32:05,600
question make sense? Yeah, I see what you mean. I would start with, you know, the easiest,

278
00:32:05,600 --> 00:32:11,360
which is, you know, selling from GPT or about, then, like, creating, like, I think we really wrote

279
00:32:11,360 --> 00:32:19,920
this paper as a sort of a design guide for using mixture of experts. Got it. So it's not, you know,

280
00:32:19,920 --> 00:32:30,320
fundamental, new, it's not, I'm not trying to, it's both disparage it. And new architectures.

281
00:32:31,600 --> 00:32:37,600
That's the technique of having mixture of experts, right? Like, now you have, you have these

282
00:32:37,600 --> 00:32:43,840
two dimensions that you can scale independently and so on. And Sparsity in general is kind of a,

283
00:32:43,840 --> 00:32:49,440
I was going to say it's a mindset, but, you know, it's a, it's a general technique, right? But if you're just,

284
00:32:51,120 --> 00:32:57,680
a practitioner, or you want to use them in, like, you don't care too much about figuring things out,

285
00:32:57,680 --> 00:33:02,560
then I recommend using sort of the architectures that people have figured out. Right, right.

286
00:33:05,040 --> 00:33:10,480
And what I'm here is not fundamentally incompatible with existing things. You just have to,

287
00:33:10,480 --> 00:33:16,480
you know, be careful in how you apply it and follow along with some of the learnings that,

288
00:33:16,480 --> 00:33:22,160
you know, this paper articulates and what will surely come after. Yeah, exactly. So the same way,

289
00:33:22,160 --> 00:33:29,200
I guess, yeah, you know, the same way we have different architectures, like CNNs, you know,

290
00:33:29,200 --> 00:33:35,920
transformers, like births, GPT, others, you can specify all of those architectures, right?

291
00:33:35,920 --> 00:33:41,920
In that paper, we walk with, uh, in color of the color transformers models. And so our recommendations

292
00:33:41,920 --> 00:33:48,400
are kind of general, but also mostly applied to in color of the color models. But if you were to,

293
00:33:48,400 --> 00:33:55,840
you know, specify a, a, a, you'd have to do something slightly different, you know, and so it's kind

294
00:33:55,840 --> 00:34:01,520
of like a new, it's like a new class in the same way that, like, it opens a new dimension for scaling

295
00:34:01,520 --> 00:34:06,480
these models. You can take any architecture and make it spouse on that spouse, basically.

296
00:34:07,040 --> 00:34:11,600
Awesome. Awesome. We had a couple of other things that we wanted to talk about and, you know,

297
00:34:11,600 --> 00:34:18,560
maybe one we can jump into is, you know, and it follows a little bit from talking about language

298
00:34:18,560 --> 00:34:26,160
models, but you are, you've got some research interests in kind of retrieval problems and alignment

299
00:34:26,160 --> 00:34:32,400
problems in that space. Let's talk a little bit about what you've seen on the retrieval side.

300
00:34:33,600 --> 00:34:40,960
Tell us about the problem and why it interests you. Yeah, so, you know, one thing I said when we're

301
00:34:40,960 --> 00:34:45,600
talking about spouse models is that one of the benefits of increasing the number of parameters

302
00:34:46,320 --> 00:34:51,920
that has two impacts, two factors, right? Like, first, you have additional compute at training

303
00:34:51,920 --> 00:34:58,320
and inference time. And so the model is smaller in a way, but you also have increased

304
00:34:58,320 --> 00:35:03,680
memorization of the training data. You know, again, there's a story that more parameters

305
00:35:04,320 --> 00:35:11,520
corresponds to more knowledge. And so rather than simply scaling the model size, one alternative

306
00:35:11,520 --> 00:35:18,880
is to equip models with ability to directly access a large database when performing predictions.

307
00:35:18,880 --> 00:35:25,840
And so this is also like what a recent line of fork is exploring, just like retrieval from

308
00:35:25,840 --> 00:35:33,840
large databases as a complementary to simply scaling language models. And so there are a few

309
00:35:33,840 --> 00:35:39,200
sort of like different directions. The first one is retrieving from the training data set.

310
00:35:39,200 --> 00:35:47,440
There's this recent paper called Retro, like retrieval enhanced transformer from friends at DeepMind,

311
00:35:48,960 --> 00:35:54,960
where they augment the training set with, you know, each chunk of the training set

312
00:35:56,000 --> 00:36:03,520
with its key nearest neighbors from the training database. And so during training and also at

313
00:36:03,520 --> 00:36:10,240
inference, the model has access to these neighbor chunks and can utilize them when making predictions.

314
00:36:10,880 --> 00:36:16,640
And so what they find is that, you know, you can match the performance of much larger

315
00:36:16,640 --> 00:36:21,680
language models with much smaller language models. So I think in their papers, they say, you know,

316
00:36:21,680 --> 00:36:30,080
we have like comparable results as GP3 with a model that's 25 times smaller. And so, you know,

317
00:36:30,080 --> 00:36:37,200
of course, you know, this retrieval process, you know, adds some latency and some, you know,

318
00:36:37,200 --> 00:36:44,400
some complexity, but, you know, this kind of like scaling gains, like 25x, also very exciting.

319
00:36:45,120 --> 00:36:49,280
That's, yeah, something else to explore besides just scaling or like sparsity.

320
00:36:50,960 --> 00:36:55,280
And I think that's very interesting. There's one thing even more recent that

321
00:36:55,280 --> 00:37:01,840
consists in like retrieving from the web, right? So, you know, when you retrieve from a fixed

322
00:37:01,840 --> 00:37:07,360
training database, you're not going to be up to date with the world's latest knowledge and

323
00:37:07,360 --> 00:37:12,960
events, right? There's this aspect of generalization that scaling doesn't help with,

324
00:37:12,960 --> 00:37:18,640
it's like temporal generalization. Like if your model was trained in 2019, it's not going to know

325
00:37:19,360 --> 00:37:24,880
about 2021. You know, you could argue that like a good enough model can predict the future,

326
00:37:24,880 --> 00:37:31,440
but I don't think we're there yet. So, so what people have have proposed is to, you know,

327
00:37:32,320 --> 00:37:38,080
continuously update the database with the latest state of the world and the latest knowledge,

328
00:37:38,080 --> 00:37:44,240
but that's basically the internet, right? So, what you can do is like outsource

329
00:37:44,240 --> 00:37:50,560
documentary retrieval to the web by using, you know, like a search engine, like Microsoft being

330
00:37:50,560 --> 00:37:57,360
or Google search. And yeah, learn, have your language model learn to interact with the web

331
00:37:58,160 --> 00:38:04,160
to retrieve documents that help for predictions. So, there's this recent paper from a

332
00:38:04,160 --> 00:38:11,280
player that I really like called WebGPT that does just that where, you know, they have humans navigate

333
00:38:11,280 --> 00:38:18,160
the web to answer questions. So, they collect demonstrations from how humans use the web,

334
00:38:18,160 --> 00:38:24,000
and then they train a language model to replicate that. And so, similarly, they show that like a

335
00:38:24,000 --> 00:38:31,520
much smaller model can match performance of much larger models and that this retrieval process helps

336
00:38:31,520 --> 00:38:39,440
into factual accuracy, truthfulness, coherence, and so on. And so, I think that's also very promising.

337
00:38:39,440 --> 00:38:47,600
Are these these ideas in the case of the the data set or the web? Are we primarily using retrieval

338
00:38:49,120 --> 00:39:00,640
to augment generation? Or one way to use retrieval is to to augment generation, right? You

339
00:39:02,240 --> 00:39:06,960
reach out to some data set, you get some chunk of text and that gives you a little bit more richness

340
00:39:06,960 --> 00:39:11,520
to manipulate when you're trying to spit something out. Another way is to think about the

341
00:39:12,160 --> 00:39:18,560
data set as as you described earlier, kind of a pure knowledge store that you're kind of,

342
00:39:18,560 --> 00:39:23,760
I think it was like operating on when you're trying to make an inference decision. How do you

343
00:39:27,600 --> 00:39:32,000
do you see aspects of both of those? And how do you think about retrieval?

344
00:39:32,000 --> 00:39:41,280
How do you distinguish also maybe the distinction between both aspects is like how clean the database is?

345
00:39:41,280 --> 00:39:54,640
Is that correct? I think what I'm trying to get at is when the how and when the data is being

346
00:39:54,640 --> 00:40:04,960
used and you know, maybe pulling it back to this analogy, you know, how kind of pure the databases

347
00:40:06,000 --> 00:40:13,360
to what degree is the data is the database really knowledge as opposed to augmenting expressiveness

348
00:40:13,360 --> 00:40:21,840
later? It's mostly the the later like. Okay. In this in this in this recent walks, you know,

349
00:40:21,840 --> 00:40:30,480
the retrieval knowledge base is just your training data set or whatever the web gives you, you know,

350
00:40:30,480 --> 00:40:36,800
and so this is going to be very noisy. This isn't like, you know, sort of entity, entity knowledge

351
00:40:36,800 --> 00:40:42,320
bases where you have, you know, like person, date of birth, like country, right? This is just like,

352
00:40:42,320 --> 00:40:47,920
you find a nearest, you find the top K nearest neighbors in all of your training

353
00:40:47,920 --> 00:40:55,760
corpora and you use that to have additional context when making predictions. And that's kind

354
00:40:55,760 --> 00:41:03,360
of enough to get really good results. You know, it could be that you get better results with sort

355
00:41:03,360 --> 00:41:11,520
of a more created database retrieve from, but I think that, you know, your efforts are and budget

356
00:41:11,520 --> 00:41:18,640
are this better spent somewhere else. We also mentioned alignment and those problems. Can you share

357
00:41:18,640 --> 00:41:23,440
a bit about what you're seeing there? Yeah. So, you know, dislodged language models are kind of

358
00:41:23,440 --> 00:41:32,000
becoming the the Swiss army knife of of an LP. They have fairly amazing like transfer learning

359
00:41:32,000 --> 00:41:39,280
capabilities in the future or zero-shot setup event. So, you train on a lot of text mostly coming

360
00:41:39,280 --> 00:41:47,120
from the internet and then at inference, the model is doing pretty good at like random tasks

361
00:41:47,120 --> 00:41:53,920
that weren't really part of the training set. But there's one sort of issue here is that the

362
00:41:53,920 --> 00:41:58,960
pre-training objective that we use to train these language models is basically predict the next

363
00:41:58,960 --> 00:42:05,120
token, right, from a web page in the internet. You know, that's where most of the data comes from.

364
00:42:05,120 --> 00:42:11,680
And so, that objective is quite different from the objective, you know, follow the user's

365
00:42:11,680 --> 00:42:17,280
instructions, you know, and preferably in a helpful and safe manner. And so, as a result,

366
00:42:17,280 --> 00:42:23,040
like large language models, you know, in spite of all the impressive results that have led to

367
00:42:23,040 --> 00:42:28,720
are not that good at following people's instructions, right? There's like a misalignment between

368
00:42:28,720 --> 00:42:35,760
how you train them and how we intend to use them. And so, there's been efforts towards sort of aligning

369
00:42:35,760 --> 00:42:42,800
these models with human intent by fine tuning them on data we care about. So, they're kind of like

370
00:42:42,800 --> 00:42:48,720
two lines of work in the direction. The first one is called instruction tuning, where during fine

371
00:42:48,720 --> 00:42:54,240
tuning, you're going to augment each task with a verbal description of the task itself, right? So,

372
00:42:54,240 --> 00:42:59,920
you should fine tune on, you know, translation data. You're going to be like,

373
00:43:00,640 --> 00:43:05,280
translate from, you know, English to French, then you're going to have your English sentence

374
00:43:05,280 --> 00:43:11,440
in the target is going to be the French sentence. And by having enough of these tasks that are

375
00:43:11,440 --> 00:43:18,640
described verbally, the hope is that at inference, the model can generalize across new tasks that are

376
00:43:18,640 --> 00:43:25,600
unseen during fine tuning. So, using that example of translating again, maybe at inference,

377
00:43:25,600 --> 00:43:32,560
I could be like head translate from legalese to regular or simple English. And because the model

378
00:43:32,560 --> 00:43:39,440
kind of knows what it's supposed to do when translating, now it just has to sort of, you know,

379
00:43:39,440 --> 00:43:44,800
understand, okay, what is legalese and what is regular simple English to perform that new unseen

380
00:43:44,800 --> 00:43:51,520
task. And so, you know, there's a similar story here that you get equivalent performance

381
00:43:51,520 --> 00:43:59,520
as much larger models. I think this paper called T0 reports, you know, 16, like similar

382
00:43:59,520 --> 00:44:06,160
performance with a model that are 16 times smaller. The second line of work in the direction is

383
00:44:06,160 --> 00:44:13,680
more direct alignment, where, you know, it's mostly driven by open AI and public. What they did is

384
00:44:13,680 --> 00:44:21,680
that they basically gave access to these models. They gave access to these models to people that,

385
00:44:21,680 --> 00:44:27,920
you know, just try to use them. And they generate like the collect actual data of how users want to

386
00:44:27,920 --> 00:44:34,880
use these models. Yeah, basically, it's kind of like assistant-assistant like data, right? Like

387
00:44:34,880 --> 00:44:40,400
help me plan a wedding or like help me write a marketing campaign for Red Bull or whatsoever.

388
00:44:40,400 --> 00:44:47,120
And so, they use this approach called reinforcement learning from human preferences,

389
00:44:48,320 --> 00:44:56,320
which is a two-step process. So the first step is to collect comparison data based on human

390
00:44:56,320 --> 00:45:02,800
preferences. And so, for example, you can output, you know, let's say, so human inputs,

391
00:45:03,440 --> 00:45:08,960
some prompt to a model and the model will propose two-candy data outputs. Then the model

392
00:45:08,960 --> 00:45:16,080
selects which one it prefers, right? Sorry, the human selects which one it prefers. And so,

393
00:45:16,080 --> 00:45:23,200
that process creates a data set of comparison data, right? Like given to, given an input and two

394
00:45:23,760 --> 00:45:29,440
candidate outputs, this is the one that is preferred by humans. So what you can do is train a model

395
00:45:30,880 --> 00:45:37,440
on that data. And this is called a reward model, because you're going to use that model as

396
00:45:37,440 --> 00:45:45,360
to provide reward to sort of, you know, like a reinforcement learning algorithm. And so,

397
00:45:45,360 --> 00:45:50,800
the second step of that process is to train your language model with a policy-gradient method,

398
00:45:50,800 --> 00:45:57,120
with the reward model providing these rewards. And by sort of going through that process iteratively,

399
00:45:57,120 --> 00:46:05,440
you get very close to the outputs matching what the humans would have preferred. And this is kind

400
00:46:05,440 --> 00:46:12,560
of like aligning models with human intention. There's a similar story here that like, you know,

401
00:46:13,600 --> 00:46:19,760
actually the results in these walks are pretty impressive, like yet. Rather than having GP3,

402
00:46:19,760 --> 00:46:27,920
you can have a model that's 100 times smaller. And by, you know, aligning it with human intentions

403
00:46:27,920 --> 00:46:37,760
with this two-step process of like collecting comparison data and using it to train your language

404
00:46:37,760 --> 00:46:44,560
model with reinforcement learning. You get better or like preferred outputs to a model that 100

405
00:46:44,560 --> 00:46:50,080
times larger. So I think something that I'm kind of really looking forward to is when people are

406
00:46:50,080 --> 00:46:56,240
going to combine all these recent advancements, figure out, you know, which dimensions to scale,

407
00:46:56,240 --> 00:47:01,760
like, yeah, I think once you do a mixture of experts, like alignments, retrieval, you know,

408
00:47:01,760 --> 00:47:06,880
you spend a lot of money and you use to scale everything, figure out how to do it the proper way.

409
00:47:06,880 --> 00:47:12,400
Yeah. No, I think we're looking at something that's going to be fairly amazing. So pretty exciting

410
00:47:12,400 --> 00:47:16,640
about that. Yeah, I mean, that's kind of the name of the game. We've got point optimizations,

411
00:47:16,640 --> 00:47:20,640
heading in different directions. And we just need to figure out how to combine the model to

412
00:47:20,640 --> 00:47:28,640
create some more general version of intelligence. Yeah, exactly. That kind of research is pretty

413
00:47:28,640 --> 00:47:36,160
costly, though, because it's very hard to extrapolate performance, you know, at small scale.

414
00:47:37,120 --> 00:47:42,240
Actually, one thing that I saw, again and again, when I was at Google, it's like, you know,

415
00:47:42,240 --> 00:47:47,040
you see a paper that that proposes a new idea or like a new architecture, you know, like Google,

416
00:47:47,040 --> 00:47:51,600
let me try it. And then you trade it like Google scale, which is usually, you know,

417
00:47:52,560 --> 00:47:57,280
in order of magnitude larger than academic scale, right? Yeah. And you find that like,

418
00:47:57,280 --> 00:48:04,320
you know, improvements completely vanish. And so it's that a is that a reproducibility problem

419
00:48:04,320 --> 00:48:13,760
or is that a fundamentally the method just didn't scale the mental issue? Yeah. But I think

420
00:48:13,760 --> 00:48:19,840
that's really like sort of a, there's something deeper behind that fact is that,

421
00:48:20,960 --> 00:48:26,880
yeah, it's very hard to extrapolate techniques. Also, people, I think this kind of stuff would be

422
00:48:27,600 --> 00:48:34,960
seen if people were, you know, designing, scaling laws in a very rigorous way. But, you know,

423
00:48:34,960 --> 00:48:41,120
that takes a lot of time. And usually people just say like, hey, you know, the baseline is this,

424
00:48:41,120 --> 00:48:45,040
I did that and I get a better number, but we have no idea what happens when you scale.

425
00:48:47,360 --> 00:48:50,720
I also think, you know, like this whole like scaling direction has made

426
00:48:51,600 --> 00:48:58,080
rigorous baselines much harder, having rigorous baselines much harder because, you know,

427
00:48:58,080 --> 00:49:06,080
these models are pretty costly both in time and of course financially. And so each data point

428
00:49:06,080 --> 00:49:13,360
that you consider for your ablation study, you know, my cost you weeks are hundreds to millions of

429
00:49:13,360 --> 00:49:19,840
dollars. And so, you know, research has kind of evolved from like this very, at least in that

430
00:49:19,840 --> 00:49:27,120
field, right, like from very rigorous settings to let's just try to, you know, figure out

431
00:49:27,840 --> 00:49:35,200
among a lot of noise, what works best. I mean, I'd say, you know, even there was already a lack of

432
00:49:35,200 --> 00:49:42,080
rigorous baselines even in overfield, so that just doesn't help. Yeah. Yeah.

433
00:49:42,960 --> 00:49:51,200
Yeah, I mean, that brings into, brings into light or the conversation, a lot of issues that we've

434
00:49:51,200 --> 00:49:57,360
been talking about in the field, the lack of rigor overfitting on, you know, a few data sets,

435
00:49:58,160 --> 00:50:04,160
you know, all these I think are accentuated by the scale that you're trying to operate at.

436
00:50:04,160 --> 00:50:08,880
Yeah. I wrote a paper about this last year, actually, like that show that in computer vision,

437
00:50:08,880 --> 00:50:15,920
a lot of the improvements that, you know, research that posted about came from training tricks,

438
00:50:15,920 --> 00:50:20,800
as opposed to the so-claim architectural improvements, that you could still get very competitive

439
00:50:20,800 --> 00:50:26,800
results by simply scaling resnets, which are, you know, like six years old or seven years old,

440
00:50:28,160 --> 00:50:32,560
that you didn't need like all these like fancy new stuff. It was just like, yeah, tech,

441
00:50:32,560 --> 00:50:39,200
the architecture that you know walks, figure out how to scale it properly and training properly,

442
00:50:39,200 --> 00:50:44,480
and usually you're going to be very close to state of DL, you know, trying all these architectural tricks,

443
00:50:44,480 --> 00:50:52,000
like, doesn't matter, basically. And I think that's something that people have come towards, like,

444
00:50:52,000 --> 00:50:57,680
now there's much less architectural research, you know, people use transformers most of the time.

445
00:50:57,680 --> 00:51:02,480
You know, there's this mixture of experts thing, which is kind of like, which I think is a big

446
00:51:02,480 --> 00:51:07,280
enough change that like, it's a real change. It's not like, you know, hey, use a depot ice convolution

447
00:51:07,280 --> 00:51:13,280
here instead of a convolution there. But yeah, I think, you know, with that scale, with the

448
00:51:13,280 --> 00:51:20,320
scaling direction, like, we're kind of moving away from the academic research setup, right? That's

449
00:51:20,320 --> 00:51:28,480
like a widening gap between what can be done in academia and industry. And only a few organizations

450
00:51:28,480 --> 00:51:34,800
have the technical expertise or the financial leeway to support this kind of work. Then I'll see

451
00:51:34,800 --> 00:51:41,040
that gap closing anytime soon. Yeah, I mean, I don't doubt if you like, you know, open efforts,

452
00:51:41,520 --> 00:51:49,600
like the third AI and big science. Stanford is also starting a, yeah, exactly. Stanford is also

453
00:51:49,600 --> 00:51:55,360
starting a group. So, you know, I think some people are rallying and, you know, realizing, hey,

454
00:51:55,360 --> 00:52:01,280
we can't just let big companies have access to those. But there are a little bit behind,

455
00:52:01,280 --> 00:52:06,240
and like, yeah, there's dark kind of questions about, you know, how long you can sustain that kind

456
00:52:06,240 --> 00:52:14,960
of research and this kind of costs. Yeah, awesome. Well, Erwan, thanks so much for taking the time

457
00:52:14,960 --> 00:52:20,560
to chat for those who are still with us. This interview has been a long time in the works.

458
00:52:21,840 --> 00:52:27,280
And Erwan's continued it to advance his research. And I'm super glad we're able to finally get

459
00:52:27,280 --> 00:52:47,920
together. And I really enjoyed the conversation. Thank you for having me. Awesome. Thank you.

