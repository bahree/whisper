1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,800
I'm your host Sam Charrington.

4
00:00:23,800 --> 00:00:27,080
It's been another exciting week here at Twimble Headquarters.

5
00:00:27,080 --> 00:00:33,480
Just a few days after hitting the 500,000 listens mark, thanks to you once again, we learned

6
00:00:33,480 --> 00:00:38,080
that at least a few of those listens came from a certain Mark Cuban.

7
00:00:38,080 --> 00:00:40,920
And yes, I mean that Mark Cuban.

8
00:00:40,920 --> 00:00:45,040
Speaking at a conference in New York City, Mark mentioned that he turns to this very

9
00:00:45,040 --> 00:00:50,120
podcast to learn about and keep up to date on advances in artificial intelligence.

10
00:00:50,120 --> 00:00:52,760
Mark, if you're listening, we love you man.

11
00:00:52,760 --> 00:00:54,280
Thanks for the shout out.

12
00:00:54,280 --> 00:00:59,680
The CNBC article that covered Mark's talk and mentioned this podcast focused on his fear

13
00:00:59,680 --> 00:01:03,280
of AI and what it might bring in the future.

14
00:01:03,280 --> 00:01:07,460
As you might imagine, this is a topic I've got some opinions on and I respond to the

15
00:01:07,460 --> 00:01:11,000
article and Mark's fears in this week's newsletter.

16
00:01:11,000 --> 00:01:16,560
If you're not already receiving it, head to twimbleai.com slash newsletter to sign up and I'll

17
00:01:16,560 --> 00:01:19,200
make sure you get the current issue.

18
00:01:19,200 --> 00:01:24,000
Last week we announced the first of two winners for our artificial intelligence conference ticket

19
00:01:24,000 --> 00:01:25,520
giveaway.

20
00:01:25,520 --> 00:01:31,400
Winners received a bronze pass to the conference which grants access to all keynotes and sessions.

21
00:01:31,400 --> 00:01:34,760
Our second winner is Richard S. from Brooklyn, New York.

22
00:01:34,760 --> 00:01:38,000
Thanks again to everyone who entered the contest.

23
00:01:38,000 --> 00:01:42,860
If you didn't win this go round but would like to join us at the conference, use the discount

24
00:01:42,860 --> 00:01:47,360
code PC Twimble for 20% off of registration.

25
00:01:47,360 --> 00:01:54,920
We'll link to the conference in the show notes which you can find at twimbleai.com slash talk slash 43.

26
00:01:54,920 --> 00:01:59,400
The first twimble online meetup was last week and was wonderful.

27
00:01:59,400 --> 00:02:05,480
The focus of the meetup was the CVPR best paper award winner learning from simulated and

28
00:02:05,480 --> 00:02:11,920
unsupervised images through adversarial training by researchers from Apple.

29
00:02:11,920 --> 00:02:17,280
The idea behind this paper is this, consider a problem like IGaze detection.

30
00:02:17,280 --> 00:02:21,560
You've got a picture from, for example, a cell phone camera and you want to determine

31
00:02:21,560 --> 00:02:24,560
which way the user is looking.

32
00:02:24,560 --> 00:02:29,640
Generating labeled IGaze training data is hard and expensive.

33
00:02:29,640 --> 00:02:34,360
Generating simulated IGaze training data sets is much easier and cheaper though and can

34
00:02:34,360 --> 00:02:38,960
be done for example by using something like a video game engine.

35
00:02:38,960 --> 00:02:43,800
The problem is that the simulated IGaze images don't look close enough to real images

36
00:02:43,800 --> 00:02:47,360
to train a model to work effectively on real data.

37
00:02:47,360 --> 00:02:53,400
This paper proposes using a generative adversarial network to train a refiner model that can

38
00:02:53,400 --> 00:03:00,360
make simulated IGaze images look like real IGaze images while preserving the gaze direction.

39
00:03:00,360 --> 00:03:05,400
Thanks again to community members Josh Manella who did a great job presenting this paper

40
00:03:05,400 --> 00:03:10,360
and to Kevin Maider for walking us through a TensorFlow implementation of the model.

41
00:03:10,360 --> 00:03:12,640
You guys are just awesome.

42
00:03:12,640 --> 00:03:17,320
We're working on getting the recording posted for those who weren't able to join us live.

43
00:03:17,320 --> 00:03:21,200
If you're signed up for the meetup or the newsletter you'll be notified when it's

44
00:03:21,200 --> 00:03:22,400
available.

45
00:03:22,400 --> 00:03:28,600
If you'd like to join the meetup, head over to twimlai.com slash meetup to register.

46
00:03:28,600 --> 00:03:34,200
Next month's meetup will be held on Wednesday September 13th at 11 a.m. Pacific time and

47
00:03:34,200 --> 00:03:38,000
we'll post the details of the program shortly.

48
00:03:38,000 --> 00:03:43,320
Before we get to the show, I'd like to give a shout out to our friends at wise.io at GE Digital

49
00:03:43,320 --> 00:03:47,760
for their sponsorship of this industrial AI podcast series.

50
00:03:47,760 --> 00:03:52,960
Hopefully you caught last week's show featuring Josh Bloom, Vice President of Data and Analytics

51
00:03:52,960 --> 00:03:54,560
at GE Digital.

52
00:03:54,560 --> 00:03:59,880
We had a great discussion about how to incorporate physics-based information into machine learning

53
00:03:59,880 --> 00:04:02,200
models among other things.

54
00:04:02,200 --> 00:04:08,440
For more information, you'll find that show at twimlai.com slash talk slash 42.

55
00:04:08,440 --> 00:04:14,200
And for more information on wise.io at GE Digital, visit wise.io.

56
00:04:14,200 --> 00:04:16,080
And now for today's show.

57
00:04:16,080 --> 00:04:20,360
If you've listened to any of the shows in the industrial AI series, you've undoubtedly

58
00:04:20,360 --> 00:04:23,360
heard me mention our friends over at bonsai.

59
00:04:23,360 --> 00:04:28,280
I'm super grateful to bonsai for taking the lead in sponsoring both the industrial AI

60
00:04:28,280 --> 00:04:32,400
podcast series as well as my paper on that topic.

61
00:04:32,400 --> 00:04:38,400
Well today's show, which concludes this first season of the industrial AI series, features

62
00:04:38,400 --> 00:04:42,760
my interview with bonsai co-founder and CEO Mark Hammond.

63
00:04:42,760 --> 00:04:48,400
Our conversation centers on the role of what he calls machine teaching and delivering practical

64
00:04:48,400 --> 00:04:54,440
machine learning solutions, particularly for enterprise or industrial AI use cases.

65
00:04:54,440 --> 00:04:58,760
I really enjoyed this conversation with Mark and I know you will too.

66
00:04:58,760 --> 00:05:01,440
And now on to the show.

67
00:05:01,440 --> 00:05:08,760
All right, everyone.

68
00:05:08,760 --> 00:05:16,400
I am here at the offices of bonsai with Mark Hammond, the co-founder and CEO of the company.

69
00:05:16,400 --> 00:05:19,120
Mark, welcome to this week in machine learning and AI.

70
00:05:19,120 --> 00:05:20,120
Thank you for having me.

71
00:05:20,120 --> 00:05:21,120
Happy to be here.

72
00:05:21,120 --> 00:05:24,200
Yeah, I'm super excited to have you on the show.

73
00:05:24,200 --> 00:05:29,160
Folks who are regular listeners will know the name bonsai without a doubt, because you

74
00:05:29,160 --> 00:05:35,320
guys have very graciously sponsored my research into industrial AI and the podcast series.

75
00:05:35,320 --> 00:05:42,120
And I'm really looking forward to digging into with you what industrial AI means for bonsai.

76
00:05:42,120 --> 00:05:46,160
But before we dive into that, why don't we, why don't you spend a few minutes telling

77
00:05:46,160 --> 00:05:47,880
us a little bit about your background?

78
00:05:47,880 --> 00:05:48,880
Sure.

79
00:05:48,880 --> 00:05:51,280
So my background is actually originally very technical.

80
00:05:51,280 --> 00:05:56,520
I started programming very young and ended up working at Microsoft while I was still in

81
00:05:56,520 --> 00:06:01,920
high school on Windows itself, Windows 95, and the first many versions of Internet Explorer.

82
00:06:01,920 --> 00:06:06,360
So definitely hands-on coding on the products themselves.

83
00:06:06,360 --> 00:06:09,000
My passion that was always been artificial intelligence.

84
00:06:09,000 --> 00:06:13,400
So even while I was there, I knew that was what I wanted to do.

85
00:06:13,400 --> 00:06:18,720
And I decided to pursue a course of studies in computation and neural systems at Caltech.

86
00:06:18,720 --> 00:06:25,920
So I was working at Microsoft attending Caltech, and it was great in all regards other than

87
00:06:25,920 --> 00:06:30,240
that it happened in the late 90s, which was a fantastic time to work at Microsoft.

88
00:06:30,240 --> 00:06:34,120
And not the best time in the world to be in the field of AI.

89
00:06:34,120 --> 00:06:37,760
It's like part of one of the AI winters.

90
00:06:37,760 --> 00:06:45,160
And so I found myself when I was completing those studies faced with, how do we use all

91
00:06:45,160 --> 00:06:46,320
this stuff in the real world?

92
00:06:46,320 --> 00:06:51,520
And at that point in time, it was really, well, do you pursue a course in academia, right?

93
00:06:51,520 --> 00:06:55,040
Is that do you go to the academic route or what else do you do?

94
00:06:55,040 --> 00:06:56,720
Because it's hard of the AI winters.

95
00:06:56,720 --> 00:07:02,880
And so I kind of decided at that point that because I have this strong impetus towards

96
00:07:02,880 --> 00:07:07,200
applying this technology in real world scenarios, if I wanted to do that, I was going to need

97
00:07:07,200 --> 00:07:10,240
to get some of the not purely technical skills.

98
00:07:10,240 --> 00:07:16,240
And so I decided, okay, I got to go try product management, developer sales and marketing,

99
00:07:16,240 --> 00:07:17,240
et cetera, et cetera.

100
00:07:17,240 --> 00:07:18,920
And so I pursued that course.

101
00:07:18,920 --> 00:07:23,160
I did find myself at one point back at Microsoft this time in sales and marketing.

102
00:07:23,160 --> 00:07:28,720
I was one of the developer evangelists who was outpitching.net when.net was brand new

103
00:07:28,720 --> 00:07:33,080
and getting everyone on the C-sharp bandwagon, so that was a lot of fun.

104
00:07:33,080 --> 00:07:35,680
And fast forward to today.

105
00:07:35,680 --> 00:07:40,680
And the market is now in a great place where the technology is very capable.

106
00:07:40,680 --> 00:07:44,480
It's the right time to start looking at applying these technologies to our real world

107
00:07:44,480 --> 00:07:49,120
industrial and commercial enterprises and looking at those use cases.

108
00:07:49,120 --> 00:07:52,040
And I had come to the insight which led to bonsai.

109
00:07:52,040 --> 00:07:56,960
And it was born through, having gone through the academic track, having gone through the

110
00:07:56,960 --> 00:08:01,560
pure business track, having looked at trying to apply AI in lots of different contexts.

111
00:08:01,560 --> 00:08:04,120
And ultimately coming on one very simple realization.

112
00:08:04,120 --> 00:08:08,200
It's one of these, one of these things where you look back and you say, well, that seems

113
00:08:08,200 --> 00:08:12,720
obvious in hindsight, but until you think about it, it's not apparent.

114
00:08:12,720 --> 00:08:18,840
And that's that no matter how good we make these algorithms, they could be as good or better

115
00:08:18,840 --> 00:08:20,880
than humans at learning.

116
00:08:20,880 --> 00:08:22,520
We will always have to teach them.

117
00:08:22,520 --> 00:08:23,600
You have to teach them something.

118
00:08:23,600 --> 00:08:24,600
It's a learning algorithm.

119
00:08:24,600 --> 00:08:26,800
It's kind of by design it has to be taught.

120
00:08:26,800 --> 00:08:32,480
And there wasn't a huge focus on how you actually teach something.

121
00:08:32,480 --> 00:08:36,720
We spend so much time in this field focused on the machine learning algorithms themselves

122
00:08:36,720 --> 00:08:38,680
that teaching is often afterthought.

123
00:08:38,680 --> 00:08:42,560
That was the spark that said, look, if we're going to be able to solve these real world

124
00:08:42,560 --> 00:08:48,280
industrial AI applications, the subject matter expertise, the ability for people to define

125
00:08:48,280 --> 00:08:52,720
what they want to teach and how to teach it, that is an area that is ripe for enabling

126
00:08:52,720 --> 00:08:53,720
the technology to be used.

127
00:08:53,720 --> 00:08:54,920
And that's what led to Founding Bones.

128
00:08:54,920 --> 00:08:56,680
I am sitting here today.

129
00:08:56,680 --> 00:08:57,680
Awesome.

130
00:08:57,680 --> 00:08:58,680
Awesome.

131
00:08:58,680 --> 00:09:00,720
It's interesting that you came across that realization.

132
00:09:00,720 --> 00:09:07,120
I tend to find the same thing that the emphasis is on kind of the machine learning.

133
00:09:07,120 --> 00:09:11,480
And maybe you put another way, the way people tend to think about this teaching process

134
00:09:11,480 --> 00:09:15,200
is throwing a bunch of data at an algorithm.

135
00:09:15,200 --> 00:09:19,040
I guess it's kind of analogous to like throwing a bunch of books at a kid and expecting them

136
00:09:19,040 --> 00:09:20,440
to learn on their own.

137
00:09:20,440 --> 00:09:21,440
Yeah, absolutely.

138
00:09:21,440 --> 00:09:22,440
Absolutely.

139
00:09:22,440 --> 00:09:24,840
The one that I often use is with my son.

140
00:09:24,840 --> 00:09:27,160
He's learning how to play baseball.

141
00:09:27,160 --> 00:09:29,200
And I tell people I don't take him out to the backyard.

142
00:09:29,200 --> 00:09:30,200
He's five, right?

143
00:09:30,200 --> 00:09:31,200
So I tell people he's five.

144
00:09:31,200 --> 00:09:36,200
I don't take him out to the backyard and throw fastballs at him because that would just be,

145
00:09:36,200 --> 00:09:37,200
that's just cruel.

146
00:09:37,200 --> 00:09:38,520
Why would you do that?

147
00:09:38,520 --> 00:09:42,040
And yet, no one even pauses for a moment when we're like, well, we're just going to throw

148
00:09:42,040 --> 00:09:44,840
giant data sets at these machine learning algorithms.

149
00:09:44,840 --> 00:09:49,920
And I bet if I threw a million fastballs at my son, he'd figure out out eventually, too.

150
00:09:49,920 --> 00:09:53,040
But it's just not a very efficient or effective mechanism for teaching.

151
00:09:53,040 --> 00:09:54,040
Yeah.

152
00:09:54,040 --> 00:09:55,560
And probably be pretty painful for your son.

153
00:09:55,560 --> 00:09:56,560
True.

154
00:09:56,560 --> 00:09:57,560
True.

155
00:09:57,560 --> 00:09:59,600
Not very responsible for me as a parent.

156
00:09:59,600 --> 00:10:03,440
So maybe we can, I guess, let's just dig into this.

157
00:10:03,440 --> 00:10:09,600
So when you talk about machine training, what does that mean or machine teaching, what

158
00:10:09,600 --> 00:10:10,960
does that mean to you?

159
00:10:10,960 --> 00:10:16,400
So machine teaching really boils down to actually looking at the art and science about

160
00:10:16,400 --> 00:10:24,080
how we teach things, distilling out the abstractions that are there, and then providing the appropriate

161
00:10:24,080 --> 00:10:30,440
platform, tooling, et cetera, that developers, we've come to expect in order to be able

162
00:10:30,440 --> 00:10:35,360
to properly construct the solution to a problem in that context.

163
00:10:35,360 --> 00:10:40,080
So that's at a very high level, but at a concrete level, what it means for us is we need to

164
00:10:40,080 --> 00:10:44,040
give you a very formal way, because it's still a computer program at the end of the day.

165
00:10:44,040 --> 00:10:48,720
We need to give you a very formal way to specify what it is you're actually trying to teach,

166
00:10:48,720 --> 00:10:54,440
and how you can go about phasing that teaching in a way that follows, again, using examples

167
00:10:54,440 --> 00:10:58,120
we're just talking about, you don't want to have it be just throw massive throws of

168
00:10:58,120 --> 00:11:02,400
data at the system or how you teach it can be broken up in ways that facilitate acquisition

169
00:11:02,400 --> 00:11:04,720
and mastery of these concepts you're trying to teach.

170
00:11:04,720 --> 00:11:10,120
So really at the end of the day, it's about empowering developers to work as teachers

171
00:11:10,120 --> 00:11:13,880
and giving them the ability to do that in a very formal, structured way.

172
00:11:13,880 --> 00:11:18,200
You can think about it if we're doing this in very humanistic terms, you talked about textbooks,

173
00:11:18,200 --> 00:11:19,200
right?

174
00:11:19,200 --> 00:11:21,240
So a textbook has a curriculum that's set out in it.

175
00:11:21,240 --> 00:11:24,280
Here's the table of contents, all the concepts we want you to master.

176
00:11:24,280 --> 00:11:25,520
We're going to go through it in this order.

177
00:11:25,520 --> 00:11:28,920
Here's phased problem sets that ramp in difficulty level.

178
00:11:28,920 --> 00:11:33,240
You go through all of these different ways of structuring the textbook in order to try

179
00:11:33,240 --> 00:11:35,840
to help guide students as they learn things.

180
00:11:35,840 --> 00:11:37,280
We do the same thing.

181
00:11:37,280 --> 00:11:39,040
We just do it in a very formal, structured way.

182
00:11:39,040 --> 00:11:43,560
We give you an actual programming language where there's no ambiguity about what you're

183
00:11:43,560 --> 00:11:44,560
trying to do.

184
00:11:44,560 --> 00:11:49,560
It's less freeform textbook and much more program that you're creating.

185
00:11:49,560 --> 00:11:53,960
Maybe you can make that more clear by walking us through an example, you know, pick a use

186
00:11:53,960 --> 00:11:58,200
case and maybe talk about how you would apply this to the use case.

187
00:11:58,200 --> 00:11:59,200
Yeah, absolutely.

188
00:11:59,200 --> 00:12:04,040
So, if we look at a robotic system as an example, so you have an industrial robot, perhaps

189
00:12:04,040 --> 00:12:09,520
it's a robotic armature being used in a manufacturing setting or a warehouse setting.

190
00:12:09,520 --> 00:12:15,040
And you want that armature to be able to create conduct various pick and place operations.

191
00:12:15,040 --> 00:12:19,600
Maybe it's doing a palatizing operation, something of that nature.

192
00:12:19,600 --> 00:12:26,200
Now, when you want to have the piece of equipment learn to accomplish this task, you can break

193
00:12:26,200 --> 00:12:28,760
it up into the constituent concepts that matter.

194
00:12:28,760 --> 00:12:34,560
It's not about, I want you to perform the entire task, let me demonstrate it for you.

195
00:12:34,560 --> 00:12:38,720
It's about, okay, you need to understand the concepts of moving between points.

196
00:12:38,720 --> 00:12:40,960
Here is where you're currently at in space.

197
00:12:40,960 --> 00:12:43,120
Here is the target you need to get to.

198
00:12:43,120 --> 00:12:46,200
How would you drive the motors to get to that target?

199
00:12:46,200 --> 00:12:48,200
That would be an example of a concept.

200
00:12:48,200 --> 00:12:53,320
Now, for a concept like that, if you're looking at a commercially available industrial robot,

201
00:12:53,320 --> 00:12:56,080
there are very good controllers for that kind of thing already, right?

202
00:12:56,080 --> 00:12:59,880
They've got the inverse kinematics all worked out, and they know how to do it not just

203
00:12:59,880 --> 00:13:04,080
efficiently and effectively, but in a way that's going to maximize the lifetime of that

204
00:13:04,080 --> 00:13:05,080
equipment.

205
00:13:05,080 --> 00:13:08,120
You don't want to unnecessarily drive it too quickly, so you're going to cause it to

206
00:13:08,120 --> 00:13:10,040
fail faster than you'd want.

207
00:13:10,040 --> 00:13:12,680
Real world concerns when you're using these kinds of robots.

208
00:13:12,680 --> 00:13:17,520
But then when you get to a grasping concept, so now we need to actually grasp the item

209
00:13:17,520 --> 00:13:21,160
that we're going to be picking as part of this action.

210
00:13:21,160 --> 00:13:23,000
That is much more complex, right?

211
00:13:23,000 --> 00:13:26,960
And you have to deal with rigid bodies and soft bodies and different packaging materials

212
00:13:26,960 --> 00:13:31,480
and all these different kinds of aspects that come to play, teaching the nuances that go

213
00:13:31,480 --> 00:13:32,800
along with that.

214
00:13:32,800 --> 00:13:36,440
That's where you can start to take your subject matter expertise and really bring it to

215
00:13:36,440 --> 00:13:37,440
bear.

216
00:13:37,440 --> 00:13:42,360
There are people within all of our organizations who already know a lot about these facets

217
00:13:42,360 --> 00:13:44,840
of things, and they're not programmers per se.

218
00:13:44,840 --> 00:13:48,560
They might be a mechanical engineer, they might be a chemical engineer, it depends what

219
00:13:48,560 --> 00:13:50,280
kind of problem you're tackling.

220
00:13:50,280 --> 00:13:54,920
But they know a lot about that area, and so if you asked them, if I asked you to teach

221
00:13:54,920 --> 00:13:58,040
a human to do this, what would you do?

222
00:13:58,040 --> 00:14:01,560
What would you actually, what are the concepts you want them to learn?

223
00:14:01,560 --> 00:14:03,200
How would you show them those things?

224
00:14:03,200 --> 00:14:05,720
How would you test whether they got it right or not?

225
00:14:05,720 --> 00:14:08,840
They usually can tell you actually, because that's their field.

226
00:14:08,840 --> 00:14:12,800
They know that pretty well, and so we just give them a mechanism to capture that.

227
00:14:12,800 --> 00:14:16,920
So in the context of grasping, for example, since we're just walking through one of these

228
00:14:16,920 --> 00:14:21,920
use cases, you might say, well, actually, if I'm teaching a human to grasp something,

229
00:14:21,920 --> 00:14:24,880
I got to rewind a lot until back when they're toddlers, right?

230
00:14:24,880 --> 00:14:26,200
But what do you do?

231
00:14:26,200 --> 00:14:30,040
You use large-scale, gross motor scale objects.

232
00:14:30,040 --> 00:14:32,480
You get them really close to their hands.

233
00:14:32,480 --> 00:14:37,440
You have set their hands on it mostly, and then you let them go through the motions.

234
00:14:37,440 --> 00:14:41,440
It's hard to think back in many of these cases for these very simplistic motions, because

235
00:14:41,440 --> 00:14:45,320
to us, it's simple, but if you watch children doing it, they have to learn, too.

236
00:14:45,320 --> 00:14:46,320
But you break it down.

237
00:14:46,320 --> 00:14:49,040
And you literally break it down into those kinds of things where I'm going to teach you

238
00:14:49,040 --> 00:14:50,040
gross motor skills.

239
00:14:50,040 --> 00:14:55,240
I'm going to teach you in these simulation environments so that you can experiment frequently.

240
00:14:55,240 --> 00:14:58,280
And you learn these concepts related to grasping.

241
00:14:58,280 --> 00:15:00,400
So you learn the concepts related to grasping.

242
00:15:00,400 --> 00:15:04,760
You're leveraging the pre-existing concepts related to moving between points.

243
00:15:04,760 --> 00:15:10,560
You want to teach concepts related to stacking or placing orientation valid grasps so that

244
00:15:10,560 --> 00:15:13,920
you can orient parts in appropriate ways for fastening them, et cetera, et cetera.

245
00:15:13,920 --> 00:15:17,920
All these kinds of things that happen when you're doing real-world industrial robotics,

246
00:15:17,920 --> 00:15:19,800
you can break the problem down.

247
00:15:19,800 --> 00:15:22,480
You break it down into these constituent concepts.

248
00:15:22,480 --> 00:15:24,840
You design a plan for how you want to teach it.

249
00:15:24,840 --> 00:15:29,080
Typically, that will involve some simulation environment in conjunction with some real-world

250
00:15:29,080 --> 00:15:30,840
physical environment.

251
00:15:30,840 --> 00:15:34,920
And then you define what that curriculum looks like to teach it.

252
00:15:34,920 --> 00:15:40,280
And then because you've done it in that way, the system can proceed to try all the various

253
00:15:40,280 --> 00:15:43,280
areas that it can explore to teach how that works.

254
00:15:43,280 --> 00:15:46,320
And mathematically, if we're looking at the low levels on the math, all that's really

255
00:15:46,320 --> 00:15:49,440
happening is you're constraining the state space the system needs to explore.

256
00:15:49,440 --> 00:15:52,080
That's what, in practice, that's what's actually happening.

257
00:15:52,080 --> 00:15:58,120
But it's happening in this more naturally-expressed way that a subject matter expert can readily latch

258
00:15:58,120 --> 00:15:59,880
on to and work well with.

259
00:15:59,880 --> 00:16:03,280
So that's an example in a robotics context.

260
00:16:03,280 --> 00:16:04,280
If you look at examples...

261
00:16:04,280 --> 00:16:06,560
Well, that's not going too far because there's so much...

262
00:16:06,560 --> 00:16:07,560
Yeah.

263
00:16:07,560 --> 00:16:08,560
Sure.

264
00:16:08,560 --> 00:16:09,560
...to unpack.

265
00:16:09,560 --> 00:16:14,920
The first thing that jumped out at me was you kind of describe these two different types

266
00:16:14,920 --> 00:16:19,680
of concepts one that you know a lot about.

267
00:16:19,680 --> 00:16:22,080
And you can help me refine this language.

268
00:16:22,080 --> 00:16:25,280
And another that you need to teach more abstractly.

269
00:16:25,280 --> 00:16:33,040
So for example, in moving, you know, in kind of the macro movement of the robot arm from

270
00:16:33,040 --> 00:16:37,360
point A to point B, you know, it's a well-understood problem, you've got the inverse kinematics

271
00:16:37,360 --> 00:16:38,360
you mentioned.

272
00:16:38,360 --> 00:16:43,360
Yeah, I get the impression that, you know, we've talked a lot in this series about kind

273
00:16:43,360 --> 00:16:50,640
of reinforcement learning from a research and academic perspective and one of the, you

274
00:16:50,640 --> 00:16:57,280
know, the problems are, I think, in that domain not decomposed in this way.

275
00:16:57,280 --> 00:17:01,680
And so I think what I heard you say was, it'd be kind of crazy to, like, throw a bunch

276
00:17:01,680 --> 00:17:06,160
of data and, like, have the robot try to figure out on its own the best way to move from

277
00:17:06,160 --> 00:17:07,160
point A to point B.

278
00:17:07,160 --> 00:17:12,280
And hey, we've already done that years and years and years, and we spent a bunch of

279
00:17:12,280 --> 00:17:14,520
time perfecting the way to do that.

280
00:17:14,520 --> 00:17:20,360
So, you know, part of what I hear you talking about is kind of, is an idea of modularity.

281
00:17:20,360 --> 00:17:21,360
Yes.

282
00:17:21,360 --> 00:17:22,360
I agree.

283
00:17:22,360 --> 00:17:27,360
And these approaches that I'm trying to, like, get at a whole lot of stuff at one point.

284
00:17:27,360 --> 00:17:28,360
Sure.

285
00:17:28,360 --> 00:17:32,960
You know, one thing that I want us to dig into is like, you know, compare contrast, you

286
00:17:32,960 --> 00:17:36,680
know, what you're doing with, you know, the way some of the things we've talked about

287
00:17:36,680 --> 00:17:41,120
on the podcast, kind of, academic approaches to reinforcement learning.

288
00:17:41,120 --> 00:17:43,880
And so one is this idea of modularity.

289
00:17:43,880 --> 00:17:52,800
Another is, you know, maybe kind of elaborating on this idea of constraining the state space

290
00:17:52,800 --> 00:18:00,600
in a way that is easily expressed by humans, like I think constraining the space, the

291
00:18:00,600 --> 00:18:07,480
state space is a huge part of, you know, this process even from an academic perspective,

292
00:18:07,480 --> 00:18:12,760
but my impression, not being an academic and at the start of this field, but my approaches

293
00:18:12,760 --> 00:18:18,120
or my sense is that their approach is constrained in the state space mathematically, right?

294
00:18:18,120 --> 00:18:20,320
As opposed to conceptually.

295
00:18:20,320 --> 00:18:21,640
Is that fair?

296
00:18:21,640 --> 00:18:22,640
That's fair.

297
00:18:22,640 --> 00:18:25,680
At the end of the day, it all does boil down to a mathematical constraint.

298
00:18:25,680 --> 00:18:28,960
It's just how you enable people to express that.

299
00:18:28,960 --> 00:18:34,080
And by virtue of building a system where you're expressing it in this more natural way for

300
00:18:34,080 --> 00:18:39,000
a subject matter expert who's actually working on these problems, you can get at the underlying

301
00:18:39,000 --> 00:18:43,880
math by allowing people to express it in these more natural terms.

302
00:18:43,880 --> 00:18:46,280
Perhaps an analogy here would be, would be useful.

303
00:18:46,280 --> 00:18:49,160
If we look back at old programming systems, right?

304
00:18:49,160 --> 00:18:53,720
So in the, in the late 90s when I was at Microsoft Visual Basic for building desktop apps

305
00:18:53,720 --> 00:18:55,320
was everywhere.

306
00:18:55,320 --> 00:18:57,600
People used that all over the place.

307
00:18:57,600 --> 00:19:02,600
And it was very popular because it enabled people who had subject matter expertise, you

308
00:19:02,600 --> 00:19:07,200
know, I'm running my veterinary clinic, I'm doing whatever it happens to be, to build

309
00:19:07,200 --> 00:19:12,080
the applications they can, they cared about because they didn't worry about comm interop

310
00:19:12,080 --> 00:19:14,560
capabilities and all these, you know, low-level stuff.

311
00:19:14,560 --> 00:19:19,240
They worried about, can I build a form and can I put the right components onto the form

312
00:19:19,240 --> 00:19:23,640
that I care about and tie that back to a database in a way that doesn't require me to go become

313
00:19:23,640 --> 00:19:29,680
an expert in assembly language and low-level binary interface technologies.

314
00:19:29,680 --> 00:19:30,880
This is the same kind of thing.

315
00:19:30,880 --> 00:19:34,320
It's about building the right abstraction at the end of the day.

316
00:19:34,320 --> 00:19:38,920
And so even if what our technology is going to do are the bonsai platform itself is going

317
00:19:38,920 --> 00:19:42,000
to take all this code that you've provided and it's going to compile it.

318
00:19:42,000 --> 00:19:44,560
And yes, at the end of the day, it's a big mathematical constraint.

319
00:19:44,560 --> 00:19:47,440
It's not that fundamentally the technology is different somehow.

320
00:19:47,440 --> 00:19:48,640
It's the same.

321
00:19:48,640 --> 00:19:53,000
It's just that we're allowing people to express it at an appropriate level of abstraction

322
00:19:53,000 --> 00:19:57,640
where it's now framed in the context of the subject matter.

323
00:19:57,640 --> 00:20:00,720
It's framed in the context of the business problem you're trying to solve.

324
00:20:00,720 --> 00:20:04,920
And that's very powerful because it takes it so that your data scientists can still play

325
00:20:04,920 --> 00:20:06,960
the role that's appropriate for them to play.

326
00:20:06,960 --> 00:20:09,360
Your programmers can play the role that's appropriate for them to play and the subject

327
00:20:09,360 --> 00:20:14,520
matter experts can participate and actually teach it the intelligence that you actually

328
00:20:14,520 --> 00:20:15,760
want the system to exhibit.

329
00:20:15,760 --> 00:20:21,720
Typically, what we find in a lot of these environments is if you have true, deep expertise

330
00:20:21,720 --> 00:20:26,720
in machine learning and data science, that is its whole own field.

331
00:20:26,720 --> 00:20:31,080
And the people who have that, if you look at the intersection with the people who have

332
00:20:31,080 --> 00:20:38,760
the expertise in building manufacturing equipment or optimizing supply chain facets rare, right?

333
00:20:38,760 --> 00:20:41,400
It's very rare that they overlap.

334
00:20:41,400 --> 00:20:46,160
So we have to provide, as an industry, we have to provide a way to enable all of these

335
00:20:46,160 --> 00:20:49,360
disparate skill sets to work together.

336
00:20:49,360 --> 00:20:53,040
Then we have to focus on the skill sets that are already within these organizations or

337
00:20:53,040 --> 00:20:55,480
we're never going to solve the real world problems.

338
00:20:55,480 --> 00:21:00,000
And so that's, that ultimately is where we're getting at with this technique.

339
00:21:00,000 --> 00:21:03,960
Yes, it's about decomposing the problems and it's about decomposing the problems in a

340
00:21:03,960 --> 00:21:09,120
way that allows these subject matter experts who know about all the different facets of

341
00:21:09,120 --> 00:21:13,800
the use case they care about to really come in and say, I need to teach you about this.

342
00:21:13,800 --> 00:21:18,800
So if I look at real world examples, we're doing a lot of work with Siemens at the moment

343
00:21:18,800 --> 00:21:21,720
as a, as they're one of our customers.

344
00:21:21,720 --> 00:21:25,400
And if we look at their manufacturing equipment that they come to us and they want to talk

345
00:21:25,400 --> 00:21:31,320
about adding intelligence to, I'm an expert at platforms and artificial intelligence and

346
00:21:31,320 --> 00:21:32,320
all this stuff.

347
00:21:32,320 --> 00:21:34,760
I'm not an expert at CNC milling, right?

348
00:21:34,760 --> 00:21:37,160
That's not my, that's not my area.

349
00:21:37,160 --> 00:21:40,600
When they come to us and they say, well, here are the real world problems we face when

350
00:21:40,600 --> 00:21:46,040
we have these gigantic pieces of manufacturing equipment and they can have an expert get

351
00:21:46,040 --> 00:21:49,640
on the line and they can, that expert once say, well, you're going to need to understand

352
00:21:49,640 --> 00:21:53,400
this facet of friction compensation and so on and so on.

353
00:21:53,400 --> 00:21:57,520
And you know, areas that I know that, nothing about, frankly, as I don't know, personal

354
00:21:57,520 --> 00:22:02,160
level, but they can tell us all sorts of things about that and we can work with them to

355
00:22:02,160 --> 00:22:06,840
say, all right, well, how do you go about this now?

356
00:22:06,840 --> 00:22:11,880
How would we break that down into something we can measure, into a set of concepts the

357
00:22:11,880 --> 00:22:13,480
system can learn?

358
00:22:13,480 --> 00:22:20,560
And it's not about how do we craft a, you know, a pit controller to solve this problem

359
00:22:20,560 --> 00:22:25,560
which would be a traditional way to solve this problem in enterprise context.

360
00:22:25,560 --> 00:22:28,760
It's about how can we tell whether it was correct or not?

361
00:22:28,760 --> 00:22:31,280
So this is where the reinforcement learning part comes in.

362
00:22:31,280 --> 00:22:35,960
You don't have to be able to specify the controller at a mathematical level.

363
00:22:35,960 --> 00:22:40,240
You have to be able to assess whether the behavior was what you wanted and the ability

364
00:22:40,240 --> 00:22:45,520
to break down that behavior into components so that you can assess for each of those

365
00:22:45,520 --> 00:22:50,080
constituent components whether or not that was what you actually wanted.

366
00:22:50,080 --> 00:22:51,560
So elaborate on that.

367
00:22:51,560 --> 00:22:57,320
Do you, I mean, ultimately in these use cases you're trying to, are you trying to create

368
00:22:57,320 --> 00:23:02,600
the controller or are you saying the controller already exists and you're trying to identify

369
00:23:02,600 --> 00:23:06,000
the right parameters for the controller or are you trying to say the controller and the

370
00:23:06,000 --> 00:23:09,560
parameters exist and you're just trying to do some kind of validation?

371
00:23:09,560 --> 00:23:14,920
So all of the above actually, we run it, unfortunately the answer is that we see all of these

372
00:23:14,920 --> 00:23:15,920
scenarios.

373
00:23:15,920 --> 00:23:20,240
So there are cases where what you have is that you have an existing controller and what

374
00:23:20,240 --> 00:23:24,600
you're looking to do is to identify deviations, right?

375
00:23:24,600 --> 00:23:30,160
So you're really trying to figure out when you've deviated and you have some condition

376
00:23:30,160 --> 00:23:33,720
that was not anticipated and you want to be able to deal with it appropriately.

377
00:23:33,720 --> 00:23:35,640
So that's something you definitely run into.

378
00:23:35,640 --> 00:23:40,840
There are areas you run into where you have existing controllers and you want to enhance

379
00:23:40,840 --> 00:23:46,760
the capabilities of those controllers beyond the already well-defined characteristics.

380
00:23:46,760 --> 00:23:48,240
So you run into that as well.

381
00:23:48,240 --> 00:23:52,200
And then there are areas where you people are still operating things by hand.

382
00:23:52,200 --> 00:23:54,920
So it is not uncommon.

383
00:23:54,920 --> 00:23:56,720
We can look at CNC machines, right?

384
00:23:56,720 --> 00:24:02,640
And we were just talking about them to have expert human operators manning these machines

385
00:24:02,640 --> 00:24:09,240
because the value to the business that is using them and creating the part, let's say you're

386
00:24:09,240 --> 00:24:11,560
making a large-scale aircraft part.

387
00:24:11,560 --> 00:24:16,440
That part might be a couple hundred thousand dollars just for that one part.

388
00:24:16,440 --> 00:24:20,160
And it might be a week-long operation to mill that part, right?

389
00:24:20,160 --> 00:24:25,160
You're not going to just turn it over to your G code script and let it run.

390
00:24:25,160 --> 00:24:28,960
And you're going to have someone there to make sure everything's going as you expect.

391
00:24:28,960 --> 00:24:32,720
If there's a mistake on day two, you want to stop it on day two.

392
00:24:32,720 --> 00:24:36,840
So that you're not wasting tons of time and money as you're going through that.

393
00:24:36,840 --> 00:24:42,800
And at the same time, one of the things that I hear over and over again is that they're

394
00:24:42,800 --> 00:24:48,920
from the perspective of trying to apply kind of modern, sane business and engineering

395
00:24:48,920 --> 00:24:55,800
practices to some of these industrial environments, a lot of what this subject matter expertise

396
00:24:55,800 --> 00:25:02,800
is, hey, when I hear this machine kind of sounding an octave or two higher and pitch or something

397
00:25:02,800 --> 00:25:06,800
like that, I know that we're probably going to lose a bit or we're going to probably

398
00:25:06,800 --> 00:25:09,320
damage the part or something like that.

399
00:25:09,320 --> 00:25:13,280
There's a lot of art in addition to the science.

400
00:25:13,280 --> 00:25:16,080
How do you begin to capture all that?

401
00:25:16,080 --> 00:25:18,000
So that's actually an excellent point.

402
00:25:18,000 --> 00:25:24,640
So the beauty of the modern machine learning technology is its ability to detect nuances

403
00:25:24,640 --> 00:25:30,720
there where the human expert, the subject matter expert, can say, yes, I hear it.

404
00:25:30,720 --> 00:25:36,320
And when it sounds off, then I know this is about to happen and you can say, well, what

405
00:25:36,320 --> 00:25:37,320
are you listening for?

406
00:25:37,320 --> 00:25:40,480
And they're not experts in acoustics, they're not going to sit there and tell you, well,

407
00:25:40,480 --> 00:25:42,400
it's exactly this is the kind of sound.

408
00:25:42,400 --> 00:25:46,160
It's more like, I just know what I'm listening for, I've heard it before.

409
00:25:46,160 --> 00:25:51,000
And so the traditional mechanism would be go through label, data set, et cetera.

410
00:25:51,000 --> 00:25:52,720
And you can still do that.

411
00:25:52,720 --> 00:25:57,680
There are techniques you can use in simulation as well to model those environments.

412
00:25:57,680 --> 00:26:01,920
But in practice, the benefit you get from modern machine learning technology versus expert

413
00:26:01,920 --> 00:26:06,960
systems, saying if we go back to the 80s, is this flexibility.

414
00:26:06,960 --> 00:26:12,040
So if I can use another analogy, which might be intuitive to a lot of people, if you play

415
00:26:12,040 --> 00:26:16,880
a sport and you really enjoy playing that sport and you practice and practice and practice

416
00:26:16,880 --> 00:26:20,280
and you get good at it and someone comes to you and they're like, wow, you're really

417
00:26:20,280 --> 00:26:21,280
good.

418
00:26:21,280 --> 00:26:22,280
What is it that you're doing?

419
00:26:22,280 --> 00:26:25,360
I'm missing so that I want to be good at this sport too.

420
00:26:25,360 --> 00:26:28,000
Oftentimes, as an amateur, you don't know.

421
00:26:28,000 --> 00:26:29,440
I just practiced a lot.

422
00:26:29,440 --> 00:26:31,040
I got good at it.

423
00:26:31,040 --> 00:26:36,480
And if you go to a professional coach or a professional athlete, they can tease it apart.

424
00:26:36,480 --> 00:26:41,240
They can say, well, actually, when you're doing this motion, you'll note that you arch

425
00:26:41,240 --> 00:26:44,440
in this way and they can get into all the subtleties and the new ones.

426
00:26:44,440 --> 00:26:46,720
That's why they're a coach or a pro.

427
00:26:46,720 --> 00:26:53,080
And so humans as a learning system, if we look at ourselves as learning systems, we have

428
00:26:53,080 --> 00:27:01,160
this remarkable ability to be able to exhibit intelligent behavior, regardless of our ability

429
00:27:01,160 --> 00:27:04,240
to explain all of it, right?

430
00:27:04,240 --> 00:27:06,760
And modern machine learning systems are like this.

431
00:27:06,760 --> 00:27:12,040
So if you take a deep reinforcement learning neural network kind of approach and you apply

432
00:27:12,040 --> 00:27:15,160
it to a problem and you say, here's the correct behavior.

433
00:27:15,160 --> 00:27:19,480
Let's look at it over and over and over again, whether that's because it's getting acoustic

434
00:27:19,480 --> 00:27:22,920
data and it's listening and you're telling it whether or not the part was about to break

435
00:27:22,920 --> 00:27:26,800
or you observe that the part broke and now it's learning what the subtleties and the acoustics

436
00:27:26,800 --> 00:27:31,400
are so that it can have that same sense that the expert operator did.

437
00:27:31,400 --> 00:27:32,400
It can do that, right?

438
00:27:32,400 --> 00:27:34,680
That's one of the benefits of the technology.

439
00:27:34,680 --> 00:27:40,120
But the more you know about the problem itself, it enables you to decompose it into

440
00:27:40,120 --> 00:27:41,440
those bits.

441
00:27:41,440 --> 00:27:46,720
And so you're not forced, you can always use the technology and get it to the point where

442
00:27:46,720 --> 00:27:51,720
again, you're the amateur athlete and you're just learned because you practice so much.

443
00:27:51,720 --> 00:27:58,160
Or in the industrial case, I have my system and it's actually monitoring the acoustics

444
00:27:58,160 --> 00:28:02,240
off of the equipment and it's learning to detect what it sounds like when a part is about

445
00:28:02,240 --> 00:28:03,400
to break.

446
00:28:03,400 --> 00:28:04,400
It can do that.

447
00:28:04,400 --> 00:28:05,400
That's fantastic.

448
00:28:05,400 --> 00:28:08,600
But then if you have that subject matter expertise and you can really decompose the

449
00:28:08,600 --> 00:28:12,760
problem, you can get a lot of benefits because you can teach faster.

450
00:28:12,760 --> 00:28:17,560
You can now have the predictions that are made explained so that the system can make

451
00:28:17,560 --> 00:28:22,640
more nuanced and more accurate behavioral decisions.

452
00:28:22,640 --> 00:28:28,400
And really getting that subtlety and nuance allows us to build and capture more knowledge

453
00:28:28,400 --> 00:28:30,320
and build more sophisticated systems.

454
00:28:30,320 --> 00:28:33,320
It's kind of like with expert systems, you are totally rigid.

455
00:28:33,320 --> 00:28:34,320
Here are the rules.

456
00:28:34,320 --> 00:28:35,320
Right.

457
00:28:35,320 --> 00:28:40,640
It's all this behavior and powerful in that sense, but very constrained.

458
00:28:40,640 --> 00:28:42,200
It was not very flexible.

459
00:28:42,200 --> 00:28:47,520
And now the pendulum has swung the entire opposite way or all the way at the other end.

460
00:28:47,520 --> 00:28:51,400
And you have your machine learning systems and it's throw lots of data at it and it's

461
00:28:51,400 --> 00:28:54,520
going to learn to predict something and great.

462
00:28:54,520 --> 00:28:57,360
It makes great predictions but no one knows why.

463
00:28:57,360 --> 00:28:58,360
Right.

464
00:28:58,360 --> 00:29:00,720
So before totally explainable, completely inflexible.

465
00:29:00,720 --> 00:29:06,400
Now totally flexible, not explainable and by virtue of using a machine teaching approach

466
00:29:06,400 --> 00:29:09,560
like the one that we've outlined, it's no longer black or white.

467
00:29:09,560 --> 00:29:11,840
You get this nice continuous gray area.

468
00:29:11,840 --> 00:29:15,400
If you don't have a lot of that, that you can provide, the system can still learn and

469
00:29:15,400 --> 00:29:16,600
that's okay.

470
00:29:16,600 --> 00:29:20,440
The more of it you provide, the more explainability you get, the faster it can learn, the more nuanced

471
00:29:20,440 --> 00:29:23,160
you can add to the decisions you're making.

472
00:29:23,160 --> 00:29:24,800
And it just opens that up.

473
00:29:24,800 --> 00:29:31,960
And so it really allows us to tackle these problems at whatever level of subject matter

474
00:29:31,960 --> 00:29:35,520
expertise, explainability is appropriate for what you're trying to do.

475
00:29:35,520 --> 00:29:36,520
Okay.

476
00:29:36,520 --> 00:29:37,520
So what?

477
00:29:37,520 --> 00:29:42,680
I think what I just heard was, well you talked about explicitly the spectrum, but when

478
00:29:42,680 --> 00:29:49,960
a company is using your tools and building a solution based on it, they've got the

479
00:29:49,960 --> 00:29:55,040
ability to, you know, you can start by at the highest level by throwing lots of data at

480
00:29:55,040 --> 00:30:01,920
the problem and not building, you know, building constraints into the system or, you know,

481
00:30:01,920 --> 00:30:06,680
conversely, you know, articulating the concepts, you know, breaking down the concepts that

482
00:30:06,680 --> 00:30:13,560
compose the system, you know, or you can do that to some varying degree of detail.

483
00:30:13,560 --> 00:30:18,160
So it sounds like that's probably one of the kind of architectural design decisions

484
00:30:18,160 --> 00:30:24,160
of someone that's implementing this, like how much decomposition do we need to go to?

485
00:30:24,160 --> 00:30:28,500
And is that what are the factors there, is it primarily performance, is it actually

486
00:30:28,500 --> 00:30:30,320
it tends to be very iterative.

487
00:30:30,320 --> 00:30:36,120
So typically, typically what ends up happening in a real life engagement is they will first

488
00:30:36,120 --> 00:30:41,320
start with the simplest possible model, which is there's only one concept and I just having,

489
00:30:41,320 --> 00:30:44,080
I'm doing the classic reinforcement learning thing.

490
00:30:44,080 --> 00:30:47,360
Here's the environment for you to go explore, go explore it.

491
00:30:47,360 --> 00:30:51,440
And that might be a robotics environment, it might be a supply chain simulation, it

492
00:30:51,440 --> 00:30:57,160
could be any number of things, but just explore and see what you can figure out.

493
00:30:57,160 --> 00:31:03,320
Typically that will learn something, not as much as one would hope, but something.

494
00:31:03,320 --> 00:31:08,360
And then you say, okay, well, let's see what would happen if I taught it about this.

495
00:31:08,360 --> 00:31:13,680
And so you add some conceptual block into the system and you break it down into teaching

496
00:31:13,680 --> 00:31:14,680
that.

497
00:31:14,680 --> 00:31:17,760
And it may or may not help, it's not always a given that it helps.

498
00:31:17,760 --> 00:31:23,400
Often times we'll get feedback from people who are more on the academic mindset.

499
00:31:23,400 --> 00:31:26,760
The whole point of deep learning was to get away from specifying these things, right?

500
00:31:26,760 --> 00:31:29,240
That was the whole point, why are you doing this?

501
00:31:29,240 --> 00:31:32,840
What happens if the person specifies this model and they're wrong, right?

502
00:31:32,840 --> 00:31:37,960
That one of the benefits of deep learning is that it's not reliant on our presupposed

503
00:31:37,960 --> 00:31:40,320
conception of what the model should be.

504
00:31:40,320 --> 00:31:41,520
How do you cope with that?

505
00:31:41,520 --> 00:31:46,240
And for us, it's like, well, that's beautiful, actually, because when you start to break

506
00:31:46,240 --> 00:31:50,720
it down and you decompose the problem and you do it in this iterative way and you see

507
00:31:50,720 --> 00:31:56,080
whether that supports a faster learning, whether it supports better explanations, better

508
00:31:56,080 --> 00:32:00,840
reuse, better generalization, all these different factors you might want to optimize for

509
00:32:00,840 --> 00:32:04,360
and care about, you learn about your own model.

510
00:32:04,360 --> 00:32:10,200
And so if your conceptual model is that friction compensation is super important for this

511
00:32:10,200 --> 00:32:15,600
manufacturing process and you go through the motions and you see that, well, the system's

512
00:32:15,600 --> 00:32:20,120
learning to make predictions and it's compensating just fine.

513
00:32:20,120 --> 00:32:24,440
But all the things I taught it that I thought I knew aren't being used.

514
00:32:24,440 --> 00:32:27,000
And the system can come back and tell you this, it's, well, you taught me this concept

515
00:32:27,000 --> 00:32:29,800
and in fact, I never use what I learned.

516
00:32:29,800 --> 00:32:31,800
I'm always doing something else.

517
00:32:31,800 --> 00:32:32,800
That's instructive.

518
00:32:32,800 --> 00:32:38,440
That tells you, hey, this model I thought made sense, there's something better because

519
00:32:38,440 --> 00:32:42,400
the system has learned the correct behavior and it's not using what I do.

520
00:32:42,400 --> 00:32:45,000
And this happens at all levels of granularity.

521
00:32:45,000 --> 00:32:47,680
So in practice, it's this iterative process.

522
00:32:47,680 --> 00:32:50,120
People start at the very simplistic model.

523
00:32:50,120 --> 00:32:53,480
They start to add more and more of the model that they believe is correct.

524
00:32:53,480 --> 00:32:59,720
It's very rarely a, here is the 120 concept model that I think maps to the problem I'm trying

525
00:32:59,720 --> 00:33:04,880
to solve and I'm going to go build the whole thing in one go and go from there.

526
00:33:04,880 --> 00:33:11,720
It's much more of an iterative refinements and expansion of the model so that you can

527
00:33:11,720 --> 00:33:15,320
have more nuance covered and more subtlety covered and learn about your model in the process

528
00:33:15,320 --> 00:33:16,320
of doing that.

529
00:33:16,320 --> 00:33:17,320
Okay.

530
00:33:17,320 --> 00:33:22,080
And one of the things that you guys talk a lot about is the notion of explainability.

531
00:33:22,080 --> 00:33:23,080
Yes.

532
00:33:23,080 --> 00:33:24,080
You just went into that.

533
00:33:24,080 --> 00:33:29,200
Very important for customers in this space, for a variety of reasons.

534
00:33:29,200 --> 00:33:35,480
I don't think I previously understood that it's this granularity of defining the concepts

535
00:33:35,480 --> 00:33:37,640
that really gets you the explainability.

536
00:33:37,640 --> 00:33:38,640
Yeah.

537
00:33:38,640 --> 00:33:39,640
Absolutely.

538
00:33:39,640 --> 00:33:40,640
Can you elaborate on that?

539
00:33:40,640 --> 00:33:41,640
Yeah, absolutely.

540
00:33:41,640 --> 00:33:46,200
So there are a lot of techniques that have been published about how you start to peer

541
00:33:46,200 --> 00:33:50,920
into the neural networks and try to tease out what is actually going on.

542
00:33:50,920 --> 00:33:52,520
You got lime and a lime and all.

543
00:33:52,520 --> 00:33:54,760
There's tons of these things.

544
00:33:54,760 --> 00:33:55,800
Our approach is different.

545
00:33:55,800 --> 00:34:01,040
As you said, because it's not magic in our system, it's not like we're going to have

546
00:34:01,040 --> 00:34:07,320
some amazing new way to peer into the neural network and tell you what these group of neurons

547
00:34:07,320 --> 00:34:08,320
meant.

548
00:34:08,320 --> 00:34:09,320
Right.

549
00:34:09,320 --> 00:34:10,320
That's not what we do.

550
00:34:10,320 --> 00:34:14,720
What we do is we say, we're going to let you break it up in this way.

551
00:34:14,720 --> 00:34:18,400
And it's always easiest to frame these things in real world scenarios.

552
00:34:18,400 --> 00:34:23,360
So let's say you are building a supply chain logistics system.

553
00:34:23,360 --> 00:34:26,760
You have got your real world data.

554
00:34:26,760 --> 00:34:28,920
So you have all the telemetry coming from that.

555
00:34:28,920 --> 00:34:32,520
You have simulation models that you've built in some discrete event simulator or something

556
00:34:32,520 --> 00:34:33,520
like that.

557
00:34:33,520 --> 00:34:36,640
So you've got all these different facets you can pull on.

558
00:34:36,640 --> 00:34:43,320
And the model that you use takes into account very coarse things like the weather and seasonality

559
00:34:43,320 --> 00:34:47,400
of goods and perishability of goods and not all these kinds of things that you might care

560
00:34:47,400 --> 00:34:48,400
about.

561
00:34:48,400 --> 00:34:52,920
And you might have more fine grain concepts that you're teaching at about the composition

562
00:34:52,920 --> 00:34:58,280
of your fleet and the land routes that are viable for you to follow and so like all these

563
00:34:58,280 --> 00:34:59,280
kinds of things.

564
00:34:59,280 --> 00:35:02,840
So you know, you can teach all of this stuff.

565
00:35:02,840 --> 00:35:07,880
What's important to emphasize here is that it is very rare for a company that we work

566
00:35:07,880 --> 00:35:11,320
with to come back and say, and now we're turning it all over to this automated system we

567
00:35:11,320 --> 00:35:12,320
trained.

568
00:35:12,320 --> 00:35:13,320
Go.

569
00:35:13,320 --> 00:35:14,320
That doesn't like me.

570
00:35:14,320 --> 00:35:15,320
Yeah.

571
00:35:15,320 --> 00:35:18,040
That's still there's not a level of comfort there yet.

572
00:35:18,040 --> 00:35:21,600
What happens is there's still a human and the human analyst is sitting there and making

573
00:35:21,600 --> 00:35:25,360
the ultimate decision and they're using the system to provide decision support.

574
00:35:25,360 --> 00:35:26,360
Yeah.

575
00:35:26,360 --> 00:35:29,920
And in that context, if you've built, I don't care how sophisticated it is.

576
00:35:29,920 --> 00:35:35,120
If you've built a very elaborate neural network or maybe you used some other machine learning

577
00:35:35,120 --> 00:35:39,600
or AI technique to build a system and it comes back and it says, I think you should have

578
00:35:39,600 --> 00:35:45,640
truck 17, which is currently in Hoboken, depart now.

579
00:35:45,640 --> 00:35:46,640
Go.

580
00:35:46,640 --> 00:35:49,600
I thought what I think you should do, the analyst is going to sit there and look at it

581
00:35:49,600 --> 00:35:52,320
and say, why?

582
00:35:52,320 --> 00:35:56,040
Okay, that's nice that you tell me that.

583
00:35:56,040 --> 00:36:01,960
And I understand that you have this visibility on massive parametric space that I'm not

584
00:36:01,960 --> 00:36:06,920
perhaps aware of as a human, but I'm still not comfortable with the fact that I have

585
00:36:06,920 --> 00:36:09,200
no context into why you're telling me to do this.

586
00:36:09,200 --> 00:36:15,440
That truck is only two thirds full and the next truck that can hit that location is

587
00:36:15,440 --> 00:36:17,600
not available for two days.

588
00:36:17,600 --> 00:36:20,360
Okay, I don't think that's a good call.

589
00:36:20,360 --> 00:36:25,720
Whereas if the decision support system comes back and says, I think you should have this

590
00:36:25,720 --> 00:36:27,720
truck leave now.

591
00:36:27,720 --> 00:36:30,120
And I think you should have it leave now because.

592
00:36:30,120 --> 00:36:33,920
And then it frames that prediction in the context, again, not magic, but in the context

593
00:36:33,920 --> 00:36:38,560
of all these concepts you've defined, that was your own model.

594
00:36:38,560 --> 00:36:42,160
So you as that analyst who is sitting there and presumably is either very familiar with

595
00:36:42,160 --> 00:36:46,440
that model or part of defining it, are going to look at that and have some rich context

596
00:36:46,440 --> 00:36:48,400
to say, oh, right.

597
00:36:48,400 --> 00:36:54,800
So we taught it about these overland routes and it sees that there is a storm coming and

598
00:36:54,800 --> 00:36:57,280
those routes are going to be cut off.

599
00:36:57,280 --> 00:37:01,080
And that's why it's telling me to do it now, even though there's not another truck for

600
00:37:01,080 --> 00:37:05,920
two days, whatever it happens to be, but it can frame it in that context.

601
00:37:05,920 --> 00:37:11,000
That gives you much more power as an analyst to make that decision in confidence in the

602
00:37:11,000 --> 00:37:12,000
results.

603
00:37:12,000 --> 00:37:13,560
You get your audit trail.

604
00:37:13,560 --> 00:37:14,560
This is why.

605
00:37:14,560 --> 00:37:15,800
This is why this happened.

606
00:37:15,800 --> 00:37:23,880
So even if those models are not ground truth, this is not exactly the state of the world,

607
00:37:23,880 --> 00:37:27,560
but it is the model that you use to run your business.

608
00:37:27,560 --> 00:37:32,840
It is the model you use to drive your robotic systems or whatever it happens to be.

609
00:37:32,840 --> 00:37:37,680
You want to have that level of explainability and that's what you currently use to frame it.

610
00:37:37,680 --> 00:37:42,800
And so we're taking a lot of the magic out of the AI, but giving you the flexibility

611
00:37:42,800 --> 00:37:47,560
that you could have a note in there where the system comes back and it says, I think

612
00:37:47,560 --> 00:37:49,560
you should have the truck to part now.

613
00:37:49,560 --> 00:37:52,920
And everything in the model, I'm not using any of that.

614
00:37:52,920 --> 00:37:57,160
If you just use that model, the model would not say, make the truck to part now, but I've

615
00:37:57,160 --> 00:38:01,280
also learned by virtue of looking at all the real world telemetry and that I believe

616
00:38:01,280 --> 00:38:05,480
making that prediction now is a good prediction.

617
00:38:05,480 --> 00:38:08,920
Then the analyst is going to look at it and say, well, I'm not really comfortable doing

618
00:38:08,920 --> 00:38:12,400
that, but let's look at what happens.

619
00:38:12,400 --> 00:38:16,360
Let's see if that was, in fact, a good prediction that comes out and that tells me I need to

620
00:38:16,360 --> 00:38:18,520
go back and enhance my model.

621
00:38:18,520 --> 00:38:23,320
My model is now deficient in some way and then you can iterate and you can keep working

622
00:38:23,320 --> 00:38:24,320
on the system.

623
00:38:24,320 --> 00:38:27,640
Specifically deficient in its expression of these concepts, right?

624
00:38:27,640 --> 00:38:29,640
It has a blind spot.

625
00:38:29,640 --> 00:38:33,520
Actually, blind spot isn't really the right way to say because the model doesn't have the

626
00:38:33,520 --> 00:38:34,520
blind spot.

627
00:38:34,520 --> 00:38:36,680
It's the lack of decomposition.

628
00:38:36,680 --> 00:38:38,680
Yeah, you can't explain it.

629
00:38:38,680 --> 00:38:41,920
It's like going to the human expert and saying, why?

630
00:38:41,920 --> 00:38:46,880
Having them say, I don't know, this is the way I do it and it works, right?

631
00:38:46,880 --> 00:38:51,440
Humans have this interesting, I'm a student of human nature as well, and humans have this

632
00:38:51,440 --> 00:38:57,040
interesting facet where we conflate the ability to explain something with the ability to justify

633
00:38:57,040 --> 00:38:59,040
something.

634
00:38:59,040 --> 00:39:03,320
It's important as you look at these systems and say, well, are you actually explaining

635
00:39:03,320 --> 00:39:08,280
why you chose to make this decision and that's really what happened or are you just looking

636
00:39:08,280 --> 00:39:10,960
at what happened and now you're justifying it?

637
00:39:10,960 --> 00:39:12,400
A lot of times it's the latter.

638
00:39:12,400 --> 00:39:14,040
It's not the former.

639
00:39:14,040 --> 00:39:17,040
That's human nature and that's just the way we are.

640
00:39:17,040 --> 00:39:22,160
But when it comes to industrial AI and really applying this technology, you really want it

641
00:39:22,160 --> 00:39:23,840
to be the former.

642
00:39:23,840 --> 00:39:29,720
In certain circumstances, you want to go back later and leverage the human strength, which

643
00:39:29,720 --> 00:39:35,120
is to say, system predicted I should do something that was out of the bounds of my model.

644
00:39:35,120 --> 00:39:38,240
There's a gap, assuming that it was the correct behavior.

645
00:39:38,240 --> 00:39:44,040
Then there's a gap, how do I try to fill that gap and then your ability to justify and

646
00:39:44,040 --> 00:39:46,520
come up with creative explanations for what that might be.

647
00:39:46,520 --> 00:39:50,080
Their ability to have your data scientists dive in and really tease apart what happened

648
00:39:50,080 --> 00:39:53,880
and try to refine that model becomes very powerful.

649
00:39:53,880 --> 00:39:55,560
It is a very fluid process.

650
00:39:55,560 --> 00:39:59,640
It is not a right once run forever proposition.

651
00:39:59,640 --> 00:40:07,360
It is a right many times, continually refine and learn as you go along and get a greater

652
00:40:07,360 --> 00:40:12,200
and greater and greater ability to explain what is actually happening as you go through

653
00:40:12,200 --> 00:40:13,200
that process.

654
00:40:13,200 --> 00:40:15,360
That's the nature of the beast.

655
00:40:15,360 --> 00:40:20,920
One of the things I found in my research and articulated in the industrial AI paper

656
00:40:20,920 --> 00:40:28,600
was an emerging maturity model in the way people are looking at deploying AI and in the enterprise

657
00:40:28,600 --> 00:40:33,520
generally, I think, but particularly in these industrial types of situations where it's

658
00:40:33,520 --> 00:40:39,680
just as you describe, there's this fundamental issue of trust.

659
00:40:39,680 --> 00:40:46,560
That trust is, I think, multifaceted part of it is repeatability part of it is explainability.

660
00:40:46,560 --> 00:40:48,880
There are a bunch of other factors.

661
00:40:48,880 --> 00:40:53,920
As people are gaining this trust, the first thing that they want to do is point these systems

662
00:40:53,920 --> 00:41:00,680
at some process and some system and just help them monitor it and tell them new things

663
00:41:00,680 --> 00:41:06,800
about it and surface new insights.

664
00:41:06,800 --> 00:41:12,400
As they gain some trust that, hey, it's actually providing me interesting information, maybe

665
00:41:12,400 --> 00:41:14,600
it should tell me what to do.

666
00:41:14,600 --> 00:41:21,480
Then they flip the switch or allow it to optimize and it becomes a decision support system

667
00:41:21,480 --> 00:41:23,960
in the way you described it.

668
00:41:23,960 --> 00:41:29,680
There's this further stage which is actually the next switch which is just do it.

669
00:41:29,680 --> 00:41:30,680
Think it so.

670
00:41:30,680 --> 00:41:38,920
I'm wondering if you see that same progression among your customers and what are some of

671
00:41:38,920 --> 00:41:43,360
the other factors that compose, what are some of the most important factors that compose

672
00:41:43,360 --> 00:41:46,120
trust beyond explainability?

673
00:41:46,120 --> 00:41:49,600
We definitely are starting to see some of that, but I would emphasize the starting two

674
00:41:49,600 --> 00:41:50,600
part.

675
00:41:50,600 --> 00:41:57,120
You yourself described there's an emerging capability maturity model that people are using as a factor

676
00:41:57,120 --> 00:41:58,120
here.

677
00:41:58,120 --> 00:41:59,480
I would wholeheartedly agree with that.

678
00:41:59,480 --> 00:42:05,720
The state of the market right now really is, it's like it's 1995 all over again.

679
00:42:05,720 --> 00:42:08,080
Hey, there's this cool visual basic thing.

680
00:42:08,080 --> 00:42:09,080
Let's go try it out.

681
00:42:09,080 --> 00:42:13,840
In this case, it's like there's this internet thing and everyone's like, wow, I really

682
00:42:13,840 --> 00:42:18,440
need to do something about this internet thing, but a lot of people, the maturity of people's

683
00:42:18,440 --> 00:42:20,560
ability to do that is all over the map, right?

684
00:42:20,560 --> 00:42:24,520
People don't even know where to start, they know it's important and you have true, deep

685
00:42:24,520 --> 00:42:29,440
experts who are doing it and so we see all of that and as you engage with customers and

686
00:42:29,440 --> 00:42:35,200
they're looking at the maturity of their trust in these systems and trust in their own models

687
00:42:35,200 --> 00:42:38,720
and own systems, they built so that they feel more comfortable turning it over.

688
00:42:38,720 --> 00:42:40,080
We definitely see that.

689
00:42:40,080 --> 00:42:44,440
You can look at, as an example, autonomous vehicles, so we talked to, of course, lots of people

690
00:42:44,440 --> 00:42:45,440
about autonomous vehicles.

691
00:42:45,440 --> 00:42:52,800
It's a hot area and there was a period of time not long ago where no trust whatsoever

692
00:42:52,800 --> 00:43:00,440
in having the systems make control decisions, using the technology for perception and identifying

693
00:43:00,440 --> 00:43:04,520
whether there's a bicycle on the road or a cow or something like that, okay, we're comfortable

694
00:43:04,520 --> 00:43:10,760
at that level right now, not steering the vehicle, like no, that's not yet.

695
00:43:10,760 --> 00:43:14,440
And you see some of those organizations now getting more and more comfortable with them.

696
00:43:14,440 --> 00:43:20,680
But really, it boils down to how well-baked is the systems that you've built.

697
00:43:20,680 --> 00:43:26,720
They can be at the point where it is just, you still want a human in the loop, it will

698
00:43:26,720 --> 00:43:30,920
get to the point where you want a human to validate what's coming out.

699
00:43:30,920 --> 00:43:35,400
And then yes, you will get to the point where frankly, you will have the system make decisions

700
00:43:35,400 --> 00:43:39,760
in an automated way and you do that because there's now sufficient trust and confidence

701
00:43:39,760 --> 00:43:44,080
in the system that it's going to do the right thing that doing the wrong thing is now

702
00:43:44,080 --> 00:43:48,760
an exceptional activity and they'll still make some mistakes, but it's rare and when

703
00:43:48,760 --> 00:43:52,280
it does make a mistake, you can capture that and you can use that to further refine the

704
00:43:52,280 --> 00:43:53,280
system.

705
00:43:53,280 --> 00:43:54,720
So yeah, I think that's a natural progression.

706
00:43:54,720 --> 00:44:00,520
We are starting to see that, but frankly, at this point in time, the market is really

707
00:44:00,520 --> 00:44:01,520
all over the map.

708
00:44:01,520 --> 00:44:07,320
And so tons of experts in a room, we will run into that with some frequency and you get

709
00:44:07,320 --> 00:44:10,960
into the occasion where people are just dipping their toes in the water and everywhere

710
00:44:10,960 --> 00:44:11,960
in between.

711
00:44:11,960 --> 00:44:16,200
So it kind of depends on the particulars of the customer and how forward-looking they

712
00:44:16,200 --> 00:44:21,920
are and how much resource they have to allocate towards exploring various facets of what

713
00:44:21,920 --> 00:44:23,080
they can do.

714
00:44:23,080 --> 00:44:29,200
But by and for the large part right now, there's a lot of desire to have explainability to

715
00:44:29,200 --> 00:44:33,840
have that audit trail, if you will, so that people can go back and test things.

716
00:44:33,840 --> 00:44:38,280
The more you get to automation towards the automation end of the spectrum, the more people

717
00:44:38,280 --> 00:44:43,600
want to be able to look not purely at the audit trail, but in generalization.

718
00:44:43,600 --> 00:44:47,920
So if you look at control systems for robotics, which we're talking about at the beginning,

719
00:44:47,920 --> 00:44:55,400
am I teaching the right concepts such that it will generalize the behavior in many scenarios,

720
00:44:55,400 --> 00:44:57,960
not just the one I'm teaching it about.

721
00:44:57,960 --> 00:45:02,520
So there tend to be more towards that end of the spectrum of trust and automation.

722
00:45:02,520 --> 00:45:04,680
So that's how they can perform their tests.

723
00:45:04,680 --> 00:45:10,680
I didn't just learn how to grasp in a way that is appropriate for this one task I'm trying

724
00:45:10,680 --> 00:45:11,680
to do.

725
00:45:11,680 --> 00:45:18,440
So did I learn grasping in a generalized way such that if I present a variety of objects

726
00:45:18,440 --> 00:45:21,840
and a variety of scenarios, it will still do the right thing.

727
00:45:21,840 --> 00:45:28,160
And consequently, you see everything, you see, you see everything on the spectrum at this

728
00:45:28,160 --> 00:45:29,960
point in time.

729
00:45:29,960 --> 00:45:34,880
Just to follow up on that last comment you made about the generalization, I imagine that's

730
00:45:34,880 --> 00:45:37,400
got to be driven by a business driver.

731
00:45:37,400 --> 00:45:41,480
You don't want to have an overly generalized system because that's going to be more expensive

732
00:45:41,480 --> 00:45:44,280
than what you need to solve your problem.

733
00:45:44,280 --> 00:45:45,280
100% agree.

734
00:45:45,280 --> 00:45:46,280
Yes.

735
00:45:46,280 --> 00:45:47,280
Yes.

736
00:45:47,280 --> 00:45:50,080
Do you find, I don't know what the question is.

737
00:45:50,080 --> 00:45:53,560
No, you're totally accurate though.

738
00:45:53,560 --> 00:45:55,960
If you have, let's just keep it in the same vein.

739
00:45:55,960 --> 00:45:57,640
It's easy to continue with that example.

740
00:45:57,640 --> 00:45:58,640
So robotics, okay.

741
00:45:58,640 --> 00:46:00,600
So I have my robotic system.

742
00:46:00,600 --> 00:46:02,800
What I have it doing, it's been retooled.

743
00:46:02,800 --> 00:46:05,480
We are part of a chain of this manufacturing chain.

744
00:46:05,480 --> 00:46:07,400
We just recently retooled for this chain.

745
00:46:07,400 --> 00:46:09,880
I really care about this one operation.

746
00:46:09,880 --> 00:46:14,120
I need to attach these two parts of whatever we're building.

747
00:46:14,120 --> 00:46:18,720
That means that the grasp has to be in a certain orientation because it's only viable to

748
00:46:18,720 --> 00:46:19,720
connect the two things.

749
00:46:19,720 --> 00:46:24,520
If it's I'm not covering up the part that needs to be combined and so on, all these kinds

750
00:46:24,520 --> 00:46:25,520
of things.

751
00:46:25,520 --> 00:46:27,280
And that's what they care about.

752
00:46:27,280 --> 00:46:30,560
They're really focused on that because that's what they're manufacturing right now and

753
00:46:30,560 --> 00:46:34,680
that's what's economically effective and efficient and capable.

754
00:46:34,680 --> 00:46:40,400
And if you look instead for the person who manufactures that piece of robotics equipment.

755
00:46:40,400 --> 00:46:47,040
So I have, I may be B or I'm, you know, you pick your robotic manufacturer of choice.

756
00:46:47,040 --> 00:46:48,840
I make robotic armatures.

757
00:46:48,840 --> 00:46:55,000
I care a lot about the generalization because for me, having it work with my customers,

758
00:46:55,000 --> 00:46:58,480
I have a variety of contexts helps them go faster.

759
00:46:58,480 --> 00:47:00,920
And so that's kind of the breakdown we tend to see now.

760
00:47:00,920 --> 00:47:07,920
And then I think I imagine it also goes back to this idea of customer and market maturity,

761
00:47:07,920 --> 00:47:08,920
right?

762
00:47:08,920 --> 00:47:12,920
So, you know, my first few projects are going to be like proofs of concept and things

763
00:47:12,920 --> 00:47:13,920
like that.

764
00:47:13,920 --> 00:47:17,600
And I'm trying to solve this thing quickly and see if this is a viable technology.

765
00:47:17,600 --> 00:47:21,720
But at some point, you know, project three or four or something like that, I want it.

766
00:47:21,720 --> 00:47:25,640
I really want to understand that, you know, if I'm going to invest in an approach or a

767
00:47:25,640 --> 00:47:32,240
platform that it can solve a broad swath of problems because there's real cost to that

768
00:47:32,240 --> 00:47:33,240
investment.

769
00:47:33,240 --> 00:47:34,240
Totally.

770
00:47:34,240 --> 00:47:36,080
And that's exactly what we see.

771
00:47:36,080 --> 00:47:42,320
So it is a typical engagement for us at the moment would be to have a, we have an early

772
00:47:42,320 --> 00:47:45,720
access program that we're actively working right now.

773
00:47:45,720 --> 00:47:49,320
And customers coming into that almost always want to run a proof of concept, as you said.

774
00:47:49,320 --> 00:47:53,520
As a first stab, we want to try to make it as aligned with the ultimate production

775
00:47:53,520 --> 00:48:00,320
use case as appropriate, but also appropriate duration and so on so that it's a, the spend

776
00:48:00,320 --> 00:48:06,600
of resource and people's time and money is aligned with pinning down exactly what you

777
00:48:06,600 --> 00:48:07,600
said.

778
00:48:07,600 --> 00:48:10,200
Is this technology something that makes sense for what we're trying to do?

779
00:48:10,200 --> 00:48:13,840
And as people get that level of comfort, then yes, then you get more into the, okay,

780
00:48:13,840 --> 00:48:17,680
now let's expand the scope and we're doing a much broader use of this technology and

781
00:48:17,680 --> 00:48:20,000
the production environment and so on.

782
00:48:20,000 --> 00:48:22,320
That is a progression that we see time and time again.

783
00:48:22,320 --> 00:48:23,800
And I think that's true.

784
00:48:23,800 --> 00:48:24,800
That's not just true about us.

785
00:48:24,800 --> 00:48:30,520
I think that's true about this technology in these industrial contexts in general.

786
00:48:30,520 --> 00:48:37,440
We talked a little bit about reinforcement learning and, you know, in some of the examples

787
00:48:37,440 --> 00:48:43,840
you gave, you start high level problem, you decompose it into concepts, you know, you

788
00:48:43,840 --> 00:48:47,480
know, if the concept is like a leaf node, you know, somewhere under there, you're doing

789
00:48:47,480 --> 00:48:50,120
reinforcement learning to kind of figure out that leaf node.

790
00:48:50,120 --> 00:48:55,560
If you don't already have a well understood model like inverse kinematics, help us understand

791
00:48:55,560 --> 00:49:03,680
the relationship between what you guys are doing and the underlying reinforcement learning

792
00:49:03,680 --> 00:49:04,680
stuff.

793
00:49:04,680 --> 00:49:09,880
Like are you, how are you, how are you architecting the, sure, the networks?

794
00:49:09,880 --> 00:49:12,800
How are you, you know, training them?

795
00:49:12,800 --> 00:49:18,120
Are there, you know, any particularly interesting things that you're doing to, you know, ensure

796
00:49:18,120 --> 00:49:22,920
that they're quickly trainable, is there any academic research that you have based your

797
00:49:22,920 --> 00:49:23,920
approaches on?

798
00:49:23,920 --> 00:49:26,400
Like what are the things that you think about in that interface?

799
00:49:26,400 --> 00:49:27,400
Sure, sure.

800
00:49:27,400 --> 00:49:30,240
That's a, that's a, there's a lot of depth that we can get into there.

801
00:49:30,240 --> 00:49:34,320
So let me, let me start at a high level so we can frame everything and then we'll, I'm

802
00:49:34,320 --> 00:49:35,320
happy to push down.

803
00:49:35,320 --> 00:49:40,840
So at a high level, the, for bonsai's platform in particular, the best way to think about

804
00:49:40,840 --> 00:49:43,400
it is in relationship to a database.

805
00:49:43,400 --> 00:49:48,240
So when you're building industrial or commercial enterprise application X for your company,

806
00:49:48,240 --> 00:49:52,600
whatever happens to be, and it's going to work with data, you're going to use a database

807
00:49:52,600 --> 00:49:53,600
almost certainly.

808
00:49:53,600 --> 00:49:55,920
That's a very common thing to do.

809
00:49:55,920 --> 00:49:57,880
And what is the database providing for you?

810
00:49:57,880 --> 00:50:03,240
It's providing that level of abstraction so that you are not focusing on how this data

811
00:50:03,240 --> 00:50:06,560
is split across disks in the cluster.

812
00:50:06,560 --> 00:50:11,320
And when you rebalance tree structures for searching and all this kind of like that, all

813
00:50:11,320 --> 00:50:13,480
that stuff's the database deals with for you.

814
00:50:13,480 --> 00:50:17,920
And it gives you this nice abstraction so you specify the structure of your data and

815
00:50:17,920 --> 00:50:21,800
the kinds of questions you want to be able to ask of it and it can take care of the rest.

816
00:50:21,800 --> 00:50:24,120
Our platform is very analogous to that, very similar.

817
00:50:24,120 --> 00:50:27,640
Now, of course, our abstraction is around this machine teaching stuff and so on.

818
00:50:27,640 --> 00:50:31,080
But in principle, when you're working with these simulation environments, when you're

819
00:50:31,080 --> 00:50:36,480
working with real world equipment and telemetry, you are nonetheless interfacing with all

820
00:50:36,480 --> 00:50:38,560
of those systems and you have to manage them.

821
00:50:38,560 --> 00:50:43,360
So if you're building a system and you're going to go through the actual training motions

822
00:50:43,360 --> 00:50:49,120
and let's say it's a supply chain logistics system and you have a discrete event simulation

823
00:50:49,120 --> 00:50:52,920
model of that system and you want to train primarily on that before you bridge to the

824
00:50:52,920 --> 00:50:58,240
real world telemetry data just so you get quicker learning and more repeatable learning.

825
00:50:58,240 --> 00:51:01,200
Then you have to manage those simulations.

826
00:51:01,200 --> 00:51:04,680
Those simulations, depending on what you're doing, those can run very quickly.

827
00:51:04,680 --> 00:51:08,120
In some cases, if you're doing a computational fluid dynamics simulation, that can run for

828
00:51:08,120 --> 00:51:12,200
hours or days and so managing all of that becomes something that matters.

829
00:51:12,200 --> 00:51:15,800
And so in the same way that you don't worry about data spanning different disks in your

830
00:51:15,800 --> 00:51:20,320
cluster on your database, we don't want you to worry about am I running 10 copies of my

831
00:51:20,320 --> 00:51:23,280
simulation environment and where am I running them?

832
00:51:23,280 --> 00:51:28,440
How do I reconfigure them between runs so that I'm maximizing the efficiency of the learning?

833
00:51:28,440 --> 00:51:31,640
All of that is the kind of stuff that our platform manages for you.

834
00:51:31,640 --> 00:51:36,280
And so in that sense, there's a lot of low-level plumbing infrastructure stuff that is really

835
00:51:36,280 --> 00:51:40,360
valuable because you don't have to worry about it and it manages that for you.

836
00:51:40,360 --> 00:51:45,440
But then when you dig down, okay, so that's the high level, now drill down a level.

837
00:51:45,440 --> 00:51:46,440
Great.

838
00:51:46,440 --> 00:51:47,440
I've provided this.

839
00:51:47,440 --> 00:51:52,520
But that's interesting stuff because at some point, someone's got to set up models and

840
00:51:52,520 --> 00:51:58,000
actually train them and have that stuff all automated out of the box is a lot of work

841
00:51:58,000 --> 00:52:00,000
that someone doesn't have to figure out how to do.

842
00:52:00,000 --> 00:52:04,320
Yeah, and in fact, that's kind of the state of the art for a lot of this work.

843
00:52:04,320 --> 00:52:08,360
If you look at the academic literature and you look at deep reinforcement learning algorithms

844
00:52:08,360 --> 00:52:14,080
in particular, you'll find DDPG networks and TRPO networks and questions around whether

845
00:52:14,080 --> 00:52:16,600
you should have learner memories or not.

846
00:52:16,600 --> 00:52:20,680
And is it on policy or off-policy method because if it, depending on which way it is, you

847
00:52:20,680 --> 00:52:26,920
might have to throw out historical data you've cashed, given what you're learning next.

848
00:52:26,920 --> 00:52:33,520
And this level of detail is great if you are focused on the mechanics of the learning.

849
00:52:33,520 --> 00:52:36,760
And from our perspective, it's the kind of thing you shouldn't have to worry about if

850
00:52:36,760 --> 00:52:41,960
you want to focus on what you're teaching and how the system should be intelligent at

851
00:52:41,960 --> 00:52:44,640
the end of the day, what the subject matter expertise is.

852
00:52:44,640 --> 00:52:47,760
I would rather abstract all that and manage it for you.

853
00:52:47,760 --> 00:52:52,520
And so by, you know, abstracting and managing it for you, you know, when you kind of punch

854
00:52:52,520 --> 00:52:56,280
into the details, can mean a bunch of things, it can mean, you know, actually we know about

855
00:52:56,280 --> 00:53:00,520
all this stuff and we know how to figure out which is the best thing, you know, and we

856
00:53:00,520 --> 00:53:01,520
do that.

857
00:53:01,520 --> 00:53:07,000
But what we really only do this one thing and you only have one choice and thus we've

858
00:53:07,000 --> 00:53:08,000
managed it for you.

859
00:53:08,000 --> 00:53:09,000
Sure.

860
00:53:09,000 --> 00:53:10,000
Sure.

861
00:53:10,000 --> 00:53:11,000
Yes.

862
00:53:11,000 --> 00:53:12,000
So it's more the former than the latter.

863
00:53:12,000 --> 00:53:13,000
Of course.

864
00:53:13,000 --> 00:53:14,000
As you'd expect.

865
00:53:14,000 --> 00:53:15,000
As you'd expect.

866
00:53:15,000 --> 00:53:16,000
You never know.

867
00:53:16,000 --> 00:53:17,000
No, it's a totally fair question.

868
00:53:17,000 --> 00:53:18,000
So what does that mean in practice?

869
00:53:18,000 --> 00:53:20,800
And let's use analogies again because it makes it easier to recognize.

870
00:53:20,800 --> 00:53:23,040
Let's say you're teaching the system something silly.

871
00:53:23,040 --> 00:53:24,560
I want it to learn how to play tic-tac-toe.

872
00:53:24,560 --> 00:53:29,080
I picked tic-tac-toe in particular because everyone's played it and everyone understands

873
00:53:29,080 --> 00:53:33,560
if you're past a certain age that it's a pointless game because you'll never win, etc.

874
00:53:33,560 --> 00:53:34,560
Right.

875
00:53:34,560 --> 00:53:37,160
The state space is not huge for tic-tac-toe.

876
00:53:37,160 --> 00:53:43,320
There's a funny xkcd comic actually where he literally maps out the entire state space

877
00:53:43,320 --> 00:53:44,320
in the comic.

878
00:53:44,320 --> 00:53:47,560
Like here is every position you can possibly enter and what the correct move is once you're

879
00:53:47,560 --> 00:53:48,560
in that.

880
00:53:48,560 --> 00:53:49,560
Like you can never, right?

881
00:53:49,560 --> 00:53:50,560
And you just do it.

882
00:53:50,560 --> 00:53:51,560
It's small enough of a state space.

883
00:53:51,560 --> 00:53:52,560
You can do that.

884
00:53:52,560 --> 00:53:53,560
We'll link to it in the show notes.

885
00:53:53,560 --> 00:53:54,560
Yeah.

886
00:53:54,560 --> 00:53:55,560
Awesome.

887
00:53:55,560 --> 00:53:56,560
Perfect.

888
00:53:56,560 --> 00:53:57,560
I can give you that link.

889
00:53:57,560 --> 00:53:58,560
That's easy.

890
00:53:58,560 --> 00:54:02,680
If you were teaching that and you did it in our system, part of what the system does when

891
00:54:02,680 --> 00:54:07,920
it's compiling and it's generating the appropriate underlying networks on your behalf is it will

892
00:54:07,920 --> 00:54:10,560
just run the simulation for a while.

893
00:54:10,560 --> 00:54:12,400
And it will collect statistics on what it sees.

894
00:54:12,400 --> 00:54:16,720
And if it's running tic-tac-toe, after it runs it a little bit, and this is not a take

895
00:54:16,720 --> 00:54:20,600
a lot of wall clock time, it's just got to run a lot of iterations of the, in this case,

896
00:54:20,600 --> 00:54:23,240
it's a game, but it runs a lot of iterations of it.

897
00:54:23,240 --> 00:54:25,960
It will see very quickly the state space isn't large.

898
00:54:25,960 --> 00:54:29,760
And consequently, maybe using a q-table is a perfectly reasonable algorithmic approach

899
00:54:29,760 --> 00:54:31,480
to solving this problem.

900
00:54:31,480 --> 00:54:32,480
Q-table being.

901
00:54:32,480 --> 00:54:38,520
Q-table being a specific approach for building a reinforcement learning network where it's

902
00:54:38,520 --> 00:54:43,720
appropriate if you don't have a lot of options to the state space is not huge.

903
00:54:43,720 --> 00:54:49,600
If you can enumerate enough of it, just a table is a perfectly reasonable thing to use.

904
00:54:49,600 --> 00:54:52,960
And I mean, that's an oversimplification of a q-table, but that's the general, that's

905
00:54:52,960 --> 00:54:56,240
the general gist, and fine, why not use that?

906
00:54:56,240 --> 00:54:58,320
That's an efficient system to use in this context.

907
00:54:58,320 --> 00:55:03,600
Whereas if I give it chess, and you say, go run some sample iterations of chess, and

908
00:55:03,600 --> 00:55:07,680
just see what's going on, it will very quickly learn that the state space is gigantic.

909
00:55:07,680 --> 00:55:08,680
It's huge.

910
00:55:08,680 --> 00:55:11,280
And using a q-table is not an appropriate choice in that context.

911
00:55:11,280 --> 00:55:15,440
You need to use a different type of network, and in fact, depending on how deep that network

912
00:55:15,440 --> 00:55:20,680
is and how well you've decomposed it, we might have to have some pretty deep layers,

913
00:55:20,680 --> 00:55:23,360
layer stacks in that network to be appropriate.

914
00:55:23,360 --> 00:55:27,880
So that's one level of how it decides which algorithmic approach is to use.

915
00:55:27,880 --> 00:55:30,120
It also looks at the kind of data that are flowing through.

916
00:55:30,120 --> 00:55:35,440
So if you're looking at chess, you can hand me the data as a array.

917
00:55:35,440 --> 00:55:41,160
If I'm looking at a robotic armature, you might hand me sensory data, which is just a collection

918
00:55:41,160 --> 00:55:46,800
of floating point values for motor torques and sensor detections and so on.

919
00:55:46,800 --> 00:55:51,720
And if I'm looking at autonomous vehicle, you might be handing me visual camera data.

920
00:55:51,720 --> 00:55:54,280
I could be handing the system any of these things.

921
00:55:54,280 --> 00:55:59,280
The appropriate network to utilize is completely dependent on that data.

922
00:55:59,280 --> 00:56:03,560
So if you hand me a visual data, I should be probably constructing an appropriately sized

923
00:56:03,560 --> 00:56:05,720
convolutional network, right?

924
00:56:05,720 --> 00:56:06,720
Exactly.

925
00:56:06,720 --> 00:56:08,040
It has spatial locality.

926
00:56:08,040 --> 00:56:13,400
If you hand me audio data, it has spatial locality in a sense, but it also has temporal locality.

927
00:56:13,400 --> 00:56:19,400
So I should be using different kinds of networks for that and all these kinds of heuristics

928
00:56:19,400 --> 00:56:23,760
and ways of looking at the environment and exploring and looking at how you deconstructed

929
00:56:23,760 --> 00:56:26,640
the problem, they're all taken into account.

930
00:56:26,640 --> 00:56:30,920
And so when the compiler outputs at the end of the day, okay, this is the network topology

931
00:56:30,920 --> 00:56:32,000
we're going to use.

932
00:56:32,000 --> 00:56:35,240
This is the algorithmic structure we're going to use, et cetera, et cetera.

933
00:56:35,240 --> 00:56:36,840
That's what's informing it all.

934
00:56:36,840 --> 00:56:40,400
And on top of that, we ourselves are a machine learning system.

935
00:56:40,400 --> 00:56:45,840
So it learns by virtue of having explored how to solve similar kinds of problems it's

936
00:56:45,840 --> 00:56:50,120
seen before, how to tune hyperparameters and all these other kinds of things.

937
00:56:50,120 --> 00:56:54,760
Again, levels of detail that the compiler should deal with for you, the platform should

938
00:56:54,760 --> 00:56:56,440
deal with for you.

939
00:56:56,440 --> 00:57:01,800
I want to, if we've done our job properly, it's much again going back to the database

940
00:57:01,800 --> 00:57:02,800
analogy.

941
00:57:02,800 --> 00:57:03,800
It's just like that.

942
00:57:03,800 --> 00:57:05,360
You don't have to tune any of those things.

943
00:57:05,360 --> 00:57:08,560
You don't even have to know what those things are or what they mean.

944
00:57:08,560 --> 00:57:11,240
You just have to be able to tell me what you want to teach and how to teach it and

945
00:57:11,240 --> 00:57:12,760
we'll build something for you.

946
00:57:12,760 --> 00:57:16,600
Is it the most efficient thing that you could possibly have hand tuned if you were an expert

947
00:57:16,600 --> 00:57:19,080
and knew how to do it yourself?

948
00:57:19,080 --> 00:57:20,080
Probably not.

949
00:57:20,080 --> 00:57:24,240
And that's why you still have database administrators and database experts who can go in and tweak

950
00:57:24,240 --> 00:57:27,880
all the, I'm going to look at the slow log and go modify the queries and I'm going to, all

951
00:57:27,880 --> 00:57:29,880
the things you do with databases.

952
00:57:29,880 --> 00:57:31,200
Same thing for us.

953
00:57:31,200 --> 00:57:35,760
If you want to go in and you want to provide a node in the system where you tell us, this

954
00:57:35,760 --> 00:57:37,080
is the structure.

955
00:57:37,080 --> 00:57:41,320
This is literally the topology it should be for this component.

956
00:57:41,320 --> 00:57:42,320
Okay.

957
00:57:42,320 --> 00:57:43,320
We're not going to stop you.

958
00:57:43,320 --> 00:57:44,800
That's fine.

959
00:57:44,800 --> 00:57:52,360
But if we give you the flexibility to decide where at what level you want to do that, that's

960
00:57:52,360 --> 00:57:57,520
better because it now lets you focus on the right people on the right levels.

961
00:57:57,520 --> 00:58:01,280
Your subject matter expertise can be focused on what to teach and how to teach it.

962
00:58:01,280 --> 00:58:05,160
Your machine learning experts and data scientists can be focused at that level and teasing

963
00:58:05,160 --> 00:58:09,160
apart when the networks are making predictions that weren't, didn't fit in your model and

964
00:58:09,160 --> 00:58:10,880
you need to now refine it.

965
00:58:10,880 --> 00:58:11,880
Great.

966
00:58:11,880 --> 00:58:12,880
That's where I want them to play.

967
00:58:12,880 --> 00:58:14,320
Your programmers have their role to play.

968
00:58:14,320 --> 00:58:17,840
I need to integrate with all these simulation environments and I need to have the telemetry

969
00:58:17,840 --> 00:58:20,680
data being tied into the system and so on.

970
00:58:20,680 --> 00:58:25,400
Every participant in this process has a role to play and our job as a platform company

971
00:58:25,400 --> 00:58:28,240
is to make sure that they all have the right tooling that's appropriate for what they're

972
00:58:28,240 --> 00:58:32,920
trying to do and they're part in the process and that we can have the discussion at the

973
00:58:32,920 --> 00:58:34,720
appropriate level.

974
00:58:34,720 --> 00:58:36,160
Is it at the level of the use case?

975
00:58:36,160 --> 00:58:38,520
Is it at the level of breaking that down?

976
00:58:38,520 --> 00:58:42,160
Is it at the level of the subject matter expertise or are we, do we need to get down to the

977
00:58:42,160 --> 00:58:46,640
level of convolutional neural network architecture?

978
00:58:46,640 --> 00:58:52,440
Is there a spec sheet or is there an enumerated list of like these are the networks that we support

979
00:58:52,440 --> 00:58:56,760
and how quickly does that evolve are there limitations or-

980
00:58:56,760 --> 00:59:00,040
So it's continuously evolving as you can imagine as a platform company we're always adding

981
00:59:00,040 --> 00:59:01,040
more.

982
00:59:01,040 --> 00:59:05,000
We have documentation up on our website so if you hit our main website-

983
00:59:05,000 --> 00:59:06,000
And so that's all there.

984
00:59:06,000 --> 00:59:07,840
Yeah just link to the docs section.

985
00:59:07,840 --> 00:59:13,560
There's documentation down to the protocol level so I have my own bespoke custom simulator

986
00:59:13,560 --> 00:59:18,720
which we do run into with some frequency and I need to be able to tie this simulator

987
00:59:18,720 --> 00:59:20,120
to the platform.

988
00:59:20,120 --> 00:59:21,120
How do I do that?

989
00:59:21,120 --> 00:59:24,640
So there's documentation on that and so it just kind of, it depends on what level you

990
00:59:24,640 --> 00:59:29,880
want to get to but yes there's documentation all up online publicly available.

991
00:59:29,880 --> 00:59:33,840
Do you get asked by customers like how many different network topologies or architectures

992
00:59:33,840 --> 00:59:35,840
do you support and-

993
00:59:35,840 --> 00:59:37,160
Yeah that happens sometimes it does.

994
00:59:37,160 --> 00:59:38,160
It depends on the-

995
00:59:38,160 --> 00:59:39,160
It depends on the-

996
00:59:39,160 --> 00:59:40,160
It's how technical they are.

997
00:59:40,160 --> 00:59:43,560
Machine learning, depth that they've gotten to.

998
00:59:43,560 --> 00:59:48,000
So yes we've had people ask that and so we can talk about TRPO networks and DDPG networks

999
00:59:48,000 --> 00:59:53,080
and or DQN networks and all the different, we don't have to numerate them all right now

1000
00:59:53,080 --> 00:59:56,560
but yes you can start to enumerate all the different ones that the platform has baked

1001
00:59:56,560 --> 00:59:59,320
in and have that conversation.

1002
00:59:59,320 --> 01:00:02,840
You'll also run into people who have been rolling their own.

1003
01:00:02,840 --> 01:00:07,200
So a lot of times that we'll get asked the question beyond which networks do you support.

1004
01:00:07,200 --> 01:00:10,400
I was just going to ask this question.

1005
01:00:10,400 --> 01:00:15,640
We'll get asked okay we have been, we'll get we'll get asked the question well who else

1006
01:00:15,640 --> 01:00:20,040
in the space should we be looking at and really there's two answers.

1007
01:00:20,040 --> 01:00:25,360
When it comes to deep reinforcement learning and applying that to solve these real problems

1008
01:00:25,360 --> 01:00:29,280
the first is rolling your own right that is actually we run into that very commonly.

1009
01:00:29,280 --> 01:00:36,200
And the second is very, very vertically specific company focused areas right.

1010
01:00:36,200 --> 01:00:40,800
So there are players in the space who only focus on deep reinforcement learning,

1011
01:00:40,800 --> 01:00:44,200
force applying, only deep reinforcement learning for robotics etc.

1012
01:00:44,200 --> 01:00:47,560
So those are the kinds of things you run into.

1013
01:00:47,560 --> 01:00:52,760
In the case of the latter if the solution they have fits your bill then that's great right.

1014
01:00:52,760 --> 01:00:54,240
They have a very specific focus.

1015
01:00:54,240 --> 01:00:59,560
We are a platform company so we're looking to allow you more flexibility in custom modeling

1016
01:00:59,560 --> 01:01:01,480
and how you expand all that out.

1017
01:01:01,480 --> 01:01:05,280
So that it's just kind of a judgment call internally for where you fall on what you want

1018
01:01:05,280 --> 01:01:06,280
to do.

1019
01:01:06,280 --> 01:01:10,760
And the former where you're rolling your own which we run into all the time.

1020
01:01:10,760 --> 01:01:15,720
That's actually a perfectly fine situation for us because the conversation quickly becomes

1021
01:01:15,720 --> 01:01:20,720
well how much time have you invested in that and have you run into this set of problems.

1022
01:01:20,720 --> 01:01:25,280
So you'll run into customers who they've decided reinforcement learning is the correct path

1023
01:01:25,280 --> 01:01:26,720
to solve their problem.

1024
01:01:26,720 --> 01:01:29,800
They've been going down that path and building out the solutions.

1025
01:01:29,800 --> 01:01:33,920
They're spending a ton of time managing their simulations, their tonning and all these

1026
01:01:33,920 --> 01:01:37,320
different assets of what we had just been talking about.

1027
01:01:37,320 --> 01:01:41,040
And we say well you know we can we're not going to magically make the problem that you're

1028
01:01:41,040 --> 01:01:45,120
facing about crafting a good reward function go away right that's part of that's still

1029
01:01:45,120 --> 01:01:48,080
part of the development process you're going to have to do that.

1030
01:01:48,080 --> 01:01:52,080
So we can take all the pain of managing those simulations off of your shoulders right

1031
01:01:52,080 --> 01:01:53,920
like the platform can help you do that.

1032
01:01:53,920 --> 01:01:56,760
And so you run into those companies that are a little bit further down the roll your

1033
01:01:56,760 --> 01:02:00,160
own and they're like oh wow I don't have to do this anymore.

1034
01:02:00,160 --> 01:02:01,160
That's that sounds great.

1035
01:02:01,160 --> 01:02:03,240
Let me let's let's talk about doing it.

1036
01:02:03,240 --> 01:02:06,000
And then you run into ones that are just dipping their toes in the water.

1037
01:02:06,000 --> 01:02:09,600
They kind of say well I could just build your built on top of TensorFlow.

1038
01:02:09,600 --> 01:02:13,480
Why wouldn't I just build it using TensorFlow and we can start to enumerate all the reasons

1039
01:02:13,480 --> 01:02:17,920
why but but generally we can say and these are the problems you will run into.

1040
01:02:17,920 --> 01:02:20,920
And some percentage of them will say I don't want to I don't want to deal with that.

1041
01:02:20,920 --> 01:02:21,920
Let's talk.

1042
01:02:21,920 --> 01:02:26,520
And some percentage of them will say I got to experience that myself and that's fine with

1043
01:02:26,520 --> 01:02:36,080
us because we know that you'll you'll come back totally totally yes exactly you got

1044
01:02:36,080 --> 01:02:39,320
to learn how to find the balance of where where it makes sense to roll your own and

1045
01:02:39,320 --> 01:02:40,320
where it doesn't.

1046
01:02:40,320 --> 01:02:44,920
Yeah and actually I was going to I was going to ask the question more aligned with kind

1047
01:02:44,920 --> 01:02:51,240
of that last way you expressed it and in particular you know TensorFlow is obviously a popular

1048
01:02:51,240 --> 01:02:56,160
platform for this stuff like is it an either or no it doesn't have to be at all in fact

1049
01:02:56,160 --> 01:03:01,440
we have a feature that we added the product recently called gears and for the developers

1050
01:03:01,440 --> 01:03:05,600
in the audience this is an interrupt feature and for people who aren't developers per

1051
01:03:05,600 --> 01:03:10,320
say it's what allows you to bridge the gap right it allows you to bring your current

1052
01:03:10,320 --> 01:03:15,880
investments that you've built whether it's in TensorFlow or you built a perception model

1053
01:03:15,880 --> 01:03:21,440
using OpenCV or whatever you happen to have done and add that to the system.

1054
01:03:21,440 --> 01:03:26,120
Facet of building any of a modern machine learning system which is not just building the

1055
01:03:26,120 --> 01:03:31,640
model but also deploying it in a production environment those there's actually completely

1056
01:03:31,640 --> 01:03:35,680
separate problems are going to face in those two arenas and so you might have built these

1057
01:03:35,680 --> 01:03:41,760
models but and want them to participate in the broader prediction pipeline and you can

1058
01:03:41,760 --> 01:03:45,680
do that very easily in our system using the gears feature it'll we allow you to add

1059
01:03:45,680 --> 01:03:52,920
that in so it's again going back to the the database analogy you don't have to if you

1060
01:03:52,920 --> 01:03:56,600
have the that expertise and you want to do it or you've already baked a bunch of things

1061
01:03:56,600 --> 01:04:00,680
and you want to leverage them we're not going to make you redo that work and we're not

1062
01:04:00,680 --> 01:04:04,000
going to stop you from tinkering with the lower level pieces if that's what you want

1063
01:04:04,000 --> 01:04:08,960
to do it just becomes an option as any good platform should it's we're going to give you

1064
01:04:08,960 --> 01:04:13,640
the ability to say I don't want to deal with any of that do it for me or I've done some

1065
01:04:13,640 --> 01:04:19,200
of this and I want to add it or now I want to tinker with the low level bits because I

1066
01:04:19,200 --> 01:04:23,600
I feel like I've learned enough and I'm an expert and I want to do that now so you can you

1067
01:04:23,600 --> 01:04:28,560
can play at any of these levels but it's not an either or in fact if you bring a TensorFlow

1068
01:04:28,560 --> 01:04:33,880
model to the system it integrates really well our systems built on TensorFlow a TensorFlow

1069
01:04:33,880 --> 01:04:38,960
based gear we can chain everything in all the appropriate ways if you have a Python function

1070
01:04:38,960 --> 01:04:42,880
that you've written you want to call out to maybe you want to invoke a cloud API because

1071
01:04:42,880 --> 01:04:48,600
you've made an investment in Watson or some other technology none of these are barriers

1072
01:04:48,600 --> 01:04:52,680
and in fact this is part of the way the system was designed if we go back to the very very

1073
01:04:52,680 --> 01:04:57,000
beginning and I talked about you have existing controllers that know how to do the inverse

1074
01:04:57,000 --> 01:05:01,440
kinematics and all you need to do is move your robotic armature between point A and point

1075
01:05:01,440 --> 01:05:06,640
B you should not be teaching that that is but and this goes to the academics right so

1076
01:05:06,640 --> 01:05:10,000
you look at the academic papers they're going to talk about how you teach all these things

1077
01:05:10,000 --> 01:05:14,880
and that's great because it's it's talking to how we further enhance the technology and

1078
01:05:14,880 --> 01:05:20,120
I love that work that's great but if we're talking about practical real world application

1079
01:05:20,120 --> 01:05:23,920
why you have tons of work spent on building that and you should just use it and that's

1080
01:05:23,920 --> 01:05:29,800
a very simple simple example but if you look at autonomous vehicles the automobile companies

1081
01:05:29,800 --> 01:05:36,640
have spent a lot of resource on building capabilities around assistive parallel parking

1082
01:05:36,640 --> 01:05:40,720
and all these other things do we really want to go reinvent all of that it it seems kind

1083
01:05:40,720 --> 01:05:43,960
of silly we should just integrate that with the rest of everything else and that

1084
01:05:43,960 --> 01:05:49,800
idea extends to your cloud based API is your TensorFlow module totally totally that's right

1085
01:05:49,800 --> 01:05:55,520
that's right it can't be a homogenous this is the only way or you know that's not that's

1086
01:05:55,520 --> 01:06:00,600
not practical that doesn't work so this has been super interesting anything that you'd

1087
01:06:00,600 --> 01:06:07,160
like to leave folks with well I would encourage the audience to take a look at our platform

1088
01:06:07,160 --> 01:06:10,320
if they're interested in the early access program of course we would love to hear from

1089
01:06:10,320 --> 01:06:14,520
you if you have a use case that you think is suitable we'd love to hear from you but

1090
01:06:14,520 --> 01:06:19,920
just in general I would encourage everyone to start thinking about machine teaching whether

1091
01:06:19,920 --> 01:06:26,560
it's with us or not with us the path forward for industrial AI in general is going to

1092
01:06:26,560 --> 01:06:32,560
rely very heavily on the marriage of human expertise and machine intelligence you need

1093
01:06:32,560 --> 01:06:38,760
both it's not enough to have one or the other you need both and so starting to explore

1094
01:06:38,760 --> 01:06:43,960
in more depth how you're teaching the system don't just throw data at it blindly don't

1095
01:06:43,960 --> 01:06:47,480
that that's that's level one right you need to move several levels beyond that and so I

1096
01:06:47,480 --> 01:06:52,200
would encourage everyone listening start to think about that start to think about strategies

1097
01:06:52,200 --> 01:06:55,720
for doing that whether you're rolling your own or whether you're using a tool like ours

1098
01:06:55,720 --> 01:07:01,000
that matters a whole lot and as practitioners using this technology a lot of our job is

1099
01:07:01,000 --> 01:07:06,680
going to be teaching it's not going to be over time the tools will get better tensor flow

1100
01:07:06,680 --> 01:07:11,800
will add more capabilities platforms like ours will become more prominent all of these

1101
01:07:11,800 --> 01:07:16,520
things will happen just as a natural evolution of the industry and the part that will remain

1102
01:07:16,520 --> 01:07:22,960
in all circumstances no matter what is how do I teach what I want the system to actually

1103
01:07:22,960 --> 01:07:26,760
be intelligent about and that's going to be with us forever and that's very particular

1104
01:07:26,760 --> 01:07:31,080
and idiosyncratic to our businesses and what we're trying to accomplish so really start

1105
01:07:31,080 --> 01:07:34,440
to think about that that would be what I would encourage the audience to do great great

1106
01:07:34,440 --> 01:07:39,080
and we'll make sure we point folks to your website and the eap and some of the other stuff

1107
01:07:39,080 --> 01:07:43,240
we talked about sounds great awesome thank you so much yeah thank you as well I really

1108
01:07:43,240 --> 01:07:53,560
appreciate it all right everyone that's our show for today thanks so much for listening and for

1109
01:07:53,560 --> 01:07:59,480
your continued feedback and support for the notes for this episode including links to mark

1110
01:07:59,480 --> 01:08:05,400
and the various resources mentioned on the show head on over to the show notes at twemolei.com

1111
01:08:05,400 --> 01:08:11,640
slash top slash 43 please be sure to comment there with your feedback or questions

1112
01:08:11,640 --> 01:08:19,960
thanks again to our sponsors for this series bonsai and wise.io at ge digital I'm so so grateful

1113
01:08:19,960 --> 01:08:25,240
for their support if you enjoyed this series it would mean a ton to me if you took a second

1114
01:08:25,240 --> 01:08:32,360
to reach out to them on twitter to thank them for their support at at wise.io and at bonsai AI

1115
01:08:33,240 --> 01:08:39,000
don't forget to register for my newsletter at twemolei.com slash newsletter and for next months

1116
01:08:39,000 --> 01:08:54,040
online meetup at twemolei.com slash meetup thanks again for listening and catch you next time

