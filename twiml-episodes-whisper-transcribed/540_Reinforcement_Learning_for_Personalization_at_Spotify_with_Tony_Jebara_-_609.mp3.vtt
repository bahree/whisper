WEBVTT

00:00.000 --> 00:05.780
All right, everyone. Welcome to another episode of the Twilmo AI podcast. I am your host Sam

00:05.780 --> 00:11.480
Charrington. And today I'm joined by Tony Jabara. Tony is a vice president of engineering

00:11.480 --> 00:16.680
and head of machine learning at Spotify. Before we get going, be sure to take a moment

00:16.680 --> 00:22.200
to hit that subscribe button wherever you're listening to today's show. Hopefully Spotify.

00:22.200 --> 00:26.800
Tony, this conversation is a long time in the works. We're super excited to have you

00:26.800 --> 00:32.520
welcome to the podcast. Thank you, Sam. Great to be with you and with all the folks joining

00:32.520 --> 00:38.320
us here today. Very excited. I'm super excited. I mentioned to Tony before we got started

00:38.320 --> 00:44.920
recording that I've got this memory of I think 2017 or something like that at the rework

00:44.920 --> 00:49.840
deep learning conference in San Francisco. I think it's the hiate that's up on the hill

00:49.840 --> 00:56.680
just by Chinatown. And I think you did a presentation while you were in Netflix about maybe machine

00:56.680 --> 01:02.800
learning for for breakfast or something. And I kind of harangued you. You were on your

01:02.800 --> 01:08.600
way to a meeting out of the hotel and been trying to get a conversation going for a while.

01:08.600 --> 01:13.720
So super excited to finally connect. Yeah. Yeah. Great. We're doing it finally. You know,

01:13.720 --> 01:19.560
thanks for for keeping up and you know, hopefully there's more folks in the audience today

01:19.560 --> 01:24.640
than in 2017 as your podcast seems to be taking off. So that's great. Yeah. Absolutely.

01:24.640 --> 01:27.880
Absolutely. Absolutely. There's no better time than the present. This episode is going to be

01:27.880 --> 01:33.920
part of our NURP series and you participated in a couple of workshops and NURPs mostly

01:33.920 --> 01:38.720
around the topic of reinforcement learning. And that'll be one of the main things we focus

01:38.720 --> 01:46.360
on in our conversation. But before we dig into RL in particular and or at least the the

01:46.360 --> 01:51.360
workshop participation in NURPs in particular, I'd love to have you share a little bit about

01:51.360 --> 01:55.880
your background and how you came to the field of machine learning. Sure. So I was, you

01:55.880 --> 02:00.360
know, excited by what computers could do a long time ago before machine learning really

02:00.360 --> 02:04.880
was a thing. I was looking at computer vision, looking at a bunch of computer science in

02:04.880 --> 02:09.640
general and saying how do we automate what people seem to be doing so well and how do they

02:09.640 --> 02:15.600
recognize faces and images. And you can write a bunch of rules, thinking here's how I would

02:15.600 --> 02:20.840
do it as a person and those were quite brittle. And so, you know, when I first started

02:20.840 --> 02:26.800
in the 90s, it was write rules for things to make computers intelligent. And then that

02:26.800 --> 02:31.560
didn't really seem to scale. It turns out it's much better to show the data and have an

02:31.560 --> 02:37.880
algorithm that learns all by itself what to do from examples of real data, real examples

02:37.880 --> 02:42.880
of intelligent behavior. And so that was, I think, how I really started and I was an academic

02:42.880 --> 02:47.680
because there wasn't much going on in industry in the 90s in the early 2000s. But then of

02:47.680 --> 02:52.400
course, industries where a lot of the big machine learning is happening these days. And

02:52.400 --> 02:57.320
so I went to Netflix and then at Spotify now where machine learning really is driving

02:57.320 --> 03:02.280
a lot of our advances and as a huge part of our business and our personalization offer.

03:02.280 --> 03:06.640
Maybe talk a little bit about your role as head of ML at Spotify. What are some of the

03:06.640 --> 03:10.080
aspects of machine learning that kind of fall under your purview there?

03:10.080 --> 03:16.000
So I'm focused on how machine learning can help across all Spotify. But also in particular,

03:16.000 --> 03:22.560
I lead the teams that build a homepage, the search engine, the programming platform, the

03:22.560 --> 03:27.000
user evaluation and understanding models, the content understanding from music understanding

03:27.000 --> 03:33.400
to talk understanding. And as well, some of the technologies that help us with playlists.

03:33.400 --> 03:40.160
And then I also advise all the business units about how they can incorporate best practices

03:40.160 --> 03:47.360
of machine learning. So our ads business unit, our let's say messaging and user membership

03:47.360 --> 03:52.360
growth business units. All of them really have to work with the same machine learning

03:52.360 --> 03:57.280
infrastructure and best practices. And so I'm also tapped into those conversations, but

03:57.280 --> 04:01.840
you should that we're pushing for the right investments for the long term across all

04:01.840 --> 04:03.960
aspects of Spotify's business.

04:03.960 --> 04:09.160
Any thoughts on how the company's use of ML has evolved since you joined?

04:09.160 --> 04:16.320
Yeah, so ML has been an important tool in our tool chest, but now we're increasingly

04:16.320 --> 04:24.040
relying on it as we have it making so many more decisions more often in every part of

04:24.040 --> 04:30.640
our personalization and our, let's see, connections between the user base and the content.

04:30.640 --> 04:35.440
And what we've seen is huge scale growth of both sides. So we've not only added music

04:35.440 --> 04:42.360
to our catalog, but we've also started looking at talk and audio books and podcasts and live

04:42.360 --> 04:47.280
and video as well. And our user base is grown across countries and cohorts and different

04:47.280 --> 04:54.080
plans. And so the complexity on the, on the people that want to listen and the creators

04:54.080 --> 04:57.560
and the content that should be listened to, that's growing on both sides. And so machine

04:57.560 --> 05:03.160
learning's actually increasingly important to figuring out what is the right valuable connections

05:03.160 --> 05:08.360
to make between the listeners and, and the artist's creators and content. And so that's

05:08.360 --> 05:14.640
been growing at a great clip. The other really interesting thing is Spotify kind of started

05:14.640 --> 05:19.920
off as a curation engine. You would come to Spotify and organize your own playlist and

05:19.920 --> 05:25.200
curate what you thought would be great audio experience. But over the years, we've evolved

05:25.200 --> 05:31.520
from a curation first experience to a recommendation and machine learning experience. So we're still

05:31.520 --> 05:35.400
having people curate their playlists and build them and share them with friends and, and

05:35.400 --> 05:41.200
in the world. But we're also creating playlists for you automatically, creating a homepage

05:41.200 --> 05:46.120
for you that's tailored to what you would be interested in driving discovery, promotions

05:46.120 --> 05:50.400
and search that are tailored for you. And so we're moving very much from a curation

05:50.400 --> 05:56.200
first product to a recommendation first product. And then even beyond that, a recommendation

05:56.200 --> 06:00.680
first product that also explains to you why you should care about this recommendation.

06:00.680 --> 06:04.880
So giving meaning to the recommendation is another aspect, understanding why this makes

06:04.880 --> 06:09.640
sense and how to explain to you and convince you to give it a try. So that's that's been

06:09.640 --> 06:14.520
the journey. So more and more machine learning throughout every stage of that journey.

06:14.520 --> 06:20.040
Can you talk a little bit about the kind of the business value that those recommendations

06:20.040 --> 06:28.120
provide for Spotify? I'm thinking back to a paper from Netflix actually may have been

06:28.120 --> 06:34.760
while you were there. I forget the names of the authors, but one was the head of product

06:34.760 --> 06:44.480
and there was this note in the paper that even back, you know, whatever that was 2015-16,

06:44.480 --> 06:49.200
they attributed a billion dollars of value to the recommendation systems that were built

06:49.200 --> 06:56.440
at Netflix. Can you talk about the, you know, how you, how the business thinks about the

06:56.440 --> 07:01.120
value of these kinds of recommendations? And I'll maybe contextualize this a little bit

07:01.120 --> 07:08.320
with my personal experience as a Spotify user. I'm not, you know, I'm not a big music person

07:08.320 --> 07:14.240
necessarily. I've got some things that I listen to and, you know, that I like and I've

07:14.240 --> 07:20.160
got enlist in Spotify. And I usually just go and, you know, play one of those and or play

07:20.160 --> 07:26.040
one of, you know, the same couple of playlists. Like, I don't necessarily, I'm not your best

07:26.040 --> 07:31.480
consumer of recommendations necessarily. And so I'm wondering kind of how broadly how

07:31.480 --> 07:37.920
they play out across the business. Sure. So we've been investing in a series of AB tested

07:37.920 --> 07:43.960
wins. Each, each test improves retention, reduces churn, increases engagement. And we

07:43.960 --> 07:47.800
can take all those tests or layered on more and more machine learning and say, well, what's

07:47.800 --> 07:52.360
the some value of all of those? The problem is that's happening while users are coming

07:52.360 --> 07:56.760
through the platform and being acquired even not just retained but acquired by machine

07:56.760 --> 08:03.000
learning. And it turns out if you go and survey people, users and, you know, premium subscribers,

08:03.000 --> 08:07.000
which is the majority of how we generate our revenue. And you ask them what drives you

08:07.000 --> 08:12.400
to Spotify? What makes you sign up for Spotify versus consume and find your music elsewhere

08:12.400 --> 08:18.200
or through some other channel or medium? 81% say it's because of the personalization.

08:18.200 --> 08:23.000
And that's really heavily driven by machine learning. So you can think of it as the majority

08:23.000 --> 08:30.600
of users are coming to Spotify's thing at Spotify because of the personalization. And

08:30.600 --> 08:34.440
of course, the content is crucial. You can't personalize when you don't have content.

08:34.440 --> 08:39.840
There's nothing to personalize. But the bigger the content catalog becomes, the more

08:39.840 --> 08:44.600
of the personalization matters. So we're now at, you know, over 100 million tracks, over

08:44.600 --> 08:50.360
100 million podcast episodes. We've added hundreds of thousands of books and we keep going.

08:50.360 --> 08:54.760
And so that means personalization constantly gets more and more important. And the machine

08:54.760 --> 08:59.880
learning algorithm is to find the right thing for you, especially when for you means one

08:59.880 --> 09:06.440
out of half a billion people almost. That value becomes much bigger as the user population

09:06.440 --> 09:13.440
grows as the content catalog grows. And as we improve and roll out more intelligent algorithms

09:13.440 --> 09:15.440
with better wins that are av tested.

09:15.440 --> 09:21.440
I guess that brings us to your presentation at the offline RL workshop, which talks about

09:21.440 --> 09:29.120
some of the ways that you apply offline RL for personalization. But before we jump into

09:29.120 --> 09:38.400
that, talk broadly about the techniques you use. Are you kind of mostly using RL for personalization

09:38.400 --> 09:44.160
or is that one of many tools that you use depending on the specific scenario?

09:44.160 --> 09:48.960
I'd say it's one of many tools. So we leverage a lot of machine learning tools. Think of the

09:48.960 --> 09:53.680
machine learning treasure chest as having, you know, many, many buckets. RL is one of those

09:53.680 --> 09:57.760
buckets. You've got deep learning in another bucket. You've got causal techniques, it

09:57.760 --> 10:03.120
causes inference in another one, probabilistic models. For one of the things we've realized

10:03.120 --> 10:09.480
is we kind of iterated our way and realized that we need more and more RL. And that's

10:09.480 --> 10:15.280
because we started doing heavily, let's say multi-arm bandits at the beginning where you

10:15.280 --> 10:20.520
basically do things like try things out, do a little exploration and then start taking

10:20.520 --> 10:25.440
action that seemed to get good responses from the user. And we did that in all sorts of

10:25.440 --> 10:31.280
places in our systems from our, you know, surfaces like our homepage, to our banners and

10:31.280 --> 10:36.880
our promotions. We use these techniques from baby RL, which is multi-arm bandits. This

10:36.880 --> 10:43.160
is like if you have a forgetful RL agent that doesn't know what state it's in, then basically

10:43.160 --> 10:49.520
it's a bandit. But that's maybe chapter one of the RL textbook. There's many more sophistications

10:49.520 --> 10:55.480
after what you start to say. Well, there's a state, you know, people change as they consume

10:55.480 --> 11:00.920
and discover and help new habits. And so you can't just think of it as a, you know,

11:00.920 --> 11:05.760
multi-arm bandit in the casino, which kind of doesn't really remember what happened before.

11:05.760 --> 11:10.520
You have to really understand users are impacted by your recommendations or what they consume,

11:10.520 --> 11:14.960
that changes who they are. And then they come back the next day and maybe the decision

11:14.960 --> 11:20.520
then is actually a little bit different. So then you understand that you're really not transacting

11:20.520 --> 11:24.600
in the moment only with the user. You're building a journey that's going to last many months

11:24.600 --> 11:29.560
with them, especially if you're a subscription service like we are. Users are with us for

11:29.560 --> 11:35.400
many months, many years. We're now thinking of our business more about building a journey

11:35.400 --> 11:40.600
rather than getting you to just click on something with a bandit and building a journey is

11:40.600 --> 11:45.560
much more of an RL style problem. If you look at what you're actually trying to do with

11:45.560 --> 11:50.840
RL, you're trying to play games and get to the end of a maze and so on or get a robot

11:50.840 --> 11:56.800
to do some useful tasks. Well, it turns out RL is also about getting a user to go on a

11:56.800 --> 12:02.880
journey and discover new things and enrich the way they use Spotify and their day-to-day

12:02.880 --> 12:09.960
life. And one of the themes that occurs in your presentation in the workshop is this

12:09.960 --> 12:19.960
idea of kind of transitioning from a single-step reward to optimizing over the lifetime value

12:19.960 --> 12:25.560
of a subscriber. Can you talk a little bit more about that and the complexities that it

12:25.560 --> 12:30.080
presents for you? Yeah, absolutely. So we started by just optimizing

12:30.080 --> 12:36.040
for the next click. And if you looked at our old-style homepage, old-style search pages,

12:36.040 --> 12:41.760
old-style playlists, it really was about just getting that next consumption, that next

12:41.760 --> 12:46.720
track, that next play session, which is really kind of click through a maximization. You

12:46.720 --> 12:52.040
want to get a good click to the rate. Which makes sense if you're serving ads, but I guess

12:52.040 --> 12:56.280
it's better than the customer closing the app. You're right. So you don't want to completely

12:56.280 --> 13:00.520
give up on instantaneous rewards. You need to do something when the customer opens the

13:00.520 --> 13:05.400
app. However, you can't just always think about what's the easy thing that gets the immediate

13:05.400 --> 13:10.080
click. Because for us, especially, the easiest thing to recommend is just listen to what

13:10.080 --> 13:15.800
you listen to yesterday. Here's the exact same playlist. Here's the exact same tracks.

13:15.800 --> 13:19.240
We know it's not going to be a trust buster. It worked yesterday, so it's not a pretty

13:19.240 --> 13:22.920
good click through rate. The problem is if you keep doing that over and over again, and

13:22.920 --> 13:27.560
you don't worry about the user tomorrow and six months from now and how happy are they

13:27.560 --> 13:35.640
this Spotify? You realize you quickly wear out this user because you haven't layered the

13:35.640 --> 13:40.840
familiar recommendations with discoveries and long-term growth. So you got to go for a

13:40.840 --> 13:46.720
short-term instantaneous reward, but also set up for long-term success. So the user keeps

13:46.720 --> 13:55.160
coming back and in the way is enriched and feels more long-term fulfillment at Spotify.

13:55.160 --> 13:59.240
Because they're building new discoveries of content, new habits, maybe now they're

13:59.240 --> 14:04.440
listening to a weekly podcast on Mondays that is a new habit for them. It's not just listening

14:04.440 --> 14:09.560
to dance music on Saturday, which might be why you started your subscription with Spotify

14:09.560 --> 14:15.440
to begin with, but we want to add new habits. Monday afternoon, listen to this podcast,

14:15.440 --> 14:22.640
maybe Thursday evening's meditation podcast on Sundays, maybe an audio book. So those

14:22.640 --> 14:27.680
habits really keep you coming back for the long term, and maybe they have lower clicks

14:27.680 --> 14:32.880
at a rate immediately, but once you do click, you keep coming back afterwards. So we've unlocked

14:32.880 --> 14:38.640
not just the next reward, but the sum of cumulative rewards into the future, which is what really

14:38.640 --> 14:44.720
RL is all about. Yeah, you've got this interesting kind of pictorial illustration of this where

14:44.720 --> 14:50.480
you talk about machine learning, kind of moving you in circles around your current state versus

14:50.480 --> 14:54.800
RL, which does a better job of getting you to that higher value state.

14:54.800 --> 14:59.200
Yeah, so I like that picture because it really captures what we actually did see in our data,

14:59.200 --> 15:03.600
where users would just go around in these circles of, you know, they play yesterday's thing,

15:03.600 --> 15:08.080
and then they play their typical kind of routine stuff, and they just circle around through

15:08.080 --> 15:13.920
their five, six frequent playlists. And you know, that's great, but if that's what you're doing

15:13.920 --> 15:18.320
eventually, you're going to get worn out by that, and you're not really, you know, getting

15:18.320 --> 15:23.280
more value out of Spotify, you kind of stuck in that rabbit hole going on the circles. And that's

15:23.280 --> 15:30.320
happening a lot with a lot of, let's say, recommendation engines. So how do you make you deliberately

15:30.320 --> 15:35.280
break out of that rabbit hole and go to a higher, you know, altitude location where you are

15:36.480 --> 15:42.960
expecting many more future rewards? Because you're now open to content categories, you didn't think

15:42.960 --> 15:48.400
existed before, you, you know, you've discovered jazz, and now you're going to open up that new source

15:48.400 --> 15:53.920
of reward, which is amazing jazz discoveries for many, many months to come. So that's how we think

15:53.920 --> 15:59.120
of altitude. You're at a higher point where, you know, much more about the audio landscape,

15:59.120 --> 16:04.080
and you can consume for the future much more easily. I'm not sure I thought of myself as stuck as

16:04.080 --> 16:09.600
a Spotify user before this conversation, but now I'm going to be super self conscious about it,

16:09.600 --> 16:15.920
and if not the recommendation systems changing my behavior, maybe this conversation.

16:16.880 --> 16:23.520
We'll take it. We'll take anyway we can. When you start thinking about, you know, this broad

16:23.520 --> 16:31.280
or longer term journey with the user and trying to make decisions around that, do you run into

16:31.280 --> 16:38.480
challenges with attribution? Yeah, absolutely. Attribution is a big problem because it's very easy

16:38.480 --> 16:44.720
to attribute an instantaneous reward to an action because the reward shows up, you know, a few

16:44.720 --> 16:49.520
hundred milliseconds later. You presented the thing I clicked. Exactly. So then it's pretty clear

16:49.520 --> 16:55.520
that, you know, our recommendation that got a click, you know, a hundred milliseconds later was a

16:55.520 --> 17:00.480
good one or a bad one. But when we're talking about, you know, the user retaining longer and

17:00.480 --> 17:04.560
building habits, it's harder to say it was one specific action, which triggered this thing that

17:04.560 --> 17:12.080
may actually have needed several nudges, we call them. You know, I nudged you to try out a podcast

17:12.080 --> 17:16.720
once. It got it into your mind. You thought about it, but you never actually followed through.

17:16.720 --> 17:20.240
And then some other time, you typed a search query that was related to that podcast and they

17:20.240 --> 17:27.120
showed up again. And so it takes a few steps before we actually get the user to build a new habit.

17:27.120 --> 17:32.240
And then you have to reinforce that habit by showing it in shortcuts at the top of the page. And so

17:32.240 --> 17:37.760
that's kind of a long term outcome. And it's harder to say, here's the thing that actually

17:38.560 --> 17:44.640
created the reward. It's several things in sequence. And it's pretty easy to lose the attribution

17:44.640 --> 17:50.400
when it's several sequences of actions that led to the reward. And the reward is delayed. So

17:50.400 --> 17:56.160
causality and attribution become much trickier. What are some of the ways that you apply causality

17:56.160 --> 18:04.480
or causal modeling in trying to model attribution? Well, I mean, we do track lots of

18:06.400 --> 18:13.760
actions. So we have the ability to stitch together an entire, let's say, set of actions that

18:13.760 --> 18:21.680
led to an actual stream. So we have linkages through our data sets, which lets us follow the user

18:21.680 --> 18:26.480
throughout the app and the sequence of actions that led to the final stream and not just think it

18:26.480 --> 18:31.920
was the last last page that triggered it, but maybe you searched and went to an artist page,

18:31.920 --> 18:36.080
then came back and then searched for something else, came to a different artist page and then finally

18:36.080 --> 18:42.000
pushed click and play. We connect those little steps along the way. So we have little trajectories.

18:42.000 --> 18:46.400
And those trajectories get stitched into longer trajectories. So we've built trajectories

18:46.400 --> 18:52.240
for all the users, the history of what they've done and what they've consumed. And that trajectory

18:52.240 --> 18:58.160
data is actually a rich data set that's perfect for things like offline RL because we see

18:58.960 --> 19:06.480
not just action reward, action reward, we actually see the whole trajectory of state action reward

19:07.200 --> 19:12.240
triplets in a time series. And then we can say, all right, this time series clearly is leading

19:12.240 --> 19:17.440
to great outcomes and long-term rewards and consistent rewards. This other time series doesn't

19:17.440 --> 19:23.040
lead to, you know, a bigger sum of cumulative rewards. So we actually are now working with sequence

19:23.040 --> 19:28.960
data and are able to do things with our offline sequence data before going into an AB test.

19:29.600 --> 19:34.000
And we also are building simulators, which is another way to capture this kind of long-term

19:35.040 --> 19:40.640
attribution problem. We simulate how our homepage will look and how users will respond to it.

19:40.640 --> 19:45.920
We simulate playlists and how the users will skip and play them. And so simulation is another

19:45.920 --> 19:50.240
key technology. And both of those were topics of the workshops. So how do you work with offline

19:50.240 --> 19:55.360
sequence data, which we have, and we've logged across our user base. And how do you work with

19:55.360 --> 20:00.880
simulation, which we've built for our homepage, at least, and our playlists? Let's jump into the

20:00.880 --> 20:08.160
offline data. How do you do that? So for every user, we don't just keep track of, let's say,

20:08.160 --> 20:13.840
the last session. We actually look at the series of actions they've taken, the recommendations

20:13.840 --> 20:18.400
we've made, the rewards they've generated for us. And so we actually have time series data.

20:19.200 --> 20:26.480
And that's been valuable. We also use this to build lifetime value models. So we look at users

20:27.200 --> 20:31.680
on the service for many months and say, okay, who is retaining after X many months and who isn't?

20:31.680 --> 20:37.680
And those models look at the history of consumption and also some sequential aspects of

20:38.480 --> 20:44.400
consumption and engagement with our app. And that sequence that is used to make a prediction,

20:44.400 --> 20:49.200
saying, how much longer is this user going to stay? How long will they survive on Spotify, for instance?

20:49.760 --> 20:54.800
And those are things we were able to build also because we have long-term historical data

20:54.800 --> 20:59.840
on retention and what led to retention. So we look at sequences of actions. We look at

20:59.840 --> 21:07.920
long-term consumption histories and how they've led to retention and survival. And then of course,

21:07.920 --> 21:15.360
we build the simulators. But one of the things we've converged on is, in a way, lifetime value models

21:15.360 --> 21:20.400
have been around for a long time. They're used in kind of subscription services. We've realized

21:20.880 --> 21:27.600
lifetime value models are really just the reward function in RL because it's the sum of cumulative

21:27.600 --> 21:33.520
rewards with a discount. And that's literally what a lifetime value is in businesses, in subscriptions.

21:34.560 --> 21:38.720
The link goes a little different, but they actually turn out to be the same thing on

21:40.560 --> 21:46.400
an equation line at least. Yeah. How does that translate to implementation land? Is there

21:47.200 --> 21:52.480
are you building some model that at the end of the month, if the users are still around,

21:52.480 --> 21:59.760
there's a 995 reward or whatever that is. Now it is. Yeah. So one aspect is we literally say

22:01.200 --> 22:04.800
each month when you subscribe, that's a big reward for us because you say, hey Spotify,

22:04.800 --> 22:10.320
you did a good job last month. I'm going to keep betting on you. Here's my 995 or 10 bucks.

22:11.120 --> 22:16.320
And so what we're trying to capture is, you know, we've got 10 bucks for this user,

22:16.320 --> 22:21.360
but how many more months are they going to keep giving us that 10 bucks? And if I can change that,

22:21.360 --> 22:27.040
make it go from 15 more months of 10 bucks to 17 more bucks, then I'm really happy. That might

22:27.040 --> 22:31.520
not show up for a while, right? That user is going to be around. We're not going to know it until

22:31.520 --> 22:36.240
15 months go by and oh, wow, they stayed a little longer. But what we're trying to do is calculate

22:37.200 --> 22:43.360
the sum of those monthly rewards. And we actually do it over a rising of 60 months. So we look at

22:43.360 --> 22:49.120
over the next five years, you know, what's the probability each month that you're going to stick

22:49.120 --> 22:56.320
around? And we sum up all those probabilities multiplied by, you know, the dollar value of that

22:56.320 --> 23:01.280
month. And it's, you know, roughly 10 bucks for subscribers a month. But for free users,

23:01.280 --> 23:05.600
it's coming from their ads and their ad load. And actually, it's a little more complicated. So it's

23:05.600 --> 23:13.040
summing all their future months and with various degrees of ad load as the dollars. And we also

23:13.040 --> 23:19.280
applied discount factor because a dollar today is worth more than a dollar tomorrow, let's say. And

23:19.280 --> 23:27.360
that's exactly what a, you know, the RL textbook problem is it's the maximize a sum of discounted

23:27.360 --> 23:33.680
cumulative rewards. And it turns out LTV is exactly the sum of discounted cumulative dollar rewards.

23:33.680 --> 23:44.080
Is that in a lot of areas in machine learning applied to kind of business types of problems,

23:44.640 --> 23:49.360
you've got to create these proxy metrics because it's, you know, either your actual metric is,

23:50.240 --> 23:57.280
you know, too opaque or too difficult to, you know, turn into a metric suitable for a machine

23:57.280 --> 24:03.760
learning model. Is this an instance where you're able to more closely map the business metrics to

24:03.760 --> 24:10.480
machine learning than in other examples in your experience? Or is it just different?

24:11.040 --> 24:17.040
It is a good proxy metric, as you say, because at the end, yes, a business wants to optimize.

24:17.040 --> 24:22.000
So it's still a proxy metric is kind of the first thing you're saying here. It's not a holy

24:22.000 --> 24:27.520
grill of like we've, you know, fully captured the business need in this computational model here.

24:28.720 --> 24:32.640
Yeah, it's not the perfect metric. It doesn't capture every aspect of the business. There's all

24:32.640 --> 24:40.480
sorts of other costs and revenue and, you know, it's not, we're not trying to put the CFO

24:40.480 --> 24:47.280
at its entire organization into one ML model, but it is a very good proxy, let's say, because

24:47.280 --> 24:52.400
it really is capturing, you know, especially for a business like ours where we have subscribers

24:52.400 --> 25:00.240
and actually multiple plans and free and, you know, churns and premium. We're trying to capture all

25:00.240 --> 25:09.440
of that with a model that really summarizes, let's say, a good portion of the revenue and the margins

25:09.440 --> 25:13.360
for the business, but not all of it. It's not a simulation of the entire business, but it's actually

25:13.360 --> 25:19.600
the best we have considering, you know, it's a very complex business at the end.

25:20.400 --> 25:26.560
So we capture, I would say, a good chunk of the business complexity with this proxy, but it's not,

25:27.280 --> 25:32.480
it's not as good as the actual real data that's showing up every day when we actually see the real

25:32.480 --> 25:37.440
dollars and the real payouts to the artists and to the creators and so on. So it's, there's still

25:37.440 --> 25:43.840
some proxy there. It's not perfect. A big portion of your talk is kind of reviewing some of your

25:43.840 --> 25:50.320
teams, papers over the past year. So the first one you talked about is LTV and survival models,

25:50.320 --> 25:56.160
and I think that's what we just kind of talked about this idea that LTV is really the sum of

25:56.160 --> 26:03.520
the probabilities of, or this weighted sum of, here it's expected gross profit, but I think you map

26:03.520 --> 26:11.920
that to the survival model and the probabilities there. What's the right way to say that?

26:11.920 --> 26:20.720
Yeah, so it's basically the sum of survival probabilities scaled by the profit for each month,

26:21.600 --> 26:28.320
and then you also do it at discount factor because, you know, there's a, you know, the capital

26:28.320 --> 26:35.360
has a time discounts. Yeah. So you could think of it as net, net present value, lifetime value,

26:36.560 --> 26:43.600
or you can literally think of it in the RL normaclature as the value function, V of S, where S is

26:43.600 --> 26:49.600
the state. So if the user is in this state, what's the value? And it's basically the sum of expected

26:49.600 --> 26:55.760
rewards. If you play well from the starting state, according to, you know, a good policy. So

26:55.760 --> 27:04.400
it depends on the policy, of course. So if you continue to act, as we've acted, this user in this

27:04.400 --> 27:11.840
state will generate the following future rewards. And that's, and we've been modeling that with these,

27:12.960 --> 27:19.360
we call them beta geometric survival models, because you don't want to just use geometric,

27:19.360 --> 27:24.400
geometric is kind of the, you know, users don't really flip a coin each month and say I'm going to

27:24.400 --> 27:30.960
stay or not, they actually have these more complicated probabilities that actually depend on

27:30.960 --> 27:36.320
something more than just a single coin. And so we look at everything we know about the user,

27:36.320 --> 27:43.440
and we actually describe their survival through basically two numbers that are computed from

27:43.440 --> 27:48.400
everything we know about them. And those describe the shape of their future survival. And that's,

27:48.400 --> 27:57.520
that's been something we've published in a paper in 2021. And then we recently, in this past summer,

27:57.520 --> 28:04.320
published a version which extends beyond survival to multi-state. So it turns out, you know,

28:04.320 --> 28:08.800
users aren't in just one of two states that's modified. They're not just either subscribed,

28:08.800 --> 28:12.080
they're not subscribed. They actually can have many states. They could be

28:13.040 --> 28:17.280
subscribed. They could be in a free state. They could be in a family plan. They can be in a duo

28:17.280 --> 28:22.480
plan. They can be churned out. They can be churned out, but still registered. We still have

28:22.480 --> 28:27.680
information. They still have emails. And we can still, you know, potentially resume their

28:27.680 --> 28:32.240
account where they left off. And then there's users who just have never even interacted with

28:32.240 --> 28:38.160
Spotify whatsoever and have yet to enter any information into our, you know, into our logins,

28:38.160 --> 28:44.160
let's say. And so, and then furthermore, you can slice those states into more granular states of,

28:44.160 --> 28:50.400
is this user in this country or that one? And we can keep going. But then we've extended the

28:50.400 --> 28:56.720
survival modeling to multi-state survival. And then that starts to look like multi-state reinforcement

28:56.720 --> 29:03.680
learning. And also, a lot of the lessons learned from survival map to this kind of multi-state world.

29:03.680 --> 29:10.560
So, you know, LTV was really about a binary survive dot survive. We've extended to multi-state.

29:10.560 --> 29:17.040
And it actually now looks much more like a nicer connection with RL because RL almost from day one

29:17.040 --> 29:23.200
was multi-state to begin with. It never was just a binary state. And that paper you get into talking about

29:25.120 --> 29:31.680
talking about categorical distributions and Dirichlet distributions. What's the, where did those

29:31.680 --> 29:38.640
come into play? So, just like, you know, in, so if you're going to stay subscribed and not

29:38.640 --> 29:43.600
stay subscribed, we said it's kind of like a user flipping a coin. Each month, the user's flipping

29:43.600 --> 29:49.120
a coin and if it lands on heads, they turn off, if it lands on tails, they stick around for another

29:49.120 --> 29:55.360
month. That model is not perfect because it turns out users aren't just flipping a coin each month.

29:56.560 --> 30:03.520
They're, think of it as they're drawing a coin from a coin factory and flipping that coin each

30:03.520 --> 30:11.040
month. And so, that's how you think about it. And that's the coin factory is called a beta

30:11.040 --> 30:17.360
distribution. And then the coin flip itself is like a Bernoulli event. And it's a coin factory

30:18.640 --> 30:25.200
their next state. And the flip is whether they go there or not. So, what we're doing for each

30:25.200 --> 30:30.800
user is trying to predict what kind of coin factory are you writing as a user. And each month,

30:30.800 --> 30:34.800
you grab a random coin from that coin factory, you flip it and that decides what you do.

30:35.520 --> 30:41.040
And so, that was the analogy. And it turns out that fits the data way better. If each user is

30:41.040 --> 30:47.040
described as having their own coin factory, and that fits the data way better than saying each

30:47.040 --> 30:53.680
user has a secret coin that they flip. So, it's analogous to the beta survival model where you had

30:53.680 --> 30:57.520
these two parameters that you're trying to figure out for each user. Now, you're trying to figure

30:57.520 --> 31:02.640
out a coin factory number of parameters. How many parameters characterize a factory?

31:02.640 --> 31:08.480
So, the beta is the coin factory. And then the Dirschley is the dice factory. And so,

31:08.480 --> 31:14.000
okay. And so, when you have multiple states, you don't just flip a coin, you roll the dice.

31:14.000 --> 31:20.000
And so, I'm a state 1, 2, 3, 4, 5, or 6. And it turns out users don't transition

31:20.000 --> 31:27.360
by rolling at, you know, the dice. What they also seem to be doing is they have their own dice factory.

31:27.360 --> 31:33.600
They grab a dice from it every day. The dice are slightly loaded differently, and then they roll

31:33.600 --> 31:38.800
the dice. And that actually fits the data better. So, it turns out, you know, human beings are not

31:38.800 --> 31:46.000
a single dice or a single die. They're not a single coin. They're acting more like a factory

31:46.000 --> 31:51.040
of these things. And there's a distribution of dice or a distribution of coins. And that captures

31:51.600 --> 31:59.360
the dynamics of multi-state transition better than what we saw with just the simple models,

31:59.360 --> 32:03.920
like the Markov models and so on, that, you know, are single dice and single coin models.

32:04.800 --> 32:08.560
And now that's what we're using in our systems. Got it. Got it.

32:08.560 --> 32:15.760
And so, the next paper you talked about is the RL and temporally consistent survival.

32:17.040 --> 32:22.480
So, this sounds like an extension of the idea to temporal consistency. What were the

32:22.480 --> 32:27.040
challenges that you were looking at there? So, this was kind of like the last, you know,

32:27.040 --> 32:32.000
put the bow around the connection between LTV and RL. And so, this was published

32:32.000 --> 32:40.320
last week at NURRIPS. And what we said was, these survival models are great. They look like RL

32:41.040 --> 32:46.240
kind of as well. But there's one aspect of RL, which is missing, which is when you estimate

32:46.240 --> 32:52.480
a survival model, let's estimate my survival model for today. Tomorrow, you're also going to

32:52.480 --> 32:57.920
look at my data and estimate my survival model. Those two survival models should be consistent.

32:57.920 --> 33:03.280
You shouldn't estimate complete different survival from one day to the next. And yes,

33:03.280 --> 33:08.320
maybe they can start to change a little bit because today, maybe I discovered one more great

33:08.320 --> 33:13.200
podcast, the Tumel podcast with Sam. And so, maybe now I want to survive much better.

33:13.520 --> 33:17.440
But there should be some consistency in time. And if you enforce that consistency,

33:18.160 --> 33:23.040
you actually get a much better estimator for these survival models that works better than just

33:23.040 --> 33:27.520
fitting them to the data with maximum likelihood, which has been how we did this before.

33:27.520 --> 33:33.280
Or Bayesian kind of marginal likelihood. So, if you enforce temporal consistency,

33:33.280 --> 33:37.520
everything also seems to work better. And that was an aha moment. And that led to, again,

33:37.520 --> 33:43.600
a performance improvement in our models. So, you add temporal consistency to survival models.

33:43.600 --> 33:49.920
And you go from point flips and dice rolls to factories of coins and factories of dice.

33:50.960 --> 33:56.480
Both of those two ideas really seem to improve how well these models fit our real human data.

33:56.480 --> 34:01.280
And that's those are the lessons learned. And it turns out, those bring survival modeling

34:01.280 --> 34:06.960
very close to RL. And we feel like now, you know, there's almost a kind of a one-to-one

34:06.960 --> 34:11.440
the source between the two communities where you can say, okay, I've got this concept of temporal

34:11.440 --> 34:18.080
consistency. Oh, that's related to, you know, how RL enforces Markovian dynamics and Bell

34:18.080 --> 34:23.280
many equations and temporal difference learning. So, there's kind of a nice the source between

34:23.280 --> 34:28.960
these two technologies that have existed in very different communities, kind of all mapping to one

34:29.920 --> 34:36.640
one real big framework that's consistent. That strikes me a lot of evolution of the around

34:36.640 --> 34:42.400
the sophistication of the way you're applying RL to your problem over the course of just a year or two.

34:43.040 --> 34:47.040
Yeah, I mean, we're we have researchers thinking about this and trying to connect it. What's

34:47.040 --> 34:50.960
great about Spotify is we're not just building, you know, science for science to say we're really

34:50.960 --> 34:54.960
thinking about the business, thinking about our users, how we can give them the most value and

34:54.960 --> 35:02.800
understand their behaviors as opposed to just building, you know, algorithms in, you know,

35:04.720 --> 35:12.320
in a, you know, isolated way. Yeah, yeah. We're also testing some of these things now in production

35:12.320 --> 35:18.800
and seeing the benefits. So, some of the learnings are now that we've understood these LTV models

35:18.800 --> 35:26.000
and we start connecting them to value functions in RL and Q functions from RL. We're now understanding

35:26.000 --> 35:33.440
how they, how they can help us better make recommendations now that we think about our recommendations

35:33.440 --> 35:39.520
as an RL problem. And what are we trying to maximize in RL? You're trying to maximize the sum

35:39.520 --> 35:48.720
of future rewards and or maximize the Q function really. That Q function now we can start to

35:48.720 --> 35:53.680
understand better and realize for our domain that the Q function is a combination of getting

35:53.680 --> 35:58.400
you to click, but also giving you something that's very valuable for the long term when you do click.

35:59.040 --> 36:07.520
And so this was in the last paper that we presented and it's actually encouraging us to view kind

36:07.520 --> 36:14.320
of recommendation as not just maximize click through rate, but maximize the click through rate

36:14.320 --> 36:20.640
of something that's going to continue to generate let's say a long, long term sum of rewards.

36:22.240 --> 36:27.520
So some high value consumption item. So don't just show me something I'm very likely to click on,

36:28.080 --> 36:33.120
but if I do click on it, it's going to increase my lifetime value by a big amount. And so that's how

36:33.120 --> 36:37.920
we're shifting our recommendations now. That's the optimizing audio recommendations for the long term

36:37.920 --> 36:43.280
paper. That's right. So we're realizing, don't just get things that are clicky, but get things that

36:43.280 --> 36:49.600
are clicky and sticky. So once I click on it, I'm going to keep coming back to it. It's going to be

36:49.600 --> 36:56.080
something that becomes a habit for me as opposed to I click consume and forget. And it's going to

36:56.080 --> 37:02.800
there's no real, you know, change in my long term value once I do click on something. So we're trying

37:02.800 --> 37:08.080
to show you something you're likely to click on and try. But and if you do try it, also will increase

37:08.080 --> 37:14.400
your LTV. And that that's kind of how we're shifting our recommendations now. So can you talk about

37:14.400 --> 37:25.040
the the process of going from the the research to actual recommendations? You, you know, there's

37:25.040 --> 37:29.920
this one idea of hey, you know, we've got these research. They identify these methods. We take

37:29.920 --> 37:36.160
these methods. We implement them against our data. They produce these models and poof. The models

37:36.160 --> 37:43.120
will recommend the constant the content. What you're describing here is the models are informing you

37:43.120 --> 37:48.800
about ways to think about how to make recommendations. And then you make different different

37:48.800 --> 37:55.840
recommendations. I'm not hearing you put models in production that make different recommendations.

37:55.840 --> 38:02.720
Like bridge the gap for me around this. Are we just talking time scales or? No, you're right.

38:02.720 --> 38:07.760
This is a longer process. I'm kind of, you know, jumping to the to the conclusion. But the reality

38:07.760 --> 38:12.880
is the way this starts is we have the ideas who write down, you know, some modeling assumptions

38:12.880 --> 38:19.440
and some, you know, aha moments, maybe a paper. We build a prototype. Then we maybe take that

38:19.440 --> 38:24.320
prototype and refine it with the offline data that I talked about. We get some good offline results.

38:24.320 --> 38:30.080
And we may even put the model through the simulator and see how well it does in simulation.

38:31.120 --> 38:36.400
Then we actually get it to be a productionized model that we can run, run live on real users.

38:36.400 --> 38:42.240
And what we then do is we A, B test it. We say, let's run this model in kind of a side-by-side

38:42.240 --> 38:47.760
horse race against what we're already doing from last year, let's say. So we got model A and

38:47.760 --> 38:53.600
model B running on half users get model A, half users get model B. And then that's really how

38:53.600 --> 38:59.120
we evaluate. We don't just stop offline or stop at the prototype. So then we have these two

38:59.120 --> 39:04.960
production models running side-by-side. And then we actually say, do we see after, you know,

39:05.600 --> 39:11.920
X many days of running this model side-by-side better engagement. Our people sticking around longer,

39:11.920 --> 39:17.040
retaining better after a couple of months of horse racing these models side-by-side. And it turns

39:17.040 --> 39:25.040
out we do see better long-term metrics with these kind of RL-inspired models. And it turns out

39:26.080 --> 39:32.560
what the models seem to do is they have lower click-through rate, the new models. But long-term,

39:32.560 --> 39:38.640
the users are streaming and retaining better. So I've given up on showing something that, you know,

39:38.640 --> 39:44.400
you're going to click on as often. But what ends up happening is they get to show you things that

39:44.400 --> 39:48.960
maybe are tiny bit lower click-through rate. But once you do start clicking on them, then you keep

39:48.960 --> 39:54.880
coming back to them and they become habits. And then you're listening more on Spotify and you're

39:54.880 --> 39:59.600
retaining better on Spotify. And so that's kind of what we're going after. We're going after the long-term

39:59.600 --> 40:07.600
outcomes. It's okay to go from a click-through rate of let's say, you know, 47% down to 45% click-through rate.

40:07.600 --> 40:15.280
What I'd much rather say is, okay, but then two months later, I'm actually getting more listening

40:15.280 --> 40:21.440
total even though my click-through is lower. Because I'm actually building longer-term habits and

40:21.440 --> 40:27.280
coming back to that same podcast. It's a habitual podcast. It's a new way of engaging with Spotify.

40:27.280 --> 40:35.360
Maybe now I listen to a, you know, a meditation podcast in the evenings. And I listen to my,

40:35.360 --> 40:42.560
you know, my news podcast in the morning and my way to work. So all these new habits now have been

40:42.560 --> 40:48.560
added. They might have actually had lower click-through rate in the moment, but long-term they generate

40:48.560 --> 40:53.120
more engagement for that user. The user spends more time on Spotify and retains better on Spotify

40:53.120 --> 40:58.560
at the end of the 60 day or 70 day trial. And then we roll it out so that everybody gets that

40:58.560 --> 41:07.520
better experience. And so, is that cycle something that, you know, the models produced by this

41:07.520 --> 41:14.640
RL approach kind of been through that full cycle and, you know, that happens on some frequency,

41:14.640 --> 41:20.800
or, you know, given that we're talking about long-term value here, is this also a long-term

41:20.800 --> 41:27.120
assessment process? And the jury's still out on the models. You're liking what you're seeing,

41:27.120 --> 41:33.520
but it's not a full flage to commitment to this particular approach. So, yeah, that's it.

41:33.520 --> 41:38.960
That's a great question. So some of these things are actually fully rolled out in parts of our

41:38.960 --> 41:45.760
product. So we've got a fleet of machine learning systems. Sure. Some of them now are now completely

41:45.760 --> 41:51.040
on this kind of approach. So we tested it, AB tested it, it was a win, we rolled it out. And so now

41:51.040 --> 41:56.560
this is the default approach in some parts of the app. Other parts of the app were still testing.

41:57.760 --> 42:03.280
Other parts of the app we haven't even tried it out yet. So the approach has legs.

42:03.280 --> 42:08.320
It doesn't mean it works everywhere, but we're past the stage of just prototyping and trying

42:08.320 --> 42:14.800
stuff in simulation or offline. It's getting real users, giving it thumbs up. And actually,

42:14.800 --> 42:18.560
in some parts of the app, it's actually fully deployed as an approach.

42:18.560 --> 42:24.080
Yeah. So this approach that is kind of being pioneered on the research side, are there

42:25.680 --> 42:31.360
ways that your data scientists and machine learning engineers need to think differently about

42:31.360 --> 42:40.000
modeling to use the RL types of approaches or to kind of embrace what you're doing here? Or

42:42.240 --> 42:47.280
is it just another tool in the toolbox for them? So it's a great question. We're obviously trying

42:47.280 --> 42:53.200
to make some of these things, you know, let's say, easy to reuse and try out in different places.

42:53.200 --> 42:57.760
So you're not starting from a blank slate in other parts of the product. What do you want to try

42:57.760 --> 43:03.760
out these techniques? But really, this is a multi-disciplinary endeavor. We worked first off with

43:03.760 --> 43:10.800
researchers and we spoke to users and got user research even to tell us people liked this idea of,

43:10.800 --> 43:15.600
you know, recommendations that aren't just click-vading for the moment, but they're actually great

43:15.600 --> 43:21.200
for the long term. And then we took those intuitions, fleshed out research prototypes,

43:21.200 --> 43:25.920
and those prototypes have to look promising. We brought in engineers who could build the

43:25.920 --> 43:31.360
scalable productionized versions of them. We A, B test them, we get data scientists to look at

43:31.360 --> 43:35.520
those results. The data scientists say, okay, this is what's happening to the metrics. This is how

43:35.520 --> 43:39.520
they move. This is what we recommend use this setting of this algorithm. That's what we would

43:39.520 --> 43:44.640
recommend rolling out. You know, and product managers are also involved. So it really is

43:44.640 --> 43:50.400
all the expertise is coming together. And it's not just researchers doing great research and

43:50.400 --> 43:54.720
then throwing it over the fence. They really sit down with the engineers, the data engineers,

43:54.720 --> 43:59.760
backend engineers, machine learning engineers. So I would say that's kind of the Spotify way.

43:59.760 --> 44:06.000
It's bring all the skills necessary for the problem to the table. So then we go from end to end,

44:06.000 --> 44:13.760
we go from an idea to an actual user productionized value add. Awesome. Awesome. Those are great

44:13.760 --> 44:20.240
case study and real world applicability of reinforcement learning. It's been an interesting topic

44:20.240 --> 44:27.680
for folks for a while. And I also hope is the first of many conversations. I'd love to have

44:27.680 --> 44:33.280
you kind of close us out with, you know, just what you're most excited about in terms of the future

44:33.280 --> 44:37.840
of this particular work where you think that goes. Well, I mean, I think reinforcement learning is

44:37.840 --> 44:44.240
about doing things with, you know, human feedback and what really matters to people for the long term,

44:44.240 --> 44:49.600
not just building algorithms like click optimization algorithms, which is maybe where the internet started,

44:49.600 --> 44:55.760
but where's it going? It's going to, you know, get the long term user feedback and human feedback.

44:55.760 --> 45:00.320
And we're trying to do that now. It's Spotify. We're seeing other companies do that, you know,

45:00.320 --> 45:06.720
for example, people are fine tuning with RL. They're large language models with human feedback. So

45:06.720 --> 45:10.720
they start to do more intelligent things. So we're really viewing reinforcement learning as a way

45:10.720 --> 45:17.360
to incorporate more valuable human feedback in how the algorithms behave. And I think this is

45:18.400 --> 45:23.600
maybe now a nice and fletching points where RL is moving out of the textbooks into the real world

45:23.600 --> 45:28.720
more and more. So I'm very excited that it will help us build algorithms that are actually more

45:28.720 --> 45:37.360
long-term intelligent and not just kind of clickbait, like myopic click chasers. That's awesome.

45:38.560 --> 45:42.640
All right. Well, thanks so much, Tony. I really appreciate you taking the time to chat with us.

45:42.640 --> 45:58.560
Thank you, Sam. What's up? I'm talking to you today.

