Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Hey everyone, Sam again with another quick Twimble
Con update. One of the things that's been especially exciting to see is the number of organizations
sending multiple people in some cases entire teams to Twimble Con to learn about scaling
and operationalizing machine learning. A full third of the companies attending are sending
groups in many cases three four and five people. This is awesome. Seeing so many teams attending
is a great indicator that folks really see the opportunity associated with improving the
efficiency of their data science and machine learning operations and are excited about
the conversations we'll be curating at the event. If you'd like to attend Twimble
Con with your team just reach out to me at Sam at Twimlai.com and let's make it happen.
Of course you're welcome to reach out to me if you want to attend as an individual or
just head over to twimblecon.com slash register to sign up.
I hearty thanks to our friends at Qualcomm for sponsoring today's show. As you hear in
my conversation with time in Qualcomm has been actively involved in AI research for
well over a decade leading to advances in power efficient on device AI through efficient
neural network quantization and compression and more. Of course Qualcomm power some of
the latest and greatest Android devices with their Snapdragon chipset family. From this
strong foundation in the mobile chipset space Qualcomm now has the goal of scaling AI across
devices and making it ubiquitous. To learn more about what Qualcomm is up to including
their AI research platforms and developer tools visit twimlai.com slash Qualcomm.
All right everyone I am on the line with time in Blunk of War. Time in as a staff engineer
at Qualcomm leading the compression and quantization research team. Time in welcome to this
week in machine learning and AI.
Hey, it's great to be here. Nice to meet you all.
Great to have you on the show. I'm looking forward to our conversation and digging into
the topic of quantization and compression. But before we do that, I'd love to hear
a little bit about your background.
Yeah, so I have a background to bachelor's degree in mathematics from Light University.
And then I moved to Artificial Intelligence in Amsterdam and during my master's I started
my own company together with the Professor Maxwelling which been on the show before I think.
And after, yeah, and that was quite successful. So our AI started got acquired by Qualcomm.
And now I'm here already since two years now working in Qualcomm.
Awesome. What was your role with with Cypher?
So at Cypher, we're basically co-founder officially called CTO. I did a lot of the technical
stuff leading the technical team. But I also did a lot of like going to different companies
talking to them about their problems that we could potentially solve a deep learning,
not stage a lot, doing some sales. I mean, if you feel like a small starter, you have to
do everything, right? So I did everything.
We're a lot of hats. Absolutely.
Yes, exactly.
And so Qualcomm now your focus has it been on quantization and compression research the
entire time or have you done a variety of things in a couple of years?
Yeah, that's been it for me. So basically two years ago when I joined, I found this topic
of neural network compression and quantization incredibly interesting. So I started working
on that both on the research and in the application end. And then as I went, the team was growing
and the topic became more and more important. So yeah, basically the last two years of my
life have been making neural networks more efficient.
Awesome. And so quantization and compression, what are defined for us this topic and are
they the same thing or are they different things? What are those terms mean? Let's start
from the top.
Yeah, that's good. So in general, what we're talking about is deep learning in an efficient
way, right? So instead of having very big large neural networks that's a convry run on,
for example, a cell phone, we want to make them as small as possible as they're very
efficient and through very, very power efficient that they are quick and fast, et cetera.
So there's a couple of ways that you can do this, right? You can take a network and make
the architecture more efficient, for example, or you can optimize the kernels that execute
or you can develop like a specific hardware from making a neural network more efficient.
And specifically what we're looking into is, well, also of course, those topics are
quite common, looking into all of those things. And specifically, my team is looking
compression and quantization, compression being, let's say, take any pre-trained network.
So a network that somebody trained for a task, you spend a lot of time on this, right?
And then comes the question, like, how can we run this more efficiently? And with that
question, you want to make sure that that same network that you have, and that you've trained
to put a lot of sweat in, and that you make that one as small as possible. So the idea
with compression is to remove individual weights, or perhaps even better, remove complete
feature maps, complete neurons, complete convolutional channels from your network to make
it more efficient, right? And parallel to that, another way of making a network more efficient
is quantization. And normally, when you train a neural network, it's done in like floating
point 32. So that means that every number is like 32-bit value. Now, you can calculate
a lot more effectively and efficiently. And you have a lot less memory transfer and energy
consumption, if you would use less bits for each of the weights and the activations in your
neural network. So the idea is that you can do calculations in 8 bits, quantize your
whole network, both the weights and all the operations in between to 8-bit operations.
And then you're a lot more efficient. So both work together, compression quantization,
to kind of skill models down. And sometimes you can do this with like a factor 80 that your
model becomes a lot and a lot smaller than the original model that you started with.
And now, how dependent on that compression factor is the specific model that you're working
with? So like, you mean how much you can compress it?
Yeah, absolutely. Yeah. Yeah. Yeah. Yeah. Yeah. It's very important what Motivus
start with. I think there's this running joke in the compression literature that everybody
always tests their models on like VGG because the VGG architecture is so inefficient that
you always get these huge numbers like, oh, I compress my models by a factor 20 or something
like that. So those models can be really, really efficiently compressed. More recently,
the smaller models are not as you can get those big compression numbers. But very frequently
we've seen, for example, MobileNet V2, which is already a very efficient fast network
can still be compressed by factor two sometimes. And that's not affected to less energy that
you're using, right? Or speed up into your latency. So that's quite significant.
And so a lot of the networks that I've heard you toss out are our CNNs. Is that the primary
place that we're focused on quantization and compression? Or does it really span the
gamut in terms of the types of models? Yeah. I think like any model that you want to run
on a phone or you want to run efficiently the clouds, be it like neural network translation,
image models, computer vision models, like cement limitation, like for all of these,
it's very, very, very important to run them efficiently. I think there's like a general
tendency in the research and compression quantization that they're focusing on computer
vision architectures. And there's some literature from like RNNs and transformers, but it's
a very large amount. But in general, what's driving that? Is that because they're starting
out very inefficient or big or power hungry? Or is it just that's what we're using more
of? I think it's also what we're using a lot of, right? Like if you look at deep learning
algorithms on the phone, yes, there's some like neural machine translation models, etc.
The large bill of data is starting in the cloud, but a lot of camera algorithms are already
on phones nowadays. So I think most of your efficiency work because of that is in the
computer vision area. And also if you look at conferences, right? Like the biggest conferences
are computer vision conferences. And those people do a lot on these efficiency compression
kind of algorithms. And so I think it's just because of the necessity of the market.
But slowly bit steadily, once more algorithms make their way to efficient devices, I think
we'll see like more work on like sequence models and other types of models.
And so the impression that I'm getting from hearing you describe compression is that,
you know, I may start with kind of an off-the-shelf model for computer vision like a VGG and then
kind of train it to, you know, solve whatever problem I'm trying to solve and then put
it through this compression process as opposed to starting with a compressed network and
train it. Is that correct? Or is the way that we're trying to get to smaller computer
vision networks starting by training big networks and then compressing them? And, you
know, as an alternative, is there a path to like just starting with the smaller network
architecture and training it? And if not, you know, why doesn't that work? What are the
issues associated with the way we approach this problem?
I think that's a great question, very fundamental to the kind of work we're doing and deep learning
in general, perhaps. And so basically, if you want to do that like make a efficient network
architecture, I would always start with just making your network smaller for something
I got, right? You know, you don't want to have a network that's like 20 times too big
and then say, okay, now we're going to compress it. I would rather say, well, if that's
the case, just tweak your architecture in some kind of way and get a more efficient architecture
and use that. But then afterwards, like if you have this efficient already efficient
architecture like a mobile network, for example, or an efficient net, it still helps a lot
to compress it afterwards. And there's this theory, like this theoretical paper got a
lottery ticket hypothesis that was recently published. And this kind of positive also proves
to a certain extent that you're better off like training a large neural network, something
that's like over parameterized. There's a lot of filters and a lot of parameters at training
that first and then making it smaller afterwards. And that's generally better than training
that smaller resulting architecture from scratch. So I think currently literature or the
quicker consensus is that it's better to do that. Just start with the big model and
then make it smaller. And why is that? What is that paper at least suggests is the reasoning
for that? The paper suggests that basically you want to find like a lottery ticket. So
the lottery ticket is basically a network architecture that's very good for your problem.
And every feature will have like a certain probability of being a good feature to use.
So what the neural network is doing during training is that you want to find features that
are initialized close to good features and those will be used. And then during training
the rest will be like pruned away or like removed or set to zero because of your regularization.
And now the more parameters you have, the more feature maps you have, the higher the
probability you have of finding those optimal features that you can find. So because of
that, you're better off training with a large network that is very of a parameterized.
You have more shots that find the correct architecture and then removing or regularizing
away the smaller ones. So each year parameters is buying a lottery ticket and a network that
works well for your problem is a winning lottery ticket. And so they're theorizing that
it's better to start with a winning lottery ticket and find a way to make it better than
it is to start with a smaller network fewer shots at finding that winning lottery ticket.
Yeah, exactly. It's like as if you were looking at, for example, you have a hole in
the big, in the big table and you can, you can randomly distribute balls over this table.
And if you want one ball in the table, you want to minimize the distance that takes you
to roll the ball from one part of the table to the, to the, to the hole, then you're better
off having 50 balls in the table to and then pick the smallest and the closest ball and
putting that in the hole, then you might have to move a few centimeters rather than picking
one ball, putting it randomly on the table and then you have like a very large distance.
So you're basically, you're trying to minimize the distance between your optimal features
and what you initialize.
And so this, the lottery ticket hypothesis kind of flies in the face of your previous
advice, which is to not start with something that's massive and, and start with something
that's smaller. How do you rationalize those two perspectives?
Yeah, so I think it's, it's a part of skill. Like, I think if you started the network
that's like 20 times too big for what you're eventually want, then you get to this point
where it's actually pretty difficult to create algorithms that compress these networks
efficiently.
At least on my experience, the current tools that we have to compress networks are not
very well equipped to prune a network by such significant amounts, because you have to
imagine that that removing parts of the network and, and then training it while removing parts
is also a very difficult task for the network to do, a lot of discreet optimization going
on there, it's very difficult to train networks and prune them at the same time.
So those numbers are too big, but definitely if your network is two times or three times
too big for the original size, then yeah, just train it on the larger size and then prune
it down and make it smaller.
And so how, how do you know if the network is two or three times too big or 20 times too
big?
I think that's trial and error. So, I mean, one of the most basic things that you can do
is just stake your network that you've already designed and see what happens if you remove
the amount of channels by a factor half, like you reduce everything by 50%.
Then I remove and then reduce it to 25% or 12.5%, etc.
And that's kind of already, if you're a pretty good idea of how over pyramids write
your network is.
Yeah, it's funny.
I talk to a lot of people, particularly folks that are on the applied side of things,
so they're not in research, they're not trying to develop new network architectures,
and 90% of the time what they're doing is they're picking up some standard network architecture
off the shelf, training it, and it kind of works.
And they're like, thumbs up, hey, we've got something that works here.
Yeah, let's go.
Let's put it in a product.
Let's put it in a product, right?
And so, granted, most of the time, they're not working on putting it in a phone, but
let's say that they are, they've developed something that can look at a medical image
and identify some kind of feature in that medical image, fairly successfully on a computer,
and they want to get that onto a phone, and they've trained it using something off the
shelf like a VGG or a ResNet, what are the specific steps that they would take to get
from there to a model that is more efficient running on that phone?
Do they toss that out and start from scratch on something smaller and then optimize that,
or do they kind of play with it in certain kind of ways, walk us through the way that
you should approach this?
I think generally those types of architectures are already quite, well, I'm part of VGG,
of course, but the ResNet architectures are generally already quite efficient for the
task that they're designed for, and the classification, or cement expectation, or object detection.
For those of you who start with compression, just take your model that you've created,
and then there's multiple algorithms that you can apply, things like tensor factorization
algorithms that decompose like one layer into multiple layers, or channel pruning type of
architectures where you're actually removing complete channels from your network, and there's
some libraries out there, right, and a lot of papers on these topics that you can look
into.
So apply these two type algorithms, and you can make your model two or three times smaller
than original.
That's generally what we see for most applications.
You mentioned a couple of these algorithms, tensor factorization, channel pruning, are
there other major ones?
I think those are the large categories.
I mean, it kind of really depends on what you're compressing your network for, right?
Are you compressing it for latency?
Are you compressing it for power consumption reduction, or do you just want a smaller model
to transfer to some other device?
But generally, if you're looking at things like latency or power consumption reduction,
then yeah, those are the major ones for compression.
And afterwards, the quantization step is arguably even more impactful, and then even bigger
numbers in terms of efficiency.
Before we get to quantization, let's stick with compressing here, so you're compressing
for latency or power, and you've got these couple of methods, tensor factorization, and channel
pruning, are they, are these kind of blunt tools that you're just kind of throwing at
your model, like you, you know, go to to GitHub or something and download a tensor factorization
script, and you just run it and, you know, it spits something out, or is this a process
that you're kind of exercising with some degree of care?
Yeah, so one of the interesting things that we're working on is to make all of these
kind of algorithms work very easily out of the box, so that you can just apply it to
any, any type of algorithm, right?
Any type of deep learning network, and then without any efforts actually doing this.
But this is still not completely solved yet, there's not a really a lot of good libraries
that you can just download and use, and then without any hassle compressing your network.
The generally it's still a process of applying these techniques, either factorizing your
rate matrix or removing some channels with some smart algorithm, and then doing a lot
of fine tuning and a lot of handcraft tuning on top of it.
So I wish it was as easy as just downloading a library and applying it, but generally it
still takes a week or a few weeks to get this done properly.
Okay, so you need to understand a little bit about these different techniques in order
to actually apply them at this point.
So maybe walk us through tensor factorization, you know, when you're starting this process,
you know, what is it that you're actually doing?
How do you need to think about the process?
Yeah, sure.
So the tensor factorization is basically a deal of decomposing your weight tensor into something
smaller.
And then in the end, what you end up with is instead of having one layer where you just
do one convolution, for example, or in the case of RNN's one matrix multiplication, is
that you do two smaller convolutions or two smaller matrix multiplication.
That's the general gist of it.
So what do you do?
Well, you take a network, then you have to somehow decide for each of the layers in
your network how much you want to prune them, so how much do you want to reduce them in
size?
There's some smart algorithms for choosing this setting.
And I have to have done that for each of the layers, you compress it, basically by applying
SVD on it.
So SVD is like a very standard algorithm of doing tensor factorization, especially for
convolutional neural networks.
SVD leading singular value decomposition.
Exactly, that's it.
Yeah.
So you basically apply a single value decomposition on it, gets two smaller sub matrices
with a lower rank.
And then you have two layers instead of one, and after you've done that for the whole network,
you fine tune it for multiple epochs to get back to the original accuracy.
That's generally how that's done.
Okay.
And you mentioned there are some algorithms to tell you which of the layers in your network
you want to apply this to.
What are those algorithms and what's the general kind of intuition behind where you want
to apply this?
Yeah.
So I mean, there's a couple of them.
I think one of the most recent ones that's interesting is called automatic model compression
or ANC, which is from Songhound Group at MIT.
And this algorithm basically applies a reinforcement learning algorithm on top of the network.
So it tells you, based on reinforcement learning, every iteration gives you a value, namely
what is the accuracy of your network after compression.
And your reinforcement learning agent optimizes the primary shows for each layer.
So how much am I compressing each layer, such that your accuracy still stays very high.
So that's like one way to do it, for example.
And is it is it deep reinforcement learning or is it more traditional RL?
It's like an extra critic model.
It's not very deep.
Okay.
Got it.
Got it.
Okay.
Pretty standard, but still recent.
Okay.
Yeah.
That's one way of doing that.
Are there others that are worth knowing about?
Yeah.
So one of the things that works very well is just looking at the singular values of your
layers, if you're doing tensor factorization, for example, that also works very well.
You're basically looking at the residual energy.
So I don't know how technical or in depth I should go here, but let's do it.
So let's go deeper.
Well, just look at the singular values, I think that that's as deep as we, we perhaps
should be going.
So the singular values are like a rough indication of the residual after doing the tensor
factorization.
So you're basically looking at the forbidden norm of your rate matrix or even better of
your activation, set of activation.
So generally you fit some data to your network, you look at the output tensor for that, and
then you decompose that.
And so you can look at the residual, basically residual energy, so that the singular values
give you an indication of that.
But now we're applying this to determine which of the layers of our network we want
to apply SVD2, I thought it sounds circular here.
Now it's not really a witch layer, so every layer you probably want to decompose, in
terms of all layers should have some redundancy in them, but some more than others.
Sometimes you have a neural network layer that is very redundant.
It almost didn't learn any useful features.
Sometimes you can even completely cut out layers out of the network, right?
Especially residual networks that happen sometimes, if they're very deep.
So it's really, what you want to do is try to figure out what the prune ratio is for
each of the layers.
So for each layer, how much should I reduce it?
Yeah, and so you're applying this layer by layer, but in order to determine these prune
ratios, you need to be looking across different layers to try to get at what the redundancy
is of a given layer, as opposed to within layers independent of the other layers.
Is that right?
Yeah, sort of.
Yeah.
And sometimes you can look at the layers independently, but it's definitely better to look
somehow at the data distributions of each of the layers.
And you're most better off starting at the start of the network, putting that one first
seeing what effect it has on the whole network, and then doing it iteratively from the start
to the end of the network.
Yeah, okay.
Got it.
Yeah.
Okay, so that's center factorization, and then there's channel pruning.
Yeah, that's a channel pruning is basically the same idea.
So except for you don't do any factorization, would you say, hey, which channels from a
network can I safely remove?
So I mean, every network is going to have some redundant channels.
If you look at a standard rest next architecture from PyTorch, for example, you'll find that
some of the layers in there, you can remove half of the features and not decrease your performance
at all.
So networks generally have a lot of channels that are useless.
And so when we're talking about channels, we're talking about the width of the network essentially
at the demo.
Yeah, yeah, exactly.
Yeah, so input and output channels, right?
So each residual block, for example, has like 64 and 28 channels, and then a lot of
those can be removed without losing any accuracy or your network.
And so what are the kind of practices for determining which of those can be removed?
Is it kind of a most significantly significant ordering, and you can just kind of draw lines
somewhere and see how the network performs, or do you have to look at it on a channel
by channel basis to see what the importance of a given channel is?
Yeah, there's a few different indications.
So you can look at the weight magnitude of each of the, let's say you want to have
prune an input channel and you want to prune, let's say, 10 out of 128, and you can just
find the ones that in terms of magnitude, the weights of the input channels, the lowest
value, you prune those, those are generally the ones that are less, less, less important.
There's some ways of calculating the sensitivity of each of the input channels based on the
Hessian, or like a recent paper called eigendamage, does this based on like a chronic
factorization?
eigendamage, yeah.
That's a great paper name.
Yeah, it's an ICML paper from this year, it's a really cool paper, it's definitely recommended
to read.
So this looks at the sensitivity function, so basically you calculate the Hessian of the
illustration, and it says, okay, which ones are more important or which ones are less
important, right?
And then the least important ones you throw away.
And so you're throwing away these various channels, you made a comment about if you want
to throw away 10 out of your 128, are you generally starting with some idea of the number
of channels you want to throw away, and then kind of testing the, so starting with an
idea of the number, then determining of the, which do you throw away to get to that number
and then testing performance, and then just kind of iterating that loop to determine
what the best number is, or how many you can get away with throwing away.
Yeah, basically like that, or you could take this AMC approach, for example, to automatically
with reinforcement learning, figure out what the prune ratios are of the layer, you can
make it dependent on the sensitivity metric that I just mentioned, there's some different
ways.
So generally, how you do it, as you see now, let's test this out.
I want to compress my model by a factor two, right?
And then the algorithm automatically finds, or finds in some way how to set the prune ratios
for each of the layers, such that I guess to a latency reduction of save effect of two.
And then you take that, and then you remove those channels, make a model smaller, and it's
going to be two times smaller.
Yeah, yeah.
And so you mentioned that we're not quite at the point where, you know, we've got kind
of auto compression for these networks, and yet it sounds like a very kind of iterative
process that we could, you know, apply automatically to networks.
What's really in the way of doing that?
Yeah, that's a great question.
I think just the literature in compression is just very recent and very new.
We have to figure out like a proper relationship between, for example, what type of error do
we introduce in the network when we compress a layer?
And if we look at that error, how does it propagate over the rest of the network, right?
How does it leads to a loss at the end of the network?
Because generally, for example, you can compress a layer and exactly figure out how to compress
the layer and minimize the residual error that you get when doing so.
But sometimes errors are very impactful at the end of the network, right?
And sometimes errors are not.
And trying to figure these things out, I don't think that has been sensibly done yet to
really solve this issue of how to do this automatically.
And so maybe asking you a different way, if, you know, let's say compute wasn't an issue
and obviously it always is and that's a big part of the challenge, but let's imagine
a world where compute was free, you know, it sounds like what we're doing here is just
kind of, you know, a big loop of loop of loops kind of thing, it could be done iteratively
and you could reinforce it.
And so do we have any kind of intuition for, you know, if we started with some kind of
reasonable, you know, reasonable defaults or something for a given class of network,
like what the, you know, the ratio between the brute force compute that we would use versus
the compute we would use if we take a more reasoned approach.
Yeah, yeah, I mean, a completely brute force, which is be a prune every channel, a fine tune
for complete epoch for every pruning that you do.
And yeah, I mean, what skill do you want it? I think you can, I can create an algorithm
for you that's going to take like a hundred million years to do this and or cost you a million
euros.
But hey, if I can have this thing running for a hundred million years and it only costs
me a million euros, that sounds like a pretty good deal.
We're going to do it on a very efficient Qualcomm chip there.
Yeah, I have no idea about this question, sorry.
Yeah, yeah, yeah.
Okay.
No good intuition here.
Yeah, I guess, you know, I'm trying to get at like where the, you know, the intuition
is in this process that we just don't know how to encode versus, you know, what is just
kind of researchers and data scientists applying, you know, wrote lead that the computer
could do itself. And I don't feel like I've fully captured like what the intuitive process
is here that, you know, requires us to manually do this.
Yeah, I also don't know yet to be 100% honest.
So I think parts of this can definitely be be automated.
And they're like a set, for example, this, this aim sees automatic reinforcement learning
based algorithm is marshy automating it, right?
Right.
I'll be it with a reinforcement learning algorithm that that's just trying a lot of things
in a smart way, rather than as having a proper understanding of what it means for a layer
to, to add a certain amount of information to your, to your, to your presentation, for
example, or how much expressive power that doesn't, doesn't neural network layer or like
group of layers need to have to really do the job that it needs to do, right?
Mm-hmm.
These are all very, very good sport questions and deep learning at the moment I feel.
Yeah, yeah.
Well, I feel like deep learning there, you know, even before we get to compression, deep
learning has a lot of unexplored issues and we're, we're managing to figure it out and
automate a lot of it.
Anyway, so those are, that's kind of the compression side of things.
And we, you know, let's, let's walk through quantization in the same way.
So quantization is, what, you gave us a summary earlier, essentially trying to reduce the
number of bits we're using in each of these channels, right, as opposed to removing channels,
now we're moving bits per channels.
Is that a fair way to look at it?
Yeah, in essence, you're, you're reducing the amount of bits of your weights, no network,
and you're doing it for the calculations.
So instead of doing calculations or floating points, 32, you could do your calculations
in int8, right?
And then you're like four times more energy efficient and it comes to memory transfer
and you're going to be 16 times more efficient when it comes to doing the actual calculations
itself.
So just by going from floating point 32, where you're doing all the calculations, if
you have some kind of MAC array, let's eat it, like unscience or science or whatever,
then they're going to build up more efficient, which is one of the most impactful things
that you can do to make a network run more efficiently on devices.
And that might be easier.
Okay, yeah.
And at my sense of where things are with regard to quantization is that a few years ago,
when I talked to folks, I think my interview with Shibos and Gupta, a very early one,
he was talking about his experiences at Baidu building their neural machine translation
system and they were experimenting with quantization and it was a very manual process, right?
They had to spend a lot of time figuring out where in their networks, they wanted to
apply quantization, how to do it without introducing too much noise, all that kind of thing.
My impression is that over the span of a couple of years, we've come a long way from
that and a lot of cases, the infrastructure and the libraries, on top of that infrastructure,
TensorFlow, for example, gives us some of that stuff for free.
We can flip a bit and quantize our network and achieve reasonable performance, is that
the case?
And if so, in a qualified way, what's the envelope around that qualification?
What does that work?
When doesn't it work?
Yeah, so that's actually a very, very interesting question, something that we've been really deeply
looking into.
Yes, some networks can be very easily quantized.
If you pick almost all of the ImageNet train networks, even though ones that are like
an okay detection or semantic segmentation on top of the base network, many of those
can just be a bit quantized with your standard TensorFlow tools, sometimes you have to find
you in a little bit with your operations in place and train the network, but generally
those, those are okay.
And some networks seem to have a lot of issues.
You can look at mobile nets, for example, so most of the mobile nets out there, like
mobile nets V2, even if V3 version, if you just not easily quantize it with the TensorFlow
operations, everything breaks.
You could like 0% accuracy.
And sometimes, so we've also looked at things like ImageToImageNetworks, networks that
do, for example, like things like style, transfer, or super resolution, or have these
types of networks.
And then sometimes in network, these networks, you can't just eat a bit quantize, no problem
with all.
And some networks don't, at all.
And it's very difficult to put our finger on why exactly that is.
And so do we have any kind of intuition at all, or are we still, you know, we recognize
that there's this distinction between the different types of networks, but really have no idea
what or why it's happening.
Yeah, so in our recent paper called Data Frequentization, we've investigated a lot of these issues
on what, why some networks are very difficult to quantize.
One of the biggest issues generally has to do with the ranges that you send.
So you have to create some kind of quantization grids where you say, these are the values
I'm going to represent with the 8 bits that I have, let's say.
Now for this grid, you have to choose like a minimum and a maximum, right?
And generally everything, all of the points in between are equidistantly spaced, and those
are the values that you can represent, both for your weight center activations.
Now, how you choose that range, the minimum and the maximum is incredibly important.
For example, you could have some outlier with weights, and those weights then determine
what your minimum max could be, if you just set your minimum max of the quantization
range to the exact minimum of your weight tensor, there could be one big outlier somewhere,
and that could ruin the representative power for the rest of your weights.
So a lot of, you can only guy with a few points that are representable in the weights that
are small, and then the big one is taking a lot of the bandwidth.
So a lot of issues in quantization come from that, the fact that you set your ranges
in properly.
And the second one is that while you're quantizing, is that you can sometimes quantize
in a not-so-smart way.
For example, you could have a bias in your quantization, especially in mobile nets, if you have
like a three by three filter that maps an input to the output.
So by accident, you quantize all the values of your three by three filter down, then your
average output of that channel is going to be lower, and you're going to incur a lot
more loss than if everything was like randomly flipped.
So those are like two major issues when it comes to quantization.
Does the data free quantization paper that you referenced, does it propose an approach
for addressing these issues?
Yeah, definitely.
Yeah, so there's multiple things in there that we've looked into.
Basically, the data free quantization paper was born out of this idea that many methods
that help with quantization require a lot of effort, right?
So generally, how this works is that somebody tells you, okay, change this neural network
architecture to this architecture, and then it's easier to quantize.
Or do a lot of fine tuning on top of your architecture and a lot of training with a lot
of hyperparameters, et cetera.
Now if you want to make sure that like 8-bit networks can be run everywhere, it's very important
that this is as hassle-free as possible, right?
If I'm a cloud provider and I want to run my customers' models in 8 bits, I don't want
to require them that they're like, they got their PhDs in quantization just so that they
can work more efficient models.
You just want to make something that's basically you snap your finger and then have your
networks gone, right?
And it's like two times smaller.
So that's kind of the idea of the paper, and then we said like, what can we do to make
that even further, right?
Can we do this without using any data at all?
So many methods use data to set the ranges, et cetera.
And then the idea of our paper is like, okay, what can we do to make sure that this is
as easy as possible?
Because you don't even have to give data to the algorithm to quantize it.
And when you say give data to it, meaning the quantization happens as part of a training
loop or process, or is there some other process that is being applied?
Yeah, it's very often that, right?
So you take like a hundred batches, you run them through the network, you get some idea
of the activation ranges, right, that you get, that you get, and the activation ranges
help you inform what your mini or max setting is for the tenser that you're quantizing.
It's basically that idea.
Yeah, so give a couple of hundred batches to the algorithm to figure out those ranges.
Or to even find you in a little bit, right?
And so this paper proposes a way to allow you to skip that.
And avoid an approach that requires that you do this with regard to data.
Exactly, yeah.
And so the complete data free part is more of like a mental exercise.
It's very interesting for the, for, for, for like a paper, and we were able to get really
good results like state-of-the-art results on compressing things like mobile nets or some
object detection networks with even that hardware strength in place.
Now in practice, you can often just use a couple of batches and improve, improve your quantization
performance.
But basically the algorithms we, we put in the paper can be used for either case.
The most important part is that you don't have to do any fine tuning.
So this is the difference between like getting a network quantized within an hour or having
to spend like a couple of weeks with like an expert in the field to make sure that your
algorithms quantize properly.
And so how do the algorithms work?
Is it multiple algorithms or is there one algorithm?
Yeah, there's a couple that that we kind of stitch together to form like a complete quantization
pipeline.
So for this paper, one of the things we do is equalization.
So remember, remember these ranges that I talked about, right?
That sometimes these outliers can cause some of the issues.
Right.
Now, for networks that are Raylu or Kraylu-based, what you can do, you can actually move scaling
factors around the network.
So for layer one, for layer one, if you have, if you look at the output, you can multiply
that with a certain scaling factor.
And then for the next layer, which is the, which takes the, as input, the output of a previous
layer, you can multiply it with one divided by that scaling factor and you get mathematically
exactly the same network.
And by moving these scaling factors around, we can make the general min-max ranges.
So these, these big ranges, we can make them smaller, such that the network is easier
to quantize.
And so these min-max ranges, are you applying them on a, a tensor by tensor basis, or is it
across the entire network?
Yeah.
So there's multiple ways of doing this.
So depending on the hardware that you're looking at, you can do it in different ways.
But generally, there's two major categories that you do for quantization.
One is that you do it per tensor.
So each layer has its own set of min-max ranges.
So that that in turn translates to its own set of, offset vectors when you do all the
calculations.
But setting that aside, you can do it per tensor or something that was more recently introduced
by a guy called Ragooh-Krishnamurti.
And he introduced, sorry, he's from Google, he introduced per channel quantization.
So each individual channel has its own min-max range.
And that's all some of the issues.
Okay.
So presumably requiring a more complex training process because you have a lot more of these
offsets to keep track of.
I think the larger drawback of it, is that not all hardware supports it?
So you have to make sure that your hardware supports, and the kernels for that as well,
supports the calculation of per channel min-max ranges.
And not all hardware does that.
Okay.
So that dependent on hardware as opposed to the training process or something else that
can be taking care of and software.
I mean, if you run things on like a CPU, of course, you can see me at everything on the
CPU and we're in a efficient lane.
But if you have like dedicated hardware for inference, you're doing low-level operations
and pushing everything down to those low-level hardware modules for speed, then you're
kind of stuck with what they're able to do.
Exactly.
Yes.
Okay.
So that's one possibility.
And another thing we do in the paper is that we correct for this bias that I mentioned
before.
So sometimes because of rounding issues, you have to choose either rounding up or am I rounding
down, each of the weight factors.
You can kind of correct for some bias that you might introduce in doing that.
And that's an algorithm we dubbed bias correction.
Those two together really make most of the networks that we've tried make them very easy
to quantize.
Okay.
And are there any kind of dependencies on the training approach you're taking in terms
of, you know, things like, you know, whether you're applying dropout or like you're learning
rate, you know, if you're doing cyclical learning rates or anything like that, or is
this all independent of the way you train your network?
Yeah.
I think it's completely independent.
So this approach is a state of free quantization approach.
In essence, you can just take any network.
You can equalize it if there's like really impregnated in it.
And then the bias correction you can always apply.
And that should give you better quantization performance.
And we do it without any fine tuning.
So there's no hyperparameters.
You just, it's like a simple API call.
You just throw your model to like an API call.
You optimize it for quantization and then you get a quantized network.
It is hardware independent or are there dependencies on the hardware with this method?
Nope, not at all.
So it's all for a tensor quantization that we do in a paper.
So that's implemented in every hardware that I know of.
So yeah, that's that's optimizes it for any any architecture.
And so what kind of results did you see relative to other ways that you might quantize?
Yeah, so one of the problems that you have, for example, with mobile SV2 is that you
actually get no performance after naive quantization with like T of light, for example.
And we gain in our paper almost all of the performance.
So usually go from like 71.7% down to zero.
We go to 71.2%.
So that's only like half the percent loss when going from flow 32 to int 8.
And that's of course like going from flow 32 to int 8 is tremendous gain in terms of power
efficiency and in terms of latency.
And to what degree it is that result generalize to other models or is it just you know, just
working for mobile net V2, you know, sufficiently applicable that you think it's important.
I think any so we we we've also not papers in gains and residual net architectures.
We've seen it on multiple tasks like object detection, semantic segmentation.
So we really because when you're doing equalization, your network stays mathematically equivalent.
So there's definitely no loss.
And it can only help for quantization.
Similarly for for the bias correction, there's no negative drawbacks to this.
So in most settings, it will help you rather than there's like no real probability that
it's going to hurt you.
And so you can always test it and I think in many of the cases we've seen this far, this
re-helps a lot.
Awesome.
So, you know, given this paper, which is relatively recently and some of the other things that
we've discussed, you know, in terms of where the field is overall, what are you most excited
about?
Where do you see this all going?
The whole model efficiency at point you think, you mean?
Yeah, yeah, just in terms of our ability to, you know, take these models which are getting
bigger and bigger and have them work on devices which we want to last longer and longer from
the battery perspective and have user experiences that are faster and faster.
Yeah, I think all of this stuff on model efficiency is like key for the adaptation of neural
networks in many practical applications, right?
I think on the cell phone, we're always bound by power and amount of power that you can use.
Nobody wants to have their battery ranges in half an hour, right?
We all expect our phones to last the whole day and the same goes for robotics and even
for cloud players, if you could reduce your amount of energy that you're using by factored
too, that would be a tremendous decrease in the energy bill that you're paying.
So, I think a lot of our performance of algorithms is very restrained by, I mean, in practical
settings, it's constrained by how efficient our algorithms are.
So, the more efficient we can make our algorithms, from a compression point of view and a
quantization point of view, the higher the adaptation of these algorithms will be.
We all know these numbers also about how the training and network, for example, how training
and network is like neural architecture search costs like the same CO2 output of 10 jumbo
jets flying back and forth, like 10 or two trips or something like that.
If you can do quantized training, like training in 8 bits, we can reduce that power consumption
by a factor 4.
So, I think all these things are crucial for the adaptation of deep learning.
And are there specific directions that you're excited about, either in your research or
in the research that other folks are doing that you think are particularly promising in
helping us get there?
Yeah, definitely.
One of the important trends I've been seeing in the compression literature, some things
from Songhands lab or Vivian says lab at MIT, is coupling the hardware and the compression
and quantization and model efficiency together.
So that you can design algorithms, like deep learning algorithms specifically for hardware.
And then you can totally different results if you optimize for the CPU versus if you optimize
them for your GPU versus if you optimize them for like a DSP and a quantum device, for
example.
So, if you're looking at a CPU algorithm, because all of the calculations are that sequentially,
these architectures are usually very deep and have like three by three convolutions.
But if you create an algorithm that's optimized for the GPU, you generally get larger, larger
layers, like five by five, seven by seven convolutions, for example.
So, I think it's a really interesting trend where given the hardware that you're going
to run it on, can we optimize the algorithm to run efficiently on that?
I think that's really cool.
Cool.
So, it's a better marriage between the hardware and the algorithms that we have.
Uh-huh.
And hopefully, ultimately, this gets us to a point where as a model developer, I have
to think less about, you know, these kinds of issues and it all just kind of works for
me.
Yeah, especially for quantization, I think that's very, very much in reach.
So, some of my vision is that literally every neural network architecture can be run
in 8 bits.
Uh-huh.
I think that that's very possible.
Uh-huh.
And currently, we're seeing almost all algorithms are still being run in 16 bits, right?
Uh-huh.
So, getting that to 8 bits would be like two times more power efficient than many cases.
And then we can run a lot more AI in our phones.
Awesome.
Awesome.
Well, Simon, thank you so much for taking the time to chat with us about what you're
up to.
Yeah.
Thanks for having me.
It was great explaining all the compression and quantization stuff we've been working
on.
Absolutely.
Absolutely.
Thank you.
All right, everyone.
That's our show for today.
If you like what you've heard, please do us a favor and tell your friends about the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you don't
miss any of the great episodes we've got in store for you.
Thanks once again to Qualcomm for their sponsorship of today's episode.
Check them out at twomolei.com slash Qualcomm.
As always, thanks so much for listening and catch you next time.
