WEBVTT

00:00.000 --> 00:16.960
All right, everyone. I am here with Constantine Rush. Constantine is a PhD student at ETH Zurich.

00:16.960 --> 00:20.640
Constantine, welcome to the podcast. Thanks for having me here, Sam.

00:21.440 --> 00:27.040
Hey, I'm really looking forward to digging into our conversation, which will focus on a couple of

00:27.040 --> 00:37.920
your recent papers, both of them focused on RNNs. But before we jump into that, I'd love to have

00:37.920 --> 00:42.560
you take a few minutes and kind of share your background. How did you get started working in

00:42.560 --> 00:48.720
machine learning? Yes, absolutely. So actually, my background is mathematics. So I did my

00:48.720 --> 00:54.080
bachelor's in pure mathematics in Germany at the University of Bonn, which is a kind of nice place

00:54.080 --> 00:59.120
for mathematics in Germany. Then I did applied mathematics for my masters in the UK.

01:00.000 --> 01:05.280
And now I'm at ETH. I'm actually affiliated with the mathematics department here.

01:05.280 --> 01:11.200
And I'm doing my PhD in applied mathematics. And I'm focusing mostly on machine learning,

01:11.200 --> 01:16.640
of course. And also not only on classical machine learning, but also on machine learning for

01:16.640 --> 01:23.920
scientific computing, you know, to solve physical systems and so on. And yeah, so how did I get

01:23.920 --> 01:28.640
interested in machine learning? I think actually two years ago, I didn't even really know what

01:28.640 --> 01:35.280
machine learning was. So I really was interested, you know, in this classical applied mathematics,

01:35.280 --> 01:39.360
like doing numerics, solving partial differential equation, equations,

01:39.360 --> 01:44.560
domestic differential equations, and so on. And I even worked for almost three years for the

01:44.560 --> 01:50.800
German aerospace center. And there I focused basically as a student researcher, and there I basically

01:50.800 --> 01:58.000
focused on this classical scientific computing problems, you know, as I said, solving PDE,

01:58.000 --> 02:03.280
and so on, and geometric problems. And then during my time in the UK, when I did my masters in

02:03.280 --> 02:08.160
applied mathematics, I did basically two really nice courses. The first one was about dynamical

02:08.160 --> 02:15.920
systems, so nonlinear dynamics and chaos. And the second one was computational cognitive neuroscience,

02:15.920 --> 02:21.680
which was super awesome. And there was actually some kind of connection because in this computational

02:21.680 --> 02:29.600
cognitive neuroscience course, we took a look at, you know, mathematical models from neurobiology.

02:29.600 --> 02:34.960
So for instance, at the Fitsunagumo model, maybe you heard of it, probably your audience had.

02:34.960 --> 02:43.200
So it's basically the mathematical model of the firing of the action potential of a single

02:43.200 --> 02:49.520
biological neuron. And you know, you can describe it more or less as a so-called relaxation oscillator,

02:49.520 --> 02:55.520
and you can do all this kind of mathematical theory around that. And it was just super interesting.

02:55.520 --> 03:00.800
And so far, it had nothing to do with classical machine learning, you know, like classifying

03:00.800 --> 03:07.920
endless digits and so on. But then I started a project and I was searching for some literature

03:07.920 --> 03:13.760
about this connection between neurobiology and dynamical systems. And then I found a lot of

03:13.760 --> 03:21.040
very recent papers about dynamical systems theory in actual classical machine learning,

03:21.040 --> 03:27.600
meaning, for instance, using dynamical systems to construct architectures, deep learning architectures,

03:27.600 --> 03:33.280
RNNs or feed forward neural networks and so on. And I was directly hooked and I really liked

03:33.280 --> 03:39.680
these papers. And then yeah, I just started to read more, read more. I had my own ideas. I tried a

03:39.680 --> 03:47.600
bit and here. So that's basically my background. Awesome. Awesome. We should maybe kind of establish

03:47.600 --> 03:52.800
the definitions. When you say dynamical systems, what exactly does that entail?

03:52.800 --> 04:02.720
Yeah. So it's basically systems, which are time dependent. So basically you have two kind of

04:02.720 --> 04:10.720
systems, a continuous time systems and discrete time systems. And mostly in physics and biology,

04:10.720 --> 04:17.600
we use continuous time systems where you know, your time parameter is actually an element of some

04:17.600 --> 04:25.120
interval 0 to capital T, for instance. And you look at so-called ordinary differential equations

04:25.120 --> 04:33.440
or systems of ordinary differential equations. And I think they're quite famous. I mean,

04:33.440 --> 04:40.640
they're also used already directly machine learning like this neural ODE stuff and so on.

04:40.640 --> 04:48.960
And then there's discrete time dynamical systems where your time is this is more like an

04:48.960 --> 04:58.080
iterator. It's not continuous anymore. And so you don't have an ordinary differential equation

04:58.080 --> 05:05.200
anymore, but you just have some recurrent update of your so-called hidden state. So of the one

05:05.200 --> 05:15.280
you're propagating forward and time basically. Cool. So you took these classes when you were

05:15.280 --> 05:22.400
working on your masters in the UK and your current work is focused on the same field. Did you

05:23.040 --> 05:31.360
come to the PhD with these ideas, with ideas for applying dynamical systems and oscillators to

05:31.360 --> 05:36.960
RNNs or is that something that you developed more recently? No, actually I already had this

05:36.960 --> 05:44.240
kind of ideas before I came to ETH. But I started more or less, I had to pause them for a bit

05:44.240 --> 05:54.480
because I came here and I first focused on some more mathematics based papers and problems. So we

05:54.480 --> 06:03.600
did some sampling related kind of problems which are only applied in scientific computing and not

06:03.600 --> 06:10.000
in classical machine learning. But it was all the time curious and I tried so many things in my

06:10.000 --> 06:15.760
free time on the weekend and I had really good results. And so I talked to my supervisor,

06:15.760 --> 06:25.520
C.D. Mischra, and we decided to go a bit deeper and really try to figure out what is the reason

06:25.520 --> 06:35.440
for these good results and what is it modeling at all and so on. Great. So let's start talking

06:35.440 --> 06:42.800
about the motivation for the paper. What's the broad problem that you are trying to solve here?

06:42.800 --> 06:50.400
Okay, here. So the motivation was that for recurrent neural networks, so all we do is we

06:51.120 --> 06:58.560
suggest a new RNN architecture, recurrent neural network architecture, and the big problem for

06:58.560 --> 07:06.880
recurrent models RNNs is that it's quite hard to learn very long time dependencies and that means

07:06.880 --> 07:15.360
okay, for RNNs, if you apply them to data, it's always sequential data. So there's some kind

07:15.360 --> 07:23.120
of sequential internal dependencies in your underlying data. And for instance, if you have

07:23.840 --> 07:29.360
internal dependencies for long range, so for instance at the beginning of your sequence,

07:29.360 --> 07:36.080
you have important information which is needed at the very end in order to classify the sequence

07:36.080 --> 07:43.440
for instance correctly, that we would call a long time dependency. And now it also depends on how

07:43.440 --> 07:50.160
long this dependency is of course, right? So RNNs are typically very, very good for sequence length

07:50.160 --> 07:57.360
of 100, 200. If you go to a thousand, it's already super hard and not mentioning doing 10,000

07:57.360 --> 08:03.040
or whatever. And the problem behind that is basically the way you train your recurrent neural

08:03.040 --> 08:09.520
network. So what you do is you use backpropagation through time where you basically unfold your RNN

08:09.520 --> 08:19.520
in time. And while you backpropagate the errors back in time more or less. And if you would write

08:19.520 --> 08:26.800
down the full gradient you're interested in computing, then you would see that there is one term,

08:26.800 --> 08:34.800
which is basically just the influence of a hidden state from a previous time, let's say at time k,

08:35.680 --> 08:42.160
on some hidden state at a later time, let's say a hidden state n. And so it's basically partial

08:42.160 --> 08:49.760
derivative of your hidden state with respect to this hidden state of the previous time. And now if

08:49.760 --> 08:57.280
you look at this term and you basically apply the chain will all over again, you will have a long

08:57.280 --> 09:05.200
product. And this product has basically n minus k plus one factors in it. And so if you imagine

09:05.200 --> 09:11.760
if on average these these factors are a bit less than one, let's say they're 0.9, then it's

09:11.760 --> 09:17.280
basically just 0.9 to the power of n minus k plus one. And if n minus k is very long, very big,

09:17.280 --> 09:23.840
then you just go to 0 exponentially fast or you go to infinity if it's a bit bigger than one

09:23.840 --> 09:29.040
on average, right? If it's like one dot one on average, then you explode. And so this is called

09:29.040 --> 09:37.920
the Wenishing and exploding gradient problem because exactly of that. And it may sound, well,

09:37.920 --> 09:44.560
okay, then just stabilize this gradient in some way. But it's actually not that easy. So of course,

09:44.560 --> 09:51.440
if for instance, if you consider your recurrent model as just the identity, right, then your

09:51.440 --> 09:57.920
gradient will just be the identity matrix. Great. Now we're super stable, right? But we learn

09:57.920 --> 10:02.720
nothing basically. And so that's the point. So basically it's twofolded. On the one hand side,

10:02.720 --> 10:08.240
you want to have gradient stability. So the gradient doesn't blow up or Wenishing. But at the same

10:08.240 --> 10:15.120
time, you also want to be expressive that you can learn actual interesting data, which the identity

10:15.120 --> 10:22.240
map won't be helpful for, right? So we have some success with tech architectures like LSTM's

10:22.240 --> 10:28.960
and GRUs and addressing this. Absolutely. Absolutely. So LSTMs, what they do, they use some kind of

10:28.960 --> 10:34.960
gating mechanism. And if you carefully write down the gradient there, you will see that you

10:34.960 --> 10:41.040
actually have a chance to mitigate the Wenishing gradient problem. However, the exploding

10:42.000 --> 10:48.160
gradient will might still happen. So the gradient can still explode for LSTMs and for GRUs.

10:48.160 --> 10:54.400
And there's a lot of papers where they have used LSTMs for long-range long-term dependencies.

10:54.400 --> 11:01.200
And they kind of start to fail at a few hundred like 1,000. So if the sequence has like a length

11:01.200 --> 11:07.440
of 1,000, it's kind of tricky for the LSTM to learn that. But LSTMs are super expressive,

11:07.440 --> 11:13.680
so that I can say already from my experience. And I think that's one of the reasons why LSTMs

11:13.680 --> 11:20.800
are still the most used are in an architecture. Maybe, I don't know, maybe the most used architecture

11:20.800 --> 11:31.840
at all, I don't know. They are quite popular. Absolutely. And so, so you've got this, this exploding

11:31.840 --> 11:38.000
vanishing gradient problem with long-term sequences. You know, there are things like LSTMs,

11:38.000 --> 11:48.240
but they're not perfect. And so your approach is inspired in some ways by the year, you know,

11:48.240 --> 11:52.720
what you learned about the neurons and the oscillators and the neurons. Tell us, you know,

11:52.720 --> 11:59.600
how it connects to what you observe there. Yeah, absolutely. So our approach was really

11:59.600 --> 12:04.720
inspired by neurobiology. As I said before, if you take a look at the dynamical systems,

12:04.720 --> 12:09.840
which model this kind of firing of the action potential, you have this kind of oscillatory

12:09.840 --> 12:14.480
behavior, right? It's actually a relaxation oscillator where you basically accumulate your

12:14.480 --> 12:20.080
stimulus. And then if you surpass a certain threshold, you just fire and then you relax.

12:20.080 --> 12:25.200
And so you have this periodically, this oscillatory behavior more or less. And you see also this kind

12:25.200 --> 12:32.240
of oscillatory behavior. If you, for instance, consider full parts of the brain, for instance,

12:32.240 --> 12:38.720
the hippocampus, they're people call it, I mean, I'm not a neurobiologist, I can just tell you

12:38.720 --> 12:44.640
what I read. And they call it also hippocampus oscillations. And you can, they're actually very

12:44.640 --> 12:50.480
beautiful papers where they measure these kind of oscillations in vivo and in vitro. And yeah,

12:50.480 --> 12:57.920
that was basically the motivation. So something which makes kind of sense in neurobiology,

12:57.920 --> 13:04.960
but we don't try to exactly mimic this kind of behavior, right? So that's just the abstract

13:04.960 --> 13:11.680
essence. And now we just forget about all this complicated underlying biological behavior.

13:11.680 --> 13:16.480
And we just take the message, okay, using oscillators might be a good idea.

13:16.480 --> 13:23.520
Yeah. Yeah. And that's what we did. And I think if you think about oscillators, I mean,

13:23.520 --> 13:29.520
this kind of harmonic oscillators where you have some linear combination of sine and cosine

13:29.520 --> 13:36.320
as a solution, it's super stable, right? If your amplitude of the oscillations doesn't blow up

13:36.320 --> 13:41.200
or vanish, it's it's perfectly stable. And also, I mean, if you take the gradient, you again

13:41.200 --> 13:45.840
have some kind of, if you take the credit of sine, you have cosine and vice versa, right?

13:45.840 --> 13:51.680
And so even the gradient is very nice behaved or you would expect that it's very nice behaved.

13:52.400 --> 13:58.080
And coming from this neurobiological background, we were also kind of hoping that, you know,

13:58.080 --> 14:03.920
it's the kind of expressive because if that's something you can find more or less in nature

14:03.920 --> 14:11.120
and in neurobiology, then why not try it, right? And so we came up with this coupled oscillatory

14:11.120 --> 14:19.200
RNN architecture, which is basically a system of ordinary of second order ordinary differential

14:19.200 --> 14:27.040
equations. So second order ODE's modeling these kind of coupled oscillators, where we also have

14:27.040 --> 14:34.080
two to additional controlling terms in there. So as I said before, if your amplitude goes to infinity

14:34.080 --> 14:41.120
or goes to zero, then we basically got nothing, right? So we have to make sure that they are also

14:41.120 --> 14:47.040
kind of nice behaved. And we can do that actually by adding some some controlling terms in there,

14:47.040 --> 14:53.280
controlling the damping. So that means like over time, how much do you, do you damp your oscillations?

14:53.280 --> 15:01.440
And and a controlling parameter for the frequency of the system. And yeah, in the end, it's it's

15:01.440 --> 15:11.520
really just just a damped controlled and coupled oscillator. And we discretize that with a so-called

15:11.520 --> 15:16.880
implicit explicit discretization scheme and IMAX scheme, just to make sure because we are not

15:16.880 --> 15:22.240
working with the continuous formulation of the system, but with the discrete time formulation of

15:22.240 --> 15:28.400
the system. And to make sure that we have all these structure preserving behavior in our discrete

15:28.400 --> 15:36.160
time system now, we use some kind of well-behaved numerical discretization scheme for ODE's,

15:36.160 --> 15:41.680
which is just this IMAX scheme. And then we come up with this discretized ODE, which we interpret

15:41.680 --> 15:49.680
as an RNN. And yeah. When you describe the oscillators as coupled, what exactly does that mean?

15:49.680 --> 15:56.800
Okay. Where are they coupled? Yeah. So the system has many dimensions, right? So basically,

15:56.800 --> 16:06.640
each dimension corresponds to one neuron in the recurrent titan state of the RNN. And so typically,

16:06.640 --> 16:13.200
for for normal problems, you use like 128 dimensions or 256 dimensions, which would mean you have in

16:13.200 --> 16:20.640
your, we are only talking for one layer RNNs right now, right? And so you typically have 128 hidden

16:20.640 --> 16:30.320
neurons or 256 hidden neurons. And coupled means that the information from one dimension can go

16:30.320 --> 16:38.000
also in the other dimension. So that means basically the hidden weight matrix we have is not

16:38.000 --> 16:46.240
sparse or has any any weird structure. It's it's it's a simple dance matrix visit.

16:48.480 --> 17:01.280
And you say you you have this system of of coupled oscillators. You write those as ODE's. And then you

17:01.280 --> 17:08.880
express those as an RNN. How does that last stage work? Is that a kind of straightforward

17:10.000 --> 17:16.240
expression of the ODE as RNN or is there, you know, are there work or tricks that you need to do to

17:16.240 --> 17:22.000
to be able to do that? Yeah, that's actually a really good question. So no, it's it's not

17:23.040 --> 17:28.720
so at least to me, it was not directly totally clear how to interpret that as an RNN. And there are

17:28.720 --> 17:34.400
different choices actually to do because for instance, you have something like a time step for

17:34.400 --> 17:40.320
your discretization scheme, right, some some kind of DT, which is just a small, small parameter.

17:40.960 --> 17:47.120
And the question is how to treat this parameter, for instance, is it a hyperparameter of your

17:47.120 --> 17:52.960
RNN architecture? Do you want to train it? You if you want to train it, you have to constrain it in

17:52.960 --> 17:57.680
some way because it shouldn't be too big and it shouldn't be negative because you don't have negative

17:57.680 --> 18:02.240
time, right? You want to be have something bigger than zero, but you also don't want to be too big.

18:02.240 --> 18:08.800
So there are actually many, many questions. And well, we went for the easiest first answer,

18:08.800 --> 18:13.440
I guess, we just treat them as a hyperparameter. And also our two controlling parameters I just

18:13.440 --> 18:19.680
mentioned are two hyperparameters. So in the end, we have an RNN with three hyperparameters,

18:21.440 --> 18:26.960
which you do not really need. So you can actually treat them as trainable parameters too,

18:26.960 --> 18:31.600
but you have to constrain them in certain ranges. As I said before, for instance, by using

18:31.600 --> 18:36.640
sigmodel activation function, or really you, because also this controlling parameters should be

18:36.640 --> 18:40.320
just non-negative or positive. Let's say they should be positive.

18:43.520 --> 18:53.680
So you have your system expresses an RNN and then what next? It sounds like maybe the next step

18:53.680 --> 18:59.120
was doing some benchmarking against existing approaches for RNNs.

18:59.680 --> 19:09.280
Yeah, not directly though. So first, all I said so far was some kind of intuition that it might

19:09.280 --> 19:16.000
be stable. Also, the credience might be well-behaved, but the key feature of our, or the key goal of

19:16.000 --> 19:22.480
our paper is actually to show, to mathematically prove now that this gradient is actually stable.

19:22.480 --> 19:28.160
So we have two to main propositions. You can also call them theorems if you want to,

19:29.680 --> 19:35.520
which we actually fully prove. And the first one is basically showing that the gradient can't

19:35.520 --> 19:42.480
explode, giving some assumptions. And now many people, I guess in the audience, will be like,

19:42.480 --> 19:48.640
okay, assumptions. Now it's not going to be met. And you know, everything was garbage. But in this

19:48.640 --> 19:53.360
case, we can actually check these assumptions that they are very mild. So we actually, we check

19:53.360 --> 19:58.480
them for each experiment we did. And they were always satisfied during the whole training procedure.

19:58.480 --> 20:06.000
So it's nothing crazy. It just means that your weights of your RNN are not too big. I mean,

20:06.000 --> 20:14.080
in the end, if they go to infinity, then what's the point of using this RNN? So that's a reasonable

20:14.080 --> 20:20.320
assumption. And with this assumption, we can actually show that the gradient is bounded from above.

20:21.200 --> 20:26.560
That's the first theorem. And the second theorem was maybe a bit harder even to show.

20:26.560 --> 20:36.480
It's some kind of asymptotic expansion of this gradient, which shows that we have some

20:36.480 --> 20:43.760
terms, some sort of asymptotic expansion means that we have some expansion in some orders.

20:43.760 --> 20:49.760
And we choose, in this case, to use the time step dt, which is quite small. And we had some

20:51.120 --> 20:57.440
expansion of orders in this small time step dt. And we can see that the leading order of this

20:57.440 --> 21:02.320
expansion is of order, I think time step to the power of three over two, something like this.

21:02.320 --> 21:09.440
And this is actually independent of your sequence length. So it doesn't matter anymore if you have

21:09.440 --> 21:16.080
a sequence of length 10 or of length 10,000. This leading order term will always be there and

21:16.080 --> 21:20.000
will always be of the same order. So that means that the wenshin gradient problem is actually

21:20.000 --> 21:26.480
mitigated and for all sequence lengths, basically. And so to recap that part,

21:26.480 --> 21:35.440
essentially, because you've got this specific form of RNN based on the coupled oscillators,

21:35.440 --> 21:47.120
you're then able to produce some closed form bounds on the, is it the convergence of the gradient?

21:47.120 --> 21:57.360
And that requires some assumptions that you make. And these are assumptions, they're not constraints,

21:57.360 --> 22:04.000
they're just assumptions. And by observation, you've not seen them being violated in kind of normal

22:04.000 --> 22:16.000
use of the RNN. Exactly. Yeah, absolutely. Yeah. And maybe one more point because it was actually

22:16.000 --> 22:23.760
quite interesting. So these two propositions, we proved in the full discrete case. So for the real

22:23.760 --> 22:32.800
RNN, because if you go discrete, some wild things can happen. So it's always a bit tougher to prove

22:32.800 --> 22:38.960
the discrete tastes and not only the continuous case. And so that's actually the case of the RNN.

22:38.960 --> 22:46.080
And interestingly, we also did the continuous case. And for that, we only can

22:47.120 --> 22:52.800
bound the gradient from above. So we can mitigate the exploding gradient problem. But because we

22:52.800 --> 22:56.800
had this expansion in this time step, right? And in the continuous case, you go with your time step

22:56.800 --> 23:03.600
to zero, because you want to be continuous in some way. We don't have this kind of feature anymore.

23:03.600 --> 23:09.440
So that's really the mitigation of the Wenzhen gradient problem is really a feature of the

23:09.440 --> 23:17.280
discretization and not of the oscillatory dynamics anymore, which I thought was quite interesting.

23:21.120 --> 23:28.960
And so I'm so very curious how this, what kind of result you saw and how it compares from a

23:28.960 --> 23:34.560
performance perspective? Yeah, that's yeah, that's even more interesting. That's true.

23:35.520 --> 23:42.480
Because if you have a good theory, okay, that's nice, but it also should work in real life, right?

23:42.480 --> 23:46.800
We have things like LSTMs that we know how to use and they work pretty well.

23:46.800 --> 23:57.600
And so the question is, we've got, like you said, you've got theories on the bounds of the gradients here,

23:57.600 --> 24:04.960
but is it useful? Yeah, absolutely. So we applied it to many long-term dependency benchmarks.

24:04.960 --> 24:11.120
So the typical benchmarks, for instance, the first one, which I really liked was, and it's already

24:11.120 --> 24:17.280
quite old, so it's the adding problem. And it was first proposed in the original LSTM paper, actually,

24:17.280 --> 24:27.200
and it's just a synthetic sequential, no, it's actually regression. And the idea here is that you

24:27.200 --> 24:34.000
have sequences of arbitrary length, and they're two-dimensional, and the first dimension are just

24:34.000 --> 24:40.480
uniform random numbers. And the second dimension are all zeros, except for two positions,

24:40.480 --> 24:47.280
they're set to one, and these positions are chosen randomly, also uniform randomly,

24:47.280 --> 24:55.200
and they are chosen in both half of the sequence. And now the goal of the RNN,

24:55.200 --> 25:03.440
especially to the output should be the two random uniform numbers of the first dimensions

25:03.440 --> 25:08.800
at the position indicated by the ones in the second dimension. And so it's just adding them

25:08.800 --> 25:14.960
together. So they call it adding, and the nice part here is that we can, you know, use sequence

25:14.960 --> 25:22.080
length of 100, we can use 1,000, and we can make it harder, harder, harder. And actually, we tried

25:22.080 --> 25:29.600
until a sequence length of 5,000. I think, actually, I've not seen that before in another paper,

25:29.600 --> 25:36.080
so normally what you do is you try 50, 100, 200, 500, something like this. Some people go up to 1,000,

25:36.080 --> 25:42.800
and you can see that, for instance, LSTMs, they already fail at 500, 750, so they are not

25:42.800 --> 25:50.080
able to converge anymore, so they can't effectively learn this kind of problem. And for corn,

25:50.080 --> 25:54.800
we actually had that, even in the case for 5,000, you get more or less direct convergence,

25:54.800 --> 26:03.440
which is really nice, yeah. Yeah, I really like it. Any interesting observations in terms of

26:03.440 --> 26:16.480
computational complexity with this approach? Yes. Yeah, so efficiency, yeah, the question is

26:16.480 --> 26:23.600
in this task, actually, how long do you try, so how long do you let the RNN model learn, right?

26:23.600 --> 26:29.280
So if I'm saying LSTM fails for 500, it doesn't mean if you do maybe a million iterations,

26:29.280 --> 26:35.440
learning steps, it might converge, who knows, right? And the interesting part really was here

26:35.440 --> 26:43.600
that you need maybe, I don't know, in terms of hundreds, we needed maybe 10 of them, and we tried

26:43.600 --> 26:50.240
LSTMs at least for 600 hundreds, so for 60,000 learning steps, and it did not converge at all,

26:50.240 --> 26:56.240
and also the other important architectures, like this exponential RNN, which is basically some

26:56.240 --> 27:02.240
kind of unitary RNN, which comes from the idea that you want to constrain your hidden to hidden

27:02.880 --> 27:10.240
structure, and all these kind of famous architectures, they did not work so well, especially in the

27:10.240 --> 27:16.560
very, very long case, so that was actually quite nice. But of course, it's just a synthetic

27:17.600 --> 27:22.800
test, right? It's just a synthetic benchmark, so maybe no one cares about this in the end,

27:22.800 --> 27:28.480
so you have to try more real world data, of course, and maybe a second one, which was quite interesting,

27:28.480 --> 27:34.640
was something like, and that's not a long-term dependency task anymore, because what I said

27:34.640 --> 27:38.320
at the very first beginning, that okay, you want to have credit instability, you want to learn

27:38.320 --> 27:42.480
long-term dependencies, but you also want to be expressive, right? You want to learn complicated

27:42.480 --> 27:49.920
problems, and so we tried this task from, it's basically human action recognition, it's

27:49.920 --> 27:57.760
basically on a smartphone with the sensors measured six different activities, like standing up,

27:57.760 --> 28:05.280
walking, running, something like this, and the idea is that you have some time series measured

28:05.280 --> 28:12.160
from your sensors over time on your Samsung S3, I guess, was it? And so in the end, you want to

28:12.160 --> 28:20.080
classify, based on these action, action time series, what the people have done, and even there,

28:20.080 --> 28:26.720
we got extremely good results. I mean, it's not a huge benchmark, so it's hard to say it was

28:26.720 --> 28:31.360
outperforming everything, what is out there, but it was at least outperforming everything we were

28:31.360 --> 28:36.800
aware of, so all the papers that have done that, we were outperforming them, which was quite nice.

28:36.800 --> 28:43.680
Awesome, awesome. You're working on a follow-on paper to the corn paper. Can you tell us a little

28:43.680 --> 28:50.880
bit about that one? Yeah, so the follow-up is called Unicorn, but with a double N in the end,

28:50.880 --> 28:56.960
so it's really like a neural network, and it stands for undamped independent controlled

28:56.960 --> 29:04.720
oscillatory RNNs. It was kind of chosen, such that we have this kind of nice Unicorn name,

29:04.720 --> 29:12.960
you know, but that's, so the idea here is that instead of using coupled oscillators,

29:12.960 --> 29:19.840
we first uncoupled them. And so basically, if you have heard of this independent RNN,

29:20.480 --> 29:25.840
where they instead having a hidden to hidden weight matrix, which is just the full,

29:26.880 --> 29:31.520
hidden dimensions, time-hidden dimensions, dense matrix, you just have a vector,

29:31.520 --> 29:41.760
and you just multiply it. So then you don't have any interconnection between the different

29:41.760 --> 29:46.080
neurons anymore, between the different hidden neurons, or in terms of dynamical systems,

29:46.080 --> 29:51.680
you don't have any interconnections between the different dimensions. So in the end,

29:51.680 --> 29:57.760
what you do is you just solve for each dimension, you solve an independent system, you don't even

29:57.760 --> 30:03.200
have to do it on the same computer. So that was one of the points, so you can solve one dimension

30:03.200 --> 30:08.320
on this computer and another dimension on the next computer. And because of this kind of

30:08.320 --> 30:15.280
independency, and we are undamped, so we don't have some damping term in there anymore,

30:15.280 --> 30:23.120
we can show that this system is actually a Hamiltonian system. And if you discretize these

30:23.120 --> 30:29.440
kind of systems with some symplectic integrators from Hamiltonian mechanics, also people in

30:30.480 --> 30:35.040
molecular simulations are very interested in that because they model basically everything

30:35.040 --> 30:44.000
with Hamiltonian systems. And so the numerics to use here is so-called symplectic integrators,

30:44.000 --> 30:48.400
and you can show that you more or less end up if you integrate them with the discrete time

30:48.400 --> 30:53.040
Hamiltonian, which is quite close to your actually continuous time Hamiltonian. And the nice feature

30:53.040 --> 31:00.800
of Hamiltonian systems is that they are invertible in time. And that means now that, for instance,

31:01.760 --> 31:07.600
if you train it with backpropagation through time, as I said, right at first you propagate

31:07.600 --> 31:13.360
forward in time your input, and then you have at the end some output, and then for backpropagating,

31:13.360 --> 31:19.840
you propagate this error back in time. And for that, you have to store each hidden state at every

31:19.840 --> 31:26.960
time. And so you can backpropagate, but in this case, because it's invertible in time, you can

31:26.960 --> 31:32.640
just store the last hidden state, and reconstruct these hidden states based on the last hidden state,

31:32.640 --> 31:38.560
because it's perfectly invertible in time. And so you know, you kind of have the same memory

31:38.560 --> 31:43.920
efficiency, like in neural ODEs, with the nice feature that you don't have to go continuous,

31:43.920 --> 31:50.640
because for standard neural ODEs, it's only true if you're going with your time step to zero,

31:50.640 --> 31:56.000
which can be very expensive. And I mean, on a digital computer, you can't go basically to zero,

31:56.000 --> 32:01.680
but you always have some rounding errors and so on. And here we don't have any problem anymore.

32:01.680 --> 32:07.680
We can even use the time step of 0.1, and it's still perfectly invertible. And so yeah,

32:07.680 --> 32:13.280
it scales just perfectly. It's very memory efficient. And on top, because we have this kind of

32:13.280 --> 32:19.600
independencies, as I said before, we don't have to train it on the same computer. And that makes a

32:19.600 --> 32:25.360
lot of sense if you use it on GPUs directly, where you basically have an independent thread,

32:25.360 --> 32:29.920
or you don't even need a thread, so it can be even threads of different blocks, which cannot

32:29.920 --> 32:36.480
communicate with each other. And you can just train each independent dimension on an independent

32:36.480 --> 32:41.600
thread, and just in the end, add everything together. And this is extremely fast. So

32:42.880 --> 32:47.680
I actually did an experiment, and we have some some really efficient code on GitHub already.

32:47.680 --> 32:53.760
You can check it out if you're interested in it. It's directly implemented in CUDA. So it's,

32:53.760 --> 33:00.080
of course, it's using PyTorch, but it's some CUDA extension, where we really do this kind of

33:00.080 --> 33:07.760
independent threading. And maybe it's interesting to hear, but on my local GPU, I had a speed up of

33:08.800 --> 33:15.200
one epoch on sequential amnesty took me like half a day on the GPU using PyTorch for a

33:15.200 --> 33:21.360
three layer unicorn, for instance. And with this CUDA extension, by really using this kind of

33:21.360 --> 33:30.880
independency, I got speed up from half a day to half a minute. And so how is the

33:30.880 --> 33:41.440
expressivity of the RNN impacted by the independence? Yeah, good question. We did some benchmarks,

33:41.440 --> 33:46.880
so one of them, the problem for for expressivity is that you know, for credit and stability,

33:46.880 --> 33:51.840
you can formulate it mathematically. You can say, okay, it should be in this in this range,

33:51.840 --> 33:55.760
and it shouldn't blow up or go to zero. You can write it down. There's no problem with that,

33:55.760 --> 34:02.240
but for expressivity, you know, what's the mathematical formulation of expressivity? So I don't

34:02.240 --> 34:07.840
know any, I think what's maybe the closest to it is some kind of universality, right? If you can

34:07.840 --> 34:14.000
prove universality, that's at least something you would need. It's absolutely not sufficient,

34:14.000 --> 34:18.720
but it's at least necessary. So if you can show some kind of universality, that's that's nice.

34:19.840 --> 34:26.240
But besides this, I'm not aware of any expressivity mathematical results. And so what you do is you

34:26.240 --> 34:33.840
classify it more or less by empirical data. So you use some some benchmark, which people think

34:33.840 --> 34:39.280
requires high expressivity. And if it performs well on that, you say, okay, it has high

34:39.280 --> 34:45.440
expressivity. At least that was my impression. I'm, you know, happy to learn, maybe I was wrong,

34:45.440 --> 34:50.960
and there's a nice mathematical formula for that, but I can't come up with one and I'm not aware

34:50.960 --> 34:57.680
of any. And so we tried it on one benchmark, which is called IMDB, which is basically just

34:57.680 --> 35:03.840
classifying its sentiment analysis. So you classify movie reviews from this IMDB database,

35:03.840 --> 35:09.520
exactly. And that can be long, and that can also have long-term dependencies, right? I mean,

35:09.520 --> 35:16.400
the front of the reviews would be, I don't know, some kind of irony thing. You know, I really love

35:16.400 --> 35:21.840
this film. And then in the end, or this movie and in the end, you say, I love it because it sucks.

35:22.480 --> 35:27.680
So you have some kind of long-term dependencies, but you also have high expressivity because

35:27.680 --> 35:34.160
you do natural language processing. I mean, they're way better benchmarks for that, I guess, but it's

35:34.160 --> 35:39.440
at least a start. And on that, we got quite good results. So it outperformed corn actually,

35:39.440 --> 35:44.640
but we have to say we are using two layers for that. And for corn, we're only using one layer.

35:44.640 --> 35:49.040
So it could also be a layer thing. But for this independent unicorn, you can't use one layer,

35:49.040 --> 35:55.600
because then you really have no interdependencies. And you also struggle a lot. So this interdependencies,

35:55.600 --> 36:01.600
you still have them, but they come from the input to hidden matrix. And so you basically

36:02.240 --> 36:09.760
delay them by one layer. Do you know more or less what I mean? So you need at least two layers to

36:09.760 --> 36:19.040
have some kind of interdependencies between the hidden neurons. Okay, awesome. Where do you see this

36:19.040 --> 36:30.080
research going next? Yeah, so my personal big goal is to, you know, so far what we did was we have

36:30.080 --> 36:35.760
some kind of great instability, which is great. We can learn long-term dependencies, but there's

36:35.760 --> 36:44.320
a reason why people use LSTMs all the time, because they are just so expressive. And for instance,

36:44.320 --> 36:52.080
if you if you try something like this PTB pantry bank language modeling task, where for instance,

36:52.080 --> 37:00.640
you have some some Wall Street articles, I guess. And the the chart level would be that you read

37:00.640 --> 37:07.920
in the sequence of the words of the characters. And you want to predict the next character,

37:07.920 --> 37:13.520
basically, in the sequence. I mean, as humans, we are quite good in that, I guess. I mean, if I would

37:13.520 --> 37:20.320
say to see the and then would say, okay, next comes an S to see this, for instance. So for us,

37:20.320 --> 37:26.240
it's it's quite quite easy to do maybe, but for RNNs, it's quite hard, especially if they're not

37:26.240 --> 37:33.840
expressive. And if you're honest and you try, you know, all these more or less famous long-term

37:33.840 --> 37:42.000
dependency RNNs, they are not performing very well. So if you if you take a look at the metric they use,

37:44.400 --> 37:51.040
then it's all the time way worse than LSTMs. So X, RNNs are worse, our corners worse or

37:51.040 --> 37:58.240
unique corners worse. All these kind of nice nice RNNs, they suffer from limited expressivity. I

37:58.240 --> 38:03.840
just want to be so honest, right? So I'm not saying that corn is like the best thing out there.

38:03.840 --> 38:08.400
It's a silver bullet, use it all the time. No, there's a real use case for this. This is long-term

38:08.400 --> 38:15.440
dependencies, but if you do like high crazy expressivity tasks where you don't require long-term

38:15.440 --> 38:22.640
dependencies, then don't use it basically. And so the idea is that LSTMs are very strong in that,

38:22.640 --> 38:28.880
but they suffer from not being very good at learning long-term dependencies to have something,

38:28.880 --> 38:33.280
you know, which kind of bridges these two things, which kind of connects these two things. So

38:33.280 --> 38:40.640
something which has good can learn long-term dependencies very efficiently, but at the same time

38:40.640 --> 38:45.920
has high expressivity. And actually we are working on something like this. We are working with

38:45.920 --> 38:56.560
the group in Berkeley right now, and maybe in a few months, you know, we will publish. Yeah,

38:56.560 --> 39:02.160
yeah, then there will be some good news. I'm really motivated. I'm really thrilled about this

39:02.160 --> 39:09.600
project. It's working really good. And yeah, that would be my ultimate goal in this kind of RNN

39:09.600 --> 39:18.400
community. Oh, that's great. That's great. Well, Constantine, thanks so much for taking the time to

39:18.400 --> 39:23.680
share a bit about what you're working on. Yeah, no worries. It was a pleasure being here. My pleasure.

39:23.680 --> 39:39.840
Thank you. Bye-bye.

