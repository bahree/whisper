Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
One of them mentioned that one of the things he misses is giveaways and he's right.
It's been way too long since we've done a giveaway here on the show.
So did he know though that we had one in the works that we're excited to kick off today.
The O'Reilly Artificial Intelligence Conference is returning to New York in April and we
have one free conference pass ready to give away to a lucky listener.
To enter, first go to twimbleai.com slash AI and Y giveaway to access the entry form.
Next, choose any or all of the nine ways to enter.
The more entries you earn, the higher your chances to win.
And finally for three bonus entries, answer the question at the bottom of the entry box.
The giveaway ends soon so be sure to get your entries in while you can.
Now about today's show.
We're joined by Kevin Tran, PhD student in the Department of Chemical Engineering at Carnegie
Mellon University.
Kevin's research focuses on creating and using active learning workflows to perform density
functional theory or DFT simulations, which are used to screen for new catalysts for a
myriad of materials applications.
In our conversation, we explore the challenges surrounding one such application, the creation
of renewable energy fuel cells, which is discussed in his recent nature paper, active learning
across intermetallics to guide discovery of electro catalysts for CO2 reduction and H2
evolution.
We dig into the role and need for good catalysts in this application, the role that quanta
mechanics plays in finding them and how Kevin uses machine learning and optimization
to predict electro catalyst performance.
Okay, let's do it.
All right, everyone, I am on the line with Kevin Tran. Kevin is a PhD student in the Department
of Chemical Engineering at Carnegie Mellon University.
Kevin, welcome to this week in machine learning and AI.
Thank you for having me, Sam.
So Kevin, you're in chemical engineering, you've practiced chemical engineering.
How did you come to get involved in the application of machine learning?
I kind of fell into it with the research that I'm doing.
And so we are looking for new materials and we need an intelligent way to screen them.
And machine learning is actually one of the tools that we found that apparently does
that pretty well.
You're using that in your graduate work, have you used machine learning prior to grad
school?
I have not actually.
So everything that we're doing now is kind of learned on the spot, so to say.
And so it's new for us and we start a lot of room for improvement, but so far it's
working pretty well.
Awesome.
So tell us a little bit about the problem that you're trying to solve there.
Yeah, so at a high level, it's really about sustainable energy, right?
And that's actually the reason that I started doing this research.
And so the idea is to help make what we call solar fuels and solar chemicals.
So what that means is we take energy dry from the sun.
So let's say solar cells.
And one of the problems right now is that when we take solar cells and make electricity
out of that, if we have a lot generated during the day and we can't use it, then it's kind
of wasted, right?
So there are a lot of people doing really nice research into figuring out how to store
that energy.
So the reason is one method, the method that we are interested in is storing it in chemical
bonds.
So what we would do is we would take something like carbon dioxide and water.
Take that and the electricity we get from the sun and convert that into more valuable
fuels or chemicals like methane or ethylene or hydrogen.
And then in turn, we can use those for whatever application that we want.
And so part of that problem to actually turn those chemicals into more valuable chemicals
is that we need good catalyst to do so.
And right now, our research focuses on finding good catalyst to do those reactions.
And it involves a lot of computer simulations that we have to do intelligently.
And so we're using machine learning to help us decide which simulations to do it early.
OK, so to dig into that a little bit more, you're trying to store solar energy and chemical
bonds.
And some ways that sounds a little bit like what a battery is doing also.
Yeah, it's fundamentally a chemical thing.
Now, or you could, you know, while the sun is up, use the solar energy to kind of power
some chemical reaction and that's kind of where your need for some kind of catalyst comes
into play.
Yeah, yeah, exactly.
So the setup we have actually looks very similar to a battery, right, or the physical setups.
And so if you think of a car battery, right, it has a solution and it has an anode and
a cathode as in more or less two metal rods on either end.
And it can either generate electricity or it can, you can put energy electricity into
it to store the energy in more or less chemical form.
And so our system, what it does is instead of storing the energy in the battery itself,
it generates methane and fuels from carbon dioxide and the water that is in the battery itself
or in this case, it's the electrical chemical cell.
You just described it as an electrical chemical cell.
Yes.
And so it's not like you could use that energy to power some kind of machine to, you know,
extract some other compound or fuel, but rather you're doing this all chemically.
Yes.
Yep.
That's the idea.
The issue there is in order to perform that reaction and a large scale that could help
a lot of people.
We needed to be fast and efficient and affordable.
And the catalyst that we have right now can meet some of those criteria, but not a lot
of them.
And so we're looking for more materials that we can scale up to a commercial scale.
And that's the problem we have right now.
In order to do this, you need these catalysts.
How do you measure the performance of one of these catalysts?
Yes.
So there are a lot of metrics for performance that experimentals usually look at, one of
which we call activity, but the idea is how fast it can actually drive the reaction.
So let's say one catalyst can transform one kilojoule of energy and it would take maybe
a few seconds and another catalyst, it could take hours.
So we want the catalyst that could take a few seconds to actually do that reaction.
Another example is the efficiency.
So just because you put one kilojoule of energy in does not mean you get one kilojoule out.
It's often much less than that.
And so certain catalysts are more efficient about transferring that energy than others.
And there's often a balance between those two and even all the properties such as how
expensive the material is, right?
And so there's a lot of things that we plan to play with and look at.
But the work we have right now, since we're just starting out, only looks at the first
thing that I mentioned, which is how fast the reaction can go.
What's the space of possible catalysts look like?
It's a good question.
So one of the issues that the field has right now is that in order to solve this problem,
we're more or less looking at most of the periodic table, right?
And so there are a lot of elements there, a lot of iterations that go through.
And even if you choose, let's say, two metals, they stole the question of what ratio do
you have between those two metals, right?
What if you have three metals, what ratios do you have there?
And so what we have is a really rough first pass of looking at two or three metal common
issues, sometimes four, for about 30 different elements on the periodic table for select
combinations of ratios between them.
And so in that sense, it sounds kind of like a constrained optimization type of a problem.
You're trying to figure out how to get the greatest yield from this set of materials that
you're working with.
Yeah, exactly.
And it's kind of a difficult problem because the search space explodes geometrically as
you add more materials, right?
And more composition blends.
And so what we have is even though it feels large and seems large to us, it's really the
tip of the iceberg of what we could do, right?
At what level are you modeling this?
Like are you modeling this at and forgive my ignorance here if I'm not asking this correctly?
And are you kind of when you're searching for high yield catalysts?
Are you modeling atomic interactions, subatomic interactions and molecular interactions?
Or could you do all of this, you know, what you need at the level of, you know, just kind
of the things you might see on a periodic table?
Yeah, yeah.
So what we are doing specifically is modeling things at the atomic level.
And so there's a type of theory in chemistry, it's called density functional theory.
So what this does, it more or less uses quantum mechanics to take a set of atoms and predict
the properties of those atoms.
And using those properties, those are actually indicative of the performance of the catalyst.
And so we take a catalyst, we perform destiny functional theory simulations really around
how the atoms interact with each other.
And that will tell us if that catalyst or those atoms really will perform well in electrochemical.
So.
And so how long do the simulations take to run?
The issue here is that these simulations take anywhere from hours to days or the big
ones sometimes even weeks to run a single one.
Oh, wow.
Can you give us a high level understanding of how these DFT simulations work?
What are they doing to determine ultimately whether these catalysts are going to be high
yield?
Yeah, yeah.
So to answer that, we can back up a second to talk about how the catalysts work.
So let's take for example, we want to turn carbon dioxide into methane, right?
So it's CO2 going to CH4.
That doesn't happen in one step.
That happens in a series of chemical reactions and we know from chemistry experts that the
elementary reaction we call it, they're a smaller reaction that matters most to convert
CO2 to methane is actually adding one hydrogen onto carbon monoxide.
And so what that means is how strongly that carbon monoxide binds onto the catalyst is
really important.
So let's say carbon monoxide binds onto the catalyst very strongly.
So it will react quickly.
But once it's done reacting, it'll actually just stay there, never come off and you will
never actually have any product.
Conversely, if carbon monoxide binds to weekly, it will never go on to the catalyst in
the first place and therefore it will never react.
So there's kind of a Goldilocks effect where the catalyst needs to have a sweet spot of
energy in the middle.
So that's where density functional theory comes in.
We call it DFT.
And so given a configuration of atoms and how the carbon monoxide is sitting on a surface
of a catalyst, DFT can tell us how strongly that carbon monoxide is sticking to the surface.
And that is indicative of how well it's going to perform.
Presumably, you're using a simulator because the laws that govern this are either too
ill-defined to apply directly or are at too granular a level to apply directly.
That's interesting.
So when we call them simulations, we just say that because it's what it feels like.
But what we're actually using is quantum mechanics and so we know the laws that govern how everything
interacts.
The reason that takes so long is because it's really just a math problem solving a lot
of partial differential equations simultaneously.
And that just takes a really long time.
You recently published a paper on your work in this field in nature, nature catalysis.
And you're using active learning to help solve this problem.
Where does active learning come into play?
Yeah.
So we have a set of elements that we want to look at, right?
And they can have different varying composition space as we thought about before.
And we want to actually keep running simulations to keep finding new materials for experimental
list to test out.
So our whole workflow is designed around the idea of we want to continuously find new
materials for other people to look at.
So that's where the active part comes in because it feels almost in there, right?
So the more simulations we have, the more data we have, and the more data we have, the
more we get a better idea of how to what sites to perform next.
So in this case, what we're doing is, let's say we start out with a small database of a
few hundred simulations.
We use machine learning on those as a training set.
And from there we can predict how well the other catalyst that we have not simulated are,
how well they will perform.
And once we have an idea of that, we can pick the ones that we think will perform well
as per the machine learning model, and actually just do the simulations, get more data,
and then perform their questions again, and just continue that loop.
And that's how we're using active learning in a sense to find new materials.
You've got this database of materials that you've run these simulations on, and they're
characterized by their different kind of material properties.
Well, actually, maybe you can talk a little bit about how they're characterized.
Are they, you know, is it strictly at the atomic level or are there other material
properties that you're using to ultimately become features in your machine learning
models?
Yeah, so what we do is we look at the location of where the carbon monoxide is sitting
on the surface.
So if you can imagine how the atoms are set up, the carbon monoxide could be bonded to,
let's say, one plinum atom on a surface, and that plinum atom could be bound to maybe
eight other plinum atoms.
So what we feed as features to our aggression model is how many atoms that the carbon monoxide
is bound to?
So in this case, it'll be one plinum.
How many atoms that are in the next nearest neighbor or the next outer shell?
So in this case, it'll be eight plinums.
We also look at the identity, the chemical identity of those atoms.
So in this case, it'll be platinum.
If there's more than one metal, we consider, let's say, this plinum and aluminum.
We consider the aluminum too.
We also look at the chemical properties of those elements.
So platinum and aluminum have what we call electronegativity, which is how much they
like electrons.
And we look at that.
And we also look at what we call the atomic number.
So where it is on the periodic table.
And one of the interesting features that we have is the average binding energy of that
absorbate on the material.
So in this sense, let's say we already have a database of maybe 20 calculations of platinum.
And we take an average of those binding energies.
And that number is associated with platinum.
It kind of gives a feel for how strongly it binds the palatine in general.
We extend, we take that number and we feed it into our alloys.
And so when we have a material that has aluminum platinum, it has a feel for how strongly
it binds the platinum and aluminum.
And it sort of averages those together in a way when we do the regression.
Okay.
And that's that number that you described earlier is wanting to have that Goldilocks effect.
That one needs to be kind of just right.
Exactly.
And so you've got all of these as features that come out of the materials that you're
using, the simulations themselves are those only used as your, your labels, your answers
or are you also gaining information from those simulations that you're using as feature
input?
So the simulations that we're performing effectively give us the labels for our regression models.
At the same time, the, so those labels are the binding energy of carbon dioxide on a
particular site on a catalyst, a site being a location where the carbon dioxide could
just attach onto the catalyst.
I was mainly curious whether the simulation was just giving you that, you know, that answer
that binding energy that you use as labels or if the simulation was also kind of illuminating
other characteristics of the, the material pairs or combinations, you know, they're qualitatively
or quantitatively that you could also use as, you know, input signal.
Sure.
So the simulations themselves really just give us the labels.
If we want to look for more holistic information around what materials might work well, we actually
just look at our database of simulations.
So I guess for each individual one, we don't really clean much chemical information from
that for from seeing patterns arise in how many simulations work well given a certain
alloy, we clean information from there.
You've got this regression model that essentially allows you to take unknown combinations of
materials and guess what the binding energy will be without running through these really
long simulations, is that that kind of the general goal there?
Yeah.
Yeah.
Exactly.
So that's what our surrogate model is doing, our machine learning model.
Yep.
And then you're using that to allow you to kind of constrain the search space for new elements.
How do you tie that surrogate model into the active learning piece or the piece that you're
using to constrain your search space?
Yeah.
So the very naive way to do this is to take the model, evaluate across our entire search
space and pick the materials and the adoption sites that actually would work best.
But there's the idea of.
And if I can interrupt that, sure, should even be an improvement over running these simulations
every time.
Right?
Yeah.
It's an intelligent way of doing the simulations, right?
So what people do in the current field is they use their intuition to decide what
materials to simulate next.
We're here.
We're actually using machine learning predictions to make that decision instead of human intuition.
How well do you trust the surrogate model when you run it and it tells you that something
should have this kind of just right bonding energy?
Do you trust it or do you use that to determine what you should actually simulate?
So that's a really good question.
At the current state of the model, I trust it with a grain of salt, right?
Does that make sense?
But what I really trust are the simulations.
And so the whole point of our workflow isn't necessarily to make machine learning model
that can predict everything perfectly.
This is really to have the machine learning model decide what simulations to run next,
which will build our database in an intelligent way.
Right.
Right.
And so you were starting to walk through that process and how you applied the machine learning
given your surrogate model.
Yeah.
Yeah.
And so I think we discussed the naive way to do it, which is to use the model to find the
best candidate and to just run that.
But if you do that, you run in, there's the idea of exploration versus exploitation.
I'm not sure if you're familiar with.
Mm-hmm.
Yeah, we talk about that quite a bit on the show.
Yeah, yeah.
And so that's pure exploitation.
And so there's a decent amount of research into ways to balance the two.
For our workflow, we had a quick and dirty way to try and balance this where we actually
made a Gaussian distribution that is centered at our optimal point, right?
So let's say our optimal point is maybe 0.1 for the binding energy.
So we made a Gaussian distribution centered there with a certain standard deviation.
And for our search space, we used that distribution to assign a probability of how of selecting
a new candidate material to simulate.
So let's say if something was at 0.1 or the model thought it was exactly at 0.1 or optimal,
then it would assign the highest probability to picking that.
And the further out you go from that space, the lower the probability gets.
And so once we assign an array of probabilities to our possible search space, then we just
picked them at random.
So in a way, we would focus on the area that we're interested in.
But still, in a way, stochastically choose other materials that we might not normally
choose.
Just retaining some of that explore characteristic.
Exactly.
Yeah.
Okay.
And then the active learning piece of this is what specifically?
So the active learning piece is the process of iteratively deciding new points to simulate
for us, right?
And so we have this loop constantly going.
So the active part is the fact that we're doing a regression and then a prediction and
then a query or simulation, in our case, to get us more data, which yields another
regression, et cetera, et cetera.
So the active part is the iterative nature of our workflow.
So you're running this kind of in serial.
And then each time you produce this distribution of possible materials to simulate based on
the tests that you've done previously, you pick one, you run that.
And then you, based on the result, you put that in your distribution, you pick another
one, you run that and you keep feeding these results back into your database, your distribution
of things to try.
Exactly.
Exactly.
Very cool.
Very cool.
And so how do you characterize the results that you saw with the paper?
Yeah.
And so that's an interesting question because we've been playing with around with different
methods to actually analyze our database.
So we, our main three methods that we thought of, we actually put inside the paper, one
is to just make a list of materials that are within an acceptable range of performance
and give that to experimentalists.
And so we have that in the paper.
Another way is to create a, almost a heat map, a two-dimensional heat map of what materials
would work well.
So on one axis would be one set of elements.
So Luminon, Platinum, Gold, Copper, and the other axis is those same elements.
And so each point would be a blend of those elements.
So the grid would be maybe choose Copper and Luminon.
And from there we can color code the grid according to the fraction of the binding sites that
would work well.
And so that was another way that we could look at it.
And the third way is actually simply using Teasney to cluster all of the points that
we've done simulations on, and then color code them by their energy and look for clusters
that have the appropriate energy for us and look for the themes within those clusters
and recommend that those materials be studied.
And so you've got these three methods, how do you know that this method is helping you?
Are you finding, are you able to, you know, ultimately is it the number of kind
of candidate combinations that you're able to find per unit of your own time as a researcher
or is there some other kind of fundamental measure here?
Yeah, that's a good question.
The two that I kind of lean towards are the number of candidates we find over time.
So we actually have a plot of that in the paper as well.
And over time we can see that number rising and when I was getting a weaning according
to the things the way we modify our workflow.
The other way is really, I guess the real way is to see if any of the candidates we have
actually work and in an experimental setup.
And so for that case, experimental validation, we do have collaborators that are testing
some of these materials and we actually have another paper in review right now where
our collaborators tested one of the things that we've suspected to be performing well.
And it turns out it did.
And so that worked out perfectly in our minds.
And so we're really excited to get that paper out.
And you might be seeing that in the next few months or so.
Nice.
And it doesn't sound like it would be enough to just demonstrate one.
There's some notion of kind of demonstrating that this is a, you know, more efficient or
more repeatable process than, you know, what folks usually do.
And that goes back to have you have you produced enough candidates and had those go through
the experimental process in order to be able to make claims around that.
Yeah.
So this is the candidates that we found so far.
We found them relatively recently.
So our, we are still in the process of testing them with experimental collaborators.
One thing that does make us feel better about our workflow is that I'd say on the order
of 40 to 60% of the candidates that we have found have already been shown in the literature
to work well for what we're looking for, but ended up not being used for other reasons
that we're not looking at right now.
And so that kind of gives us a gut check to say that, hey, the things that our workflow
are finding are things that people have already looked at.
And so hopefully the things that we haven't experimentally tested yet might be good candidates
too.
Oh, that's right.
Because you're only looking at one of many important categories that you need to, or
properties that you need to consider to actually commercialize something.
So your workflow is spinning out candidates that according to your criteria are good ones.
But you know, may not, you know, may have already been proven to be not commercially viable
for other reasons.
Exactly.
Exactly.
And so that's where some of our research might be going in the future to start looking
at those other properties as well so that we can really trim down our list and reduce
the chances of giving a bad recommendation to an experimentalist.
And so how do you envision scaling your workflow to these multiple criteria?
So that's something that we're actually looking to now.
And we're still hoping that out.
But the rough idea is to find the Pareto front of multiple objectives and to try and balance
where we want to be on that Pareto front, if you're familiar with that term.
A library on that.
How do you go about doing that practically?
Yeah.
So practically, we would first have to find another property that we'd be interested
in, let's say stability of the catalyst or how long it'll stay there because if a catalyst
operates for a few hours and then degrades into something else and stops working, then
that's not a good catalyst, right?
So that's one of the things that we're going to be looking into in the future.
And our simulation, our simulations can actually get a handle on how stable something is.
And so what we would do is we would have a metric for what we call activity or how fast
the reaction goes and we would have a metric for stability.
And for all the candidates that we want to look at, we could evaluate how well it performs
in each.
And I mean, there's no free lingerie.
And so each candidate is probably going to perform well on one and maybe not for the other.
And so what we call a Pareto front is where let's say you have a certain activity, right?
And you find, let's say you want to have the ideal activity of point one.
We can find the candidates that have that idea, that ideal activity, but also are the
most stable.
And so a lot of give us one answer for that particular activity.
And then we can take a step to say, okay, let's look around materials that have an activity
of point two a little further away and of that, find the subset that are the most stable.
And then we can continue that across the spectrum of activities and find a list of materials
that are active and generally more stable than other candidates.
And we would still get a list from there.
But using that, that we could down select our material set even further.
What else do you plan on doing the kind of further this research line?
Yeah.
So other things we're looking at are more intelligent ways to select the next experiments
or the next simulations.
And so what I told you was the Gaussian selection.
So right now we're actually having conversations with machine learning people at Carnegie Mellon
University to figure out what type of algorithms might be best suited for our application.
So there's that.
And another way is to actually simply improve their regression methods that we're using
right now to see if we can get better serial models that can be more intelligent about
what this light didn't have a better prediction rate.
With the first of those kind of an alternative to Gaussian selection is there some intuition
that you're pursuing as to what methods might be better or what might the characteristics
of better methods be.
Yeah.
So again, this is something that we're just starting to dabble in.
So we haven't flushed the ideas out, but a lot of the active learning literature that
we've seen so far is really centered around Bayesian prediction and processes.
And so that's something that we might look into to model our system with and maybe start
selecting new candidates with.
And on the second direction that you mentioned, improved models, what are you thinking there
or what directions are you looking at?
Yeah.
So that kind of has an interaction with other things we're looking at.
Like I said, if we end up using Bayesian statistics to do the active learning, well then that
means our regression is going to be Bayesian based.
If we don't end up doing that, I've toyed with the idea of actually using neural networks
to do these predictions.
But there's a decent amount of overhead work that we need to go into that.
So we're still thinking about that, not sure if we want to go that direction.
But using neural networks is probably, if we choose to go that route, probably what
we would do.
Okay.
And when you say overhead, what are you specifically speaking of computational or the
effort that goes into just learning, you know, how to build out the models or what?
I would say the effort going into building the models because the way in which we turn
our system into features really matters a lot.
And that's really very active area of research in the field right now.
But it requires a lot of intuition, a lot of luck, and a lot of time to fit these models
well, especially when we're looking at search spaces as large as ours.
Well, Kevin, thanks so much for taking some time to share with us what you're working
on.
It's really cool stuff.
Of course.
You're welcome.
All right, everyone, that's our show for today.
For more information on Kevin or any of the topics covered in this show, visit twimmelai.com
slash talk slash 238.
Remember to enter our AI conference ticket giveaway at twimmelai.com slash AI and Y giveaway.
And of course, as always, thanks so much for listening and catch you next time.
