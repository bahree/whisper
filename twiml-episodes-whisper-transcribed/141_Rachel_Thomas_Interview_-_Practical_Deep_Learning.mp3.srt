1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,200
Tomorrow, May 15, the Twimble online meetup is back.

5
00:00:36,200 --> 00:00:43,140
Our main event will be a presentation by Santosh GSK on the paper Yolo 9000, Better Faster

6
00:00:43,140 --> 00:00:48,560
Stronger, which was written by Joseph Redman and Ali Farhadi.

7
00:00:48,560 --> 00:00:54,880
Yolo, or you only look once, is a very popular image object detection system and will thoroughly

8
00:00:54,880 --> 00:00:59,640
review it, along with the broader object detection landscape, the current state of detection

9
00:00:59,640 --> 00:01:03,240
algorithms, and the various challenges ahead.

10
00:01:03,240 --> 00:01:09,280
If you aren't already signed up, head over to twimbleai.com slash meetup to register.

11
00:01:09,280 --> 00:01:11,600
See you there.

12
00:01:11,600 --> 00:01:17,240
In this episode, I'm joined by Rachel Thomas, founder and researcher at Fast AI.

13
00:01:17,240 --> 00:01:22,320
If you're not familiar with Fast AI, the company offers a series of courses including practical

14
00:01:22,320 --> 00:01:27,940
deep learning for coders, cutting edge deep learning for coders, and Rachel's computational

15
00:01:27,940 --> 00:01:31,280
linear algebra course, and they're all free.

16
00:01:31,280 --> 00:01:35,240
The courses are designed to make deep learning more accessible to those without the extensive

17
00:01:35,240 --> 00:01:39,160
math backgrounds that some other courses assume.

18
00:01:39,160 --> 00:01:43,400
Rachel and I cover a lot of ground in this conversation, starting with the philosophy and

19
00:01:43,400 --> 00:01:46,240
goals behind the Fast AI courses.

20
00:01:46,240 --> 00:01:51,920
We also cover Fast AI's recent decision to switch their courses from TensorFlow to PyTorch,

21
00:01:51,920 --> 00:01:56,200
the reasons for this, and the lessons they've learned in the process.

22
00:01:56,200 --> 00:02:00,520
We chat about the role of the Fast AI deep learning in library as well, and how it was

23
00:02:00,520 --> 00:02:06,200
recently used to help their team achieve top results on a popular industry benchmark of

24
00:02:06,200 --> 00:02:10,560
training time and training cost by a factor of more than 10.

25
00:02:10,560 --> 00:02:14,920
I also announce on this show that I'll be working through the Fast AI practical deep learning

26
00:02:14,920 --> 00:02:20,120
for coders course starting in June, and I'm organizing a study and support group via

27
00:02:20,120 --> 00:02:22,400
the Twimble Online Meetup.

28
00:02:22,400 --> 00:02:26,040
So if you're like me, and taking this course has been on your list for a while, but you've

29
00:02:26,040 --> 00:02:28,720
put it off, let's knock it out together.

30
00:02:28,720 --> 00:02:33,680
You can join in by registering for the meetup at twimlai.com slash meetup, and letting

31
00:02:33,680 --> 00:02:38,360
us know that you're in on the Fast AI channel that I've created there.

32
00:02:38,360 --> 00:02:45,800
And now on to the interview.

33
00:02:45,800 --> 00:02:46,800
All right, everyone.

34
00:02:46,800 --> 00:02:49,200
I am on the line with Rachel Thomas.

35
00:02:49,200 --> 00:02:56,240
Rachel is founder and researcher at Fast AI and an assistant professor with a data institute

36
00:02:56,240 --> 00:02:58,720
at the University of San Francisco.

37
00:02:58,720 --> 00:03:01,400
Rachel, welcome to this weekend machine learning in AI.

38
00:03:01,400 --> 00:03:02,400
Oh, thank you.

39
00:03:02,400 --> 00:03:03,920
Thanks for having me.

40
00:03:03,920 --> 00:03:09,200
Why don't we get started by having you tell us about your background and how you made

41
00:03:09,200 --> 00:03:12,760
your way into machine learning in AI?

42
00:03:12,760 --> 00:03:13,760
Sure.

43
00:03:13,760 --> 00:03:14,760
Yeah.

44
00:03:14,760 --> 00:03:20,480
So I studied math and computer science in college, but I was really focused on kind of stuff

45
00:03:20,480 --> 00:03:23,360
that was as theoretical as possible.

46
00:03:23,360 --> 00:03:27,880
I did a PhD in math planning, planning to become a professor, and then kind of changing

47
00:03:27,880 --> 00:03:34,000
my mind towards the end, and stumbled into working into finance as a quant.

48
00:03:34,000 --> 00:03:38,080
And that's where I first kind of started working with data a lot, and really enjoyed

49
00:03:38,080 --> 00:03:39,080
it.

50
00:03:39,080 --> 00:03:43,360
And so I read about data science being kind of this new field and tech.

51
00:03:43,360 --> 00:03:46,720
And yeah, I moved to San Francisco and started working for tech startups.

52
00:03:46,720 --> 00:03:53,720
I was an early engineer and data scientist at Uber, and then I returned to teaching.

53
00:03:53,720 --> 00:03:58,360
So I love teaching always, I think, come back to it in some form and taught full-stacks

54
00:03:58,360 --> 00:04:01,240
offer development to women at Hackbright.

55
00:04:01,240 --> 00:04:06,760
And then two years ago, Jeremy Howard and I started, fast, AI with the goal of making

56
00:04:06,760 --> 00:04:09,800
deep learning more accessible and easier to use.

57
00:04:09,800 --> 00:04:16,040
This kind of brings together my enjoyment of machine learning as well as, yeah, trying

58
00:04:16,040 --> 00:04:23,040
to create a more inclusive field, yeah, and really make the field more accessible.

59
00:04:23,040 --> 00:04:27,200
And at Fast Day, I would do a mix of research and education.

60
00:04:27,200 --> 00:04:32,200
So our goal is we teach a course, practical deep learning for coders that's available

61
00:04:32,200 --> 00:04:36,800
for free online, and we want every time we teach it to be able to teach it to an even

62
00:04:36,800 --> 00:04:41,600
broader audience and have even better, faster results.

63
00:04:41,600 --> 00:04:45,640
And so part of that is building the libraries and tools we need to make that possible.

64
00:04:45,640 --> 00:04:46,640
Okay.

65
00:04:46,640 --> 00:04:47,640
Awesome.

66
00:04:47,640 --> 00:04:49,000
Well, we'll dig into all of that.

67
00:04:49,000 --> 00:04:53,240
Before we do, I will fess up to not having gone through the course yet.

68
00:04:53,240 --> 00:04:55,840
It has been on my list.

69
00:04:55,840 --> 00:04:57,720
And I do plan to do it.

70
00:04:57,720 --> 00:04:59,360
And in fact, I'll throw this out there.

71
00:04:59,360 --> 00:05:08,840
If anyone in the listening audience wants to do a kind of a support group or a group for

72
00:05:08,840 --> 00:05:12,880
going through the course, I will commit to doing it in June.

73
00:05:12,880 --> 00:05:15,280
So we can start in June 1st.

74
00:05:15,280 --> 00:05:16,480
On June 1st, I should say.

75
00:05:16,480 --> 00:05:22,120
We've got a in our meetup, we've got a slight channel, and I think I've already created

76
00:05:22,120 --> 00:05:25,080
a channel for doing the course because I'd hope to do it sooner.

77
00:05:25,080 --> 00:05:29,720
But if anyone wants to join me in getting started with it in June, I think it'd be great

78
00:05:29,720 --> 00:05:31,480
to do it as a group.

79
00:05:31,480 --> 00:05:32,480
Yeah, that's great.

80
00:05:32,480 --> 00:05:38,560
I was going to say we have, when we have forums, forums.fast.ai, so definitely check those

81
00:05:38,560 --> 00:05:40,960
out for asking questions and getting help.

82
00:05:40,960 --> 00:05:45,440
But yeah, we really encourage people to find groups to do it with because I know a lot

83
00:05:45,440 --> 00:05:51,040
of people, it really helps to have that accountability and people to talk with as they're working

84
00:05:51,040 --> 00:05:52,040
through the course.

85
00:05:52,040 --> 00:05:55,040
And I know a few different companies where groups have worked through the course together

86
00:05:55,040 --> 00:05:58,680
and you know, like have lunch once a week to discuss how they're doing.

87
00:05:58,680 --> 00:06:00,440
Oh, yeah, that sounds great.

88
00:06:00,440 --> 00:06:06,160
Yeah, there's also this program, AI Saturdays, that we only found out about this after a

89
00:06:06,160 --> 00:06:07,160
while.

90
00:06:07,160 --> 00:06:10,360
But I think they're in like 60 countries and it's like people get together on Saturdays

91
00:06:10,360 --> 00:06:12,720
and in the morning, they work through the course together.

92
00:06:12,720 --> 00:06:13,720
Oh, wow.

93
00:06:13,720 --> 00:06:14,720
Oh, very cool.

94
00:06:14,720 --> 00:06:15,720
Yeah, yeah.

95
00:06:15,720 --> 00:06:16,720
So that's AI Saturdays.

96
00:06:16,720 --> 00:06:17,720
Very cool.

97
00:06:17,720 --> 00:06:24,480
Well, why don't we start by having you talk a little bit about the course and go a little

98
00:06:24,480 --> 00:06:30,240
bit deeper into the motivation behind the course, what makes it unique, relative to all

99
00:06:30,240 --> 00:06:37,960
of the other courses that are out there and kind of how you see the education landscape

100
00:06:37,960 --> 00:06:39,200
around deep learning.

101
00:06:39,200 --> 00:06:40,200
Sure.

102
00:06:40,200 --> 00:06:41,200
Yeah.

103
00:06:41,200 --> 00:06:45,200
This is the course that I wish had existed five years ago when I was first getting interested

104
00:06:45,200 --> 00:06:46,200
in deep learning.

105
00:06:46,200 --> 00:06:47,200
Mm-hmm.

106
00:06:47,200 --> 00:06:52,080
And it kind of came with, I think, a lot of me and Jeremy's frustrations at the time with

107
00:06:52,080 --> 00:06:54,640
a kind of existing materials.

108
00:06:54,640 --> 00:07:00,320
But a lot of resources for deep learning are either they're very theoretical.

109
00:07:00,320 --> 00:07:05,440
And so, you know, they're not too accessible to people that don't have a graduate math background.

110
00:07:05,440 --> 00:07:09,520
And even as someone that did have a graduate math background, it was not that helpful for

111
00:07:09,520 --> 00:07:10,520
coding.

112
00:07:10,520 --> 00:07:14,000
It was like, I want to build kind of like practical applications using this.

113
00:07:14,000 --> 00:07:17,800
And yeah, reading the theory wasn't that helpful to me.

114
00:07:17,800 --> 00:07:23,280
We're starting to see, I think, a lot more practical courses and tutorials out there.

115
00:07:23,280 --> 00:07:28,600
But many of them kind of settle for these, you know, they work on toy problems and have

116
00:07:28,600 --> 00:07:29,920
like, okay, results.

117
00:07:29,920 --> 00:07:33,160
But we really wanted something that would get you to the state of the art and that you

118
00:07:33,160 --> 00:07:38,440
could use in the workplace and have state of the art results, but have it be super practical.

119
00:07:38,440 --> 00:07:42,680
So our course is distinctive and that there are no math prerequisites.

120
00:07:42,680 --> 00:07:46,440
The only prerequisite is one year of coding experience.

121
00:07:46,440 --> 00:07:49,000
And it gets you to the state of the art.

122
00:07:49,000 --> 00:07:54,400
Something else that's pretty unusual about it is we use a stop down teaching approach.

123
00:07:54,400 --> 00:07:59,000
So most technical education is, we call it bottom up, but it's where you have to learn

124
00:07:59,000 --> 00:08:03,120
each individual like underlying component that you'll be using.

125
00:08:03,120 --> 00:08:06,480
And then, you know, eventually you can put them together to do something interesting.

126
00:08:06,480 --> 00:08:08,480
And this is how math is taught as well.

127
00:08:08,480 --> 00:08:11,680
And so it's like, for years, students are kind of like, what's the big picture?

128
00:08:11,680 --> 00:08:14,080
Why am I learning, you know, like all these little components?

129
00:08:14,080 --> 00:08:18,160
And you can do really awesome stuff later, but so many people lose the motivation when

130
00:08:18,160 --> 00:08:20,080
they don't have that big picture.

131
00:08:20,080 --> 00:08:26,320
And so our goal with the course is to get you training and training a model right away,

132
00:08:26,320 --> 00:08:28,160
like in three lines of code.

133
00:08:28,160 --> 00:08:31,360
And then as time goes on, we get into these underlying details.

134
00:08:31,360 --> 00:08:36,160
And so this is a lot more similar to how they sports or music are taught, where, you

135
00:08:36,160 --> 00:08:40,360
know, kids can be playing baseball, even if they don't know the formal rules, they might

136
00:08:40,360 --> 00:08:44,040
not have, you know, a full team or a full nine innings.

137
00:08:44,040 --> 00:08:46,400
And as they get older, they learn more rules.

138
00:08:46,400 --> 00:08:48,800
And so that's kind of what we're doing with people learning is like, show you how to

139
00:08:48,800 --> 00:08:49,800
use it.

140
00:08:49,800 --> 00:08:51,840
And then we explain how it works later.

141
00:08:51,840 --> 00:08:55,520
And that's kind of the opposite approach for many, many courses.

142
00:08:55,520 --> 00:09:01,960
And there are two courses or at least two parts to the second version of the course.

143
00:09:01,960 --> 00:09:05,160
How are the two parts differentiated?

144
00:09:05,160 --> 00:09:06,160
Sure.

145
00:09:06,160 --> 00:09:07,160
Yeah.

146
00:09:07,160 --> 00:09:12,200
So part one, we call practical deep learning for coders, and that kind of goes over a lot

147
00:09:12,200 --> 00:09:18,880
of, I guess, like core areas of, you know, like using convolutional neural networks for

148
00:09:18,880 --> 00:09:19,880
image classification.

149
00:09:19,880 --> 00:09:23,400
We do a little bit of language with RNNs.

150
00:09:23,400 --> 00:09:30,400
We also do, we cover how to work on tabular data, collaborative filtering, so for, you

151
00:09:30,400 --> 00:09:35,360
know, making predictions of, you know, like movie recommendations, so that's all in part

152
00:09:35,360 --> 00:09:36,360
one.

153
00:09:36,360 --> 00:09:39,600
Just kind of really to get you using the tools proficiently.

154
00:09:39,600 --> 00:09:43,360
And then part two is cutting edge, cutting edge deep learning.

155
00:09:43,360 --> 00:09:47,440
And so that's a lot more, which you had to start reading and implementing papers.

156
00:09:47,440 --> 00:09:48,440
Oh, nice.

157
00:09:48,440 --> 00:09:52,120
And it's, yeah, it's exciting to see because we've had a lot of students are like, oh,

158
00:09:52,120 --> 00:09:53,440
my gosh, this is so intimidating.

159
00:09:53,440 --> 00:09:57,040
You know, I did not think I would be able to like read one of these papers and implement

160
00:09:57,040 --> 00:09:59,240
it to have them have them doing that now.

161
00:09:59,240 --> 00:10:00,240
Oh, fantastic.

162
00:10:00,240 --> 00:10:03,720
Well, I'll plan on hitting part two as well this summer.

163
00:10:03,720 --> 00:10:04,720
Yeah.

164
00:10:04,720 --> 00:10:05,720
Yeah.

165
00:10:05,720 --> 00:10:07,960
We'll be releasing the updated version of that in the next month or two.

166
00:10:07,960 --> 00:10:10,320
So we just wrapped up the in person version.

167
00:10:10,320 --> 00:10:14,080
So we kind of teach it in person and then we really sit online.

168
00:10:14,080 --> 00:10:19,200
But that includes stuff like bounding boxes and gans.

169
00:10:19,200 --> 00:10:24,560
We did some really neat stuff with language as we're using transfer learning to kind of

170
00:10:24,560 --> 00:10:30,320
use a language model for then text classification problems, style, transfer.

171
00:10:30,320 --> 00:10:33,040
Oh, very cool.

172
00:10:33,040 --> 00:10:37,040
And the course is offered both online and in person.

173
00:10:37,040 --> 00:10:42,040
What's the motivation behind doing the in person course given that you can reach so many

174
00:10:42,040 --> 00:10:43,800
people doing it online?

175
00:10:43,800 --> 00:10:49,360
So I think with the in person course, it's really helpful to have that kind of energy

176
00:10:49,360 --> 00:10:51,120
and feedback.

177
00:10:51,120 --> 00:10:56,320
I think it's hard to record a course just in an empty room, but to be getting student

178
00:10:56,320 --> 00:10:57,320
questions.

179
00:10:57,320 --> 00:11:00,920
And we have to really get to know a lot of the students taking the in person course.

180
00:11:00,920 --> 00:11:03,680
There's been a pretty good community around it.

181
00:11:03,680 --> 00:11:08,320
The in person course, I should say it meets one evening a week in San Francisco.

182
00:11:08,320 --> 00:11:13,680
So most people taking the course are working full-time, most of them work in tech.

183
00:11:13,680 --> 00:11:17,040
As part of the in person course, we've also had two programs.

184
00:11:17,040 --> 00:11:22,080
We have diversity fellows, and so this is to encourage more women, people of color and

185
00:11:22,080 --> 00:11:27,120
LGBTQ people to take the course, and that's really, I think, helped us get a more diverse

186
00:11:27,120 --> 00:11:30,200
audience, which is great.

187
00:11:30,200 --> 00:11:34,440
And then we also have an international fellows program, and that's people that are remote

188
00:11:34,440 --> 00:11:39,560
from all over the world, but they are participating in the course in real time.

189
00:11:39,560 --> 00:11:45,080
And so I think that those have been really important components of the in person course.

190
00:11:45,080 --> 00:11:52,400
One of the things that I noticed not too long ago was there were some announcements

191
00:11:52,400 --> 00:11:59,120
about the course shifting from, actually, I forget the framework that it was using before.

192
00:11:59,120 --> 00:12:05,440
So version one part one was in caros, and then version one part two was a blend of caros

193
00:12:05,440 --> 00:12:06,440
and tensorflow.

194
00:12:06,440 --> 00:12:11,640
I should say, yeah, we were using caros on top of the ono for the original version, and

195
00:12:11,640 --> 00:12:17,080
then we introduced some tensorflow, but then kind of last summer when we started working

196
00:12:17,080 --> 00:12:19,960
on version two, we switched to pie torch.

197
00:12:19,960 --> 00:12:24,320
And I guess, actually now that I think about it, we had used some pie torch in version

198
00:12:24,320 --> 00:12:29,320
one part two, because we found there were some stuff that was just almost impossible for

199
00:12:29,320 --> 00:12:33,360
us to do in tensorflow, and so we had started using pie torches, it was pretty soon after

200
00:12:33,360 --> 00:12:38,200
it released, and it was just such a fun language to use, and made a lot of things feel so much

201
00:12:38,200 --> 00:12:43,360
more intuitive and easier, so this year, yeah, the course was entirely in pie torch, as

202
00:12:43,360 --> 00:12:48,480
well as a high level API that we've written in our own library, fast AI that sits on top

203
00:12:48,480 --> 00:12:49,680
of pie torch.

204
00:12:49,680 --> 00:12:54,640
What were some of the things that were impossible to do in tensorflow that you were able to do

205
00:12:54,640 --> 00:12:55,640
in pie torch?

206
00:12:55,640 --> 00:13:03,000
So I remember we were having a lot of trouble with teacher forcing, so this is in natural

207
00:13:03,000 --> 00:13:10,080
language processing, where for text model, we're trying to predict what the next word

208
00:13:10,080 --> 00:13:16,400
in a sentence will be, you initially, as you're training it, you want to give it the right

209
00:13:16,400 --> 00:13:22,480
answer initially, and then kind of with a probability, you want to be reducing the chance,

210
00:13:22,480 --> 00:13:29,680
so since this is an RNN, you're predicting one word and then the next, and then the next,

211
00:13:29,680 --> 00:13:35,280
if you always give it what the actual next word is, the network is going to make kind of

212
00:13:35,280 --> 00:13:40,000
be more willing to make wild predictions, because it's not going to like hurt it long-term,

213
00:13:40,000 --> 00:13:44,760
but if you never give it, you can kind of get too off, and so that was something to be

214
00:13:44,760 --> 00:13:49,920
able to kind of have this probabilistic change happening while you're training that we found

215
00:13:49,920 --> 00:13:50,920
pretty difficult.

216
00:13:50,920 --> 00:13:56,480
Is that something that teacher forcing, is that an issue around initialization, or is

217
00:13:56,480 --> 00:14:01,280
it an issue around labels, or neither?

218
00:14:01,280 --> 00:14:07,360
It's more, it's more an issue on kind of how you're training, although actually I don't

219
00:14:07,360 --> 00:14:11,480
want to focus too much on that, I should say kind of like high level, like the big reasons

220
00:14:11,480 --> 00:14:18,200
we changed were one, I think a key thing for me is pie torch is easier to debug, and

221
00:14:18,200 --> 00:14:24,640
I think in any sort of coding, just being able to debug easily is really important.

222
00:14:24,640 --> 00:14:31,720
So with TensorFlow, and so this TensorFlow has now released a dynamic version, but for the

223
00:14:31,720 --> 00:14:37,280
first few years of its life, TensorFlow was just a new construct, it's called a static

224
00:14:37,280 --> 00:14:42,600
computation graph, and so you're kind of constructing the graph, and then you execute

225
00:14:42,600 --> 00:14:47,040
stuff on it, and so by the time you get an error, it can feel very removed from the line

226
00:14:47,040 --> 00:14:53,720
of code that actually caused that error, whereas pie torch is a dynamic graph, and so when

227
00:14:53,720 --> 00:14:57,480
you get an error, it's right at the line of code that caused it, and so I think that feels

228
00:14:57,480 --> 00:14:59,160
much easier to debug.

229
00:14:59,160 --> 00:15:05,720
I should note that TensorFlow has released an early execution mode that does this and

230
00:15:05,720 --> 00:15:10,680
is more like pie torch, and this is something I think that TensorFlow is having to play

231
00:15:10,680 --> 00:15:15,480
ketchup to pie torch, I think they kind of saw how successful pie torch was with the

232
00:15:15,480 --> 00:15:20,560
dynamic graph, and so I think TensorFlow is still behind in this area, and it's tough

233
00:15:20,560 --> 00:15:25,720
because TensorFlow has such a huge code base that I think it's harder for them to be nimble

234
00:15:25,720 --> 00:15:26,920
when they make changes.

235
00:15:26,920 --> 00:15:27,920
Right.

236
00:15:27,920 --> 00:15:33,680
So that's one area, something else that's great about pie torch is it's written in kind

237
00:15:33,680 --> 00:15:40,280
of this very standard Python object-oriented paradigm, and so I think for people that

238
00:15:40,280 --> 00:15:44,520
have done other Python programming or either other object-oriented programming in different

239
00:15:44,520 --> 00:15:50,200
languages, I think it feels a lot more natural and intuitive, just kind of how it's how

240
00:15:50,200 --> 00:15:56,360
it's structured, whereas TensorFlow has, it has a lot of TensorFlow specific conventions

241
00:15:56,360 --> 00:16:01,800
that you have to learn around sessions and scope, but don't, yeah, just kind of aren't

242
00:16:01,800 --> 00:16:05,720
as commonly used in other programming languages.

243
00:16:05,720 --> 00:16:08,800
And so do you see pie torch?

244
00:16:08,800 --> 00:16:14,480
And I also, I'll say one more thing about the makes pie torch great is, so TensorFlow

245
00:16:14,480 --> 00:16:19,920
is a much larger library, and that can be difficult because it's like, I don't know, they're

246
00:16:19,920 --> 00:16:25,680
both four ways to do anything you want to do, whereas pie torch is kind of a much smaller

247
00:16:25,680 --> 00:16:29,040
set of features, but they were designed to be super flexible.

248
00:16:29,040 --> 00:16:33,320
And so it's very easy to kind of build, build what you want because you have these very

249
00:16:33,320 --> 00:16:36,000
flexible pieces that you can combine well.

250
00:16:36,000 --> 00:16:37,000
Okay.

251
00:16:37,000 --> 00:16:44,280
And so do you think all of those have made pie torch a better choice for education, but

252
00:16:44,280 --> 00:16:50,880
maybe not a strong that choice for production use cases, or do you think it's a solid

253
00:16:50,880 --> 00:16:52,880
choice for production as well?

254
00:16:52,880 --> 00:16:57,000
I think it's a solid choice for production as well.

255
00:16:57,000 --> 00:17:04,080
I think something to remember is in a lot of pieces, you don't need to be training in

256
00:17:04,080 --> 00:17:09,800
production, and so you can train a model and then kind of have your, really, you can

257
00:17:09,800 --> 00:17:14,040
just take your predictions and put them on like an end point for, you know, if you have

258
00:17:14,040 --> 00:17:18,680
like a flask app or, you know, any other sort of app, like you don't necessarily kind of

259
00:17:18,680 --> 00:17:25,720
need this machine learning component in production when you're making your inferences,

260
00:17:25,720 --> 00:17:30,280
like you can just kind of like have that function, yeah, attach to something else.

261
00:17:30,280 --> 00:17:35,920
I think that yeah, TensorFlow, like if you, I mean, if you need to be doing deep learning

262
00:17:35,920 --> 00:17:41,480
on an edge device, yeah, TensorFlow is definitely way more developed for that.

263
00:17:41,480 --> 00:17:47,120
And if you're doing something at Google scale, where you do want to be using, yeah, which

264
00:17:47,120 --> 00:17:50,560
I think very few people other than Google are.

265
00:17:50,560 --> 00:17:55,680
So I think I think that PyTorch can, yeah, can be good for production for most people.

266
00:17:55,680 --> 00:18:02,640
And to what degree has the relative lack of, you know, library, community contributed

267
00:18:02,640 --> 00:18:06,440
components and that kind of thing, you know, to what degree do you think that holds back

268
00:18:06,440 --> 00:18:07,440
PyTorch?

269
00:18:07,440 --> 00:18:11,720
I mean, I think, I think some of their members, PyTorch, it's just, I mean, it's crazy

270
00:18:11,720 --> 00:18:17,400
to think it's really, I was like January or February 2017 that it came out.

271
00:18:17,400 --> 00:18:19,480
So it's really just a little bit more than a year.

272
00:18:19,480 --> 00:18:22,160
And so I think that they've made great progress in that time.

273
00:18:22,160 --> 00:18:24,720
It is still, yeah, this kind of very young project.

274
00:18:24,720 --> 00:18:33,240
And I remember when, you know, there was this point in time, I forget how long ago it was,

275
00:18:33,240 --> 00:18:40,560
but I remember this point in time where, you know, prior to, prior to, you know, the general

276
00:18:40,560 --> 00:18:47,640
sense was that TensorFlow had pretty much locked things up in terms of deep learning frameworks

277
00:18:47,640 --> 00:18:50,240
and all of a sudden, PyTorch came out of nowhere.

278
00:18:50,240 --> 00:18:51,240
Yeah.

279
00:18:51,240 --> 00:18:56,000
And then I think you, yeah, you all piled on soon after that.

280
00:18:56,000 --> 00:19:02,800
I really, like, Google has just spent so much on marketing TensorFlow that it's hard,

281
00:19:02,800 --> 00:19:06,560
like, and they just have really, really marketed TensorFlow because I feel like I talked to

282
00:19:06,560 --> 00:19:11,200
so many people that, like, don't do deep learning, have never used TensorFlow, but they've heard

283
00:19:11,200 --> 00:19:12,200
about it.

284
00:19:12,200 --> 00:19:15,680
And I think it's great because they, but it, like, what they've heard sounds like mostly

285
00:19:15,680 --> 00:19:17,120
kind of like marketing from Google.

286
00:19:17,120 --> 00:19:24,880
So I think that that has, that has really impacted or kind of skewed, skewed things towards

287
00:19:24,880 --> 00:19:29,240
making TensorFlow seem more popular for, I mean, this is also hard to remember, you know,

288
00:19:29,240 --> 00:19:33,200
across, you know, it's been incorporated into core TensorFlow now, but that was not originally

289
00:19:33,200 --> 00:19:34,920
a part of TensorFlow.

290
00:19:34,920 --> 00:19:39,240
And so, yeah, like part very first version of our course, like we were using caros on top

291
00:19:39,240 --> 00:19:44,480
with the ono, although yeah, the ono has now been, been deprecated.

292
00:19:44,480 --> 00:19:46,720
The space is changing so quickly.

293
00:19:46,720 --> 00:19:47,720
It is.

294
00:19:47,720 --> 00:19:48,720
Yeah.

295
00:19:48,720 --> 00:19:55,720
Do you feel like it's, it's stabilized or do you think it could change any day?

296
00:19:55,720 --> 00:20:00,080
Yeah, I think it'll continue changing, tear me actually wrote in our blog post last year

297
00:20:00,080 --> 00:20:03,760
when we announced we were switching to PyTorch that basically everyone working in the

298
00:20:03,760 --> 00:20:08,400
field should assume we'll have to switch languages and frameworks a few more times because

299
00:20:08,400 --> 00:20:11,160
it's just, it's just going to keep changing possibly.

300
00:20:11,160 --> 00:20:15,800
That sounds really scary for, you know, putting on my enterprise hat, someone that wants

301
00:20:15,800 --> 00:20:22,720
to start, you know, really building, you know, real stuff and, and, you know, business

302
00:20:22,720 --> 00:20:27,360
critical functionality of the fluidity.

303
00:20:27,360 --> 00:20:32,520
What do you think about efforts like Onyx and some of these other things that are trying

304
00:20:32,520 --> 00:20:40,120
to create, you know, either portability between frameworks or cross compilation to, you

305
00:20:40,120 --> 00:20:41,120
know, different frameworks.

306
00:20:41,120 --> 00:20:42,120
Yeah.

307
00:20:42,120 --> 00:20:46,520
I'm not as familiar with those, but from what I know, I think that's a great idea.

308
00:20:46,520 --> 00:20:50,400
Yeah, and I meant to bring up Onyx earlier as another potential solution to someone that's

309
00:20:50,400 --> 00:20:53,720
working in PyTorch and wants to deploy to production.

310
00:20:53,720 --> 00:20:56,720
Yeah, I know, I think those are good efforts.

311
00:20:56,720 --> 00:20:59,120
And yeah, I know, I said that it's changing a lot.

312
00:20:59,120 --> 00:21:03,360
I mean, I think that they're, you know, if you want to be doing this stuff in production,

313
00:21:03,360 --> 00:21:08,080
like you absolutely can and should be right now, I don't feel like it's, everything's

314
00:21:08,080 --> 00:21:10,280
going to change tomorrow.

315
00:21:10,280 --> 00:21:16,960
But I think, I think there's something exciting about being in a relatively young field that

316
00:21:16,960 --> 00:21:22,400
is, is so dynamic and where, you know, a lot of the changes we're seeing are, you know,

317
00:21:22,400 --> 00:21:24,800
are these huge improvements?

318
00:21:24,800 --> 00:21:25,800
Mm-hmm.

319
00:21:25,800 --> 00:21:33,840
So you mentioned that in addition to the course, there's also a fast AI library that you

320
00:21:33,840 --> 00:21:36,400
distribute and use in the course.

321
00:21:36,400 --> 00:21:41,600
And I've recently seen you publish some benchmarks that I think have something to do with that

322
00:21:41,600 --> 00:21:42,600
library.

323
00:21:42,600 --> 00:21:45,920
Can you talk about the library and its purpose and the benchmarks?

324
00:21:45,920 --> 00:21:46,920
Yeah.

325
00:21:46,920 --> 00:21:51,840
And so the library, the goal of it, and I should also put the disclaimer, we need to release

326
00:21:51,840 --> 00:21:55,920
more documentation about around it and that's the thing that we're working on right now,

327
00:21:55,920 --> 00:22:00,160
and we'll be, we'll be coming out this summer.

328
00:22:00,160 --> 00:22:07,880
But the goal of the library was to kind of give a high level API and code, and also to

329
00:22:07,880 --> 00:22:12,600
encode kind of a lot of best practices and smart defaults.

330
00:22:12,600 --> 00:22:17,320
Like I think something that can be overwhelming, like when you're first learning, deep learning

331
00:22:17,320 --> 00:22:20,280
and the kind of the impression that you get from it, some places are, you know, there

332
00:22:20,280 --> 00:22:23,560
are just so many hyper parameters, and it's like, oh my gosh, I've got all these hyper

333
00:22:23,560 --> 00:22:24,560
parameters.

334
00:22:24,560 --> 00:22:25,560
What do I choose?

335
00:22:25,560 --> 00:22:26,560
And how do I tune those?

336
00:22:26,560 --> 00:22:32,160
And so we really just kind of wanted to make it easier and give you good defaults.

337
00:22:32,160 --> 00:22:36,120
And so if you want, you can have to really just think about like one hyper parameter.

338
00:22:36,120 --> 00:22:39,040
And then, you know, later on, if there's more stuff you want to change, that's easy to

339
00:22:39,040 --> 00:22:40,880
do as well.

340
00:22:40,880 --> 00:22:45,240
And to kind of have nice high level abstractions.

341
00:22:45,240 --> 00:22:46,920
So that was the goal of the library.

342
00:22:46,920 --> 00:22:52,360
And so it's a great teaching tool, but I think it's also a good thing to use in practice.

343
00:22:52,360 --> 00:22:55,480
And you mentioned our benchmarks.

344
00:22:55,480 --> 00:23:01,720
And so this was part of Stanford, the Stanford Dawn lab hosted a competition called Stanford

345
00:23:01,720 --> 00:23:02,720
Dawn bench.

346
00:23:02,720 --> 00:23:07,920
And so this was for, there are two categories, ImageNet and CIFAR 10, which are these

347
00:23:07,920 --> 00:23:11,600
classic, image classification problems.

348
00:23:11,600 --> 00:23:17,040
However, typically, you know, like the ImageNet competition was about being most accurate.

349
00:23:17,040 --> 00:23:19,680
This was about being fastest and cheapest.

350
00:23:19,680 --> 00:23:21,600
And, you know, there was a baseline.

351
00:23:21,600 --> 00:23:27,200
You had to be at least 93% accurate, but past that point, yeah, what was fastest and

352
00:23:27,200 --> 00:23:28,200
cheapest?

353
00:23:28,200 --> 00:23:34,240
Sounds a lot like a TPC benchmarks, if you're familiar with those.

354
00:23:34,240 --> 00:23:35,240
I am not.

355
00:23:35,240 --> 00:23:36,920
I think it's not TPS.

356
00:23:36,920 --> 00:23:39,080
That's the office-based thing.

357
00:23:39,080 --> 00:23:41,880
It's a transaction processing council.

358
00:23:41,880 --> 00:23:49,280
It's like they do a transaction per second benchmarks across, like, you know, big iron.

359
00:23:49,280 --> 00:23:54,520
And there was a point in time where commodity computers, clusters of commodity compute

360
00:23:54,520 --> 00:24:00,760
started to supplant the, you know, the big expensive monolithic hardware boxes.

361
00:24:00,760 --> 00:24:05,680
And it sounds like from just kind of casually seeing some of the tweets and stuff about

362
00:24:05,680 --> 00:24:10,120
your benchmarks, that that's kind of what you, you know, what some of your results were

363
00:24:10,120 --> 00:24:11,120
about.

364
00:24:11,120 --> 00:24:16,760
Well, so what we found, so for the CIFAR 10, which is a smaller data set, although I think

365
00:24:16,760 --> 00:24:23,360
it's a size that's more, it's like 160 megabytes, that is representative of what a lot of

366
00:24:23,360 --> 00:24:25,440
businesses and companies have.

367
00:24:25,440 --> 00:24:30,840
We won both sections, fastest and cheapest.

368
00:24:30,840 --> 00:24:36,120
For the image net, these are larger data set, it's like 160 gigabytes.

369
00:24:36,120 --> 00:24:42,040
We were the fastest on publicly available infrastructure, fastest on GPUs, fastest on

370
00:24:42,040 --> 00:24:45,640
a single machine, and lowest actual cost.

371
00:24:45,640 --> 00:24:50,520
And I should say this was, this was Jeremy working with a group of our fast-day-eye students

372
00:24:50,520 --> 00:24:53,840
who kind of have this in-person study group that's been meeting every day.

373
00:24:53,840 --> 00:25:02,680
But I think it was really exciting to prove that the fast-day-eye library, you know, was

374
00:25:02,680 --> 00:25:03,680
super helpful to this.

375
00:25:03,680 --> 00:25:06,840
And so I mean, this was, you know, it's like fast-day-eye library, which is built on top

376
00:25:06,840 --> 00:25:07,840
of a pie torch.

377
00:25:07,840 --> 00:25:14,920
We're using Nvidia GPUs on AWS and AWS spot instances.

378
00:25:14,920 --> 00:25:20,080
But a lot of, so like Google and Intel, like their general strategy in this competition

379
00:25:20,080 --> 00:25:24,840
was kind of just having way more hardware.

380
00:25:24,840 --> 00:25:29,360
And we really tried to approach it as kind of using more algorithmic creativity because

381
00:25:29,360 --> 00:25:33,720
kind of a core, I think, part of our mission and like the thesis we're trying to prove

382
00:25:33,720 --> 00:25:40,600
with fast-day-eye is that deep learning can be accessible to people from all backgrounds

383
00:25:40,600 --> 00:25:45,600
and that you don't have to, you know, be able to afford like these very, very expensive

384
00:25:45,600 --> 00:25:49,880
clusters of machines and that you don't, you know, you don't need to have a PhD from

385
00:25:49,880 --> 00:25:51,040
Stanford.

386
00:25:51,040 --> 00:25:56,920
And so here, you know, this was a group of like part-time students and we're trying to

387
00:25:56,920 --> 00:25:58,720
try to do things cheaply.

388
00:25:58,720 --> 00:26:00,440
Her David and Goliath story.

389
00:26:00,440 --> 00:26:01,440
Yes.

390
00:26:01,440 --> 00:26:02,440
Yes, exactly.

391
00:26:02,440 --> 00:26:09,120
Yeah, our, we had an entry on a single machine that was faster than Intel's entry on a cluster

392
00:26:09,120 --> 00:26:10,120
of 128 machines.

393
00:26:10,120 --> 00:26:11,120
Oh, wow.

394
00:26:11,120 --> 00:26:17,560
And so, and so what we were doing here was just, yeah, kind of being creative with our algorithms

395
00:26:17,560 --> 00:26:21,640
and also using, I don't know, in the deep learning community, there's some results that

396
00:26:21,640 --> 00:26:27,120
have really kind of been neglected because they're not from, there's a lot of attention

397
00:26:27,120 --> 00:26:29,440
in the community to, you know, like what is Stanford doing?

398
00:26:29,440 --> 00:26:31,320
What is OpenAI doing?

399
00:26:31,320 --> 00:26:34,480
And, you know, these people that kind of have this name, Cache, or recognition.

400
00:26:34,480 --> 00:26:38,040
And so part of what we were doing was kind of implementing results from other people

401
00:26:38,040 --> 00:26:43,280
that are at these, you know, less famous or less prestigious institutions and showing

402
00:26:43,280 --> 00:26:49,000
like, hey, you know, we can, we can use this to, to get, to get faster results.

403
00:26:49,000 --> 00:26:55,040
How would you attribute the benefits, the performance, the ability to, to run at low

404
00:26:55,040 --> 00:26:59,720
cost between the fast AI library and pie torch?

405
00:26:59,720 --> 00:27:03,720
In other words, were you competing against other pie torch based entrants?

406
00:27:03,720 --> 00:27:06,320
Oh, now we have a shoe who gets, yeah.

407
00:27:06,320 --> 00:27:09,120
So I know the Google team was definitely using TensorFlow.

408
00:27:09,120 --> 00:27:13,560
And I am pretty sure Intel was using TensorFlow as well.

409
00:27:13,560 --> 00:27:15,480
Or even what about the structure?

410
00:27:15,480 --> 00:27:22,360
Tell us about the structure of the task and the, some of the, the key things, and it's

411
00:27:22,360 --> 00:27:27,600
interesting, the stuff we were implementing, they're actually not that complicated as ideas.

412
00:27:27,600 --> 00:27:33,080
So one of them is this idea of super convergence, and this is something that Leslie Smith,

413
00:27:33,080 --> 00:27:38,240
who works at the Naval, Naval Research Laboratory found, but it's the idea that you can

414
00:27:38,240 --> 00:27:44,480
use way higher learning rates if you lower your momentum.

415
00:27:44,480 --> 00:27:50,680
And momentum is kind of a factor used in, in optimization of, you know, they're all

416
00:27:50,680 --> 00:27:54,040
these variants on stochastic gradient is that, dissented.

417
00:27:54,040 --> 00:27:57,080
So momentum is kind of a commonly used part of that.

418
00:27:57,080 --> 00:28:03,320
And I don't know if anybody else is lowering though the, the momentum part, the, dynamically

419
00:28:03,320 --> 00:28:05,040
as training happens.

420
00:28:05,040 --> 00:28:07,920
So backing off momentum as you're converging?

421
00:28:07,920 --> 00:28:13,200
Well, as, well, actually, it's, it's backing off momentum as you're increasing your

422
00:28:13,200 --> 00:28:18,440
learning rates, these high rates, and then you decrease your learning rate again and increase

423
00:28:18,440 --> 00:28:19,440
your momentum.

424
00:28:19,440 --> 00:28:24,640
So kind of keeping, you kind of have this triangle shape with both, and they're like inversely

425
00:28:24,640 --> 00:28:25,640
related.

426
00:28:25,640 --> 00:28:26,640
Interesting.

427
00:28:26,640 --> 00:28:27,640
Yeah.

428
00:28:27,640 --> 00:28:31,200
So it's like, and you can check that we have a blog post where it kind of writes more details

429
00:28:31,200 --> 00:28:32,200
about what we were doing.

430
00:28:32,200 --> 00:28:36,240
But yeah, it was like learning rates, increasing, all the momentum's decreasing, and then

431
00:28:36,240 --> 00:28:39,560
learning rate decreases, well, momentum increases.

432
00:28:39,560 --> 00:28:43,200
But that, yeah, allows for much faster training.

433
00:28:43,200 --> 00:28:48,120
And so here, you know, this is something that, these are things that, the combination

434
00:28:48,120 --> 00:28:52,480
of fast AI and high torch made very easy to implement.

435
00:28:52,480 --> 00:28:57,080
And having, you know, having that kind of dynamic change as you're learning is something

436
00:28:57,080 --> 00:29:01,480
that is typically harder to do in TensorFlow.

437
00:29:01,480 --> 00:29:05,960
So this was, yeah, I mean, I think it's kind of hard to talk about fast AI.

438
00:29:05,960 --> 00:29:10,920
And, you know, it's the flexibility of high torch that may make fast AI possible.

439
00:29:10,920 --> 00:29:16,120
And, you know, fast AI is fairly flexible as well, since it's written on high torch.

440
00:29:16,120 --> 00:29:18,360
So that was one component.

441
00:29:18,360 --> 00:29:24,240
And then there was this other idea of progressive resizing where you start out training your

442
00:29:24,240 --> 00:29:28,960
network on small versions of your images, which makes sense.

443
00:29:28,960 --> 00:29:35,920
It's kind of just trying to learn, you know, like, very, very rough things, and then as

444
00:29:35,920 --> 00:29:40,560
training happens, you're using kind of larger and larger versions of the images.

445
00:29:40,560 --> 00:29:47,200
And so again, yeah, high torch is great to kind of have that dynamic nature.

446
00:29:47,200 --> 00:29:49,440
Oh, that sounds really interesting as well.

447
00:29:49,440 --> 00:29:53,920
And this is also something I think it's exciting, because it's like the ideas on their own

448
00:29:53,920 --> 00:29:59,480
like are not, you know, it's not like this, I don't know, like complicated thing of math

449
00:29:59,480 --> 00:30:03,120
equations, you know, it's, you know, it's like, hey, this makes sense, you know, intuitively

450
00:30:03,120 --> 00:30:06,680
of like, start training on small images and then train on larger ones.

451
00:30:06,680 --> 00:30:10,480
And, you know, it's also like that way your network kind of has information about images

452
00:30:10,480 --> 00:30:11,720
of different sizes.

453
00:30:11,720 --> 00:30:17,120
And so those are, you know, a lot of, a lot of the breakthroughs and Jeremy points

454
00:30:17,120 --> 00:30:20,280
this out in the blog post, a lot of these breakthroughs that have really helped us in

455
00:30:20,280 --> 00:30:27,720
deep learning, you know, things like using rectified linear units for activations and drop

456
00:30:27,720 --> 00:30:33,520
out where you kind of randomly drop a lot of, a lot of your weights to avoid overfitting

457
00:30:33,520 --> 00:30:38,080
each time or batch normalization, they're actually fairly simple ideas.

458
00:30:38,080 --> 00:30:43,760
And they need things in some ways like easier to understand while at the same time really

459
00:30:43,760 --> 00:30:47,160
improving the performance of neural networks.

460
00:30:47,160 --> 00:30:52,240
So that we really think a lot of the breakthroughs are going to come by kind of, you know, these

461
00:30:52,240 --> 00:30:57,480
new insight where you can like do something differently and not just from getting bigger

462
00:30:57,480 --> 00:31:00,880
and bigger clusters of computers.

463
00:31:00,880 --> 00:31:01,880
Interesting.

464
00:31:01,880 --> 00:31:11,000
And to what extent do you think the techniques that were used in the benchmark are, you

465
00:31:11,000 --> 00:31:12,520
know, practical techniques?

466
00:31:12,520 --> 00:31:18,400
In other words, are they kind of not gaming the benchmark with a negative connotation,

467
00:31:18,400 --> 00:31:22,520
but were they kind of designed to, you know, compete well in the benchmark, but not something

468
00:31:22,520 --> 00:31:27,000
that you would necessarily do in practice or, you know, they're very, very practical.

469
00:31:27,000 --> 00:31:28,000
Yeah.

470
00:31:28,000 --> 00:31:30,360
So there are all things you could do in practice.

471
00:31:30,360 --> 00:31:34,640
And some of them do still kind of raise issues, it's a one issue that we ran into and

472
00:31:34,640 --> 00:31:40,640
I think it's a kind of understudied issue in the field is that how to best use multiple

473
00:31:40,640 --> 00:31:46,760
GPUs, I think often people, they kind of release with new GPUs, you know, like how many calculations

474
00:31:46,760 --> 00:31:52,000
can be done on them, how quickly, but that doesn't take into account that like training

475
00:31:52,000 --> 00:31:56,520
your network really changes when you go from one GPU to many GPUs.

476
00:31:56,520 --> 00:32:00,320
And so you don't necessarily, well, you don't get the speed up of, you know, going from

477
00:32:00,320 --> 00:32:05,200
one GPU to a GPU is does not mean you're going to do stuff eight times faster.

478
00:32:05,200 --> 00:32:08,320
And so this is an area where I think that definitely needs to be more researched on on kind

479
00:32:08,320 --> 00:32:14,400
of how do you get the, get the most out of going from, yeah, one GPU to multi GPUs.

480
00:32:14,400 --> 00:32:19,400
Did you do anything with, with low precision in this, in this particular benchmark?

481
00:32:19,400 --> 00:32:20,400
We did.

482
00:32:20,400 --> 00:32:21,400
Yes.

483
00:32:21,400 --> 00:32:27,280
And so, and this is key to Nvidia's new bolt to architecture with key to this because

484
00:32:27,280 --> 00:32:31,400
it gives you half precision floating point data.

485
00:32:31,400 --> 00:32:36,960
And this was something that, I guess Nvidia had provided an open source demonstration

486
00:32:36,960 --> 00:32:41,040
and then we had a student that worked on this to incorporate the ideas into fast AI.

487
00:32:41,040 --> 00:32:43,080
And this is Andrew Shaw.

488
00:32:43,080 --> 00:32:49,240
But the kind of issue is that so half precision, you know, is kind of letting you do stuff

489
00:32:49,240 --> 00:32:55,200
using less space, but you do need to convert to full precision for some of the steps.

490
00:32:55,200 --> 00:32:59,720
So like when you multiply by your learning rate, but so that's something we've got, we've

491
00:32:59,720 --> 00:33:00,720
got implemented now.

492
00:33:00,720 --> 00:33:06,840
Yeah, that's something that I have the sense gets glossed over a bit and Nvidia's presentations

493
00:33:06,840 --> 00:33:07,840
about this.

494
00:33:07,840 --> 00:33:11,400
It's not like flipping a half, you know, a low precision switch and everything gets

495
00:33:11,400 --> 00:33:12,400
faster.

496
00:33:12,400 --> 00:33:13,400
Yeah.

497
00:33:13,400 --> 00:33:14,400
You have to really dig into it.

498
00:33:14,400 --> 00:33:16,640
Yeah, because you have to think about where, you know, where is it useful and not going

499
00:33:16,640 --> 00:33:17,640
to hurt you?

500
00:33:17,640 --> 00:33:20,640
Yeah, versus, okay, like where the places where you need full precision.

501
00:33:20,640 --> 00:33:25,080
And I think, I mean, I think that happens in a lot of kind of talking about hardware

502
00:33:25,080 --> 00:33:28,640
specs, of course, you know, because you're just like, you know, focused on this piece.

503
00:33:28,640 --> 00:33:32,400
And, you know, deep learning is this, you know, system that's got so many components

504
00:33:32,400 --> 00:33:34,840
and thinking about how is it work as a whole.

505
00:33:34,840 --> 00:33:39,040
Mm-hmm. And so where do you see the library going?

506
00:33:39,040 --> 00:33:41,200
Is it still evolving quickly?

507
00:33:41,200 --> 00:33:42,200
Is it?

508
00:33:42,200 --> 00:33:43,200
Yeah.

509
00:33:43,200 --> 00:33:44,200
Yes.

510
00:33:44,200 --> 00:33:45,200
Still evolving quickly.

511
00:33:45,200 --> 00:33:47,480
And I think we definitely still, like even coming from the competition, have a few

512
00:33:47,480 --> 00:33:50,400
more things we need to get incorporated.

513
00:33:50,400 --> 00:33:54,920
And then we also, we really want to work on documentation and usability in terms of

514
00:33:54,920 --> 00:33:55,920
for new users.

515
00:33:55,920 --> 00:33:59,240
I think if you're, you know, working through the course, that really gives you a lot

516
00:33:59,240 --> 00:34:04,720
of a lot of information and context of kind of seeing how things are built that we

517
00:34:04,720 --> 00:34:08,560
want to, yeah, kind of make it even more, more user friendly beyond that.

518
00:34:08,560 --> 00:34:13,920
And we'll continue, you know, as we, as we teach the course and as new papers come out,

519
00:34:13,920 --> 00:34:17,960
I think, and as we're doing more research, you know, that gives us kind of more ideas

520
00:34:17,960 --> 00:34:20,120
of things to implement.

521
00:34:20,120 --> 00:34:27,560
And do you see it primarily as an educational tool or something that someone would use?

522
00:34:27,560 --> 00:34:28,560
Yes.

523
00:34:28,560 --> 00:34:31,000
We see it as something that would be useful in the workplace.

524
00:34:31,000 --> 00:34:32,840
Okay.

525
00:34:32,840 --> 00:34:36,640
And is the, to what extent, I mean, you kind of addressed this in introducing it, but,

526
00:34:36,640 --> 00:34:46,920
you know, when I think of higher level abstractions, I certainly get the notion of, you know,

527
00:34:46,920 --> 00:34:52,640
saving folks that are new to the field from a lot of the details, but that saving, you

528
00:34:52,640 --> 00:34:58,240
know, usually comes from hiding, which can be a bit of an impediment to education and

529
00:34:58,240 --> 00:34:59,240
understanding.

530
00:34:59,240 --> 00:35:00,880
How do you balance those two?

531
00:35:00,880 --> 00:35:02,880
Yeah, that's a good question.

532
00:35:02,880 --> 00:35:06,920
So I mean, some of it is this, you know, this top down teaching approach I described earlier

533
00:35:06,920 --> 00:35:12,280
of like, yeah, like initially we are hiding a lot of things, but I think having the code

534
00:35:12,280 --> 00:35:18,080
written in such a way that it, like, we definitely then get into the details later on, and it

535
00:35:18,080 --> 00:35:24,120
is written in a way that, like, makes it easy for people to customize and if they need

536
00:35:24,120 --> 00:35:28,440
to modify things on a, on a lower level, they can do so.

537
00:35:28,440 --> 00:35:32,120
And so, like, our goal with the library is to kind of be providing both of those.

538
00:35:32,120 --> 00:35:36,240
And this is also, I mean, I think this is something that PyTorch is good at as well, but

539
00:35:36,240 --> 00:35:40,280
that if, yeah, if you want to, you know, add something to the library or change how it's

540
00:35:40,280 --> 00:35:46,720
doing something like there are places to add custom hooks and to use, I think the library

541
00:35:46,720 --> 00:35:49,520
is constructed to make that easy and reasonable.

542
00:35:49,520 --> 00:35:56,400
And is the idea that you would, you know, maybe use the library as a wrapper to some things

543
00:35:56,400 --> 00:36:01,040
that you don't want to, you know, mess with the details about, but then, you know, when

544
00:36:01,040 --> 00:36:06,120
you want to dig into those details, you would not use the library and go straight to PyTorch

545
00:36:06,120 --> 00:36:07,120
or is there a-

546
00:36:07,120 --> 00:36:10,480
I mean, I think it's, I think you would continue to use the library, it would more just

547
00:36:10,480 --> 00:36:14,600
be if you needed to change some, I don't know, if you're running some experiments, you

548
00:36:14,600 --> 00:36:19,200
know, your researcher kind of doing your own thing, you can, or you're, you know, you're

549
00:36:19,200 --> 00:36:24,440
trying out a new architecture that maybe then you would need to dip into changing some

550
00:36:24,440 --> 00:36:29,400
of the details, but I think that our goal is to have it, you know, I think part of what

551
00:36:29,400 --> 00:36:34,160
this competition showed is that the library itself is achieving state-of-the-art results.

552
00:36:34,160 --> 00:36:38,280
And so it's not that it's been a hurt your performance or something to be using this high

553
00:36:38,280 --> 00:36:39,280
level library.

554
00:36:39,280 --> 00:36:40,280
Right.

555
00:36:40,280 --> 00:36:48,280
So the performance and cost objectives of this benchmark are clearly important, particularly

556
00:36:48,280 --> 00:36:53,560
as folks are running these types of workloads, not just on machines that are sitting on

557
00:36:53,560 --> 00:37:00,400
their desk, but in the cloud where they're paying for them by the minute or hour.

558
00:37:00,400 --> 00:37:10,280
But another key often maybe overlooked benchmark or metric is around productivity and the ability

559
00:37:10,280 --> 00:37:12,400
to iterate, innovate quickly.

560
00:37:12,400 --> 00:37:14,640
Is anyone, are there any benchmarks for that?

561
00:37:14,640 --> 00:37:15,640
It's harder to do.

562
00:37:15,640 --> 00:37:18,160
Yeah, I don't know how you would benchmark that.

563
00:37:18,160 --> 00:37:23,040
I mean, I would say like, it's almost a bit of a meme on Twitter of a lot of people saying

564
00:37:23,040 --> 00:37:28,040
that like they have to, they use TensorFlow for their jobs, but they use PyTorch for their

565
00:37:28,040 --> 00:37:29,040
experimentation.

566
00:37:29,040 --> 00:37:32,640
I feel like that's what they're doing in the evenings.

567
00:37:32,640 --> 00:37:36,440
And I mean, there are plenty of companies using PyTorch as well, but just that I think

568
00:37:36,440 --> 00:37:42,480
that's part of the reason people love PyTorch is it's so easy to iterate and kind of

569
00:37:42,480 --> 00:37:46,440
experiment the way you're talking about, because I think, you know, there are people that

570
00:37:46,440 --> 00:37:50,960
will say like, oh, you know, competition graphs, like theoretically, that's letting the

571
00:37:50,960 --> 00:37:56,000
compiler optimize more, so you should get better performance with a, or sorry, with a static

572
00:37:56,000 --> 00:38:00,240
competition graph and with dynamic, but it's just that's not what people have found in

573
00:38:00,240 --> 00:38:01,240
practice.

574
00:38:01,240 --> 00:38:05,360
And I think part of that is with, with the dynamic framework, you're able to iterate

575
00:38:05,360 --> 00:38:06,360
so much quicker.

576
00:38:06,360 --> 00:38:10,760
So you do end up kind of getting better performance, but yeah, I don't, I don't know if any benchmarks

577
00:38:10,760 --> 00:38:14,160
that are formally, formally studying the distinction.

578
00:38:14,160 --> 00:38:15,160
Okay.

579
00:38:15,160 --> 00:38:23,480
So one of the often cited barriers to folks getting into this field is the level of math

580
00:38:23,480 --> 00:38:28,880
that's required and one of the things that you've done personally to try to address

581
00:38:28,880 --> 00:38:36,320
this is develop a class on linear algebra, computational linear algebra in particular.

582
00:38:36,320 --> 00:38:38,160
Can you talk a little bit about that?

583
00:38:38,160 --> 00:38:39,160
Sure.

584
00:38:39,160 --> 00:38:40,160
Yeah.

585
00:38:40,160 --> 00:38:41,160
So computational linear algebra.

586
00:38:41,160 --> 00:38:46,680
So this is a course I teach in the master's program at USF, but I've released all the

587
00:38:46,680 --> 00:38:49,400
videos and notebooks online.

588
00:38:49,400 --> 00:38:55,680
So it's a study of getting matrix, or so you're getting computers to do matrix calculations

589
00:38:55,680 --> 00:38:59,360
with acceptable speed and acceptable accuracy.

590
00:38:59,360 --> 00:39:05,680
And so I would say whether or not you have taken or liked kind of traditional linear algebra,

591
00:39:05,680 --> 00:39:10,000
this is a very different field because a traditional linear algebra course is typically,

592
00:39:10,000 --> 00:39:13,800
you know, having students kind of do these matrix computations by hand, which is just

593
00:39:13,800 --> 00:39:17,600
like a whole different set of considerations from when you're getting a computer to do

594
00:39:17,600 --> 00:39:18,600
them.

595
00:39:18,600 --> 00:39:25,320
But one of the kind of key themes of the course is the idea of decomposing matrices.

596
00:39:25,320 --> 00:39:30,480
As you can think of this, it's kind of analogous to, you know, like you can factor a number

597
00:39:30,480 --> 00:39:36,240
into primes and that's useful, you know, because primes have the special property, the

598
00:39:36,240 --> 00:39:41,520
same idea with matrices, you can kind of factor them into component matrices that have special

599
00:39:41,520 --> 00:39:42,520
properties.

600
00:39:42,520 --> 00:39:48,760
And so the course is, it's similar teaching philosophy to the deep learning course and

601
00:39:48,760 --> 00:39:51,640
that it's all, it's very code based.

602
00:39:51,640 --> 00:39:55,880
It's all in Jupyter notebooks and it's all centered around applications and so applications

603
00:39:55,880 --> 00:40:02,480
we look at are removing the background from a surveillance video of, you know, identifying

604
00:40:02,480 --> 00:40:08,200
what's foreground and background, topic modeling for a corpus of documents, kind of digging

605
00:40:08,200 --> 00:40:14,440
into, you know, Google's PageRank algorithm, because that's actually a matrix decomposition.

606
00:40:14,440 --> 00:40:21,400
And so kind of going, singular value decomposition, which is used for compressing data.

607
00:40:21,400 --> 00:40:24,760
It's also used for, you can remove errors.

608
00:40:24,760 --> 00:40:30,520
There's a really, really neat example of kind of a highly corrupted data set where there's

609
00:40:30,520 --> 00:40:34,200
just all these errors in the picture and you can actually remove it and kind of find what

610
00:40:34,200 --> 00:40:38,680
the underlying picture probably was of, this is of people's faces.

611
00:40:38,680 --> 00:40:42,640
So yeah, the course is very kind of application focused, but yeah, computational linear algebra

612
00:40:42,640 --> 00:40:48,400
includes a lot of things like approximate algorithms or randomized algorithms, you know,

613
00:40:48,400 --> 00:40:52,280
like often you, you know, like what are the cases where you don't need to be super precise

614
00:40:52,280 --> 00:40:55,800
or even, you know, often your data only has so much precision, so it doesn't make sense

615
00:40:55,800 --> 00:40:59,400
to, you know, try to get an algorithm that's going to give you something accurate to the

616
00:40:59,400 --> 00:41:02,440
10th decimal place.

617
00:41:02,440 --> 00:41:07,560
It also looks at, you know, issues around a machine epsilon and the kind of errors that

618
00:41:07,560 --> 00:41:11,240
can get introduced through through computer calculations.

619
00:41:11,240 --> 00:41:13,400
And what's machine epsilon?

620
00:41:13,400 --> 00:41:19,480
And so that's just, so the way, you know, it's like numbers are infinite and continuous

621
00:41:19,480 --> 00:41:23,200
and it's like computers are, you know, finite and limited.

622
00:41:23,200 --> 00:41:25,200
So floating point representation noise.

623
00:41:25,200 --> 00:41:27,040
Yeah, floating point representation, exactly.

624
00:41:27,040 --> 00:41:28,040
Okay.

625
00:41:28,040 --> 00:41:30,040
Well, that sounds like a really interesting class, too.

626
00:41:30,040 --> 00:41:35,040
This is how my classes to take lists gets really long.

627
00:41:35,040 --> 00:41:36,040
Right.

628
00:41:36,040 --> 00:41:38,040
Right.

629
00:41:38,040 --> 00:41:43,600
And so this one is in a structured class, but you've published all the materials online.

630
00:41:43,600 --> 00:41:45,080
Yeah, all the materials online.

631
00:41:45,080 --> 00:41:49,400
So yeah, on GitHub or all the Jupyter notebooks and then all the videos are on YouTube.

632
00:41:49,400 --> 00:41:51,080
Oh, that sounds very cool.

633
00:41:51,080 --> 00:41:56,760
Yeah, I feel like I've been talking a lot about like why I like pi torch more than TensorFlow.

634
00:41:56,760 --> 00:42:00,880
But I did want to bring up that I think TensorFlow has some really neat developments happening.

635
00:42:00,880 --> 00:42:06,040
And so I'm definitely still keeping my eye on TensorFlow.

636
00:42:06,040 --> 00:42:12,200
I think they, so they Chris Lattner announced at the TensorFlow Dev Summit two months ago

637
00:42:12,200 --> 00:42:18,160
that they're going to be releasing its Swift for TensorFlow.

638
00:42:18,160 --> 00:42:22,680
And so, you know, Swift is the language that Chris Lattner invented for iOS development.

639
00:42:22,680 --> 00:42:28,600
And I see I spent several months, I guess, in 2015 kind of like learning Swift and learning

640
00:42:28,600 --> 00:42:30,600
how to build mobile apps.

641
00:42:30,600 --> 00:42:36,400
And it is a really neat language that they're going to be kind of having a version of that.

642
00:42:36,400 --> 00:42:42,440
So it's like a whole nother language that would be, it would not be just for iOS.

643
00:42:42,440 --> 00:42:47,160
And it's, you know, specifically for neural networks, this Swift for TensorFlow version.

644
00:42:47,160 --> 00:42:49,680
That's something that really interests me.

645
00:42:49,680 --> 00:42:54,680
And then it was kind of exciting to hear some of the new releases.

646
00:42:54,680 --> 00:43:01,680
Like they now have a TensorFlow JavaScript version of TensorFlow, which is I think really great.

647
00:43:01,680 --> 00:43:09,680
They have released these Google Colab notebooks where you can kind of like basically launch these interactive examples.

648
00:43:09,680 --> 00:43:16,680
So I felt bad that I was perhaps criticizing TensorFlow earlier when I do think that there are some interesting developments happening there.

649
00:43:16,680 --> 00:43:21,680
Yeah, I think there's no question that Google's making some tremendous contributions with TensorFlow

650
00:43:21,680 --> 00:43:24,680
and that broader ecosystem.

651
00:43:24,680 --> 00:43:27,680
Yeah, and like, I mean, right now we're using PyTorch for the course.

652
00:43:27,680 --> 00:43:31,680
And you know, there are things I love about PyTorch, but I'm definitely not.

653
00:43:31,680 --> 00:43:36,680
And in general, I'm not until like, you know, saying like, hey, this is the language I'm always going to use.

654
00:43:36,680 --> 00:43:38,680
I hate other languages or anything.

655
00:43:38,680 --> 00:43:43,680
You know, it's you want to recognize kind of what each language is contributing and be flexible.

656
00:43:43,680 --> 00:43:51,680
So should we move on to them versus Emax now?

657
00:43:51,680 --> 00:43:54,680
I am Twitter and Twitter recently.

658
00:43:54,680 --> 00:43:59,680
I just did this poll about what would you call this type of data?

659
00:43:59,680 --> 00:44:08,680
It got more comments and engagement than like I think anything else I've ever tweeted.

660
00:44:08,680 --> 00:44:11,680
I was like asking asking programmers to name something as a.

661
00:44:11,680 --> 00:44:13,680
Tabs and spaces, anyone?

662
00:44:13,680 --> 00:44:14,680
Yeah.

663
00:44:14,680 --> 00:44:17,680
It's underrated a lot of debate.

664
00:44:17,680 --> 00:44:18,680
Nice, nice.

665
00:44:18,680 --> 00:44:19,680
Awesome.

666
00:44:19,680 --> 00:44:25,680
Is there anything else that you'd like to touch on or any final parting words for the audience?

667
00:44:25,680 --> 00:44:28,680
Definitely. I just want to encourage people to check out the deep learning course.

668
00:44:28,680 --> 00:44:31,680
That's a course dot fast dot AI.

669
00:44:31,680 --> 00:44:38,680
Absolutely. And if if anyone who's listening wants to join me in going through the course,

670
00:44:38,680 --> 00:44:47,680
starting the beginning of June, either hit me up on Twitter or via the twomla.com website

671
00:44:47,680 --> 00:44:53,680
or just go ahead and join our meetup and you'll get an invitation to our Slack channel.

672
00:44:53,680 --> 00:44:57,680
And just chime in there and we will we will get it going.

673
00:44:57,680 --> 00:44:58,680
Great.

674
00:44:58,680 --> 00:44:59,680
No, I think it's awesome.

675
00:44:59,680 --> 00:45:00,680
You're doing that.

676
00:45:00,680 --> 00:45:02,680
Awesome. Well, thanks so much, Rachel.

677
00:45:02,680 --> 00:45:04,680
Thank you, Sam.

678
00:45:04,680 --> 00:45:10,680
All right, everyone. That's our show for today.

679
00:45:10,680 --> 00:45:15,680
For more information on Rachel or any of the topics covered in this episode,

680
00:45:15,680 --> 00:45:20,680
head on over to twomla.com slash talk slash 138.

681
00:45:20,680 --> 00:45:25,680
Remember, the twomla online meetup is tomorrow and starting in June,

682
00:45:25,680 --> 00:45:30,680
we'll be organizing a group to take the fast AI practical deep learning course.

683
00:45:30,680 --> 00:45:36,680
Don't miss either and sign up for both at twomla.com slash meetup.

684
00:45:36,680 --> 00:46:03,680
Okay, thanks so much for listening and catch you next time.

