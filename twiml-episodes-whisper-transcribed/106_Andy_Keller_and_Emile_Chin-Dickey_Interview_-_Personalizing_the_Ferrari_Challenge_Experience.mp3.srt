1
00:00:00,000 --> 00:00:16,120
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,120 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:33,840
Contest alert.

5
00:00:33,840 --> 00:00:38,960
This week we have a jam-packed intro, including a new contest we're launching.

6
00:00:38,960 --> 00:00:43,040
So please bear with me, you don't want to miss this one.

7
00:00:43,040 --> 00:00:46,720
First, a bit about this week's shows.

8
00:00:46,720 --> 00:00:51,440
As you may know, I spent a few days at CES earlier this month.

9
00:00:51,440 --> 00:00:56,400
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,

10
00:00:56,400 --> 00:01:01,040
and I'm including you in those conversations via this series of shows.

11
00:01:01,040 --> 00:01:05,440
Stay tuned as we explore some of the very cool ways that machine learning and AI are being

12
00:01:05,440 --> 00:01:08,600
used to enhance our everyday lives.

13
00:01:08,600 --> 00:01:13,680
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered

14
00:01:13,680 --> 00:01:15,280
robot.

15
00:01:15,280 --> 00:01:22,120
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.

16
00:01:22,120 --> 00:01:28,120
Intel, who's using the single-shot multi-box image detection algorithm to personalize video

17
00:01:28,120 --> 00:01:31,680
fees for the Ferrari Challenge North America.

18
00:01:31,680 --> 00:01:36,480
First beat, a company whose machine learning algorithms analyzed your heartbeat data to

19
00:01:36,480 --> 00:01:42,360
provide personalized insights into stress, exercise, and sleep patterns.

20
00:01:42,360 --> 00:01:48,400
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams

21
00:01:48,400 --> 00:01:52,280
or automatically adjusting high beams to the U.S.

22
00:01:52,280 --> 00:01:59,480
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to

23
00:01:59,480 --> 00:02:05,640
enable some really interesting home automation and healthcare applications.

24
00:02:05,640 --> 00:02:11,000
Now, as if six amazing interviews wasn't enough, a few of these companies have been so

25
00:02:11,000 --> 00:02:15,520
kind as to provide us with products for you, the Twimmel community.

26
00:02:15,520 --> 00:02:19,360
And keeping with the theme of this series, our contest will be a little different this

27
00:02:19,360 --> 00:02:20,360
time.

28
00:02:20,360 --> 00:02:25,400
To enter, we want to hear from you about the role AI is playing in your home and personal

29
00:02:25,400 --> 00:02:28,640
life, and where you see it going.

30
00:02:28,640 --> 00:02:36,240
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera, and

31
00:02:36,240 --> 00:02:39,120
tell us your story in two minutes or less.

32
00:02:39,120 --> 00:02:43,400
We'll post the videos to YouTube, and the video with the most likes wins their choice

33
00:02:43,400 --> 00:02:50,480
of great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.

34
00:02:50,480 --> 00:02:55,040
Submissions will be taken until February 11th, and voting will remain open until February

35
00:02:55,040 --> 00:02:56,040
18th.

36
00:02:56,040 --> 00:03:02,360
Good luck.

37
00:03:02,360 --> 00:03:06,560
Before we dive into today's show, I'd like to thank our friends at Intel AI for their

38
00:03:06,560 --> 00:03:09,520
continued support of this podcast.

39
00:03:09,520 --> 00:03:14,760
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving

40
00:03:14,760 --> 00:03:17,200
and VR related announcements.

41
00:03:17,200 --> 00:03:21,440
One of the more interesting partnerships they announced was a collaboration with the Ferrari

42
00:03:21,440 --> 00:03:24,720
Challenge North America race series.

43
00:03:24,720 --> 00:03:29,440
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience

44
00:03:29,440 --> 00:03:35,360
more personalized by using deep computer vision to detect and monitor individual race

45
00:03:35,360 --> 00:03:40,640
cars via camera feeds and allow viewers to choose the specific cars feeds that they'd

46
00:03:40,640 --> 00:03:42,760
like to watch.

47
00:03:42,760 --> 00:03:47,080
You'll learn much more about this application in today's show, which features Intel's

48
00:03:47,080 --> 00:03:49,800
Andy Keller and Emil Chinnicki.

49
00:03:49,800 --> 00:03:55,040
Andy is a deep learning data scientist at Intel, and Emil Chinnicki is senior manager of marketing

50
00:03:55,040 --> 00:03:57,120
partnerships at the company.

51
00:03:57,120 --> 00:04:02,080
In this show, Emil gives us a high level overview of the Ferrari Challenge partnership and the

52
00:04:02,080 --> 00:04:04,560
goals of the collaboration.

53
00:04:04,560 --> 00:04:08,920
Andy and I then dive into the various machine learning aspects of this project, including

54
00:04:08,920 --> 00:04:13,880
how the training data was collected, the techniques they used to perform fine-grained object

55
00:04:13,880 --> 00:04:19,280
detection in the video streams, how they built the analytics platform, some of their remaining

56
00:04:19,280 --> 00:04:21,960
challenges, and more.

57
00:04:21,960 --> 00:04:23,960
And now on to the show.

58
00:04:23,960 --> 00:04:24,960
All right, everyone.

59
00:04:24,960 --> 00:04:31,240
We are here at CES, and I have the pleasure of being seated with Andy Keller, who is

60
00:04:31,240 --> 00:04:37,400
a deep learning data scientist at Intel, and Emil Chinnicki, who is a senior manager of

61
00:04:37,400 --> 00:04:43,040
marketing partnerships at Intel, and we've got an opportunity to chat about one of the

62
00:04:43,040 --> 00:04:48,000
cool announcements that was made yesterday, or today it was kind of announced yesterday,

63
00:04:48,000 --> 00:04:51,040
and then kind of more fully announced today.

64
00:04:51,040 --> 00:04:56,080
I'm going to keep you in suspense for just a little bit longer, and actually ask these

65
00:04:56,080 --> 00:05:01,440
guys to introduce themselves, and then we'll get into what that announcement was.

66
00:05:01,440 --> 00:05:02,840
So why don't we start with you, Andy?

67
00:05:02,840 --> 00:05:05,160
Yeah, thanks for having me.

68
00:05:05,160 --> 00:05:07,840
So I'm a deep learning data scientist at Intel.

69
00:05:07,840 --> 00:05:16,480
I started at Nirvana before the acquisition, working as an intern, doing some of the same

70
00:05:16,480 --> 00:05:25,000
kind of object localization stuff that we're using for this soon-to-be-in-house project,

71
00:05:25,000 --> 00:05:30,000
and specifically I was implementing some other models, like FasterRCNN.

72
00:05:30,000 --> 00:05:31,000
Like what?

73
00:05:31,000 --> 00:05:32,000
FasterRCNN?

74
00:05:32,000 --> 00:05:33,000
Okay.

75
00:05:33,000 --> 00:05:42,800
More recently, I've moved to kind of more similar work that I did for my masters at UCSD,

76
00:05:42,800 --> 00:05:49,800
which is natural language processing and dialogue systems and question-answer systems.

77
00:05:49,800 --> 00:05:50,800
Okay.

78
00:05:50,800 --> 00:05:59,800
And so now I'm started full-time after finishing my degree, and I love being here.

79
00:05:59,800 --> 00:06:00,800
Awesome.

80
00:06:00,800 --> 00:06:01,800
Awesome.

81
00:06:01,800 --> 00:06:02,800
The mail?

82
00:06:02,800 --> 00:06:03,800
My name's Emil Tinnicki.

83
00:06:03,800 --> 00:06:12,680
I joined last year the Artificial Intelligence Products Group at Intel, and I'm responsible

84
00:06:12,680 --> 00:06:13,880
for marketing partnerships.

85
00:06:13,880 --> 00:06:20,600
So what that means is that I work with our partner companies to showcase both partner

86
00:06:20,600 --> 00:06:28,040
technologies, as well as intel technologies, showcasing our efforts on an artificial intelligence.

87
00:06:28,040 --> 00:06:34,840
And in fact, there's one of these partnerships that Intel CEO Brian Krozenich announced

88
00:06:34,840 --> 00:06:41,640
at his CES keynote last night, and it's a partnership with Ferrari Racing.

89
00:06:41,640 --> 00:06:49,160
And I think for me, thinking back to Brian's keynote, it was a little surprising for

90
00:06:49,160 --> 00:06:55,560
me, I guess, you know, I kind of exist in this AI bubble, and I expected it to be just

91
00:06:55,560 --> 00:06:59,400
pure AI consumer and devices stuff.

92
00:06:59,400 --> 00:07:06,800
But Intel as a company is way more all-in into this virtual reality and immersive experience

93
00:07:06,800 --> 00:07:09,480
that then I knew.

94
00:07:09,480 --> 00:07:15,480
I didn't know anything about Intel's efforts around true VR and the other virtual reality

95
00:07:15,480 --> 00:07:20,760
plays in the immersive experience that he talked about.

96
00:07:20,760 --> 00:07:28,200
I mean, it's everything from outfitting the Olympics that are coming up.

97
00:07:28,200 --> 00:07:36,600
You guys built a studio, like a volume metric studio, and there was a partnership with Paramount

98
00:07:36,600 --> 00:07:38,400
Theaters announced.

99
00:07:38,400 --> 00:07:44,560
There was a ton of discussion around like how you would use the stuff in sports.

100
00:07:44,560 --> 00:07:48,840
And then one of the partnerships that was announced in that context was this partnership

101
00:07:48,840 --> 00:07:50,360
with Ferrari.

102
00:07:50,360 --> 00:07:54,960
So tell us a little bit about that partnership and what you're hoping to achieve with it.

103
00:07:54,960 --> 00:08:00,200
Yeah, so what was announced was a three-year partnership with Ferrari Challenge.

104
00:08:00,200 --> 00:08:07,480
Ferrari challenges a race series put on by Ferrari, and it takes place in different regions.

105
00:08:07,480 --> 00:08:10,600
We're specifically partnering with the North America team.

106
00:08:10,600 --> 00:08:16,320
So one of the areas that we're applying artificial intelligence is to the race stream.

107
00:08:16,320 --> 00:08:25,560
So that's basically applying AI techniques around a single shot detection and fine green

108
00:08:25,560 --> 00:08:33,400
classification to recognize the cars that are being captured in each of the camera feeds.

109
00:08:33,400 --> 00:08:42,440
And then the intent is to use that as metadata to curate camera feeds for viewers that are

110
00:08:42,440 --> 00:08:45,840
specific to each car or driver.

111
00:08:45,840 --> 00:08:53,840
So basically a fan could pick their favorite driver and we could deliver a curated feed

112
00:08:53,840 --> 00:08:58,920
specific to that driver as they make their way around the entire track.

113
00:08:58,920 --> 00:09:07,880
As I understand it, it's not just fans, but today, as every all of the folks that are

114
00:09:07,880 --> 00:09:13,680
broadcasting a race like the Ferrari Challenge, they're all operating off of the same feed,

115
00:09:13,680 --> 00:09:15,800
like everybody gets the same feed.

116
00:09:15,800 --> 00:09:17,200
Is that how it's working today?

117
00:09:17,200 --> 00:09:23,320
Yeah, so if you think of a traditional race broadcast or more typical race broadcast,

118
00:09:23,320 --> 00:09:30,880
you have kind of a director's cut of the action and normally the feed is focused on the

119
00:09:30,880 --> 00:09:33,800
first three or four cars in the race.

120
00:09:33,800 --> 00:09:40,520
So if your favorite driver is not among those couple cars, then you may not actually see

121
00:09:40,520 --> 00:09:44,360
too much of them during a broadcast.

122
00:09:44,360 --> 00:09:52,480
We notice that people's consumption of media is changing and we expect that there's no

123
00:09:52,480 --> 00:09:59,440
reason why that shouldn't change for motorsports broadcasts as well.

124
00:09:59,440 --> 00:10:05,960
This basically puts more choice in the hands of the fan to kind of get a more tailored

125
00:10:05,960 --> 00:10:09,840
customized viewing experience.

126
00:10:09,840 --> 00:10:13,360
How would I configure my feed?

127
00:10:13,360 --> 00:10:20,000
Is this something that is this being delivered for me like via a live stream and I'm choosing

128
00:10:20,000 --> 00:10:26,400
a car or something like that or what's the experience of tailoring the feed?

129
00:10:26,400 --> 00:10:32,760
Yeah, so the intention is that this would be a stream that would be broadcast live over

130
00:10:32,760 --> 00:10:40,320
an app or a website and exactly what you said, you could pick your favorite driver and

131
00:10:40,320 --> 00:10:45,480
kind of get that tailored stream according to that driver.

132
00:10:45,480 --> 00:10:51,840
And this isn't being done by just putting traditional cameras around the track?

133
00:10:51,840 --> 00:10:54,160
Well, yeah, so there's two components.

134
00:10:54,160 --> 00:10:59,080
We're using drones to capture the race footage.

135
00:10:59,080 --> 00:11:06,360
So that actually adds a rather dramatic element to the race experience, if you will.

136
00:11:06,360 --> 00:11:09,360
So these are not fixed point cameras.

137
00:11:09,360 --> 00:11:15,640
And then the notion is that you can apply these recognition techniques to each of those

138
00:11:15,640 --> 00:11:20,560
camera feeds so that at any given point in time, you know exactly which cars are showing

139
00:11:20,560 --> 00:11:22,160
up in which camera feeds.

140
00:11:22,160 --> 00:11:23,160
Okay.

141
00:11:23,160 --> 00:11:28,840
So with that information, you can kind of tailor that stream to a end user and viewer.

142
00:11:28,840 --> 00:11:29,840
Interesting.

143
00:11:29,840 --> 00:11:34,240
And how many drones are going to be flying around the Ferrari challenge when this thing

144
00:11:34,240 --> 00:11:35,240
is running?

145
00:11:35,240 --> 00:11:39,680
It's somewhat a function of the track configuration and length.

146
00:11:39,680 --> 00:11:45,800
But for starters, we're looking at between five and six drones.

147
00:11:45,800 --> 00:11:47,240
And we'll kind of go from there.

148
00:11:47,240 --> 00:11:54,240
And are the drones mostly kind of fixed in each one covers an area of the track or are

149
00:11:54,240 --> 00:11:57,720
they like following the cars or something?

150
00:11:57,720 --> 00:11:58,720
Yeah.

151
00:11:58,720 --> 00:12:06,000
So there's, they're not fixed in one location, they have kind of more of like a territory

152
00:12:06,000 --> 00:12:08,280
if you will that cover.

153
00:12:08,280 --> 00:12:10,520
So they may, you may see them like zipping around.

154
00:12:10,520 --> 00:12:11,880
Yeah, they'll be moving around.

155
00:12:11,880 --> 00:12:12,880
Territory.

156
00:12:12,880 --> 00:12:13,880
Yeah.

157
00:12:13,880 --> 00:12:14,880
Exactly.

158
00:12:14,880 --> 00:12:15,880
Okay.

159
00:12:15,880 --> 00:12:20,360
So I'm going to someone on your team yesterday about this at the, like, at the keynote,

160
00:12:20,360 --> 00:12:22,840
all right, before the keynote.

161
00:12:22,840 --> 00:12:28,640
And you know, we're going to talk a little bit about the kind of the AI elements and

162
00:12:28,640 --> 00:12:30,680
challenges associated with this.

163
00:12:30,680 --> 00:12:36,080
But, you know, just thinking about even like the, you know, this day and age, you've got

164
00:12:36,080 --> 00:12:41,960
to be capturing like 4K, 8K streams, like off of a drone and like getting those down

165
00:12:41,960 --> 00:12:45,400
to, you know, some place where you're going to process.

166
00:12:45,400 --> 00:12:49,200
I mean, aside, there's a ton of stuff that, you know, keeping the drones in the air is

167
00:12:49,200 --> 00:12:50,600
going to challenge.

168
00:12:50,600 --> 00:12:54,840
But a ton of technical challenges here, really, really interesting.

169
00:12:54,840 --> 00:12:59,600
So maybe this is a good segue to actually talk about some of the AI stuff.

170
00:12:59,600 --> 00:13:03,280
And Andy, this is where some of your work is come in.

171
00:13:03,280 --> 00:13:04,280
Yeah.

172
00:13:04,280 --> 00:13:09,680
Maybe tell us a little bit about kind of the underlying AI technologies and what the primary

173
00:13:09,680 --> 00:13:12,280
goals are for the project that you worked on.

174
00:13:12,280 --> 00:13:13,280
Yeah.

175
00:13:13,280 --> 00:13:15,920
So we thought a good place to start.

176
00:13:15,920 --> 00:13:21,920
We needed a foundation on which to kind of build other analytics about all of the drivers

177
00:13:21,920 --> 00:13:23,440
and the racing that was going on.

178
00:13:23,440 --> 00:13:30,960
So we realized we needed something else to build some sort of analytics platform on.

179
00:13:30,960 --> 00:13:36,440
And to start that, we really needed to be able to know, okay, we're going to be using

180
00:13:36,440 --> 00:13:37,680
the video footage.

181
00:13:37,680 --> 00:13:41,320
We need to know which drivers we're looking at in any given feed and we need to know where

182
00:13:41,320 --> 00:13:42,320
they are.

183
00:13:42,320 --> 00:13:44,040
We need to actually be able to localize them.

184
00:13:44,040 --> 00:13:48,560
So obviously the first thing that makes sense is some sort of object detection or object

185
00:13:48,560 --> 00:13:55,280
localization model, which can draw boxes around all the cars on the track and then potentially

186
00:13:55,280 --> 00:14:00,920
classify them uniquely as who's in what car.

187
00:14:00,920 --> 00:14:08,400
So kind of as a starting point for that, we had tested out using models that were pre-trained

188
00:14:08,400 --> 00:14:13,800
on some other data sets and realized that pretty quickly realized we were going to need to

189
00:14:13,800 --> 00:14:23,320
gather domain specific data set just because like Emule mentioned, the drone footage is

190
00:14:23,320 --> 00:14:28,360
dramatic in some sense and you don't find a lot of that out on YouTube.

191
00:14:28,360 --> 00:14:31,120
Exactly.

192
00:14:31,120 --> 00:14:33,640
What did you start with?

193
00:14:33,640 --> 00:14:37,720
What data sets did you try to train on initially?

194
00:14:37,720 --> 00:14:43,760
Yeah, we were using the kitty data set, which is kind of what in the self-driving

195
00:14:43,760 --> 00:14:52,160
data sets, but so it has bounding boxes around all of the cars and you can build a basic

196
00:14:52,160 --> 00:14:56,480
car detector off of that, but it's all from the viewpoint of kind of the dashboard of

197
00:14:56,480 --> 00:14:58,240
a car.

198
00:14:58,240 --> 00:15:03,560
So some of those viewpoints apply from a drone, but a lot of our shots are really long

199
00:15:03,560 --> 00:15:10,600
distance and kind of unique angles that we'd never seen before in that data set.

200
00:15:10,600 --> 00:15:15,400
Right, that was kind of our main driving factor in generating this new data set, which

201
00:15:15,400 --> 00:15:20,040
was a lot of the portion of the beginning of this project.

202
00:15:20,040 --> 00:15:28,120
So in the typical race, you'll have these five or six drones kind of zipping around

203
00:15:28,120 --> 00:15:34,680
their territories, capturing data of the field as a dozen or two dozen cars typically.

204
00:15:34,680 --> 00:15:38,440
Yeah, somewhere around there.

205
00:15:38,440 --> 00:15:44,840
And for each of these drones is instrumented with a camera and you're pulling down a

206
00:15:44,840 --> 00:15:49,280
feed and you're doing what otherwise might seem like a typical autonomous driving

207
00:15:49,280 --> 00:15:55,280
task, like putting bounding boxes around the cars that are on the track.

208
00:15:55,280 --> 00:15:59,760
So the first step is building a model based on this kitty data set.

209
00:15:59,760 --> 00:16:03,960
And then that wasn't giving you the accuracy that you're looking for.

210
00:16:03,960 --> 00:16:09,080
So you started building your own data set and was this like buys, you know, running

211
00:16:09,080 --> 00:16:12,800
drones on top of racetracks or yeah, basically.

212
00:16:12,800 --> 00:16:20,320
So we had a substantial amount of footage from across the entire 2017 season of recording

213
00:16:20,320 --> 00:16:23,400
from a bunch of drones at every single race.

214
00:16:23,400 --> 00:16:24,400
Okay.

215
00:16:24,400 --> 00:16:26,400
And this was in 4K.

216
00:16:26,400 --> 00:16:33,360
And so we knew that a lot of this footage maybe wasn't as useful.

217
00:16:33,360 --> 00:16:41,440
So we needed to kind of sort through that as a first step to generating a training set.

218
00:16:41,440 --> 00:16:49,280
So we had probably hundreds of hours of 4K footage and it was some of the data scientists

219
00:16:49,280 --> 00:16:53,120
jobs actually look through and find kind of really high variance shots, shots with

220
00:16:53,120 --> 00:16:57,160
differences in lighting, differences in size of the cars.

221
00:16:57,160 --> 00:17:01,840
Since these drones will move anywhere from 10 feet off the ground to 100 feet off the

222
00:17:01,840 --> 00:17:06,520
ground, that really changes the appearance of the car and what the model is going to be

223
00:17:06,520 --> 00:17:08,200
able to learn.

224
00:17:08,200 --> 00:17:11,680
So we really tried to get as much variance as possible.

225
00:17:11,680 --> 00:17:16,960
And what was the approach to doing that was this something that, you know, that was kind

226
00:17:16,960 --> 00:17:23,600
of manually coming through footage and hand annotating or did you, did you automate it

227
00:17:23,600 --> 00:17:26,320
in some way?

228
00:17:26,320 --> 00:17:34,920
We did have some of the cut from the live broadcast that was kind of curated by the drone

229
00:17:34,920 --> 00:17:38,560
team that was broadcast on the stream during the race.

230
00:17:38,560 --> 00:17:43,840
So we knew a little bit of kind of what shots to follow and what was the general.

231
00:17:43,840 --> 00:17:48,280
I mean, obviously there are at least going to have cars in the shot if it's in the broadcast.

232
00:17:48,280 --> 00:17:55,600
So we were able to use those and then some amount of manually coming through unfortunately.

233
00:17:55,600 --> 00:18:02,720
That wasn't the most exciting way that you spent time.

234
00:18:02,720 --> 00:18:10,680
And with that, did you, the manual coming through, like, what, did you use some, like, some

235
00:18:10,680 --> 00:18:14,680
kind of off-to-shelf tools to facilitate that?

236
00:18:14,680 --> 00:18:21,360
Luckily, we were kind of able to just write down timestamps in videos.

237
00:18:21,360 --> 00:18:26,320
And then I had written some scripts and FFM Peg is kind of the default tool for-

238
00:18:26,320 --> 00:18:30,280
To just snip out snippets and dump them in a drive or something like that.

239
00:18:30,280 --> 00:18:34,960
And so once we had kind of parsed through all of those and extracted some percentage of

240
00:18:34,960 --> 00:18:42,560
the frames, we were able to send those off to a labeling company and then provide them

241
00:18:42,560 --> 00:18:47,560
with a series of guidelines and kind of helpful data sheets about each of the different

242
00:18:47,560 --> 00:18:53,840
cars that we wanted labeled since we had something close to 40 or 50 different cars, or some

243
00:18:53,840 --> 00:18:55,880
of them looked pretty similar.

244
00:18:55,880 --> 00:19:01,480
So it was a, that was kind of the, one of the larger challenges was making sure the labeling

245
00:19:01,480 --> 00:19:06,600
team was actually able to do their job correctly such that the model itself was able to do

246
00:19:06,600 --> 00:19:07,600
it.

247
00:19:07,600 --> 00:19:12,920
It's just, it's, it's, so how long do you have a sense for how long you spent on just this

248
00:19:12,920 --> 00:19:22,680
kind of data collection and pre, like pre labeling task?

249
00:19:22,680 --> 00:19:30,560
The data collection itself, the recording of the video was over the entire 2017 race season,

250
00:19:30,560 --> 00:19:35,240
so I guess that maybe shouldn't be included.

251
00:19:35,240 --> 00:19:41,280
But kind of the curation of what we were going to provide to them, I would say happened

252
00:19:41,280 --> 00:19:46,520
over a week or two, we were able to get a couple of different data scientists looking

253
00:19:46,520 --> 00:19:49,520
at it and people who.

254
00:19:49,520 --> 00:19:55,480
And you started with 100 or so hours, I think you said, what was the, how, what was the

255
00:19:55,480 --> 00:20:01,800
size, like if you added up all the snippets that you sent on to get annotated, how many

256
00:20:01,800 --> 00:20:04,400
hours did you have of that?

257
00:20:04,400 --> 00:20:06,680
It was close to I think one hour.

258
00:20:06,680 --> 00:20:07,680
Okay.

259
00:20:07,680 --> 00:20:08,680
So significant.

260
00:20:08,680 --> 00:20:14,440
So dense, set, yeah.

261
00:20:14,440 --> 00:20:21,400
We even, after that, we obviously, because from frame to frame, there's really not that

262
00:20:21,400 --> 00:20:22,920
much difference.

263
00:20:22,920 --> 00:20:28,960
We were pulling something like maybe 10% of the frames, so that we can get more variety

264
00:20:28,960 --> 00:20:35,800
with a lower number of frames, since it really, every single frame is more effort for the

265
00:20:35,800 --> 00:20:39,840
labeling team and you're really trying to reduce that as much as possible.

266
00:20:39,840 --> 00:20:40,840
Right.

267
00:20:40,840 --> 00:20:41,840
Right.

268
00:20:41,840 --> 00:20:48,720
So meaning you used 10% of the frames in the one hour of footage.

269
00:20:48,720 --> 00:20:54,840
And then you had this label by the labeling team, I would have imagined that you needed

270
00:20:54,840 --> 00:21:02,520
a lot more training examples to achieve, you know, acceptable performance on this.

271
00:21:02,520 --> 00:21:09,960
Were you using these examples in conjunction with the previous, like was this a transfer

272
00:21:09,960 --> 00:21:15,280
learning type of task, or did you train from scratch with these new examples?

273
00:21:15,280 --> 00:21:16,280
Yeah.

274
00:21:16,280 --> 00:21:19,120
We ended up actually going straight from scratch.

275
00:21:19,120 --> 00:21:23,720
We kind of tried to approximate the size of some of the other popular object detection

276
00:21:23,720 --> 00:21:26,080
data sets, like Pascal VOC.

277
00:21:26,080 --> 00:21:27,080
Okay.

278
00:21:27,080 --> 00:21:35,800
And so that was kind of our goal to begin with, and basically from scratch was a little

279
00:21:35,800 --> 00:21:42,240
bit challenging, but we realized we probably needed to do it that way just because of kind

280
00:21:42,240 --> 00:21:51,480
of some of the uniqueness of this problem, the fact that these cars are very small and

281
00:21:51,480 --> 00:21:54,200
we have 50 different classes, but they all look very similar.

282
00:21:54,200 --> 00:21:58,240
So it's more of a fine grain classification problem, which a lot of these typical object

283
00:21:58,240 --> 00:22:03,920
detection data sets don't really cover.

284
00:22:03,920 --> 00:22:11,920
And is it generally the case that for these fine grain types of object identification

285
00:22:11,920 --> 00:22:17,800
problems that you need less data than, you know, if you were training up kind of object,

286
00:22:17,800 --> 00:22:23,160
you know, coarse grain object detection scratch?

287
00:22:23,160 --> 00:22:29,160
I don't know if I'd say less data, I think it depends on also how you develop the model.

288
00:22:29,160 --> 00:22:35,760
So there are some kind of future steps and kind of some of the state of the art and fine

289
00:22:35,760 --> 00:22:41,680
grain detection that we're planning to implement where you'll train a deep network and then

290
00:22:41,680 --> 00:22:46,360
chop off the top and add an SVM on top.

291
00:22:46,360 --> 00:22:51,520
And that was able to do a lot better at these tasks where the features between the two

292
00:22:51,520 --> 00:22:58,640
classes are very similar, but there is a boundary versus kind of the traditional something

293
00:22:58,640 --> 00:23:06,600
with something like what we use for SSD where it's an end-to-end neural network.

294
00:23:06,600 --> 00:23:16,000
So I think for the end-to-end case, you probably need about the same amount of data.

295
00:23:16,000 --> 00:23:26,720
We ended up having probably close to 1,000 labels per car, maybe 2,000, which seemed to

296
00:23:26,720 --> 00:23:29,920
be similar to some of these other data sets.

297
00:23:29,920 --> 00:23:30,920
Okay.

298
00:23:30,920 --> 00:23:34,920
Obviously some cars were more frequent than others and there wasn't a ton we could do besides

299
00:23:34,920 --> 00:23:42,080
rebalancing afterwards, but yeah, it turned out to work pretty well.

300
00:23:42,080 --> 00:23:43,080
Oh, nice.

301
00:23:43,080 --> 00:23:45,720
Thank you.

302
00:23:45,720 --> 00:23:52,520
What kind of model approach model architecture did you end up using and how did you arrive

303
00:23:52,520 --> 00:23:53,520
at that?

304
00:23:53,520 --> 00:23:54,520
Yeah.

305
00:23:54,520 --> 00:24:03,680
So we ended up using a single shot multi-box detector, which I think Andres, who was on

306
00:24:03,680 --> 00:24:07,160
your show a few weeks ago talking about the NASA project.

307
00:24:07,160 --> 00:24:11,840
They also used that for the crater detection.

308
00:24:11,840 --> 00:24:18,960
So because of some of the optimizations that we have in Neon for Intel, that made sense

309
00:24:18,960 --> 00:24:19,960
on that front.

310
00:24:19,960 --> 00:24:24,240
We were able to get kind of the live speed that we were looking for.

311
00:24:24,240 --> 00:24:27,960
And then also kind of the general architecture itself.

312
00:24:27,960 --> 00:24:36,200
It's a one shot, not one shot in one shot learning sense, but in a single shot like it, it's

313
00:24:36,200 --> 00:24:37,200
a single architecture.

314
00:24:37,200 --> 00:24:42,600
You don't have kind of a two-step process like faster or CNN has where first it proposes

315
00:24:42,600 --> 00:24:47,680
boxes and then it does the classification, which kind of happens all at once.

316
00:24:47,680 --> 00:24:54,240
So that is partially one of the reasons why it's significantly faster, which we liked.

317
00:24:54,240 --> 00:24:59,240
And it also has kind of feature maps of multiple different scales.

318
00:24:59,240 --> 00:25:02,960
So what does that mean?

319
00:25:02,960 --> 00:25:10,240
We have a convolutional networks when they operate and you go up and hire layers, the kind

320
00:25:10,240 --> 00:25:14,600
of the feature maps continue to shrink down and down further.

321
00:25:14,600 --> 00:25:22,000
So if we provide all of those to the end classifier, it's able to kind of get the same object

322
00:25:22,000 --> 00:25:23,840
at a bunch of different resolutions.

323
00:25:23,840 --> 00:25:30,120
So it's able to, in the end, basically means we're able to classify objects better at a

324
00:25:30,120 --> 00:25:31,400
lot of different scales.

325
00:25:31,400 --> 00:25:35,480
So small objects, big objects, which was really one of the main challenges of this problem

326
00:25:35,480 --> 00:25:37,640
in this data set.

327
00:25:37,640 --> 00:25:42,560
You mentioned that one of the things that you looked at or are looking at or considered

328
00:25:42,560 --> 00:25:49,520
was doing a network where you kind of chopped off the end of the network and replace it

329
00:25:49,520 --> 00:25:52,240
with an SVM.

330
00:25:52,240 --> 00:25:54,040
How far did you go down that path?

331
00:25:54,040 --> 00:25:56,960
We haven't gotten there at all yet.

332
00:25:56,960 --> 00:26:01,720
So there's a lot of next steps that we're looking into.

333
00:26:01,720 --> 00:26:08,760
I'm not sure how many I can discuss, but it's, yeah, that's definitely something we're

334
00:26:08,760 --> 00:26:09,760
considering.

335
00:26:09,760 --> 00:26:10,760
Okay.

336
00:26:10,760 --> 00:26:18,200
What are the kind of outstanding challenges in trying to productionalize this?

337
00:26:18,200 --> 00:26:25,480
I always kind of change whether I say productize, productionize, productionize, but it's a production.

338
00:26:25,480 --> 00:26:26,480
Yeah.

339
00:26:26,480 --> 00:26:34,240
What's remaining for you to kind of take on with this project?

340
00:26:34,240 --> 00:26:35,240
Yeah.

341
00:26:35,240 --> 00:26:43,360
One of the big ones is the fact that the appearance of the cars isn't necessarily static

342
00:26:43,360 --> 00:26:50,040
in between races or even in between days of a given race.

343
00:26:50,040 --> 00:26:55,960
A driver may change the color of their rims on their tires or even completely change

344
00:26:55,960 --> 00:26:58,880
the wrap on their car or crash the car at one day.

345
00:26:58,880 --> 00:27:07,840
So if we're trying to do a purely supervised kind of one of these, like just pound it with

346
00:27:07,840 --> 00:27:13,520
as much data as possible to get it to learn, then that makes it a little more challenging

347
00:27:13,520 --> 00:27:14,520
or to keep up.

348
00:27:14,520 --> 00:27:15,520
Hard to keep up.

349
00:27:15,520 --> 00:27:16,520
Yeah.

350
00:27:16,520 --> 00:27:20,560
So how do you address that?

351
00:27:20,560 --> 00:27:22,520
So there's a couple ways.

352
00:27:22,520 --> 00:27:27,680
The way that we're thinking to address it at this point is we would like to be able

353
00:27:27,680 --> 00:27:38,000
to just during practice laps, maybe take five, three, four, five pictures of a given car.

354
00:27:38,000 --> 00:27:42,240
We can know when a car has changed appearance slightly.

355
00:27:42,240 --> 00:27:47,200
And then if we have a model that's able to learn to classify cars just based on that

356
00:27:47,200 --> 00:27:52,680
small support set, that is kind of that would be ideal.

357
00:27:52,680 --> 00:27:57,720
So there's some architectures out there that are kind of attempting to do this today and

358
00:27:57,720 --> 00:28:02,520
it's kind of in the one shot, few shot learning field.

359
00:28:02,520 --> 00:28:09,040
And so stuff like matching networks or some of the other metal learning techniques, I think

360
00:28:09,040 --> 00:28:13,040
are what we're going to be exploring in the immediate future.

361
00:28:13,040 --> 00:28:17,480
So much of you dug into that stuff so far.

362
00:28:17,480 --> 00:28:22,080
One shot, few shot, metal learning or things that I, you know, are on my list of things

363
00:28:22,080 --> 00:28:25,280
to dig into a bit more on the podcast this year.

364
00:28:25,280 --> 00:28:32,560
So if you've learned about some of this stuff already, I'd love to get a sense for, you

365
00:28:32,560 --> 00:28:34,880
know, what you've seen out there and what you find interesting.

366
00:28:34,880 --> 00:28:35,880
Yeah.

367
00:28:35,880 --> 00:28:40,280
I mean, I saw a ton of interesting metal learning talks this year at NIPs.

368
00:28:40,280 --> 00:28:41,280
Okay.

369
00:28:41,280 --> 00:28:48,560
And a lot of significant portion of them focused around kind of this few shot learning idea.

370
00:28:48,560 --> 00:28:51,840
And it seems like there's a lot of different ways to approach it.

371
00:28:51,840 --> 00:28:55,120
And metal learning as a framework is pretty general.

372
00:28:55,120 --> 00:29:02,160
And so some people were approaching it from the, just like kind of the transfer learning

373
00:29:02,160 --> 00:29:07,400
perspective where you say, okay, we're actually going to design our optimization function

374
00:29:07,400 --> 00:29:13,240
such that our goal is that we learn a set of parameters that, with a single gradient

375
00:29:13,240 --> 00:29:20,160
step, we're able to achieve a variety of different tasks or achieve high performance on a variety

376
00:29:20,160 --> 00:29:25,760
of different tasks, which is different than kind of just optimize for a single specific

377
00:29:25,760 --> 00:29:27,240
task.

378
00:29:27,240 --> 00:29:32,080
I think that work is called mammal out of Berkeley.

379
00:29:32,080 --> 00:29:37,800
And so that, that's something that I think is generally applicable to, I mean, obviously

380
00:29:37,800 --> 00:29:40,480
any model, which is cool.

381
00:29:40,480 --> 00:29:45,840
And then some stuff like the matching network where it's almost like a can yours neighbors

382
00:29:45,840 --> 00:29:55,240
type approximation of classification where you say, okay, I'm going to give it five images

383
00:29:55,240 --> 00:29:59,840
of this type of dog and now show it a new dog.

384
00:29:59,840 --> 00:30:05,080
And it's going to compare with these images that it knows and see which one is the closest

385
00:30:05,080 --> 00:30:09,600
and then or do some sort of majority vote based on the class using that.

386
00:30:09,600 --> 00:30:14,280
So the cool thing is that they're all kind of differentiable and not necessarily related

387
00:30:14,280 --> 00:30:16,160
to a specific data set.

388
00:30:16,160 --> 00:30:19,400
So I think that is an interesting.

389
00:30:19,400 --> 00:30:25,600
So is the idea that you would have someone like at the track who is, or I guess they

390
00:30:25,600 --> 00:30:30,840
could be remotely, but someone who is looking at these live practice run videos and like

391
00:30:30,840 --> 00:30:40,080
coming up with an annotated sample and shooting that like, are you, are you then triggering

392
00:30:40,080 --> 00:30:46,280
an entire retrain of some model, you know, just before the race, that sounds like that.

393
00:30:46,280 --> 00:30:49,280
I hope as we don't have to do that.

394
00:30:49,280 --> 00:30:50,280
Yeah.

395
00:30:50,280 --> 00:30:51,280
That would be dangerous.

396
00:30:51,280 --> 00:30:53,280
Yeah, sounds dangerous.

397
00:30:53,280 --> 00:30:59,600
I think we had discussed that before and we were like, well, what could go wrong?

398
00:30:59,600 --> 00:31:03,080
What if we just doesn't finish training in front of?

399
00:31:03,080 --> 00:31:04,080
Right.

400
00:31:04,080 --> 00:31:05,080
Yeah.

401
00:31:05,080 --> 00:31:11,600
So I think we want to have an interface where you can kind of just draw, either click

402
00:31:11,600 --> 00:31:15,880
on the cars or draw a couple of boxes around the cars.

403
00:31:15,880 --> 00:31:22,920
Our models will work as a general car detector from drone footage, kind of regardless of

404
00:31:22,920 --> 00:31:24,840
how the appearance of the car changes.

405
00:31:24,840 --> 00:31:30,080
So it should be as simple as just clicking on the new car over a series of frames and

406
00:31:30,080 --> 00:31:33,520
the boxes will already be there.

407
00:31:33,520 --> 00:31:38,600
Hopefully if we use something like a matching network, those can just be cropped out and dumped

408
00:31:38,600 --> 00:31:42,600
into a directory and we actually don't have to do any retraining.

409
00:31:42,600 --> 00:31:50,600
So the model uses those images as part of it's inference procedure, which is kind of the

410
00:31:50,600 --> 00:31:52,320
ideal scenario.

411
00:31:52,320 --> 00:32:00,560
So in what way, how is the model incorporating those images as part of the inference?

412
00:32:00,560 --> 00:32:08,280
So you talked about the five images of the dog and getting a new one, is it?

413
00:32:08,280 --> 00:32:13,080
These annotated car images would be presumably the five images of the dog, but that's happening

414
00:32:13,080 --> 00:32:15,800
at inference time, not training time.

415
00:32:15,800 --> 00:32:16,800
Yeah.

416
00:32:16,800 --> 00:32:22,280
So maybe we have five images of every single car associated with that as the labels.

417
00:32:22,280 --> 00:32:26,400
And this is kind of a really rough overview of the matching network.

418
00:32:26,400 --> 00:32:27,400
They did.

419
00:32:27,400 --> 00:32:32,120
They added some fancy stuff on top of this, but basically like a can yours neighbor.

420
00:32:32,120 --> 00:32:37,840
So all of those images, so maybe you have 10 different cars, you have 15 images total.

421
00:32:37,840 --> 00:32:43,640
They're all embedded with some learned embedding function, just a mapping.

422
00:32:43,640 --> 00:32:48,680
And then an embedding function into what space?

423
00:32:48,680 --> 00:32:54,800
One lower dimensional, so you just have like a fully connected neural network layer and

424
00:32:54,800 --> 00:33:00,600
it goes to some smaller space, so you have new, now features for all of those pictures.

425
00:33:00,600 --> 00:33:05,720
You also do the same mapping on your new input.

426
00:33:05,720 --> 00:33:12,240
And then you can just compare through some distance metric or some similarity metric, that

427
00:33:12,240 --> 00:33:17,080
new input and all of your 50 kind of just loaded images.

428
00:33:17,080 --> 00:33:21,000
So these images were potentially never trained on.

429
00:33:21,000 --> 00:33:24,000
You're really just learning that embedding function.

430
00:33:24,000 --> 00:33:29,600
And then once you compare and find, okay, it's closest to these five or these three or

431
00:33:29,600 --> 00:33:35,400
this one, you can take a weighted combination of those classes and then whichever class

432
00:33:35,400 --> 00:33:40,160
kind of has the highest vote is what you end up going with.

433
00:33:40,160 --> 00:33:46,480
And how does the train model interact with and inform the, you know, this matching process

434
00:33:46,480 --> 00:33:49,200
that happens at inference time?

435
00:33:49,200 --> 00:33:58,200
Is it that in this model, like the model that you've trained, the network is only doing object

436
00:33:58,200 --> 00:34:05,680
detection and then this matching process is doing the object identification or does having

437
00:34:05,680 --> 00:34:11,080
the train model somehow inform the identification as well, but you've got this extra layer that

438
00:34:11,080 --> 00:34:12,080
refines it.

439
00:34:12,080 --> 00:34:15,040
Yeah, so I think we could do it either way.

440
00:34:15,040 --> 00:34:24,160
We would like to be able to keep it as a single model, just for speed reasons, but that's

441
00:34:24,160 --> 00:34:27,200
still kind of in the research phase.

442
00:34:27,200 --> 00:34:33,880
An alternative is to kind of have the SSD model just do the object detection and then

443
00:34:33,880 --> 00:34:40,360
have the whatever additional network that we have on top does kind of the few shop learning

444
00:34:40,360 --> 00:34:44,560
and classification of the different cars.

445
00:34:44,560 --> 00:34:49,160
So I think, yeah, a lot of these are things to explore in the year.

446
00:34:49,160 --> 00:34:50,160
Right.

447
00:34:50,160 --> 00:34:51,760
You mentioned SSD a couple of times.

448
00:34:51,760 --> 00:34:58,800
What is, what is SSD I'm sorry, it's the single shot multi box, single shot multi box

449
00:34:58,800 --> 00:34:59,800
detector.

450
00:34:59,800 --> 00:35:00,800
Right.

451
00:35:00,800 --> 00:35:09,040
Okay, as opposed to your hard drive on, and we have an implementation of SSD that's open

452
00:35:09,040 --> 00:35:15,760
source on the neon get a repo, okay, so that's actually what we ended up using for this

453
00:35:15,760 --> 00:35:16,760
project.

454
00:35:16,760 --> 00:35:17,760
Okay.

455
00:35:17,760 --> 00:35:22,120
And it's pretty much just plug and play with a new data set and a little bit of tuning

456
00:35:22,120 --> 00:35:25,400
and we were able to achieve pretty good performance.

457
00:35:25,400 --> 00:35:26,400
Nice.

458
00:35:26,400 --> 00:35:32,080
And now is the, the data set that you've, like are you publishing this data set or anything

459
00:35:32,080 --> 00:35:34,880
like that?

460
00:35:34,880 --> 00:35:38,880
Not at this point, okay.

461
00:35:38,880 --> 00:35:45,520
Or any of the, any of the models or anything like that, are you publishing them or do you

462
00:35:45,520 --> 00:35:49,520
have like technical blog posts or something that folks can take a look at if they want

463
00:35:49,520 --> 00:35:53,200
to get more detailist into, like how you approach this problem?

464
00:35:53,200 --> 00:35:59,120
Yeah, we have a technical blog post that should be out.

465
00:35:59,120 --> 00:36:04,440
Or will we coming out, describing some of the data collection and the modeling procedure

466
00:36:04,440 --> 00:36:10,040
and preprocessing steps that we used, as well as a little bit of the training, so that

467
00:36:10,040 --> 00:36:12,280
should give people a pretty good idea.

468
00:36:12,280 --> 00:36:18,880
And are there, are there other kind of types or classes of problem that you think that

469
00:36:18,880 --> 00:36:22,640
this same kind of approach would lend itself to?

470
00:36:22,640 --> 00:36:28,080
Or is this, like do you think this is very custom to the unique challenges of having six

471
00:36:28,080 --> 00:36:33,920
drones flying around trying to identify, you know, formula racing cars or Ferrari racing

472
00:36:33,920 --> 00:36:34,920
cars?

473
00:36:34,920 --> 00:36:41,440
Yeah, I definitely, even today, here at CES of the booth, we had a lot of people coming

474
00:36:41,440 --> 00:36:48,080
up and saying, oh, this could work for this sport that I participate in, or we haven't

475
00:36:48,080 --> 00:36:52,480
quite explored those areas in terms of partnerships or how that would work yet.

476
00:36:52,480 --> 00:36:59,640
But I think it's definitely applicable to all sorts of different either broadcast races

477
00:36:59,640 --> 00:37:06,120
or sports or when just when there's a fast moving object that it's difficult for viewers

478
00:37:06,120 --> 00:37:12,000
to follow and it helps a lot to have some sort of either AI assistants or overlay or automatic

479
00:37:12,000 --> 00:37:14,320
broadcast control.

480
00:37:14,320 --> 00:37:19,600
So Andy, so this is kind of, you know, this project was just announced here.

481
00:37:19,600 --> 00:37:22,560
What are some of the other things that you're working on?

482
00:37:22,560 --> 00:37:28,960
Yeah, so I'm working on, obviously, continuing this project and some of the optimizations

483
00:37:28,960 --> 00:37:31,120
that I mentioned.

484
00:37:31,120 --> 00:37:37,880
My other work is more related, like I said, to what I had done my master's on, which is

485
00:37:37,880 --> 00:37:49,360
kind of end to end question-answer systems and dialogue systems, using similar network

486
00:37:49,360 --> 00:37:54,360
topologies if you look at it kind of squint and from a distance.

487
00:37:54,360 --> 00:38:03,240
And stuff like memory networks are a large part of my work.

488
00:38:03,240 --> 00:38:08,920
So I'm really interested in kind of memory augmented neural networks and the role that

489
00:38:08,920 --> 00:38:15,960
can play and not just question answering, but kind of a bunch of different challenges.

490
00:38:15,960 --> 00:38:23,440
And our memory networks and memory augmented networks is that of these related to like LSTM's

491
00:38:23,440 --> 00:38:30,080
and attention mechanisms and that kind of thing or yeah, very similar to attention.

492
00:38:30,080 --> 00:38:41,720
It's typically the memory augmented network means it has kind of some sort of fixed readable

493
00:38:41,720 --> 00:38:45,920
memory that you can either load items into.

494
00:38:45,920 --> 00:38:50,320
So for the case of the memory networks for question answering, you'll load in a story,

495
00:38:50,320 --> 00:38:58,280
like a text story of something that happened and then ask it questions about that story.

496
00:38:58,280 --> 00:39:03,000
And so similar to attention, the memory network will kind of compare whatever its input

497
00:39:03,000 --> 00:39:08,320
is with all of the story that's loaded in memory.

498
00:39:08,320 --> 00:39:09,960
So those are very similar.

499
00:39:09,960 --> 00:39:10,960
Awesome.

500
00:39:10,960 --> 00:39:15,800
Well, thanks so much guys for taking a hike over in the rain and the traffic.

501
00:39:15,800 --> 00:39:20,600
I was great chatting with you and I enjoyed learning more about what you're doing with

502
00:39:20,600 --> 00:39:21,920
the Ferrari challenge.

503
00:39:21,920 --> 00:39:22,920
Absolutely.

504
00:39:22,920 --> 00:39:23,920
Thank you.

505
00:39:23,920 --> 00:39:25,920
Thanks for having us.

506
00:39:25,920 --> 00:39:28,200
Alright, everyone.

507
00:39:28,200 --> 00:39:30,360
That's our show for today.

508
00:39:30,360 --> 00:39:34,920
Thanks so much for listening and for your continued feedback and support.

509
00:39:34,920 --> 00:39:41,440
Remember, for your chance to win in our AI at home giveaway, head on over to twimlai.com

510
00:39:41,440 --> 00:39:45,920
slash my AI contest for complete details.

511
00:39:45,920 --> 00:39:50,680
For more information on Andy, Amille, or any of the topics covered in this episode, head

512
00:39:50,680 --> 00:39:55,200
on over to twimlai.com slash talk slash 104.

513
00:39:55,200 --> 00:39:59,240
Thanks once again to Intel AI for their sponsorship of this series.

514
00:39:59,240 --> 00:40:03,240
To learn more about their partnership with Ferrari North America challenge and the other

515
00:40:03,240 --> 00:40:07,840
things they've been up to, visit ai.intel.com.

516
00:40:07,840 --> 00:40:12,920
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

517
00:40:12,920 --> 00:40:19,240
or via Twitter directly to me at at Sam Charrington or to the show at at twimlai.

518
00:40:19,240 --> 00:40:40,080
Thanks once again for listening and catch you next time.

