WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.800
I'm your host Sam Charrington.

00:23.800 --> 00:27.080
It's been another exciting week here at Twimble Headquarters.

00:27.080 --> 00:33.480
Just a few days after hitting the 500,000 listens mark, thanks to you once again, we learned

00:33.480 --> 00:38.080
that at least a few of those listens came from a certain Mark Cuban.

00:38.080 --> 00:40.920
And yes, I mean that Mark Cuban.

00:40.920 --> 00:45.040
Speaking at a conference in New York City, Mark mentioned that he turns to this very

00:45.040 --> 00:50.120
podcast to learn about and keep up to date on advances in artificial intelligence.

00:50.120 --> 00:52.760
Mark, if you're listening, we love you man.

00:52.760 --> 00:54.280
Thanks for the shout out.

00:54.280 --> 00:59.680
The CNBC article that covered Mark's talk and mentioned this podcast focused on his fear

00:59.680 --> 01:03.280
of AI and what it might bring in the future.

01:03.280 --> 01:07.460
As you might imagine, this is a topic I've got some opinions on and I respond to the

01:07.460 --> 01:11.000
article and Mark's fears in this week's newsletter.

01:11.000 --> 01:16.560
If you're not already receiving it, head to twimbleai.com slash newsletter to sign up and I'll

01:16.560 --> 01:19.200
make sure you get the current issue.

01:19.200 --> 01:24.000
Last week we announced the first of two winners for our artificial intelligence conference ticket

01:24.000 --> 01:25.520
giveaway.

01:25.520 --> 01:31.400
Winners received a bronze pass to the conference which grants access to all keynotes and sessions.

01:31.400 --> 01:34.760
Our second winner is Richard S. from Brooklyn, New York.

01:34.760 --> 01:38.000
Thanks again to everyone who entered the contest.

01:38.000 --> 01:42.860
If you didn't win this go round but would like to join us at the conference, use the discount

01:42.860 --> 01:47.360
code PC Twimble for 20% off of registration.

01:47.360 --> 01:54.920
We'll link to the conference in the show notes which you can find at twimbleai.com slash talk slash 43.

01:54.920 --> 01:59.400
The first twimble online meetup was last week and was wonderful.

01:59.400 --> 02:05.480
The focus of the meetup was the CVPR best paper award winner learning from simulated and

02:05.480 --> 02:11.920
unsupervised images through adversarial training by researchers from Apple.

02:11.920 --> 02:17.280
The idea behind this paper is this, consider a problem like IGaze detection.

02:17.280 --> 02:21.560
You've got a picture from, for example, a cell phone camera and you want to determine

02:21.560 --> 02:24.560
which way the user is looking.

02:24.560 --> 02:29.640
Generating labeled IGaze training data is hard and expensive.

02:29.640 --> 02:34.360
Generating simulated IGaze training data sets is much easier and cheaper though and can

02:34.360 --> 02:38.960
be done for example by using something like a video game engine.

02:38.960 --> 02:43.800
The problem is that the simulated IGaze images don't look close enough to real images

02:43.800 --> 02:47.360
to train a model to work effectively on real data.

02:47.360 --> 02:53.400
This paper proposes using a generative adversarial network to train a refiner model that can

02:53.400 --> 03:00.360
make simulated IGaze images look like real IGaze images while preserving the gaze direction.

03:00.360 --> 03:05.400
Thanks again to community members Josh Manella who did a great job presenting this paper

03:05.400 --> 03:10.360
and to Kevin Maider for walking us through a TensorFlow implementation of the model.

03:10.360 --> 03:12.640
You guys are just awesome.

03:12.640 --> 03:17.320
We're working on getting the recording posted for those who weren't able to join us live.

03:17.320 --> 03:21.200
If you're signed up for the meetup or the newsletter you'll be notified when it's

03:21.200 --> 03:22.400
available.

03:22.400 --> 03:28.600
If you'd like to join the meetup, head over to twimlai.com slash meetup to register.

03:28.600 --> 03:34.200
Next month's meetup will be held on Wednesday September 13th at 11 a.m. Pacific time and

03:34.200 --> 03:38.000
we'll post the details of the program shortly.

03:38.000 --> 03:43.320
Before we get to the show, I'd like to give a shout out to our friends at wise.io at GE Digital

03:43.320 --> 03:47.760
for their sponsorship of this industrial AI podcast series.

03:47.760 --> 03:52.960
Hopefully you caught last week's show featuring Josh Bloom, Vice President of Data and Analytics

03:52.960 --> 03:54.560
at GE Digital.

03:54.560 --> 03:59.880
We had a great discussion about how to incorporate physics-based information into machine learning

03:59.880 --> 04:02.200
models among other things.

04:02.200 --> 04:08.440
For more information, you'll find that show at twimlai.com slash talk slash 42.

04:08.440 --> 04:14.200
And for more information on wise.io at GE Digital, visit wise.io.

04:14.200 --> 04:16.080
And now for today's show.

04:16.080 --> 04:20.360
If you've listened to any of the shows in the industrial AI series, you've undoubtedly

04:20.360 --> 04:23.360
heard me mention our friends over at bonsai.

04:23.360 --> 04:28.280
I'm super grateful to bonsai for taking the lead in sponsoring both the industrial AI

04:28.280 --> 04:32.400
podcast series as well as my paper on that topic.

04:32.400 --> 04:38.400
Well today's show, which concludes this first season of the industrial AI series, features

04:38.400 --> 04:42.760
my interview with bonsai co-founder and CEO Mark Hammond.

04:42.760 --> 04:48.400
Our conversation centers on the role of what he calls machine teaching and delivering practical

04:48.400 --> 04:54.440
machine learning solutions, particularly for enterprise or industrial AI use cases.

04:54.440 --> 04:58.760
I really enjoyed this conversation with Mark and I know you will too.

04:58.760 --> 05:01.440
And now on to the show.

05:01.440 --> 05:08.760
All right, everyone.

05:08.760 --> 05:16.400
I am here at the offices of bonsai with Mark Hammond, the co-founder and CEO of the company.

05:16.400 --> 05:19.120
Mark, welcome to this week in machine learning and AI.

05:19.120 --> 05:20.120
Thank you for having me.

05:20.120 --> 05:21.120
Happy to be here.

05:21.120 --> 05:24.200
Yeah, I'm super excited to have you on the show.

05:24.200 --> 05:29.160
Folks who are regular listeners will know the name bonsai without a doubt, because you

05:29.160 --> 05:35.320
guys have very graciously sponsored my research into industrial AI and the podcast series.

05:35.320 --> 05:42.120
And I'm really looking forward to digging into with you what industrial AI means for bonsai.

05:42.120 --> 05:46.160
But before we dive into that, why don't we, why don't you spend a few minutes telling

05:46.160 --> 05:47.880
us a little bit about your background?

05:47.880 --> 05:48.880
Sure.

05:48.880 --> 05:51.280
So my background is actually originally very technical.

05:51.280 --> 05:56.520
I started programming very young and ended up working at Microsoft while I was still in

05:56.520 --> 06:01.920
high school on Windows itself, Windows 95, and the first many versions of Internet Explorer.

06:01.920 --> 06:06.360
So definitely hands-on coding on the products themselves.

06:06.360 --> 06:09.000
My passion that was always been artificial intelligence.

06:09.000 --> 06:13.400
So even while I was there, I knew that was what I wanted to do.

06:13.400 --> 06:18.720
And I decided to pursue a course of studies in computation and neural systems at Caltech.

06:18.720 --> 06:25.920
So I was working at Microsoft attending Caltech, and it was great in all regards other than

06:25.920 --> 06:30.240
that it happened in the late 90s, which was a fantastic time to work at Microsoft.

06:30.240 --> 06:34.120
And not the best time in the world to be in the field of AI.

06:34.120 --> 06:37.760
It's like part of one of the AI winters.

06:37.760 --> 06:45.160
And so I found myself when I was completing those studies faced with, how do we use all

06:45.160 --> 06:46.320
this stuff in the real world?

06:46.320 --> 06:51.520
And at that point in time, it was really, well, do you pursue a course in academia, right?

06:51.520 --> 06:55.040
Is that do you go to the academic route or what else do you do?

06:55.040 --> 06:56.720
Because it's hard of the AI winters.

06:56.720 --> 07:02.880
And so I kind of decided at that point that because I have this strong impetus towards

07:02.880 --> 07:07.200
applying this technology in real world scenarios, if I wanted to do that, I was going to need

07:07.200 --> 07:10.240
to get some of the not purely technical skills.

07:10.240 --> 07:16.240
And so I decided, okay, I got to go try product management, developer sales and marketing,

07:16.240 --> 07:17.240
et cetera, et cetera.

07:17.240 --> 07:18.920
And so I pursued that course.

07:18.920 --> 07:23.160
I did find myself at one point back at Microsoft this time in sales and marketing.

07:23.160 --> 07:28.720
I was one of the developer evangelists who was outpitching.net when.net was brand new

07:28.720 --> 07:33.080
and getting everyone on the C-sharp bandwagon, so that was a lot of fun.

07:33.080 --> 07:35.680
And fast forward to today.

07:35.680 --> 07:40.680
And the market is now in a great place where the technology is very capable.

07:40.680 --> 07:44.480
It's the right time to start looking at applying these technologies to our real world

07:44.480 --> 07:49.120
industrial and commercial enterprises and looking at those use cases.

07:49.120 --> 07:52.040
And I had come to the insight which led to bonsai.

07:52.040 --> 07:56.960
And it was born through, having gone through the academic track, having gone through the

07:56.960 --> 08:01.560
pure business track, having looked at trying to apply AI in lots of different contexts.

08:01.560 --> 08:04.120
And ultimately coming on one very simple realization.

08:04.120 --> 08:08.200
It's one of these, one of these things where you look back and you say, well, that seems

08:08.200 --> 08:12.720
obvious in hindsight, but until you think about it, it's not apparent.

08:12.720 --> 08:18.840
And that's that no matter how good we make these algorithms, they could be as good or better

08:18.840 --> 08:20.880
than humans at learning.

08:20.880 --> 08:22.520
We will always have to teach them.

08:22.520 --> 08:23.600
You have to teach them something.

08:23.600 --> 08:24.600
It's a learning algorithm.

08:24.600 --> 08:26.800
It's kind of by design it has to be taught.

08:26.800 --> 08:32.480
And there wasn't a huge focus on how you actually teach something.

08:32.480 --> 08:36.720
We spend so much time in this field focused on the machine learning algorithms themselves

08:36.720 --> 08:38.680
that teaching is often afterthought.

08:38.680 --> 08:42.560
That was the spark that said, look, if we're going to be able to solve these real world

08:42.560 --> 08:48.280
industrial AI applications, the subject matter expertise, the ability for people to define

08:48.280 --> 08:52.720
what they want to teach and how to teach it, that is an area that is ripe for enabling

08:52.720 --> 08:53.720
the technology to be used.

08:53.720 --> 08:54.920
And that's what led to Founding Bones.

08:54.920 --> 08:56.680
I am sitting here today.

08:56.680 --> 08:57.680
Awesome.

08:57.680 --> 08:58.680
Awesome.

08:58.680 --> 09:00.720
It's interesting that you came across that realization.

09:00.720 --> 09:07.120
I tend to find the same thing that the emphasis is on kind of the machine learning.

09:07.120 --> 09:11.480
And maybe you put another way, the way people tend to think about this teaching process

09:11.480 --> 09:15.200
is throwing a bunch of data at an algorithm.

09:15.200 --> 09:19.040
I guess it's kind of analogous to like throwing a bunch of books at a kid and expecting them

09:19.040 --> 09:20.440
to learn on their own.

09:20.440 --> 09:21.440
Yeah, absolutely.

09:21.440 --> 09:22.440
Absolutely.

09:22.440 --> 09:24.840
The one that I often use is with my son.

09:24.840 --> 09:27.160
He's learning how to play baseball.

09:27.160 --> 09:29.200
And I tell people I don't take him out to the backyard.

09:29.200 --> 09:30.200
He's five, right?

09:30.200 --> 09:31.200
So I tell people he's five.

09:31.200 --> 09:36.200
I don't take him out to the backyard and throw fastballs at him because that would just be,

09:36.200 --> 09:37.200
that's just cruel.

09:37.200 --> 09:38.520
Why would you do that?

09:38.520 --> 09:42.040
And yet, no one even pauses for a moment when we're like, well, we're just going to throw

09:42.040 --> 09:44.840
giant data sets at these machine learning algorithms.

09:44.840 --> 09:49.920
And I bet if I threw a million fastballs at my son, he'd figure out out eventually, too.

09:49.920 --> 09:53.040
But it's just not a very efficient or effective mechanism for teaching.

09:53.040 --> 09:54.040
Yeah.

09:54.040 --> 09:55.560
And probably be pretty painful for your son.

09:55.560 --> 09:56.560
True.

09:56.560 --> 09:57.560
True.

09:57.560 --> 09:59.600
Not very responsible for me as a parent.

09:59.600 --> 10:03.440
So maybe we can, I guess, let's just dig into this.

10:03.440 --> 10:09.600
So when you talk about machine training, what does that mean or machine teaching, what

10:09.600 --> 10:10.960
does that mean to you?

10:10.960 --> 10:16.400
So machine teaching really boils down to actually looking at the art and science about

10:16.400 --> 10:24.080
how we teach things, distilling out the abstractions that are there, and then providing the appropriate

10:24.080 --> 10:30.440
platform, tooling, et cetera, that developers, we've come to expect in order to be able

10:30.440 --> 10:35.360
to properly construct the solution to a problem in that context.

10:35.360 --> 10:40.080
So that's at a very high level, but at a concrete level, what it means for us is we need to

10:40.080 --> 10:44.040
give you a very formal way, because it's still a computer program at the end of the day.

10:44.040 --> 10:48.720
We need to give you a very formal way to specify what it is you're actually trying to teach,

10:48.720 --> 10:54.440
and how you can go about phasing that teaching in a way that follows, again, using examples

10:54.440 --> 10:58.120
we're just talking about, you don't want to have it be just throw massive throws of

10:58.120 --> 11:02.400
data at the system or how you teach it can be broken up in ways that facilitate acquisition

11:02.400 --> 11:04.720
and mastery of these concepts you're trying to teach.

11:04.720 --> 11:10.120
So really at the end of the day, it's about empowering developers to work as teachers

11:10.120 --> 11:13.880
and giving them the ability to do that in a very formal, structured way.

11:13.880 --> 11:18.200
You can think about it if we're doing this in very humanistic terms, you talked about textbooks,

11:18.200 --> 11:19.200
right?

11:19.200 --> 11:21.240
So a textbook has a curriculum that's set out in it.

11:21.240 --> 11:24.280
Here's the table of contents, all the concepts we want you to master.

11:24.280 --> 11:25.520
We're going to go through it in this order.

11:25.520 --> 11:28.920
Here's phased problem sets that ramp in difficulty level.

11:28.920 --> 11:33.240
You go through all of these different ways of structuring the textbook in order to try

11:33.240 --> 11:35.840
to help guide students as they learn things.

11:35.840 --> 11:37.280
We do the same thing.

11:37.280 --> 11:39.040
We just do it in a very formal, structured way.

11:39.040 --> 11:43.560
We give you an actual programming language where there's no ambiguity about what you're

11:43.560 --> 11:44.560
trying to do.

11:44.560 --> 11:49.560
It's less freeform textbook and much more program that you're creating.

11:49.560 --> 11:53.960
Maybe you can make that more clear by walking us through an example, you know, pick a use

11:53.960 --> 11:58.200
case and maybe talk about how you would apply this to the use case.

11:58.200 --> 11:59.200
Yeah, absolutely.

11:59.200 --> 12:04.040
So, if we look at a robotic system as an example, so you have an industrial robot, perhaps

12:04.040 --> 12:09.520
it's a robotic armature being used in a manufacturing setting or a warehouse setting.

12:09.520 --> 12:15.040
And you want that armature to be able to create conduct various pick and place operations.

12:15.040 --> 12:19.600
Maybe it's doing a palatizing operation, something of that nature.

12:19.600 --> 12:26.200
Now, when you want to have the piece of equipment learn to accomplish this task, you can break

12:26.200 --> 12:28.760
it up into the constituent concepts that matter.

12:28.760 --> 12:34.560
It's not about, I want you to perform the entire task, let me demonstrate it for you.

12:34.560 --> 12:38.720
It's about, okay, you need to understand the concepts of moving between points.

12:38.720 --> 12:40.960
Here is where you're currently at in space.

12:40.960 --> 12:43.120
Here is the target you need to get to.

12:43.120 --> 12:46.200
How would you drive the motors to get to that target?

12:46.200 --> 12:48.200
That would be an example of a concept.

12:48.200 --> 12:53.320
Now, for a concept like that, if you're looking at a commercially available industrial robot,

12:53.320 --> 12:56.080
there are very good controllers for that kind of thing already, right?

12:56.080 --> 12:59.880
They've got the inverse kinematics all worked out, and they know how to do it not just

12:59.880 --> 13:04.080
efficiently and effectively, but in a way that's going to maximize the lifetime of that

13:04.080 --> 13:05.080
equipment.

13:05.080 --> 13:08.120
You don't want to unnecessarily drive it too quickly, so you're going to cause it to

13:08.120 --> 13:10.040
fail faster than you'd want.

13:10.040 --> 13:12.680
Real world concerns when you're using these kinds of robots.

13:12.680 --> 13:17.520
But then when you get to a grasping concept, so now we need to actually grasp the item

13:17.520 --> 13:21.160
that we're going to be picking as part of this action.

13:21.160 --> 13:23.000
That is much more complex, right?

13:23.000 --> 13:26.960
And you have to deal with rigid bodies and soft bodies and different packaging materials

13:26.960 --> 13:31.480
and all these different kinds of aspects that come to play, teaching the nuances that go

13:31.480 --> 13:32.800
along with that.

13:32.800 --> 13:36.440
That's where you can start to take your subject matter expertise and really bring it to

13:36.440 --> 13:37.440
bear.

13:37.440 --> 13:42.360
There are people within all of our organizations who already know a lot about these facets

13:42.360 --> 13:44.840
of things, and they're not programmers per se.

13:44.840 --> 13:48.560
They might be a mechanical engineer, they might be a chemical engineer, it depends what

13:48.560 --> 13:50.280
kind of problem you're tackling.

13:50.280 --> 13:54.920
But they know a lot about that area, and so if you asked them, if I asked you to teach

13:54.920 --> 13:58.040
a human to do this, what would you do?

13:58.040 --> 14:01.560
What would you actually, what are the concepts you want them to learn?

14:01.560 --> 14:03.200
How would you show them those things?

14:03.200 --> 14:05.720
How would you test whether they got it right or not?

14:05.720 --> 14:08.840
They usually can tell you actually, because that's their field.

14:08.840 --> 14:12.800
They know that pretty well, and so we just give them a mechanism to capture that.

14:12.800 --> 14:16.920
So in the context of grasping, for example, since we're just walking through one of these

14:16.920 --> 14:21.920
use cases, you might say, well, actually, if I'm teaching a human to grasp something,

14:21.920 --> 14:24.880
I got to rewind a lot until back when they're toddlers, right?

14:24.880 --> 14:26.200
But what do you do?

14:26.200 --> 14:30.040
You use large-scale, gross motor scale objects.

14:30.040 --> 14:32.480
You get them really close to their hands.

14:32.480 --> 14:37.440
You have set their hands on it mostly, and then you let them go through the motions.

14:37.440 --> 14:41.440
It's hard to think back in many of these cases for these very simplistic motions, because

14:41.440 --> 14:45.320
to us, it's simple, but if you watch children doing it, they have to learn, too.

14:45.320 --> 14:46.320
But you break it down.

14:46.320 --> 14:49.040
And you literally break it down into those kinds of things where I'm going to teach you

14:49.040 --> 14:50.040
gross motor skills.

14:50.040 --> 14:55.240
I'm going to teach you in these simulation environments so that you can experiment frequently.

14:55.240 --> 14:58.280
And you learn these concepts related to grasping.

14:58.280 --> 15:00.400
So you learn the concepts related to grasping.

15:00.400 --> 15:04.760
You're leveraging the pre-existing concepts related to moving between points.

15:04.760 --> 15:10.560
You want to teach concepts related to stacking or placing orientation valid grasps so that

15:10.560 --> 15:13.920
you can orient parts in appropriate ways for fastening them, et cetera, et cetera.

15:13.920 --> 15:17.920
All these kinds of things that happen when you're doing real-world industrial robotics,

15:17.920 --> 15:19.800
you can break the problem down.

15:19.800 --> 15:22.480
You break it down into these constituent concepts.

15:22.480 --> 15:24.840
You design a plan for how you want to teach it.

15:24.840 --> 15:29.080
Typically, that will involve some simulation environment in conjunction with some real-world

15:29.080 --> 15:30.840
physical environment.

15:30.840 --> 15:34.920
And then you define what that curriculum looks like to teach it.

15:34.920 --> 15:40.280
And then because you've done it in that way, the system can proceed to try all the various

15:40.280 --> 15:43.280
areas that it can explore to teach how that works.

15:43.280 --> 15:46.320
And mathematically, if we're looking at the low levels on the math, all that's really

15:46.320 --> 15:49.440
happening is you're constraining the state space the system needs to explore.

15:49.440 --> 15:52.080
That's what, in practice, that's what's actually happening.

15:52.080 --> 15:58.120
But it's happening in this more naturally-expressed way that a subject matter expert can readily latch

15:58.120 --> 15:59.880
on to and work well with.

15:59.880 --> 16:03.280
So that's an example in a robotics context.

16:03.280 --> 16:04.280
If you look at examples...

16:04.280 --> 16:06.560
Well, that's not going too far because there's so much...

16:06.560 --> 16:07.560
Yeah.

16:07.560 --> 16:08.560
Sure.

16:08.560 --> 16:09.560
...to unpack.

16:09.560 --> 16:14.920
The first thing that jumped out at me was you kind of describe these two different types

16:14.920 --> 16:19.680
of concepts one that you know a lot about.

16:19.680 --> 16:22.080
And you can help me refine this language.

16:22.080 --> 16:25.280
And another that you need to teach more abstractly.

16:25.280 --> 16:33.040
So for example, in moving, you know, in kind of the macro movement of the robot arm from

16:33.040 --> 16:37.360
point A to point B, you know, it's a well-understood problem, you've got the inverse kinematics

16:37.360 --> 16:38.360
you mentioned.

16:38.360 --> 16:43.360
Yeah, I get the impression that, you know, we've talked a lot in this series about kind

16:43.360 --> 16:50.640
of reinforcement learning from a research and academic perspective and one of the, you

16:50.640 --> 16:57.280
know, the problems are, I think, in that domain not decomposed in this way.

16:57.280 --> 17:01.680
And so I think what I heard you say was, it'd be kind of crazy to, like, throw a bunch

17:01.680 --> 17:06.160
of data and, like, have the robot try to figure out on its own the best way to move from

17:06.160 --> 17:07.160
point A to point B.

17:07.160 --> 17:12.280
And hey, we've already done that years and years and years, and we spent a bunch of

17:12.280 --> 17:14.520
time perfecting the way to do that.

17:14.520 --> 17:20.360
So, you know, part of what I hear you talking about is kind of, is an idea of modularity.

17:20.360 --> 17:21.360
Yes.

17:21.360 --> 17:22.360
I agree.

17:22.360 --> 17:27.360
And these approaches that I'm trying to, like, get at a whole lot of stuff at one point.

17:27.360 --> 17:28.360
Sure.

17:28.360 --> 17:32.960
You know, one thing that I want us to dig into is like, you know, compare contrast, you

17:32.960 --> 17:36.680
know, what you're doing with, you know, the way some of the things we've talked about

17:36.680 --> 17:41.120
on the podcast, kind of, academic approaches to reinforcement learning.

17:41.120 --> 17:43.880
And so one is this idea of modularity.

17:43.880 --> 17:52.800
Another is, you know, maybe kind of elaborating on this idea of constraining the state space

17:52.800 --> 18:00.600
in a way that is easily expressed by humans, like I think constraining the space, the

18:00.600 --> 18:07.480
state space is a huge part of, you know, this process even from an academic perspective,

18:07.480 --> 18:12.760
but my impression, not being an academic and at the start of this field, but my approaches

18:12.760 --> 18:18.120
or my sense is that their approach is constrained in the state space mathematically, right?

18:18.120 --> 18:20.320
As opposed to conceptually.

18:20.320 --> 18:21.640
Is that fair?

18:21.640 --> 18:22.640
That's fair.

18:22.640 --> 18:25.680
At the end of the day, it all does boil down to a mathematical constraint.

18:25.680 --> 18:28.960
It's just how you enable people to express that.

18:28.960 --> 18:34.080
And by virtue of building a system where you're expressing it in this more natural way for

18:34.080 --> 18:39.000
a subject matter expert who's actually working on these problems, you can get at the underlying

18:39.000 --> 18:43.880
math by allowing people to express it in these more natural terms.

18:43.880 --> 18:46.280
Perhaps an analogy here would be, would be useful.

18:46.280 --> 18:49.160
If we look back at old programming systems, right?

18:49.160 --> 18:53.720
So in the, in the late 90s when I was at Microsoft Visual Basic for building desktop apps

18:53.720 --> 18:55.320
was everywhere.

18:55.320 --> 18:57.600
People used that all over the place.

18:57.600 --> 19:02.600
And it was very popular because it enabled people who had subject matter expertise, you

19:02.600 --> 19:07.200
know, I'm running my veterinary clinic, I'm doing whatever it happens to be, to build

19:07.200 --> 19:12.080
the applications they can, they cared about because they didn't worry about comm interop

19:12.080 --> 19:14.560
capabilities and all these, you know, low-level stuff.

19:14.560 --> 19:19.240
They worried about, can I build a form and can I put the right components onto the form

19:19.240 --> 19:23.640
that I care about and tie that back to a database in a way that doesn't require me to go become

19:23.640 --> 19:29.680
an expert in assembly language and low-level binary interface technologies.

19:29.680 --> 19:30.880
This is the same kind of thing.

19:30.880 --> 19:34.320
It's about building the right abstraction at the end of the day.

19:34.320 --> 19:38.920
And so even if what our technology is going to do are the bonsai platform itself is going

19:38.920 --> 19:42.000
to take all this code that you've provided and it's going to compile it.

19:42.000 --> 19:44.560
And yes, at the end of the day, it's a big mathematical constraint.

19:44.560 --> 19:47.440
It's not that fundamentally the technology is different somehow.

19:47.440 --> 19:48.640
It's the same.

19:48.640 --> 19:53.000
It's just that we're allowing people to express it at an appropriate level of abstraction

19:53.000 --> 19:57.640
where it's now framed in the context of the subject matter.

19:57.640 --> 20:00.720
It's framed in the context of the business problem you're trying to solve.

20:00.720 --> 20:04.920
And that's very powerful because it takes it so that your data scientists can still play

20:04.920 --> 20:06.960
the role that's appropriate for them to play.

20:06.960 --> 20:09.360
Your programmers can play the role that's appropriate for them to play and the subject

20:09.360 --> 20:14.520
matter experts can participate and actually teach it the intelligence that you actually

20:14.520 --> 20:15.760
want the system to exhibit.

20:15.760 --> 20:21.720
Typically, what we find in a lot of these environments is if you have true, deep expertise

20:21.720 --> 20:26.720
in machine learning and data science, that is its whole own field.

20:26.720 --> 20:31.080
And the people who have that, if you look at the intersection with the people who have

20:31.080 --> 20:38.760
the expertise in building manufacturing equipment or optimizing supply chain facets rare, right?

20:38.760 --> 20:41.400
It's very rare that they overlap.

20:41.400 --> 20:46.160
So we have to provide, as an industry, we have to provide a way to enable all of these

20:46.160 --> 20:49.360
disparate skill sets to work together.

20:49.360 --> 20:53.040
Then we have to focus on the skill sets that are already within these organizations or

20:53.040 --> 20:55.480
we're never going to solve the real world problems.

20:55.480 --> 21:00.000
And so that's, that ultimately is where we're getting at with this technique.

21:00.000 --> 21:03.960
Yes, it's about decomposing the problems and it's about decomposing the problems in a

21:03.960 --> 21:09.120
way that allows these subject matter experts who know about all the different facets of

21:09.120 --> 21:13.800
the use case they care about to really come in and say, I need to teach you about this.

21:13.800 --> 21:18.800
So if I look at real world examples, we're doing a lot of work with Siemens at the moment

21:18.800 --> 21:21.720
as a, as they're one of our customers.

21:21.720 --> 21:25.400
And if we look at their manufacturing equipment that they come to us and they want to talk

21:25.400 --> 21:31.320
about adding intelligence to, I'm an expert at platforms and artificial intelligence and

21:31.320 --> 21:32.320
all this stuff.

21:32.320 --> 21:34.760
I'm not an expert at CNC milling, right?

21:34.760 --> 21:37.160
That's not my, that's not my area.

21:37.160 --> 21:40.600
When they come to us and they say, well, here are the real world problems we face when

21:40.600 --> 21:46.040
we have these gigantic pieces of manufacturing equipment and they can have an expert get

21:46.040 --> 21:49.640
on the line and they can, that expert once say, well, you're going to need to understand

21:49.640 --> 21:53.400
this facet of friction compensation and so on and so on.

21:53.400 --> 21:57.520
And you know, areas that I know that, nothing about, frankly, as I don't know, personal

21:57.520 --> 22:02.160
level, but they can tell us all sorts of things about that and we can work with them to

22:02.160 --> 22:06.840
say, all right, well, how do you go about this now?

22:06.840 --> 22:11.880
How would we break that down into something we can measure, into a set of concepts the

22:11.880 --> 22:13.480
system can learn?

22:13.480 --> 22:20.560
And it's not about how do we craft a, you know, a pit controller to solve this problem

22:20.560 --> 22:25.560
which would be a traditional way to solve this problem in enterprise context.

22:25.560 --> 22:28.760
It's about how can we tell whether it was correct or not?

22:28.760 --> 22:31.280
So this is where the reinforcement learning part comes in.

22:31.280 --> 22:35.960
You don't have to be able to specify the controller at a mathematical level.

22:35.960 --> 22:40.240
You have to be able to assess whether the behavior was what you wanted and the ability

22:40.240 --> 22:45.520
to break down that behavior into components so that you can assess for each of those

22:45.520 --> 22:50.080
constituent components whether or not that was what you actually wanted.

22:50.080 --> 22:51.560
So elaborate on that.

22:51.560 --> 22:57.320
Do you, I mean, ultimately in these use cases you're trying to, are you trying to create

22:57.320 --> 23:02.600
the controller or are you saying the controller already exists and you're trying to identify

23:02.600 --> 23:06.000
the right parameters for the controller or are you trying to say the controller and the

23:06.000 --> 23:09.560
parameters exist and you're just trying to do some kind of validation?

23:09.560 --> 23:14.920
So all of the above actually, we run it, unfortunately the answer is that we see all of these

23:14.920 --> 23:15.920
scenarios.

23:15.920 --> 23:20.240
So there are cases where what you have is that you have an existing controller and what

23:20.240 --> 23:24.600
you're looking to do is to identify deviations, right?

23:24.600 --> 23:30.160
So you're really trying to figure out when you've deviated and you have some condition

23:30.160 --> 23:33.720
that was not anticipated and you want to be able to deal with it appropriately.

23:33.720 --> 23:35.640
So that's something you definitely run into.

23:35.640 --> 23:40.840
There are areas you run into where you have existing controllers and you want to enhance

23:40.840 --> 23:46.760
the capabilities of those controllers beyond the already well-defined characteristics.

23:46.760 --> 23:48.240
So you run into that as well.

23:48.240 --> 23:52.200
And then there are areas where you people are still operating things by hand.

23:52.200 --> 23:54.920
So it is not uncommon.

23:54.920 --> 23:56.720
We can look at CNC machines, right?

23:56.720 --> 24:02.640
And we were just talking about them to have expert human operators manning these machines

24:02.640 --> 24:09.240
because the value to the business that is using them and creating the part, let's say you're

24:09.240 --> 24:11.560
making a large-scale aircraft part.

24:11.560 --> 24:16.440
That part might be a couple hundred thousand dollars just for that one part.

24:16.440 --> 24:20.160
And it might be a week-long operation to mill that part, right?

24:20.160 --> 24:25.160
You're not going to just turn it over to your G code script and let it run.

24:25.160 --> 24:28.960
And you're going to have someone there to make sure everything's going as you expect.

24:28.960 --> 24:32.720
If there's a mistake on day two, you want to stop it on day two.

24:32.720 --> 24:36.840
So that you're not wasting tons of time and money as you're going through that.

24:36.840 --> 24:42.800
And at the same time, one of the things that I hear over and over again is that they're

24:42.800 --> 24:48.920
from the perspective of trying to apply kind of modern, sane business and engineering

24:48.920 --> 24:55.800
practices to some of these industrial environments, a lot of what this subject matter expertise

24:55.800 --> 25:02.800
is, hey, when I hear this machine kind of sounding an octave or two higher and pitch or something

25:02.800 --> 25:06.800
like that, I know that we're probably going to lose a bit or we're going to probably

25:06.800 --> 25:09.320
damage the part or something like that.

25:09.320 --> 25:13.280
There's a lot of art in addition to the science.

25:13.280 --> 25:16.080
How do you begin to capture all that?

25:16.080 --> 25:18.000
So that's actually an excellent point.

25:18.000 --> 25:24.640
So the beauty of the modern machine learning technology is its ability to detect nuances

25:24.640 --> 25:30.720
there where the human expert, the subject matter expert, can say, yes, I hear it.

25:30.720 --> 25:36.320
And when it sounds off, then I know this is about to happen and you can say, well, what

25:36.320 --> 25:37.320
are you listening for?

25:37.320 --> 25:40.480
And they're not experts in acoustics, they're not going to sit there and tell you, well,

25:40.480 --> 25:42.400
it's exactly this is the kind of sound.

25:42.400 --> 25:46.160
It's more like, I just know what I'm listening for, I've heard it before.

25:46.160 --> 25:51.000
And so the traditional mechanism would be go through label, data set, et cetera.

25:51.000 --> 25:52.720
And you can still do that.

25:52.720 --> 25:57.680
There are techniques you can use in simulation as well to model those environments.

25:57.680 --> 26:01.920
But in practice, the benefit you get from modern machine learning technology versus expert

26:01.920 --> 26:06.960
systems, saying if we go back to the 80s, is this flexibility.

26:06.960 --> 26:12.040
So if I can use another analogy, which might be intuitive to a lot of people, if you play

26:12.040 --> 26:16.880
a sport and you really enjoy playing that sport and you practice and practice and practice

26:16.880 --> 26:20.280
and you get good at it and someone comes to you and they're like, wow, you're really

26:20.280 --> 26:21.280
good.

26:21.280 --> 26:22.280
What is it that you're doing?

26:22.280 --> 26:25.360
I'm missing so that I want to be good at this sport too.

26:25.360 --> 26:28.000
Oftentimes, as an amateur, you don't know.

26:28.000 --> 26:29.440
I just practiced a lot.

26:29.440 --> 26:31.040
I got good at it.

26:31.040 --> 26:36.480
And if you go to a professional coach or a professional athlete, they can tease it apart.

26:36.480 --> 26:41.240
They can say, well, actually, when you're doing this motion, you'll note that you arch

26:41.240 --> 26:44.440
in this way and they can get into all the subtleties and the new ones.

26:44.440 --> 26:46.720
That's why they're a coach or a pro.

26:46.720 --> 26:53.080
And so humans as a learning system, if we look at ourselves as learning systems, we have

26:53.080 --> 27:01.160
this remarkable ability to be able to exhibit intelligent behavior, regardless of our ability

27:01.160 --> 27:04.240
to explain all of it, right?

27:04.240 --> 27:06.760
And modern machine learning systems are like this.

27:06.760 --> 27:12.040
So if you take a deep reinforcement learning neural network kind of approach and you apply

27:12.040 --> 27:15.160
it to a problem and you say, here's the correct behavior.

27:15.160 --> 27:19.480
Let's look at it over and over and over again, whether that's because it's getting acoustic

27:19.480 --> 27:22.920
data and it's listening and you're telling it whether or not the part was about to break

27:22.920 --> 27:26.800
or you observe that the part broke and now it's learning what the subtleties and the acoustics

27:26.800 --> 27:31.400
are so that it can have that same sense that the expert operator did.

27:31.400 --> 27:32.400
It can do that, right?

27:32.400 --> 27:34.680
That's one of the benefits of the technology.

27:34.680 --> 27:40.120
But the more you know about the problem itself, it enables you to decompose it into

27:40.120 --> 27:41.440
those bits.

27:41.440 --> 27:46.720
And so you're not forced, you can always use the technology and get it to the point where

27:46.720 --> 27:51.720
again, you're the amateur athlete and you're just learned because you practice so much.

27:51.720 --> 27:58.160
Or in the industrial case, I have my system and it's actually monitoring the acoustics

27:58.160 --> 28:02.240
off of the equipment and it's learning to detect what it sounds like when a part is about

28:02.240 --> 28:03.400
to break.

28:03.400 --> 28:04.400
It can do that.

28:04.400 --> 28:05.400
That's fantastic.

28:05.400 --> 28:08.600
But then if you have that subject matter expertise and you can really decompose the

28:08.600 --> 28:12.760
problem, you can get a lot of benefits because you can teach faster.

28:12.760 --> 28:17.560
You can now have the predictions that are made explained so that the system can make

28:17.560 --> 28:22.640
more nuanced and more accurate behavioral decisions.

28:22.640 --> 28:28.400
And really getting that subtlety and nuance allows us to build and capture more knowledge

28:28.400 --> 28:30.320
and build more sophisticated systems.

28:30.320 --> 28:33.320
It's kind of like with expert systems, you are totally rigid.

28:33.320 --> 28:34.320
Here are the rules.

28:34.320 --> 28:35.320
Right.

28:35.320 --> 28:40.640
It's all this behavior and powerful in that sense, but very constrained.

28:40.640 --> 28:42.200
It was not very flexible.

28:42.200 --> 28:47.520
And now the pendulum has swung the entire opposite way or all the way at the other end.

28:47.520 --> 28:51.400
And you have your machine learning systems and it's throw lots of data at it and it's

28:51.400 --> 28:54.520
going to learn to predict something and great.

28:54.520 --> 28:57.360
It makes great predictions but no one knows why.

28:57.360 --> 28:58.360
Right.

28:58.360 --> 29:00.720
So before totally explainable, completely inflexible.

29:00.720 --> 29:06.400
Now totally flexible, not explainable and by virtue of using a machine teaching approach

29:06.400 --> 29:09.560
like the one that we've outlined, it's no longer black or white.

29:09.560 --> 29:11.840
You get this nice continuous gray area.

29:11.840 --> 29:15.400
If you don't have a lot of that, that you can provide, the system can still learn and

29:15.400 --> 29:16.600
that's okay.

29:16.600 --> 29:20.440
The more of it you provide, the more explainability you get, the faster it can learn, the more nuanced

29:20.440 --> 29:23.160
you can add to the decisions you're making.

29:23.160 --> 29:24.800
And it just opens that up.

29:24.800 --> 29:31.960
And so it really allows us to tackle these problems at whatever level of subject matter

29:31.960 --> 29:35.520
expertise, explainability is appropriate for what you're trying to do.

29:35.520 --> 29:36.520
Okay.

29:36.520 --> 29:37.520
So what?

29:37.520 --> 29:42.680
I think what I just heard was, well you talked about explicitly the spectrum, but when

29:42.680 --> 29:49.960
a company is using your tools and building a solution based on it, they've got the

29:49.960 --> 29:55.040
ability to, you know, you can start by at the highest level by throwing lots of data at

29:55.040 --> 30:01.920
the problem and not building, you know, building constraints into the system or, you know,

30:01.920 --> 30:06.680
conversely, you know, articulating the concepts, you know, breaking down the concepts that

30:06.680 --> 30:13.560
compose the system, you know, or you can do that to some varying degree of detail.

30:13.560 --> 30:18.160
So it sounds like that's probably one of the kind of architectural design decisions

30:18.160 --> 30:24.160
of someone that's implementing this, like how much decomposition do we need to go to?

30:24.160 --> 30:28.500
And is that what are the factors there, is it primarily performance, is it actually

30:28.500 --> 30:30.320
it tends to be very iterative.

30:30.320 --> 30:36.120
So typically, typically what ends up happening in a real life engagement is they will first

30:36.120 --> 30:41.320
start with the simplest possible model, which is there's only one concept and I just having,

30:41.320 --> 30:44.080
I'm doing the classic reinforcement learning thing.

30:44.080 --> 30:47.360
Here's the environment for you to go explore, go explore it.

30:47.360 --> 30:51.440
And that might be a robotics environment, it might be a supply chain simulation, it

30:51.440 --> 30:57.160
could be any number of things, but just explore and see what you can figure out.

30:57.160 --> 31:03.320
Typically that will learn something, not as much as one would hope, but something.

31:03.320 --> 31:08.360
And then you say, okay, well, let's see what would happen if I taught it about this.

31:08.360 --> 31:13.680
And so you add some conceptual block into the system and you break it down into teaching

31:13.680 --> 31:14.680
that.

31:14.680 --> 31:17.760
And it may or may not help, it's not always a given that it helps.

31:17.760 --> 31:23.400
Often times we'll get feedback from people who are more on the academic mindset.

31:23.400 --> 31:26.760
The whole point of deep learning was to get away from specifying these things, right?

31:26.760 --> 31:29.240
That was the whole point, why are you doing this?

31:29.240 --> 31:32.840
What happens if the person specifies this model and they're wrong, right?

31:32.840 --> 31:37.960
That one of the benefits of deep learning is that it's not reliant on our presupposed

31:37.960 --> 31:40.320
conception of what the model should be.

31:40.320 --> 31:41.520
How do you cope with that?

31:41.520 --> 31:46.240
And for us, it's like, well, that's beautiful, actually, because when you start to break

31:46.240 --> 31:50.720
it down and you decompose the problem and you do it in this iterative way and you see

31:50.720 --> 31:56.080
whether that supports a faster learning, whether it supports better explanations, better

31:56.080 --> 32:00.840
reuse, better generalization, all these different factors you might want to optimize for

32:00.840 --> 32:04.360
and care about, you learn about your own model.

32:04.360 --> 32:10.200
And so if your conceptual model is that friction compensation is super important for this

32:10.200 --> 32:15.600
manufacturing process and you go through the motions and you see that, well, the system's

32:15.600 --> 32:20.120
learning to make predictions and it's compensating just fine.

32:20.120 --> 32:24.440
But all the things I taught it that I thought I knew aren't being used.

32:24.440 --> 32:27.000
And the system can come back and tell you this, it's, well, you taught me this concept

32:27.000 --> 32:29.800
and in fact, I never use what I learned.

32:29.800 --> 32:31.800
I'm always doing something else.

32:31.800 --> 32:32.800
That's instructive.

32:32.800 --> 32:38.440
That tells you, hey, this model I thought made sense, there's something better because

32:38.440 --> 32:42.400
the system has learned the correct behavior and it's not using what I do.

32:42.400 --> 32:45.000
And this happens at all levels of granularity.

32:45.000 --> 32:47.680
So in practice, it's this iterative process.

32:47.680 --> 32:50.120
People start at the very simplistic model.

32:50.120 --> 32:53.480
They start to add more and more of the model that they believe is correct.

32:53.480 --> 32:59.720
It's very rarely a, here is the 120 concept model that I think maps to the problem I'm trying

32:59.720 --> 33:04.880
to solve and I'm going to go build the whole thing in one go and go from there.

33:04.880 --> 33:11.720
It's much more of an iterative refinements and expansion of the model so that you can

33:11.720 --> 33:15.320
have more nuance covered and more subtlety covered and learn about your model in the process

33:15.320 --> 33:16.320
of doing that.

33:16.320 --> 33:17.320
Okay.

33:17.320 --> 33:22.080
And one of the things that you guys talk a lot about is the notion of explainability.

33:22.080 --> 33:23.080
Yes.

33:23.080 --> 33:24.080
You just went into that.

33:24.080 --> 33:29.200
Very important for customers in this space, for a variety of reasons.

33:29.200 --> 33:35.480
I don't think I previously understood that it's this granularity of defining the concepts

33:35.480 --> 33:37.640
that really gets you the explainability.

33:37.640 --> 33:38.640
Yeah.

33:38.640 --> 33:39.640
Absolutely.

33:39.640 --> 33:40.640
Can you elaborate on that?

33:40.640 --> 33:41.640
Yeah, absolutely.

33:41.640 --> 33:46.200
So there are a lot of techniques that have been published about how you start to peer

33:46.200 --> 33:50.920
into the neural networks and try to tease out what is actually going on.

33:50.920 --> 33:52.520
You got lime and a lime and all.

33:52.520 --> 33:54.760
There's tons of these things.

33:54.760 --> 33:55.800
Our approach is different.

33:55.800 --> 34:01.040
As you said, because it's not magic in our system, it's not like we're going to have

34:01.040 --> 34:07.320
some amazing new way to peer into the neural network and tell you what these group of neurons

34:07.320 --> 34:08.320
meant.

34:08.320 --> 34:09.320
Right.

34:09.320 --> 34:10.320
That's not what we do.

34:10.320 --> 34:14.720
What we do is we say, we're going to let you break it up in this way.

34:14.720 --> 34:18.400
And it's always easiest to frame these things in real world scenarios.

34:18.400 --> 34:23.360
So let's say you are building a supply chain logistics system.

34:23.360 --> 34:26.760
You have got your real world data.

34:26.760 --> 34:28.920
So you have all the telemetry coming from that.

34:28.920 --> 34:32.520
You have simulation models that you've built in some discrete event simulator or something

34:32.520 --> 34:33.520
like that.

34:33.520 --> 34:36.640
So you've got all these different facets you can pull on.

34:36.640 --> 34:43.320
And the model that you use takes into account very coarse things like the weather and seasonality

34:43.320 --> 34:47.400
of goods and perishability of goods and not all these kinds of things that you might care

34:47.400 --> 34:48.400
about.

34:48.400 --> 34:52.920
And you might have more fine grain concepts that you're teaching at about the composition

34:52.920 --> 34:58.280
of your fleet and the land routes that are viable for you to follow and so like all these

34:58.280 --> 34:59.280
kinds of things.

34:59.280 --> 35:02.840
So you know, you can teach all of this stuff.

35:02.840 --> 35:07.880
What's important to emphasize here is that it is very rare for a company that we work

35:07.880 --> 35:11.320
with to come back and say, and now we're turning it all over to this automated system we

35:11.320 --> 35:12.320
trained.

35:12.320 --> 35:13.320
Go.

35:13.320 --> 35:14.320
That doesn't like me.

35:14.320 --> 35:15.320
Yeah.

35:15.320 --> 35:18.040
That's still there's not a level of comfort there yet.

35:18.040 --> 35:21.600
What happens is there's still a human and the human analyst is sitting there and making

35:21.600 --> 35:25.360
the ultimate decision and they're using the system to provide decision support.

35:25.360 --> 35:26.360
Yeah.

35:26.360 --> 35:29.920
And in that context, if you've built, I don't care how sophisticated it is.

35:29.920 --> 35:35.120
If you've built a very elaborate neural network or maybe you used some other machine learning

35:35.120 --> 35:39.600
or AI technique to build a system and it comes back and it says, I think you should have

35:39.600 --> 35:45.640
truck 17, which is currently in Hoboken, depart now.

35:45.640 --> 35:46.640
Go.

35:46.640 --> 35:49.600
I thought what I think you should do, the analyst is going to sit there and look at it

35:49.600 --> 35:52.320
and say, why?

35:52.320 --> 35:56.040
Okay, that's nice that you tell me that.

35:56.040 --> 36:01.960
And I understand that you have this visibility on massive parametric space that I'm not

36:01.960 --> 36:06.920
perhaps aware of as a human, but I'm still not comfortable with the fact that I have

36:06.920 --> 36:09.200
no context into why you're telling me to do this.

36:09.200 --> 36:15.440
That truck is only two thirds full and the next truck that can hit that location is

36:15.440 --> 36:17.600
not available for two days.

36:17.600 --> 36:20.360
Okay, I don't think that's a good call.

36:20.360 --> 36:25.720
Whereas if the decision support system comes back and says, I think you should have this

36:25.720 --> 36:27.720
truck leave now.

36:27.720 --> 36:30.120
And I think you should have it leave now because.

36:30.120 --> 36:33.920
And then it frames that prediction in the context, again, not magic, but in the context

36:33.920 --> 36:38.560
of all these concepts you've defined, that was your own model.

36:38.560 --> 36:42.160
So you as that analyst who is sitting there and presumably is either very familiar with

36:42.160 --> 36:46.440
that model or part of defining it, are going to look at that and have some rich context

36:46.440 --> 36:48.400
to say, oh, right.

36:48.400 --> 36:54.800
So we taught it about these overland routes and it sees that there is a storm coming and

36:54.800 --> 36:57.280
those routes are going to be cut off.

36:57.280 --> 37:01.080
And that's why it's telling me to do it now, even though there's not another truck for

37:01.080 --> 37:05.920
two days, whatever it happens to be, but it can frame it in that context.

37:05.920 --> 37:11.000
That gives you much more power as an analyst to make that decision in confidence in the

37:11.000 --> 37:12.000
results.

37:12.000 --> 37:13.560
You get your audit trail.

37:13.560 --> 37:14.560
This is why.

37:14.560 --> 37:15.800
This is why this happened.

37:15.800 --> 37:23.880
So even if those models are not ground truth, this is not exactly the state of the world,

37:23.880 --> 37:27.560
but it is the model that you use to run your business.

37:27.560 --> 37:32.840
It is the model you use to drive your robotic systems or whatever it happens to be.

37:32.840 --> 37:37.680
You want to have that level of explainability and that's what you currently use to frame it.

37:37.680 --> 37:42.800
And so we're taking a lot of the magic out of the AI, but giving you the flexibility

37:42.800 --> 37:47.560
that you could have a note in there where the system comes back and it says, I think

37:47.560 --> 37:49.560
you should have the truck to part now.

37:49.560 --> 37:52.920
And everything in the model, I'm not using any of that.

37:52.920 --> 37:57.160
If you just use that model, the model would not say, make the truck to part now, but I've

37:57.160 --> 38:01.280
also learned by virtue of looking at all the real world telemetry and that I believe

38:01.280 --> 38:05.480
making that prediction now is a good prediction.

38:05.480 --> 38:08.920
Then the analyst is going to look at it and say, well, I'm not really comfortable doing

38:08.920 --> 38:12.400
that, but let's look at what happens.

38:12.400 --> 38:16.360
Let's see if that was, in fact, a good prediction that comes out and that tells me I need to

38:16.360 --> 38:18.520
go back and enhance my model.

38:18.520 --> 38:23.320
My model is now deficient in some way and then you can iterate and you can keep working

38:23.320 --> 38:24.320
on the system.

38:24.320 --> 38:27.640
Specifically deficient in its expression of these concepts, right?

38:27.640 --> 38:29.640
It has a blind spot.

38:29.640 --> 38:33.520
Actually, blind spot isn't really the right way to say because the model doesn't have the

38:33.520 --> 38:34.520
blind spot.

38:34.520 --> 38:36.680
It's the lack of decomposition.

38:36.680 --> 38:38.680
Yeah, you can't explain it.

38:38.680 --> 38:41.920
It's like going to the human expert and saying, why?

38:41.920 --> 38:46.880
Having them say, I don't know, this is the way I do it and it works, right?

38:46.880 --> 38:51.440
Humans have this interesting, I'm a student of human nature as well, and humans have this

38:51.440 --> 38:57.040
interesting facet where we conflate the ability to explain something with the ability to justify

38:57.040 --> 38:59.040
something.

38:59.040 --> 39:03.320
It's important as you look at these systems and say, well, are you actually explaining

39:03.320 --> 39:08.280
why you chose to make this decision and that's really what happened or are you just looking

39:08.280 --> 39:10.960
at what happened and now you're justifying it?

39:10.960 --> 39:12.400
A lot of times it's the latter.

39:12.400 --> 39:14.040
It's not the former.

39:14.040 --> 39:17.040
That's human nature and that's just the way we are.

39:17.040 --> 39:22.160
But when it comes to industrial AI and really applying this technology, you really want it

39:22.160 --> 39:23.840
to be the former.

39:23.840 --> 39:29.720
In certain circumstances, you want to go back later and leverage the human strength, which

39:29.720 --> 39:35.120
is to say, system predicted I should do something that was out of the bounds of my model.

39:35.120 --> 39:38.240
There's a gap, assuming that it was the correct behavior.

39:38.240 --> 39:44.040
Then there's a gap, how do I try to fill that gap and then your ability to justify and

39:44.040 --> 39:46.520
come up with creative explanations for what that might be.

39:46.520 --> 39:50.080
Their ability to have your data scientists dive in and really tease apart what happened

39:50.080 --> 39:53.880
and try to refine that model becomes very powerful.

39:53.880 --> 39:55.560
It is a very fluid process.

39:55.560 --> 39:59.640
It is not a right once run forever proposition.

39:59.640 --> 40:07.360
It is a right many times, continually refine and learn as you go along and get a greater

40:07.360 --> 40:12.200
and greater and greater ability to explain what is actually happening as you go through

40:12.200 --> 40:13.200
that process.

40:13.200 --> 40:15.360
That's the nature of the beast.

40:15.360 --> 40:20.920
One of the things I found in my research and articulated in the industrial AI paper

40:20.920 --> 40:28.600
was an emerging maturity model in the way people are looking at deploying AI and in the enterprise

40:28.600 --> 40:33.520
generally, I think, but particularly in these industrial types of situations where it's

40:33.520 --> 40:39.680
just as you describe, there's this fundamental issue of trust.

40:39.680 --> 40:46.560
That trust is, I think, multifaceted part of it is repeatability part of it is explainability.

40:46.560 --> 40:48.880
There are a bunch of other factors.

40:48.880 --> 40:53.920
As people are gaining this trust, the first thing that they want to do is point these systems

40:53.920 --> 41:00.680
at some process and some system and just help them monitor it and tell them new things

41:00.680 --> 41:06.800
about it and surface new insights.

41:06.800 --> 41:12.400
As they gain some trust that, hey, it's actually providing me interesting information, maybe

41:12.400 --> 41:14.600
it should tell me what to do.

41:14.600 --> 41:21.480
Then they flip the switch or allow it to optimize and it becomes a decision support system

41:21.480 --> 41:23.960
in the way you described it.

41:23.960 --> 41:29.680
There's this further stage which is actually the next switch which is just do it.

41:29.680 --> 41:30.680
Think it so.

41:30.680 --> 41:38.920
I'm wondering if you see that same progression among your customers and what are some of

41:38.920 --> 41:43.360
the other factors that compose, what are some of the most important factors that compose

41:43.360 --> 41:46.120
trust beyond explainability?

41:46.120 --> 41:49.600
We definitely are starting to see some of that, but I would emphasize the starting two

41:49.600 --> 41:50.600
part.

41:50.600 --> 41:57.120
You yourself described there's an emerging capability maturity model that people are using as a factor

41:57.120 --> 41:58.120
here.

41:58.120 --> 41:59.480
I would wholeheartedly agree with that.

41:59.480 --> 42:05.720
The state of the market right now really is, it's like it's 1995 all over again.

42:05.720 --> 42:08.080
Hey, there's this cool visual basic thing.

42:08.080 --> 42:09.080
Let's go try it out.

42:09.080 --> 42:13.840
In this case, it's like there's this internet thing and everyone's like, wow, I really

42:13.840 --> 42:18.440
need to do something about this internet thing, but a lot of people, the maturity of people's

42:18.440 --> 42:20.560
ability to do that is all over the map, right?

42:20.560 --> 42:24.520
People don't even know where to start, they know it's important and you have true, deep

42:24.520 --> 42:29.440
experts who are doing it and so we see all of that and as you engage with customers and

42:29.440 --> 42:35.200
they're looking at the maturity of their trust in these systems and trust in their own models

42:35.200 --> 42:38.720
and own systems, they built so that they feel more comfortable turning it over.

42:38.720 --> 42:40.080
We definitely see that.

42:40.080 --> 42:44.440
You can look at, as an example, autonomous vehicles, so we talked to, of course, lots of people

42:44.440 --> 42:45.440
about autonomous vehicles.

42:45.440 --> 42:52.800
It's a hot area and there was a period of time not long ago where no trust whatsoever

42:52.800 --> 43:00.440
in having the systems make control decisions, using the technology for perception and identifying

43:00.440 --> 43:04.520
whether there's a bicycle on the road or a cow or something like that, okay, we're comfortable

43:04.520 --> 43:10.760
at that level right now, not steering the vehicle, like no, that's not yet.

43:10.760 --> 43:14.440
And you see some of those organizations now getting more and more comfortable with them.

43:14.440 --> 43:20.680
But really, it boils down to how well-baked is the systems that you've built.

43:20.680 --> 43:26.720
They can be at the point where it is just, you still want a human in the loop, it will

43:26.720 --> 43:30.920
get to the point where you want a human to validate what's coming out.

43:30.920 --> 43:35.400
And then yes, you will get to the point where frankly, you will have the system make decisions

43:35.400 --> 43:39.760
in an automated way and you do that because there's now sufficient trust and confidence

43:39.760 --> 43:44.080
in the system that it's going to do the right thing that doing the wrong thing is now

43:44.080 --> 43:48.760
an exceptional activity and they'll still make some mistakes, but it's rare and when

43:48.760 --> 43:52.280
it does make a mistake, you can capture that and you can use that to further refine the

43:52.280 --> 43:53.280
system.

43:53.280 --> 43:54.720
So yeah, I think that's a natural progression.

43:54.720 --> 44:00.520
We are starting to see that, but frankly, at this point in time, the market is really

44:00.520 --> 44:01.520
all over the map.

44:01.520 --> 44:07.320
And so tons of experts in a room, we will run into that with some frequency and you get

44:07.320 --> 44:10.960
into the occasion where people are just dipping their toes in the water and everywhere

44:10.960 --> 44:11.960
in between.

44:11.960 --> 44:16.200
So it kind of depends on the particulars of the customer and how forward-looking they

44:16.200 --> 44:21.920
are and how much resource they have to allocate towards exploring various facets of what

44:21.920 --> 44:23.080
they can do.

44:23.080 --> 44:29.200
But by and for the large part right now, there's a lot of desire to have explainability to

44:29.200 --> 44:33.840
have that audit trail, if you will, so that people can go back and test things.

44:33.840 --> 44:38.280
The more you get to automation towards the automation end of the spectrum, the more people

44:38.280 --> 44:43.600
want to be able to look not purely at the audit trail, but in generalization.

44:43.600 --> 44:47.920
So if you look at control systems for robotics, which we're talking about at the beginning,

44:47.920 --> 44:55.400
am I teaching the right concepts such that it will generalize the behavior in many scenarios,

44:55.400 --> 44:57.960
not just the one I'm teaching it about.

44:57.960 --> 45:02.520
So there tend to be more towards that end of the spectrum of trust and automation.

45:02.520 --> 45:04.680
So that's how they can perform their tests.

45:04.680 --> 45:10.680
I didn't just learn how to grasp in a way that is appropriate for this one task I'm trying

45:10.680 --> 45:11.680
to do.

45:11.680 --> 45:18.440
So did I learn grasping in a generalized way such that if I present a variety of objects

45:18.440 --> 45:21.840
and a variety of scenarios, it will still do the right thing.

45:21.840 --> 45:28.160
And consequently, you see everything, you see, you see everything on the spectrum at this

45:28.160 --> 45:29.960
point in time.

45:29.960 --> 45:34.880
Just to follow up on that last comment you made about the generalization, I imagine that's

45:34.880 --> 45:37.400
got to be driven by a business driver.

45:37.400 --> 45:41.480
You don't want to have an overly generalized system because that's going to be more expensive

45:41.480 --> 45:44.280
than what you need to solve your problem.

45:44.280 --> 45:45.280
100% agree.

45:45.280 --> 45:46.280
Yes.

45:46.280 --> 45:47.280
Yes.

45:47.280 --> 45:50.080
Do you find, I don't know what the question is.

45:50.080 --> 45:53.560
No, you're totally accurate though.

45:53.560 --> 45:55.960
If you have, let's just keep it in the same vein.

45:55.960 --> 45:57.640
It's easy to continue with that example.

45:57.640 --> 45:58.640
So robotics, okay.

45:58.640 --> 46:00.600
So I have my robotic system.

46:00.600 --> 46:02.800
What I have it doing, it's been retooled.

46:02.800 --> 46:05.480
We are part of a chain of this manufacturing chain.

46:05.480 --> 46:07.400
We just recently retooled for this chain.

46:07.400 --> 46:09.880
I really care about this one operation.

46:09.880 --> 46:14.120
I need to attach these two parts of whatever we're building.

46:14.120 --> 46:18.720
That means that the grasp has to be in a certain orientation because it's only viable to

46:18.720 --> 46:19.720
connect the two things.

46:19.720 --> 46:24.520
If it's I'm not covering up the part that needs to be combined and so on, all these kinds

46:24.520 --> 46:25.520
of things.

46:25.520 --> 46:27.280
And that's what they care about.

46:27.280 --> 46:30.560
They're really focused on that because that's what they're manufacturing right now and

46:30.560 --> 46:34.680
that's what's economically effective and efficient and capable.

46:34.680 --> 46:40.400
And if you look instead for the person who manufactures that piece of robotics equipment.

46:40.400 --> 46:47.040
So I have, I may be B or I'm, you know, you pick your robotic manufacturer of choice.

46:47.040 --> 46:48.840
I make robotic armatures.

46:48.840 --> 46:55.000
I care a lot about the generalization because for me, having it work with my customers,

46:55.000 --> 46:58.480
I have a variety of contexts helps them go faster.

46:58.480 --> 47:00.920
And so that's kind of the breakdown we tend to see now.

47:00.920 --> 47:07.920
And then I think I imagine it also goes back to this idea of customer and market maturity,

47:07.920 --> 47:08.920
right?

47:08.920 --> 47:12.920
So, you know, my first few projects are going to be like proofs of concept and things

47:12.920 --> 47:13.920
like that.

47:13.920 --> 47:17.600
And I'm trying to solve this thing quickly and see if this is a viable technology.

47:17.600 --> 47:21.720
But at some point, you know, project three or four or something like that, I want it.

47:21.720 --> 47:25.640
I really want to understand that, you know, if I'm going to invest in an approach or a

47:25.640 --> 47:32.240
platform that it can solve a broad swath of problems because there's real cost to that

47:32.240 --> 47:33.240
investment.

47:33.240 --> 47:34.240
Totally.

47:34.240 --> 47:36.080
And that's exactly what we see.

47:36.080 --> 47:42.320
So it is a typical engagement for us at the moment would be to have a, we have an early

47:42.320 --> 47:45.720
access program that we're actively working right now.

47:45.720 --> 47:49.320
And customers coming into that almost always want to run a proof of concept, as you said.

47:49.320 --> 47:53.520
As a first stab, we want to try to make it as aligned with the ultimate production

47:53.520 --> 48:00.320
use case as appropriate, but also appropriate duration and so on so that it's a, the spend

48:00.320 --> 48:06.600
of resource and people's time and money is aligned with pinning down exactly what you

48:06.600 --> 48:07.600
said.

48:07.600 --> 48:10.200
Is this technology something that makes sense for what we're trying to do?

48:10.200 --> 48:13.840
And as people get that level of comfort, then yes, then you get more into the, okay,

48:13.840 --> 48:17.680
now let's expand the scope and we're doing a much broader use of this technology and

48:17.680 --> 48:20.000
the production environment and so on.

48:20.000 --> 48:22.320
That is a progression that we see time and time again.

48:22.320 --> 48:23.800
And I think that's true.

48:23.800 --> 48:24.800
That's not just true about us.

48:24.800 --> 48:30.520
I think that's true about this technology in these industrial contexts in general.

48:30.520 --> 48:37.440
We talked a little bit about reinforcement learning and, you know, in some of the examples

48:37.440 --> 48:43.840
you gave, you start high level problem, you decompose it into concepts, you know, you

48:43.840 --> 48:47.480
know, if the concept is like a leaf node, you know, somewhere under there, you're doing

48:47.480 --> 48:50.120
reinforcement learning to kind of figure out that leaf node.

48:50.120 --> 48:55.560
If you don't already have a well understood model like inverse kinematics, help us understand

48:55.560 --> 49:03.680
the relationship between what you guys are doing and the underlying reinforcement learning

49:03.680 --> 49:04.680
stuff.

49:04.680 --> 49:09.880
Like are you, how are you, how are you architecting the, sure, the networks?

49:09.880 --> 49:12.800
How are you, you know, training them?

49:12.800 --> 49:18.120
Are there, you know, any particularly interesting things that you're doing to, you know, ensure

49:18.120 --> 49:22.920
that they're quickly trainable, is there any academic research that you have based your

49:22.920 --> 49:23.920
approaches on?

49:23.920 --> 49:26.400
Like what are the things that you think about in that interface?

49:26.400 --> 49:27.400
Sure, sure.

49:27.400 --> 49:30.240
That's a, that's a, there's a lot of depth that we can get into there.

49:30.240 --> 49:34.320
So let me, let me start at a high level so we can frame everything and then we'll, I'm

49:34.320 --> 49:35.320
happy to push down.

49:35.320 --> 49:40.840
So at a high level, the, for bonsai's platform in particular, the best way to think about

49:40.840 --> 49:43.400
it is in relationship to a database.

49:43.400 --> 49:48.240
So when you're building industrial or commercial enterprise application X for your company,

49:48.240 --> 49:52.600
whatever happens to be, and it's going to work with data, you're going to use a database

49:52.600 --> 49:53.600
almost certainly.

49:53.600 --> 49:55.920
That's a very common thing to do.

49:55.920 --> 49:57.880
And what is the database providing for you?

49:57.880 --> 50:03.240
It's providing that level of abstraction so that you are not focusing on how this data

50:03.240 --> 50:06.560
is split across disks in the cluster.

50:06.560 --> 50:11.320
And when you rebalance tree structures for searching and all this kind of like that, all

50:11.320 --> 50:13.480
that stuff's the database deals with for you.

50:13.480 --> 50:17.920
And it gives you this nice abstraction so you specify the structure of your data and

50:17.920 --> 50:21.800
the kinds of questions you want to be able to ask of it and it can take care of the rest.

50:21.800 --> 50:24.120
Our platform is very analogous to that, very similar.

50:24.120 --> 50:27.640
Now, of course, our abstraction is around this machine teaching stuff and so on.

50:27.640 --> 50:31.080
But in principle, when you're working with these simulation environments, when you're

50:31.080 --> 50:36.480
working with real world equipment and telemetry, you are nonetheless interfacing with all

50:36.480 --> 50:38.560
of those systems and you have to manage them.

50:38.560 --> 50:43.360
So if you're building a system and you're going to go through the actual training motions

50:43.360 --> 50:49.120
and let's say it's a supply chain logistics system and you have a discrete event simulation

50:49.120 --> 50:52.920
model of that system and you want to train primarily on that before you bridge to the

50:52.920 --> 50:58.240
real world telemetry data just so you get quicker learning and more repeatable learning.

50:58.240 --> 51:01.200
Then you have to manage those simulations.

51:01.200 --> 51:04.680
Those simulations, depending on what you're doing, those can run very quickly.

51:04.680 --> 51:08.120
In some cases, if you're doing a computational fluid dynamics simulation, that can run for

51:08.120 --> 51:12.200
hours or days and so managing all of that becomes something that matters.

51:12.200 --> 51:15.800
And so in the same way that you don't worry about data spanning different disks in your

51:15.800 --> 51:20.320
cluster on your database, we don't want you to worry about am I running 10 copies of my

51:20.320 --> 51:23.280
simulation environment and where am I running them?

51:23.280 --> 51:28.440
How do I reconfigure them between runs so that I'm maximizing the efficiency of the learning?

51:28.440 --> 51:31.640
All of that is the kind of stuff that our platform manages for you.

51:31.640 --> 51:36.280
And so in that sense, there's a lot of low-level plumbing infrastructure stuff that is really

51:36.280 --> 51:40.360
valuable because you don't have to worry about it and it manages that for you.

51:40.360 --> 51:45.440
But then when you dig down, okay, so that's the high level, now drill down a level.

51:45.440 --> 51:46.440
Great.

51:46.440 --> 51:47.440
I've provided this.

51:47.440 --> 51:52.520
But that's interesting stuff because at some point, someone's got to set up models and

51:52.520 --> 51:58.000
actually train them and have that stuff all automated out of the box is a lot of work

51:58.000 --> 52:00.000
that someone doesn't have to figure out how to do.

52:00.000 --> 52:04.320
Yeah, and in fact, that's kind of the state of the art for a lot of this work.

52:04.320 --> 52:08.360
If you look at the academic literature and you look at deep reinforcement learning algorithms

52:08.360 --> 52:14.080
in particular, you'll find DDPG networks and TRPO networks and questions around whether

52:14.080 --> 52:16.600
you should have learner memories or not.

52:16.600 --> 52:20.680
And is it on policy or off-policy method because if it, depending on which way it is, you

52:20.680 --> 52:26.920
might have to throw out historical data you've cashed, given what you're learning next.

52:26.920 --> 52:33.520
And this level of detail is great if you are focused on the mechanics of the learning.

52:33.520 --> 52:36.760
And from our perspective, it's the kind of thing you shouldn't have to worry about if

52:36.760 --> 52:41.960
you want to focus on what you're teaching and how the system should be intelligent at

52:41.960 --> 52:44.640
the end of the day, what the subject matter expertise is.

52:44.640 --> 52:47.760
I would rather abstract all that and manage it for you.

52:47.760 --> 52:52.520
And so by, you know, abstracting and managing it for you, you know, when you kind of punch

52:52.520 --> 52:56.280
into the details, can mean a bunch of things, it can mean, you know, actually we know about

52:56.280 --> 53:00.520
all this stuff and we know how to figure out which is the best thing, you know, and we

53:00.520 --> 53:01.520
do that.

53:01.520 --> 53:07.000
But what we really only do this one thing and you only have one choice and thus we've

53:07.000 --> 53:08.000
managed it for you.

53:08.000 --> 53:09.000
Sure.

53:09.000 --> 53:10.000
Sure.

53:10.000 --> 53:11.000
Yes.

53:11.000 --> 53:12.000
So it's more the former than the latter.

53:12.000 --> 53:13.000
Of course.

53:13.000 --> 53:14.000
As you'd expect.

53:14.000 --> 53:15.000
As you'd expect.

53:15.000 --> 53:16.000
You never know.

53:16.000 --> 53:17.000
No, it's a totally fair question.

53:17.000 --> 53:18.000
So what does that mean in practice?

53:18.000 --> 53:20.800
And let's use analogies again because it makes it easier to recognize.

53:20.800 --> 53:23.040
Let's say you're teaching the system something silly.

53:23.040 --> 53:24.560
I want it to learn how to play tic-tac-toe.

53:24.560 --> 53:29.080
I picked tic-tac-toe in particular because everyone's played it and everyone understands

53:29.080 --> 53:33.560
if you're past a certain age that it's a pointless game because you'll never win, etc.

53:33.560 --> 53:34.560
Right.

53:34.560 --> 53:37.160
The state space is not huge for tic-tac-toe.

53:37.160 --> 53:43.320
There's a funny xkcd comic actually where he literally maps out the entire state space

53:43.320 --> 53:44.320
in the comic.

53:44.320 --> 53:47.560
Like here is every position you can possibly enter and what the correct move is once you're

53:47.560 --> 53:48.560
in that.

53:48.560 --> 53:49.560
Like you can never, right?

53:49.560 --> 53:50.560
And you just do it.

53:50.560 --> 53:51.560
It's small enough of a state space.

53:51.560 --> 53:52.560
You can do that.

53:52.560 --> 53:53.560
We'll link to it in the show notes.

53:53.560 --> 53:54.560
Yeah.

53:54.560 --> 53:55.560
Awesome.

53:55.560 --> 53:56.560
Perfect.

53:56.560 --> 53:57.560
I can give you that link.

53:57.560 --> 53:58.560
That's easy.

53:58.560 --> 54:02.680
If you were teaching that and you did it in our system, part of what the system does when

54:02.680 --> 54:07.920
it's compiling and it's generating the appropriate underlying networks on your behalf is it will

54:07.920 --> 54:10.560
just run the simulation for a while.

54:10.560 --> 54:12.400
And it will collect statistics on what it sees.

54:12.400 --> 54:16.720
And if it's running tic-tac-toe, after it runs it a little bit, and this is not a take

54:16.720 --> 54:20.600
a lot of wall clock time, it's just got to run a lot of iterations of the, in this case,

54:20.600 --> 54:23.240
it's a game, but it runs a lot of iterations of it.

54:23.240 --> 54:25.960
It will see very quickly the state space isn't large.

54:25.960 --> 54:29.760
And consequently, maybe using a q-table is a perfectly reasonable algorithmic approach

54:29.760 --> 54:31.480
to solving this problem.

54:31.480 --> 54:32.480
Q-table being.

54:32.480 --> 54:38.520
Q-table being a specific approach for building a reinforcement learning network where it's

54:38.520 --> 54:43.720
appropriate if you don't have a lot of options to the state space is not huge.

54:43.720 --> 54:49.600
If you can enumerate enough of it, just a table is a perfectly reasonable thing to use.

54:49.600 --> 54:52.960
And I mean, that's an oversimplification of a q-table, but that's the general, that's

54:52.960 --> 54:56.240
the general gist, and fine, why not use that?

54:56.240 --> 54:58.320
That's an efficient system to use in this context.

54:58.320 --> 55:03.600
Whereas if I give it chess, and you say, go run some sample iterations of chess, and

55:03.600 --> 55:07.680
just see what's going on, it will very quickly learn that the state space is gigantic.

55:07.680 --> 55:08.680
It's huge.

55:08.680 --> 55:11.280
And using a q-table is not an appropriate choice in that context.

55:11.280 --> 55:15.440
You need to use a different type of network, and in fact, depending on how deep that network

55:15.440 --> 55:20.680
is and how well you've decomposed it, we might have to have some pretty deep layers,

55:20.680 --> 55:23.360
layer stacks in that network to be appropriate.

55:23.360 --> 55:27.880
So that's one level of how it decides which algorithmic approach is to use.

55:27.880 --> 55:30.120
It also looks at the kind of data that are flowing through.

55:30.120 --> 55:35.440
So if you're looking at chess, you can hand me the data as a array.

55:35.440 --> 55:41.160
If I'm looking at a robotic armature, you might hand me sensory data, which is just a collection

55:41.160 --> 55:46.800
of floating point values for motor torques and sensor detections and so on.

55:46.800 --> 55:51.720
And if I'm looking at autonomous vehicle, you might be handing me visual camera data.

55:51.720 --> 55:54.280
I could be handing the system any of these things.

55:54.280 --> 55:59.280
The appropriate network to utilize is completely dependent on that data.

55:59.280 --> 56:03.560
So if you hand me a visual data, I should be probably constructing an appropriately sized

56:03.560 --> 56:05.720
convolutional network, right?

56:05.720 --> 56:06.720
Exactly.

56:06.720 --> 56:08.040
It has spatial locality.

56:08.040 --> 56:13.400
If you hand me audio data, it has spatial locality in a sense, but it also has temporal locality.

56:13.400 --> 56:19.400
So I should be using different kinds of networks for that and all these kinds of heuristics

56:19.400 --> 56:23.760
and ways of looking at the environment and exploring and looking at how you deconstructed

56:23.760 --> 56:26.640
the problem, they're all taken into account.

56:26.640 --> 56:30.920
And so when the compiler outputs at the end of the day, okay, this is the network topology

56:30.920 --> 56:32.000
we're going to use.

56:32.000 --> 56:35.240
This is the algorithmic structure we're going to use, et cetera, et cetera.

56:35.240 --> 56:36.840
That's what's informing it all.

56:36.840 --> 56:40.400
And on top of that, we ourselves are a machine learning system.

56:40.400 --> 56:45.840
So it learns by virtue of having explored how to solve similar kinds of problems it's

56:45.840 --> 56:50.120
seen before, how to tune hyperparameters and all these other kinds of things.

56:50.120 --> 56:54.760
Again, levels of detail that the compiler should deal with for you, the platform should

56:54.760 --> 56:56.440
deal with for you.

56:56.440 --> 57:01.800
I want to, if we've done our job properly, it's much again going back to the database

57:01.800 --> 57:02.800
analogy.

57:02.800 --> 57:03.800
It's just like that.

57:03.800 --> 57:05.360
You don't have to tune any of those things.

57:05.360 --> 57:08.560
You don't even have to know what those things are or what they mean.

57:08.560 --> 57:11.240
You just have to be able to tell me what you want to teach and how to teach it and

57:11.240 --> 57:12.760
we'll build something for you.

57:12.760 --> 57:16.600
Is it the most efficient thing that you could possibly have hand tuned if you were an expert

57:16.600 --> 57:19.080
and knew how to do it yourself?

57:19.080 --> 57:20.080
Probably not.

57:20.080 --> 57:24.240
And that's why you still have database administrators and database experts who can go in and tweak

57:24.240 --> 57:27.880
all the, I'm going to look at the slow log and go modify the queries and I'm going to, all

57:27.880 --> 57:29.880
the things you do with databases.

57:29.880 --> 57:31.200
Same thing for us.

57:31.200 --> 57:35.760
If you want to go in and you want to provide a node in the system where you tell us, this

57:35.760 --> 57:37.080
is the structure.

57:37.080 --> 57:41.320
This is literally the topology it should be for this component.

57:41.320 --> 57:42.320
Okay.

57:42.320 --> 57:43.320
We're not going to stop you.

57:43.320 --> 57:44.800
That's fine.

57:44.800 --> 57:52.360
But if we give you the flexibility to decide where at what level you want to do that, that's

57:52.360 --> 57:57.520
better because it now lets you focus on the right people on the right levels.

57:57.520 --> 58:01.280
Your subject matter expertise can be focused on what to teach and how to teach it.

58:01.280 --> 58:05.160
Your machine learning experts and data scientists can be focused at that level and teasing

58:05.160 --> 58:09.160
apart when the networks are making predictions that weren't, didn't fit in your model and

58:09.160 --> 58:10.880
you need to now refine it.

58:10.880 --> 58:11.880
Great.

58:11.880 --> 58:12.880
That's where I want them to play.

58:12.880 --> 58:14.320
Your programmers have their role to play.

58:14.320 --> 58:17.840
I need to integrate with all these simulation environments and I need to have the telemetry

58:17.840 --> 58:20.680
data being tied into the system and so on.

58:20.680 --> 58:25.400
Every participant in this process has a role to play and our job as a platform company

58:25.400 --> 58:28.240
is to make sure that they all have the right tooling that's appropriate for what they're

58:28.240 --> 58:32.920
trying to do and they're part in the process and that we can have the discussion at the

58:32.920 --> 58:34.720
appropriate level.

58:34.720 --> 58:36.160
Is it at the level of the use case?

58:36.160 --> 58:38.520
Is it at the level of breaking that down?

58:38.520 --> 58:42.160
Is it at the level of the subject matter expertise or are we, do we need to get down to the

58:42.160 --> 58:46.640
level of convolutional neural network architecture?

58:46.640 --> 58:52.440
Is there a spec sheet or is there an enumerated list of like these are the networks that we support

58:52.440 --> 58:56.760
and how quickly does that evolve are there limitations or-

58:56.760 --> 59:00.040
So it's continuously evolving as you can imagine as a platform company we're always adding

59:00.040 --> 59:01.040
more.

59:01.040 --> 59:05.000
We have documentation up on our website so if you hit our main website-

59:05.000 --> 59:06.000
And so that's all there.

59:06.000 --> 59:07.840
Yeah just link to the docs section.

59:07.840 --> 59:13.560
There's documentation down to the protocol level so I have my own bespoke custom simulator

59:13.560 --> 59:18.720
which we do run into with some frequency and I need to be able to tie this simulator

59:18.720 --> 59:20.120
to the platform.

59:20.120 --> 59:21.120
How do I do that?

59:21.120 --> 59:24.640
So there's documentation on that and so it just kind of, it depends on what level you

59:24.640 --> 59:29.880
want to get to but yes there's documentation all up online publicly available.

59:29.880 --> 59:33.840
Do you get asked by customers like how many different network topologies or architectures

59:33.840 --> 59:35.840
do you support and-

59:35.840 --> 59:37.160
Yeah that happens sometimes it does.

59:37.160 --> 59:38.160
It depends on the-

59:38.160 --> 59:39.160
It depends on the-

59:39.160 --> 59:40.160
It's how technical they are.

59:40.160 --> 59:43.560
Machine learning, depth that they've gotten to.

59:43.560 --> 59:48.000
So yes we've had people ask that and so we can talk about TRPO networks and DDPG networks

59:48.000 --> 59:53.080
and or DQN networks and all the different, we don't have to numerate them all right now

59:53.080 --> 59:56.560
but yes you can start to enumerate all the different ones that the platform has baked

59:56.560 --> 59:59.320
in and have that conversation.

59:59.320 --> 01:00:02.840
You'll also run into people who have been rolling their own.

01:00:02.840 --> 01:00:07.200
So a lot of times that we'll get asked the question beyond which networks do you support.

01:00:07.200 --> 01:00:10.400
I was just going to ask this question.

01:00:10.400 --> 01:00:15.640
We'll get asked okay we have been, we'll get we'll get asked the question well who else

01:00:15.640 --> 01:00:20.040
in the space should we be looking at and really there's two answers.

01:00:20.040 --> 01:00:25.360
When it comes to deep reinforcement learning and applying that to solve these real problems

01:00:25.360 --> 01:00:29.280
the first is rolling your own right that is actually we run into that very commonly.

01:00:29.280 --> 01:00:36.200
And the second is very, very vertically specific company focused areas right.

01:00:36.200 --> 01:00:40.800
So there are players in the space who only focus on deep reinforcement learning,

01:00:40.800 --> 01:00:44.200
force applying, only deep reinforcement learning for robotics etc.

01:00:44.200 --> 01:00:47.560
So those are the kinds of things you run into.

01:00:47.560 --> 01:00:52.760
In the case of the latter if the solution they have fits your bill then that's great right.

01:00:52.760 --> 01:00:54.240
They have a very specific focus.

01:00:54.240 --> 01:00:59.560
We are a platform company so we're looking to allow you more flexibility in custom modeling

01:00:59.560 --> 01:01:01.480
and how you expand all that out.

01:01:01.480 --> 01:01:05.280
So that it's just kind of a judgment call internally for where you fall on what you want

01:01:05.280 --> 01:01:06.280
to do.

01:01:06.280 --> 01:01:10.760
And the former where you're rolling your own which we run into all the time.

01:01:10.760 --> 01:01:15.720
That's actually a perfectly fine situation for us because the conversation quickly becomes

01:01:15.720 --> 01:01:20.720
well how much time have you invested in that and have you run into this set of problems.

01:01:20.720 --> 01:01:25.280
So you'll run into customers who they've decided reinforcement learning is the correct path

01:01:25.280 --> 01:01:26.720
to solve their problem.

01:01:26.720 --> 01:01:29.800
They've been going down that path and building out the solutions.

01:01:29.800 --> 01:01:33.920
They're spending a ton of time managing their simulations, their tonning and all these

01:01:33.920 --> 01:01:37.320
different assets of what we had just been talking about.

01:01:37.320 --> 01:01:41.040
And we say well you know we can we're not going to magically make the problem that you're

01:01:41.040 --> 01:01:45.120
facing about crafting a good reward function go away right that's part of that's still

01:01:45.120 --> 01:01:48.080
part of the development process you're going to have to do that.

01:01:48.080 --> 01:01:52.080
So we can take all the pain of managing those simulations off of your shoulders right

01:01:52.080 --> 01:01:53.920
like the platform can help you do that.

01:01:53.920 --> 01:01:56.760
And so you run into those companies that are a little bit further down the roll your

01:01:56.760 --> 01:02:00.160
own and they're like oh wow I don't have to do this anymore.

01:02:00.160 --> 01:02:01.160
That's that sounds great.

01:02:01.160 --> 01:02:03.240
Let me let's let's talk about doing it.

01:02:03.240 --> 01:02:06.000
And then you run into ones that are just dipping their toes in the water.

01:02:06.000 --> 01:02:09.600
They kind of say well I could just build your built on top of TensorFlow.

01:02:09.600 --> 01:02:13.480
Why wouldn't I just build it using TensorFlow and we can start to enumerate all the reasons

01:02:13.480 --> 01:02:17.920
why but but generally we can say and these are the problems you will run into.

01:02:17.920 --> 01:02:20.920
And some percentage of them will say I don't want to I don't want to deal with that.

01:02:20.920 --> 01:02:21.920
Let's talk.

01:02:21.920 --> 01:02:26.520
And some percentage of them will say I got to experience that myself and that's fine with

01:02:26.520 --> 01:02:36.080
us because we know that you'll you'll come back totally totally yes exactly you got

01:02:36.080 --> 01:02:39.320
to learn how to find the balance of where where it makes sense to roll your own and

01:02:39.320 --> 01:02:40.320
where it doesn't.

01:02:40.320 --> 01:02:44.920
Yeah and actually I was going to I was going to ask the question more aligned with kind

01:02:44.920 --> 01:02:51.240
of that last way you expressed it and in particular you know TensorFlow is obviously a popular

01:02:51.240 --> 01:02:56.160
platform for this stuff like is it an either or no it doesn't have to be at all in fact

01:02:56.160 --> 01:03:01.440
we have a feature that we added the product recently called gears and for the developers

01:03:01.440 --> 01:03:05.600
in the audience this is an interrupt feature and for people who aren't developers per

01:03:05.600 --> 01:03:10.320
say it's what allows you to bridge the gap right it allows you to bring your current

01:03:10.320 --> 01:03:15.880
investments that you've built whether it's in TensorFlow or you built a perception model

01:03:15.880 --> 01:03:21.440
using OpenCV or whatever you happen to have done and add that to the system.

01:03:21.440 --> 01:03:26.120
Facet of building any of a modern machine learning system which is not just building the

01:03:26.120 --> 01:03:31.640
model but also deploying it in a production environment those there's actually completely

01:03:31.640 --> 01:03:35.680
separate problems are going to face in those two arenas and so you might have built these

01:03:35.680 --> 01:03:41.760
models but and want them to participate in the broader prediction pipeline and you can

01:03:41.760 --> 01:03:45.680
do that very easily in our system using the gears feature it'll we allow you to add

01:03:45.680 --> 01:03:52.920
that in so it's again going back to the the database analogy you don't have to if you

01:03:52.920 --> 01:03:56.600
have the that expertise and you want to do it or you've already baked a bunch of things

01:03:56.600 --> 01:04:00.680
and you want to leverage them we're not going to make you redo that work and we're not

01:04:00.680 --> 01:04:04.000
going to stop you from tinkering with the lower level pieces if that's what you want

01:04:04.000 --> 01:04:08.960
to do it just becomes an option as any good platform should it's we're going to give you

01:04:08.960 --> 01:04:13.640
the ability to say I don't want to deal with any of that do it for me or I've done some

01:04:13.640 --> 01:04:19.200
of this and I want to add it or now I want to tinker with the low level bits because I

01:04:19.200 --> 01:04:23.600
I feel like I've learned enough and I'm an expert and I want to do that now so you can you

01:04:23.600 --> 01:04:28.560
can play at any of these levels but it's not an either or in fact if you bring a TensorFlow

01:04:28.560 --> 01:04:33.880
model to the system it integrates really well our systems built on TensorFlow a TensorFlow

01:04:33.880 --> 01:04:38.960
based gear we can chain everything in all the appropriate ways if you have a Python function

01:04:38.960 --> 01:04:42.880
that you've written you want to call out to maybe you want to invoke a cloud API because

01:04:42.880 --> 01:04:48.600
you've made an investment in Watson or some other technology none of these are barriers

01:04:48.600 --> 01:04:52.680
and in fact this is part of the way the system was designed if we go back to the very very

01:04:52.680 --> 01:04:57.000
beginning and I talked about you have existing controllers that know how to do the inverse

01:04:57.000 --> 01:05:01.440
kinematics and all you need to do is move your robotic armature between point A and point

01:05:01.440 --> 01:05:06.640
B you should not be teaching that that is but and this goes to the academics right so

01:05:06.640 --> 01:05:10.000
you look at the academic papers they're going to talk about how you teach all these things

01:05:10.000 --> 01:05:14.880
and that's great because it's it's talking to how we further enhance the technology and

01:05:14.880 --> 01:05:20.120
I love that work that's great but if we're talking about practical real world application

01:05:20.120 --> 01:05:23.920
why you have tons of work spent on building that and you should just use it and that's

01:05:23.920 --> 01:05:29.800
a very simple simple example but if you look at autonomous vehicles the automobile companies

01:05:29.800 --> 01:05:36.640
have spent a lot of resource on building capabilities around assistive parallel parking

01:05:36.640 --> 01:05:40.720
and all these other things do we really want to go reinvent all of that it it seems kind

01:05:40.720 --> 01:05:43.960
of silly we should just integrate that with the rest of everything else and that

01:05:43.960 --> 01:05:49.800
idea extends to your cloud based API is your TensorFlow module totally totally that's right

01:05:49.800 --> 01:05:55.520
that's right it can't be a homogenous this is the only way or you know that's not that's

01:05:55.520 --> 01:06:00.600
not practical that doesn't work so this has been super interesting anything that you'd

01:06:00.600 --> 01:06:07.160
like to leave folks with well I would encourage the audience to take a look at our platform

01:06:07.160 --> 01:06:10.320
if they're interested in the early access program of course we would love to hear from

01:06:10.320 --> 01:06:14.520
you if you have a use case that you think is suitable we'd love to hear from you but

01:06:14.520 --> 01:06:19.920
just in general I would encourage everyone to start thinking about machine teaching whether

01:06:19.920 --> 01:06:26.560
it's with us or not with us the path forward for industrial AI in general is going to

01:06:26.560 --> 01:06:32.560
rely very heavily on the marriage of human expertise and machine intelligence you need

01:06:32.560 --> 01:06:38.760
both it's not enough to have one or the other you need both and so starting to explore

01:06:38.760 --> 01:06:43.960
in more depth how you're teaching the system don't just throw data at it blindly don't

01:06:43.960 --> 01:06:47.480
that that's that's level one right you need to move several levels beyond that and so I

01:06:47.480 --> 01:06:52.200
would encourage everyone listening start to think about that start to think about strategies

01:06:52.200 --> 01:06:55.720
for doing that whether you're rolling your own or whether you're using a tool like ours

01:06:55.720 --> 01:07:01.000
that matters a whole lot and as practitioners using this technology a lot of our job is

01:07:01.000 --> 01:07:06.680
going to be teaching it's not going to be over time the tools will get better tensor flow

01:07:06.680 --> 01:07:11.800
will add more capabilities platforms like ours will become more prominent all of these

01:07:11.800 --> 01:07:16.520
things will happen just as a natural evolution of the industry and the part that will remain

01:07:16.520 --> 01:07:22.960
in all circumstances no matter what is how do I teach what I want the system to actually

01:07:22.960 --> 01:07:26.760
be intelligent about and that's going to be with us forever and that's very particular

01:07:26.760 --> 01:07:31.080
and idiosyncratic to our businesses and what we're trying to accomplish so really start

01:07:31.080 --> 01:07:34.440
to think about that that would be what I would encourage the audience to do great great

01:07:34.440 --> 01:07:39.080
and we'll make sure we point folks to your website and the eap and some of the other stuff

01:07:39.080 --> 01:07:43.240
we talked about sounds great awesome thank you so much yeah thank you as well I really

01:07:43.240 --> 01:07:53.560
appreciate it all right everyone that's our show for today thanks so much for listening and for

01:07:53.560 --> 01:07:59.480
your continued feedback and support for the notes for this episode including links to mark

01:07:59.480 --> 01:08:05.400
and the various resources mentioned on the show head on over to the show notes at twemolei.com

01:08:05.400 --> 01:08:11.640
slash top slash 43 please be sure to comment there with your feedback or questions

01:08:11.640 --> 01:08:19.960
thanks again to our sponsors for this series bonsai and wise.io at ge digital I'm so so grateful

01:08:19.960 --> 01:08:25.240
for their support if you enjoyed this series it would mean a ton to me if you took a second

01:08:25.240 --> 01:08:32.360
to reach out to them on twitter to thank them for their support at at wise.io and at bonsai AI

01:08:33.240 --> 01:08:39.000
don't forget to register for my newsletter at twemolei.com slash newsletter and for next months

01:08:39.000 --> 01:08:54.040
online meetup at twemolei.com slash meetup thanks again for listening and catch you next time

