WEBVTT

00:00.000 --> 00:15.280
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

00:23.280 --> 00:28.720
All right everyone, I am super excited to be on the line with Yashua Benjiyo. Yashua is a

00:28.720 --> 00:34.240
professor in the Department of Computer Science and Operations Research at the University of Montreal

00:34.240 --> 00:40.000
and the founder and scientific director of Miele. Yashua, welcome to the Twimal AI Podcast.

00:40.000 --> 00:47.600
Hi, it's good to be here. It is super exciting for me to have you on the line. I've already said

00:47.600 --> 00:53.680
super exciting twice, so maybe that's an indication of my enthusiasm for the opportunity to chat with

00:53.680 --> 01:00.800
you a little bit about your work. Your name is certainly well-known among our audience and your

01:00.800 --> 01:07.520
contributions to this field. I read somewhere that you are ranked as the most cited computer

01:07.520 --> 01:14.560
scientist worldwide or one of. In terms of recent contributions, yes. There are people who have been

01:14.560 --> 01:24.320
in the field longer than me and I'm among the young old people. Nice and just last year, you along

01:24.320 --> 01:32.240
with Jeff Hinton and Jan Lecune received the ACM Touring Award for your contributions with

01:32.240 --> 01:36.720
deep learning, deep neural networks. Why don't we start with having you tell us a little bit about

01:36.720 --> 01:43.680
that journey and how you came to work in AI and on deep learning and of course we'll get to what

01:43.680 --> 01:48.560
you're working on now, but let's start with some of that background. Yeah, deep learning,

01:48.560 --> 01:55.120
neural nets, it's been my life, my professional life at least. I had started as an adolescent reading

01:55.120 --> 01:59.200
a lot of science fiction and getting acquainted with the notion of AI and robots and so on

02:00.320 --> 02:07.760
with the three laws of robotics and all that and movies like 2001 Space Odyssey. And then at

02:07.760 --> 02:14.480
university, I studied first computer and electrical engineering and then computer science

02:15.680 --> 02:20.880
in my masters and I had to choose a topic and by chance I got to read some of the early neural

02:20.880 --> 02:26.000
net papers from people like Jeff Hinton and I realized that this was the field I wanted to work on

02:26.000 --> 02:34.720
because it was about trying to understand the principles that would explain our own intelligence,

02:34.720 --> 02:41.040
humans and intelligence is very central to who we are and then building machines thanks to

02:41.040 --> 02:46.080
understanding these principles like the way that we understand how burst fly but we're not

02:46.080 --> 02:50.160
necessarily copying the birds, we are trying to understand those principles so we can build airplanes

02:50.160 --> 02:57.360
that also fly. A lot of your recent work is focused on this idea of consciousness. What's the

02:57.360 --> 03:04.480
relationship between consciousness and intelligence? Very good question. So first of all, let me put

03:04.480 --> 03:09.840
things in perspective about deep learning. So a lot of the progress we've made in deep learning

03:09.840 --> 03:18.000
and neural nets in the last few decades has helped us to build machines that can do pretty well

03:18.000 --> 03:22.800
at some of the things that brains are good at including perception, the ability to understand

03:22.800 --> 03:30.000
images, sounds and so on. But there are also things that humans do with their brain, the things

03:30.000 --> 03:34.480
that we are conscious of. So when you decide to do something and you're able to report what you're

03:34.480 --> 03:41.040
thinking about or why you do something, that kind of cognitive ability which is associated with

03:41.040 --> 03:47.680
being conscious of it is not something that we're good at in AI right now and some people might

03:47.680 --> 03:52.080
even think that it's incompatible with all of the ideas that have been put forward with the

03:52.080 --> 03:58.320
neural nets and deep learning and it's more related to some of the older ideas in AI based on

03:58.320 --> 04:04.080
symbols and logic and expert systems. But actually your brain is a huge neural net that we need to

04:04.080 --> 04:11.200
understand better. And the good news is that in the last two or three decades, neuroscientists,

04:11.200 --> 04:16.320
cognitive neuroscientists have made a lot of progress in understanding what is going on in your

04:16.320 --> 04:22.000
brain when you're doing something consciously which parts of your brain get activated in what

04:22.000 --> 04:27.920
order and so on. And so whereas the research on consciousness was almost something taboo in the 20th

04:27.920 --> 04:33.600
century, in this century it is something that has become an important topic of serious science.

04:35.520 --> 04:41.440
And so my research now is at the cusp of this transformation where computer science and AI

04:41.440 --> 04:49.040
research are starting to take stock of what has been discovered in neuroscience and try to take

04:49.040 --> 04:56.240
some of these ideas and import them into new types of deep learning systems that would come with

04:56.240 --> 05:00.800
the advantages of being conscious, which I can also explain. One of the ways that you describe the

05:00.800 --> 05:06.400
relationship between the type of deep learning systems that we have today and this model that you

05:06.400 --> 05:12.160
see as a possibility as we start to incorporate these ideas of consciousness is system one and

05:12.160 --> 05:18.240
system two coming from Daniel, condiments book thinking fast and slow. Maybe you can elaborate a

05:18.240 --> 05:26.240
little bit on these two different systems as he puts it and from my curiosity was that an

05:26.240 --> 05:32.240
inspiration for the way you're thinking about this now or did it become a handy explanation

05:32.240 --> 05:39.280
for some of what you're doing? Oh, it's definitely an inspiration. His work and the work of

05:39.280 --> 05:47.840
psychologists and neuroscientists who have helped clarify the different types of functions that are

05:47.840 --> 05:55.920
being that are happening in your brain has really helped me and others think about how modern AI

05:55.920 --> 06:01.120
systems could could incorporate these things. So first of all realizing that you have these two

06:01.120 --> 06:07.360
very different types of cognitive abilities. A system one is the kinds of things that deep learning

06:07.360 --> 06:12.880
is good at right now. The kinds of things you can do in half a second. So you see an image,

06:12.880 --> 06:17.200
you know that it's a cat and you don't need to think about it. It happens automatically.

06:17.920 --> 06:23.200
In fact, you don't even need to be conscious of it. It's something that we can see in your brain

06:23.680 --> 06:30.480
that if it's maybe happening on the side or you have not enough time, your brain will record

06:30.480 --> 06:35.840
that it's a cat but it doesn't even get to your consciousness. So these things are intuitive

06:35.840 --> 06:43.200
and they're hard to verbalize. Like you can't explain why you recognize that this was a cat.

06:43.200 --> 06:50.000
Or you could try to explain it but it's not a good explanation. In fact, a lot of the earlier failures

06:50.000 --> 06:54.880
of computer vision research was that we were trying to take our own internal explanation. So,

06:54.880 --> 06:59.760
you know, what is it that makes a cat and take that in trying to go through all this. But it was

06:59.760 --> 07:04.320
never, you know, very good because there's a lot of the way that our brain does it which we don't

07:04.320 --> 07:11.120
have access to. So, everything that is intuitive that is at a, you know, subverbal's unconscious

07:11.120 --> 07:16.560
level is roughly system one. There are some differences between conscious and conscious and system

07:16.560 --> 07:24.400
one system two but it's a good way to first order to understand things. And system two is instead

07:24.400 --> 07:29.360
the things you typically do consciously. The things that you can report, the things that you,

07:29.360 --> 07:33.360
you know, you can verbalize, not everything can be verbalized but a lot of the things that are

07:33.360 --> 07:39.680
your conscious of can be verbalized. And that you do in sequence. It's like at each step you,

07:39.680 --> 07:45.520
you might be involving intuitive computation in your brain but you're going to sequence these things

07:45.520 --> 07:53.760
in your mind in a way that you control and that, you know, gives you an extra power, an extra

07:53.760 --> 08:02.400
flexibility which allows us to do things like learn to drive or learn to drive in a new city

08:02.400 --> 08:08.320
or figure out what to do in some unusual circumstances where we have to be creative and find solutions

08:08.320 --> 08:14.640
on the spot and it doesn't look like anything we've seen before. This kind of very powerful,

08:14.640 --> 08:23.440
dynamically adaptive behavior that humans employ to solve new problems is a very typical of system two.

08:23.440 --> 08:31.120
And current AI isn't very good at these things. Current AI, if you train the systems on some data

08:31.120 --> 08:39.600
and then you deploy them in situations that are not exactly the same kind, you get a big hit

08:39.600 --> 08:44.160
in performance. It doesn't work as well whereas humans are able to transfer their knowledge to

08:45.040 --> 08:53.120
new domains to, you know, new environments much more easily. And very often this is when you use

08:53.120 --> 08:59.840
your conscious strengths. So when you're doing something habitual, you don't need to think about

08:59.840 --> 09:05.280
it. So the example I give is when you're driving on the usual path, you can talk to the person

09:05.280 --> 09:09.680
besides you. You're conscious of the conversation but you don't need to be conscious of the details

09:09.680 --> 09:14.160
of the road. I mean, of course you're taking chances when you're doing that but the point is a lot

09:14.160 --> 09:20.640
is going on unconsciously because it's something familiar that you have seen a lot and you don't

09:20.640 --> 09:27.840
need to be mindful of it. Whereas when you're doing something that's more unusual and you need to

09:27.840 --> 09:35.040
practice a new skill or to have a new coherent way of putting together pieces of knowledge,

09:35.040 --> 09:39.600
you're using system two and you're using conscious processing. It was interesting to me that in

09:39.600 --> 09:46.160
your description of these two systems, you talked about unvocalize or subverbal and, you know,

09:46.160 --> 09:55.440
things that we can vocalize and put into words. And in your descriptions of consciousness, you

09:55.440 --> 10:02.000
use the examples of language a lot to represent these ideas. I'm curious to relationship between

10:02.000 --> 10:09.680
language as a construct and consciousness. Yeah. This is obviously an open question. So you have

10:09.680 --> 10:14.640
to realize that the science of consciousness is something fairly new, as I said, like a couple of

10:14.640 --> 10:20.080
decades a bit more, and there are a lot that we need to understand. So we have to be very humble

10:20.080 --> 10:25.200
about how much we don't know. But yeah, it seems very obvious that there's a very strong connection

10:25.200 --> 10:32.880
between language and consciousness, but they're not equivalent. For example, you might have

10:34.240 --> 10:39.120
things that are happening in the area of your brain that deals with language that are kind of

10:39.120 --> 10:43.760
different from the things that have to do with being conscious of something. So, but there are

10:43.760 --> 10:51.680
strong connections. And in fact, one of my current research tracks is to exploit hypothesis

10:52.400 --> 11:00.640
that that connection is very tight in the sense that the high level concepts that you manipulate

11:00.640 --> 11:07.680
consciously also correspond to things like words, things that you can verbalize. And so the

11:07.680 --> 11:11.440
representation level, the two things are very close to each other. Yeah, there's been

11:11.440 --> 11:17.760
a past work, nothing that I could cite or that I know intimately, but that we kind of talk about

11:19.360 --> 11:25.520
generally that, for example, in some cultures, they don't have words for certain types of things.

11:25.520 --> 11:30.960
And the implication being that that kind of conditions their thought in certain directions

11:30.960 --> 11:36.640
that differs from other cultures. And it seems very much in line with this idea of

11:36.640 --> 11:43.520
consciousness as you're defining it. Yes, absolutely. And in fact, the way that I want to use this

11:45.040 --> 11:53.680
hypothetical connection is to help the learning of these high level concepts. So, what deep learning

11:53.680 --> 11:58.240
is about, which I didn't have time to say earlier, is learning good representations.

11:59.520 --> 12:04.960
And we've been very good at learning, let's say, low level and mid-level representations with deep

12:04.960 --> 12:12.480
learning, especially on the visual input. But we don't yet have good algorithms to discover

12:12.480 --> 12:18.720
the right variables or high level concepts that humans manipulate consciously. Of course, we can,

12:18.720 --> 12:23.680
you can kind of cheat by telling machines, this is a cap, this is a dog, and then they kind of know

12:23.680 --> 12:28.800
what is a cat and a dog by example. But what we don't have is an ability to discover these

12:28.800 --> 12:36.480
high level things. And so, by connecting, say, video input with a corresponding language, in the

12:36.480 --> 12:43.200
right way, we can force a deep learning system to learn representations, which at the top level

12:43.760 --> 12:50.400
would have features, if you want, that correspond to words or phrases or linguistic constructs.

12:50.400 --> 12:57.040
And that would help the representation learning system discover those high level concepts,

12:57.040 --> 13:03.360
in the same way that when we talk to children, using words in particular context helps them

13:03.360 --> 13:08.160
build the corresponding meaning of those words in their mind. But of course, they don't need the

13:08.160 --> 13:13.600
words. They're building meaning absent of the words. Initially, they learn a lot of things that

13:13.600 --> 13:18.640
they don't have words for. But it's actually difficult to disentangle these things, because even though

13:18.640 --> 13:24.160
it might take a couple of years before they start talking, during those couple of years, they also

13:24.160 --> 13:30.320
hear a lot of language. Now, the interesting clue is that in some cultures, parents don't talk to

13:30.320 --> 13:35.200
their children. They talk with each other, and the children just listen. Whereas in our western

13:35.200 --> 13:39.600
cultures, we tend to talk a lot to our babies. So you might think that they're learning from that

13:39.600 --> 13:46.000
interaction. But it looks like even without a direct sort of naming, oh, this is a cat, this is a dog,

13:46.000 --> 13:54.560
the babies can catch the connection between the labels, the words, and the things in the world.

13:54.560 --> 14:01.120
So we jumped in pretty quickly and started talking about some of the implications of how you're

14:01.120 --> 14:09.360
defining consciousness. But you really launched into this field with a paper in 2017, the consciousness

14:09.360 --> 14:14.480
prior. That's right. What exactly is the consciousness prior? And maybe tell us the, you know,

14:14.480 --> 14:19.680
the main points that you're trying to convey in that paper. Okay. I'm going to try to do that

14:19.680 --> 14:28.240
in an accessible way. Yeah. So first of all, what is the word prior means? It's a term we use

14:28.240 --> 14:37.840
in machine learning research to say that the learning system is exploiting some kind of assumption

14:37.840 --> 14:43.120
about the world. And there's even theorems that say that you have to have assumptions, at least

14:43.120 --> 14:50.640
minimal ones in order to be able to learn successfully. And so the brain has these kinds of assumptions

14:50.640 --> 14:56.080
about the world that, you know, we inherit from our ancestors through that have, you know,

14:56.080 --> 15:01.440
these assumptions have encoded somehow in our genes. And we are born with those things.

15:02.240 --> 15:08.240
And they help us to learn faster and better in the world around us. So when I said at the

15:08.240 --> 15:12.880
beginning that one of my research goals is to understand the principles that give rise to

15:12.880 --> 15:18.080
intelligence, well, an important part of this is what are the kinds of assumptions that humans

15:18.080 --> 15:24.320
exploit about the world that allow us to learn efficiently about it, to build, to understand how it

15:24.320 --> 15:31.200
works, to learn language, to learn to model the world, to act in the world and so on. So notions,

15:31.200 --> 15:36.880
you can think of things like, well, notions of time and space of agency that, you know, I do things

15:36.880 --> 15:44.480
and their effects and so on, are probably things that we got from our genes and that our brain

15:45.360 --> 15:51.040
is exploiting as some kind of assumptions about the world. And so the consciousness prior is

15:51.040 --> 15:56.400
one such assumption which would be connected with a notion of consciousness, but they actually,

15:56.400 --> 16:02.720
I actually have a whole list of related assumptions, but this one is very central. So what it says is

16:02.720 --> 16:11.200
there are two kinds of knowledge about the world, which is somewhere in our brain. And that's

16:11.200 --> 16:15.360
basically the system one knowledge and the system two knowledge, right? So the system one knowledge

16:15.920 --> 16:21.520
is knowledge that is difficult to put in words, as we were defining system one this way essentially,

16:22.080 --> 16:27.440
whereas system two knowledge is knowledge that is easy to put in words. Now what kind of knowledge

16:27.440 --> 16:35.440
has the property that it can easily put in words? Well, there is a nice structural property

16:35.440 --> 16:42.160
of the kind of knowledge we can communicate with words. And the property is that we are able to

16:42.160 --> 16:49.200
make predictions about things that can happen, about words, given other things that we know.

16:49.200 --> 16:53.920
So for example, if I say, if I drop my phone, it will fall on the ground,

16:53.920 --> 17:00.560
that sentence only involves a few concepts. It involves a phone, it involves the ground,

17:00.560 --> 17:07.840
it involves the act of dropping and the result, right? That's very few elements if you think about

17:07.840 --> 17:14.880
it. Normally when you try to make a prediction about something in the world, you need a whole lot

17:14.880 --> 17:21.520
of other things to make that prediction accurate. So if I'm trying to predict the next pixel that

17:21.520 --> 17:27.840
will show up at some position in the video that I'm seeing right now, I need to know about all the

17:27.840 --> 17:33.120
other pixels and all the pixels that I've seen in the last few seconds or something or minutes.

17:34.880 --> 17:41.440
And that's like millions of numbers that come into that prediction. So pixels are difficult to

17:41.440 --> 17:48.400
manipulate to explain with language. They don't have this property. But instead, if I explain the

17:48.400 --> 17:55.120
world in terms of objects that I can name, like I did with my phone that could drop on the ground,

17:55.120 --> 18:00.000
this way of representing information, of representing knowledge is one where we can make

18:00.880 --> 18:05.680
statements about things that should be true and things that should not be true.

18:06.880 --> 18:13.120
Where each of those statements only involves very few concepts. So with natural language,

18:13.120 --> 18:17.600
we can communicate a lot of knowledge about the world, but it's decomposing to little pieces

18:17.600 --> 18:25.280
like sentences. In each of these sentences only involves very few concepts. So in machine learning

18:25.280 --> 18:32.240
jargon, we say we can summarize this in a single phrase. We could say that the joint distribution

18:32.240 --> 18:39.680
of the high level concepts is sparse. Sparse here means that if you draw the connections between

18:39.680 --> 18:47.200
all the concepts as a graph, the graph has very few edges coming out of each node. So each

18:47.200 --> 18:56.240
node corresponds to a concept. And each concept is attached to other concepts through very few edges.

18:56.240 --> 19:03.360
So the graph between the connections between the concepts in that sentence or the concepts in

19:03.360 --> 19:07.360
all the things that you could say, if you think about all the things that you could say,

19:07.920 --> 19:12.880
you decompose all the things that you could say into sentences just to make it keep it simple.

19:12.880 --> 19:18.480
Now each sentence is like a special kind of node in the graph. And each sentence connects to

19:18.480 --> 19:24.640
very few words. Now the words are connected to many, all of the sentences that they could appear in.

19:25.840 --> 19:31.360
But each of these sentences only involves very few words. And so overall, the structure of that

19:31.360 --> 19:37.760
graph is extremely sparse. Whereas if I were to draw a graph of the statistical dependencies

19:37.760 --> 19:42.560
between pixels, it would all most need to be like fully connected. That every pixel needs to talk

19:42.560 --> 19:49.600
to every pixel. So what happens is your brain has created these high-level variables, high-level

19:49.600 --> 19:55.520
concepts, which allow to decouple a lot of the complicated dependencies that would otherwise

19:55.520 --> 20:00.240
seem to exist. And in addition, so that's the consciousness probably, but there are like

20:00.240 --> 20:05.200
side statements to this. So one of them is that those high-level variables have to do with

20:05.200 --> 20:10.240
causality. So those dependencies, when I said that if I dropped the phone, it's going to fall on

20:10.240 --> 20:16.240
the ground. It's also like a causal statement. It says something about if I do a particular action,

20:16.960 --> 20:22.480
this is going to be the consequence. And there's an object which is going to be affected by the

20:22.480 --> 20:32.000
action of dropping the phone. And so this is important because causality allows us to understand

20:32.000 --> 20:37.200
the world in a strong sense. That goes beyond making predictions. It allows us to

20:37.200 --> 20:44.400
imagine what could happen, for example, or what could have happened. These are called counterfactuals.

20:45.520 --> 20:54.080
You give an example in some of your talks on this about putting on sunglasses and that simple

20:54.080 --> 21:02.160
act which can be reduced to one bit. Change has a significant implications on all the pixels

21:02.160 --> 21:09.760
that are firing in your retina, but it's a single bit. Yeah. So exactly. And so I was talking about

21:09.760 --> 21:14.560
causality earlier. And one of the particularly interesting aspects of causality is that it tells us

21:14.560 --> 21:22.640
about how the world typically changes. So it's not just about how the concepts are related to

21:22.640 --> 21:30.480
each other, but how these relations changes over time due to agents like people or robots,

21:30.480 --> 21:37.760
eventually, and animals doing things. And what happens is that those changes are very localized

21:38.400 --> 21:44.320
in the sense that they involve only one or a few concepts. So typically when you come up with an

21:44.320 --> 21:48.800
explanation for something that has happened and our brain constantly tries to come up with explaining

21:48.800 --> 21:54.960
what is going on verbally, we end up being able to provide a very short explanation like one sentence.

21:54.960 --> 22:00.800
Oh, he dropped the phone. But this is amazing, right? Because the world has changed like I've put

22:00.800 --> 22:07.280
on these dark glasses. And I'm able to explain it by referring to very few things. I have put on

22:07.280 --> 22:13.520
my dark glasses. Whereas if you didn't have that assumption, you might imagine that everything

22:13.520 --> 22:18.560
has changed and you can't do any prediction anymore. But now the fact that is very few things

22:18.560 --> 22:22.480
that have changed allow you to recover from those changes. And this is what humans are good at.

22:22.480 --> 22:28.480
They're good at recovering from changes that are happening in the world. So that's this adaptive

22:28.480 --> 22:34.400
strength skill that humans have that we would like to put in machines. So this whole research

22:34.400 --> 22:39.680
on consciousness is not just about understanding an important part of who we are, who we are,

22:39.680 --> 22:45.040
which is clearly consciousness is a big part of it. But it's also building those abilities in

22:45.040 --> 22:50.400
machines to be more robust to the changes that can happen in the environment. The idea of

22:50.400 --> 22:56.240
consciousness, then you kind of express it as this low dimensional representation of the broader

22:56.240 --> 23:02.160
connections that are in the brain or that are in some system that we don't have yet. That's related

23:02.160 --> 23:08.000
to the idea of attention that we've been experimenting with in neural networks. What's the connection

23:08.000 --> 23:12.880
between those? Yes, yes. Very good question. And actually, it's a tough one because a lot of people

23:12.880 --> 23:20.000
still don't see the connection. So let me try to explain it. So remember I said that we're exploiting

23:20.000 --> 23:27.440
this assumption that the dependencies between concepts have this sparsity, like pieces of knowledge

23:27.440 --> 23:36.400
each would be like a sentence. Now, if you want a machine or brain to compute over that knowledge

23:36.400 --> 23:42.080
base that we have in our brain or in a computer, you want to take advantage of that sparsity.

23:42.080 --> 23:49.120
And a good way to do it is to focus the computation on just the right pieces at each step,

23:49.120 --> 23:53.040
because then you only need to consider the interactions between a few variables.

23:53.600 --> 23:59.200
And that's much easier both in terms of computations and in terms of what we call the statistical

23:59.200 --> 24:03.840
advantage. So if something changes then we can learn it quickly if it involves only a few variables.

24:03.840 --> 24:13.680
So attention is a way to exploit this inherent sparsity, the assumed sparsity that I'm talking about,

24:14.400 --> 24:18.960
so that we can do this very special kind of computation that involves very few variables at a time.

24:18.960 --> 24:23.280
So attention selects just these few variables that come into our working memory.

24:24.480 --> 24:32.000
And then we can utter a sentence to share what we have in our mind. But there's more to this,

24:32.880 --> 24:38.640
the fact that we have this attention bottleneck, which is also this bottleneck is a central piece of

24:39.280 --> 24:45.280
current theories of consciousness, forces the part of the knowledge about the world which

24:45.280 --> 24:53.120
should go in system two to go there. So only the things that can go through this bottleneck

24:53.120 --> 24:58.640
that involves little pieces of knowledge involving a few variables at a time are being

24:58.640 --> 25:02.800
represented at that level. The stuff that you can't handle with consciously goes, you know,

25:02.800 --> 25:08.400
system one, and it takes more time to learn because it about the interaction of many things together,

25:08.400 --> 25:15.840
like extracting information from images, but the stuff that somehow has this property,

25:15.840 --> 25:23.680
the consciousness prior property, that will be processed by system two, and now you have

25:23.680 --> 25:29.200
advantages because you can do things at that level because you exploit that sparsity,

25:29.200 --> 25:37.120
you can quickly reason and do all kinds of things plan that are harder to do otherwise.

25:37.120 --> 25:44.720
Is the idea there that this system two, if we think of it as like a memory,

25:44.720 --> 25:50.960
you know, the bottleneck says that it can't just store anything, it can only store these higher

25:50.960 --> 25:56.320
level concepts or representations, is that? You're right. We only store the stuff that goes

25:56.320 --> 26:00.880
through our consciousness. The things that you're unconscious of are going to stay in your mind,

26:00.880 --> 26:05.520
in your brain, you know, very short time, and then you're going to forget them. Whereas the things

26:05.520 --> 26:10.320
that you've been conscious of are more likely to be stored in long-term memory. But in addition,

26:10.320 --> 26:16.320
when they go through this short-term memory where you can operate on them, you can do pretty

26:16.320 --> 26:24.960
fancy things that is what we call thinking or reasoning or planning or discovering something in

26:24.960 --> 26:34.160
coherent and that we do consciously. So is the idea of attention as applied to consciousness

26:34.160 --> 26:41.520
that we might use attention as a way to train this consciousness prior? Yes. Yes. Yes. So attention

26:41.520 --> 26:47.280
is part of the architecture of the neural net in order to enforce the prior. What's interesting

26:47.280 --> 26:53.920
is it has already brought a kind of revolution within deep learning. So there's been amazing

26:53.920 --> 26:57.600
progress in natural language processing. It started with the work we did in which

26:57.600 --> 27:05.360
translation around 2014. And then since 2016, this has been put in Google Translate and then,

27:05.360 --> 27:11.600
you know, it's become the dominant kind of technology for machine translation. But since then,

27:12.320 --> 27:18.480
with new architectures that exploit consciousness like what's called the Transformers,

27:19.680 --> 27:25.920
the progress in all kinds of natural language tasks has really, really progressed a lot.

27:25.920 --> 27:34.320
And it's changing the very nature of the way we think about neural nets. So in the traditional

27:34.320 --> 27:42.000
neural net, we think of the computation as operating one step at a time on these vectors. So like

27:42.000 --> 27:48.000
a fixed set of numbers, which correspond to a bunch of neurons having some activity. But when

27:48.000 --> 27:55.280
you have attention mechanisms, what it makes it possible is to operate on sets of objects rather

27:55.280 --> 28:04.160
than on these fixed size vectors. So already, this is having a big impact in natural language

28:04.160 --> 28:10.000
processing because language has this property that, you know, you want to just take some elements

28:10.000 --> 28:15.760
of what I've been talking about in the last five minutes. And then reuse it in order to say

28:15.760 --> 28:21.920
something, you know, that makes sense with respect to what I said. So it tends to select a few

28:21.920 --> 28:27.360
elements, combine them in new ways, add some new things. But at each step, you only consider a

28:27.360 --> 28:35.920
few things at a time. And current transformer architectures have this inherent structure.

28:35.920 --> 28:41.280
Another thing that's going on with these attention-based systems is that in a way we are introducing

28:41.280 --> 28:48.880
some of the old ideas from AI of indirection and naming things and having things that have a

28:48.880 --> 28:54.960
type. So these are concepts that have been there since the beginning of programming,

28:54.960 --> 29:00.640
but haven't really, it wasn't clear how to incorporate these ideas in your nets. And so

29:01.600 --> 29:06.640
with attention, there's something really interesting going on, which is now you select

29:07.280 --> 29:11.040
which neuron, for example, is going to talk to which neuron. So it's kind of like dynamically

29:11.040 --> 29:18.560
changing the connection pattern between neurons or groups of neurons. And so when you have

29:18.560 --> 29:24.960
this dynamic connectivity going on, you need to carry information about where is the signal coming

29:24.960 --> 29:30.800
from. So it's not just the information I send you, but it's coming from me and you need to keep

29:30.800 --> 29:39.600
track of that. In a lot of ways, the idea of a consciousness prior makes me think of the work

29:39.600 --> 29:46.000
that's happening around model-based machine learning. So in reinforcement learning, for example,

29:46.000 --> 29:53.120
we're starting to see a lot of work or we've been seeing quite a bit of work around models.

29:53.120 --> 29:59.760
It is the idea that the consciousness prior is like the specific type of model that's analogous

29:59.760 --> 30:05.280
to what you would call our consciousness. So the connection with model-based reinforcement

30:05.280 --> 30:14.480
learning is the following. When you do model-free reinforcement learning, you train a policy,

30:14.480 --> 30:19.120
in other words, you train a neural net which is going to be called whenever you have to take a

30:19.120 --> 30:25.360
decision and it takes a decision and it's sort of automatic. So it's like when you're driving

30:25.360 --> 30:29.920
and a habitual route and you don't need to think about it, it knows what to do and there's no need

30:29.920 --> 30:36.640
to involve consciousness. But when you plan in your route on the fly, let's say there's some funny

30:36.640 --> 30:41.520
construction going on on your road and suddenly you realize that you're going to have to use a

30:41.520 --> 30:47.680
different path and you think about it. That is planning on the fly and that kind of planning on

30:47.680 --> 30:54.160
the fly is a form of reasoning and it's something that's conscious and that allows you to deal with

30:54.160 --> 31:00.960
as unexpected occurrences and that in principle is exactly what model-based reinforcement learning

31:00.960 --> 31:07.600
is about. So once you have a model of how the world works, you can create a new policy on the fly,

31:07.600 --> 31:12.240
something you've never done before. By combining the pieces of knowledge you already know, say about

31:12.240 --> 31:21.440
pieces of roads and so on, in order to come up with a new plan and that dynamic decision-making

31:21.440 --> 31:28.000
about the future and imagining the future in order to take decision in a very flexible way

31:28.000 --> 31:33.600
is very, very characteristic of conscious behavior and system two. So I think that at the end of the

31:33.600 --> 31:39.840
day we're going to have reinforcement learning that has both model-free elements and model-based

31:39.840 --> 31:45.120
elements because they both have their strengths. So earlier I referred to this idea of conscious

31:45.120 --> 31:54.560
as far as a memory. Is it strictly speaking memory in nature or is it more active? Does that question

31:54.560 --> 32:00.960
make sense? Yes, it's more active. So it's more about how you process information but the connection

32:00.960 --> 32:08.320
to memory is it's also connected to how we represent a lot of conscious knowledge. So the things

32:08.320 --> 32:14.640
that you can access in memory are conscious pieces of information. I mean they're not conscious

32:14.640 --> 32:20.480
until you retrieve them from memory but they can become conscious. And so there's a sense in which

32:20.480 --> 32:24.480
a lot of your conscious knowledge is stored in your long-term memory and then it could be retrieved

32:24.480 --> 32:32.640
as needed in order to solve problems that you're facing today. And conscious processing is dealing

32:32.640 --> 32:38.320
with the computations that your brain is doing to do these things, to retrieve things from memory,

32:38.320 --> 32:43.520
to interpret what you're seeing now and potentially also to visualize things that could happen

32:43.520 --> 32:50.240
in the future. I think that the memory analogy was that it was kind of the store of the relationships

32:50.240 --> 33:00.240
between the things. But the planning based on that information is not necessarily the same thing.

33:00.240 --> 33:07.440
That's right. In a sense there is declarative knowledge. So all the pieces of knowledge

33:07.440 --> 33:12.800
that are in memory and then there's the computation that you do on the fly in order to combine these

33:12.800 --> 33:20.240
pieces of knowledge with what is going on now or what you're imagining in order to come up with

33:20.240 --> 33:23.920
sometimes better explanations about the past or about what you'd like to do in the future.

33:24.800 --> 33:31.360
What's kind of the current state of this line of research and how do you see it evolving?

33:32.000 --> 33:40.160
Oh it's still in its infancy. I think how many decades it took for like Neonets and Deep Learning as

33:40.160 --> 33:47.440
it stands now to really reach the maturity that it has. So I think this is a long-term project

33:48.160 --> 33:56.880
and I also think that there's a huge importance in the collaboration between the brain sciences.

33:57.840 --> 34:03.920
That includes cognitive science, neuroscience, but also philosophers of mind which have been

34:03.920 --> 34:09.360
thinking about consciousness and mind for a long time. So all of these people who have been thinking

34:09.360 --> 34:16.800
about the human side of the equation should be collaborating with the people in AI and Deep Learning

34:18.000 --> 34:28.160
who are interested in understanding those principles and trying different ways of capturing

34:28.160 --> 34:34.560
these things in computers. And that can go also back in the other direction because one of the

34:34.560 --> 34:43.280
problems with say neuroscience or philosophy is that it's difficult to come up with good theories

34:43.280 --> 34:50.880
that explain the many observations that we have. But machine learning can come up with interesting

34:50.880 --> 34:55.360
theories because they are motivated from the learning theory point of view. In other words,

34:55.360 --> 35:00.960
so my conscious prior idea is something that makes sense from a machine learning perspective

35:00.960 --> 35:06.880
because as soon as you start making assumptions about the world, that means you could learn faster,

35:06.880 --> 35:13.440
you could adapt faster. And so these kinds of justification can help

35:14.560 --> 35:20.560
constrain theories that neuroscience or philosophers might be considering. So at the end of the day,

35:20.560 --> 35:26.800
be theories that both are consistent with what we know about humans. And that makes sense from

35:28.080 --> 35:34.480
a computational perspective, from a learning perspective. Before we got started talking about

35:34.480 --> 35:39.840
consciousness, we were actually before we started recording. We were talking a little bit about

35:40.480 --> 35:47.760
kind of what's going on in the world now as we record this mid-March and COVID-19. And one of the

35:47.760 --> 35:54.320
other topics that you've been spending some time working on is how machine learning and AI

35:54.320 --> 36:00.240
could make a difference in that setting. Can you share with us a little bit of what you're doing there?

36:00.240 --> 36:04.960
Yes, just put a bit of context. So I've been involved, of course, in basic research in machine

36:04.960 --> 36:10.400
learning and deep learning for many decades. But also in the last few years, I've realized the

36:10.400 --> 36:20.160
importance of thinking about how AI is deployed, will be deployed in society. And how we can steer

36:21.200 --> 36:28.000
our collective boat in directions that will make AI a useful force for the world and for humanity.

36:28.720 --> 36:35.600
So this is why I got, I'm barked, for example, in the project of writing the Montreal Declaration

36:35.600 --> 36:41.840
for Responsible Development of AI. This is why I'm involved in the project called AI Commons

36:41.840 --> 36:52.480
to try to help developers and NGOs and philanthropy work together on applying AI to areas that

36:53.600 --> 36:58.560
really AI for social good that may not be necessarily profitable but are important to do for

36:58.560 --> 37:05.200
humanity. This is also why I've been involved in the last couple of years in working on how AI could

37:05.200 --> 37:13.600
be used to fight climate change. So we wrote a very long paper that does a survey of many different

37:13.600 --> 37:22.480
areas in which machine learning can be used to help reducing greenhouse emissions, to design new

37:22.480 --> 37:28.880
materials, to better use the renewable energy sources that we have or to even help people understand

37:28.880 --> 37:33.200
better what is going on with climate change. And then, of course, in the last few weeks,

37:33.200 --> 37:41.680
like everyone else, I've been, you know, into this tornado to try to think with many of my

37:41.680 --> 37:48.720
colleagues, not just in AI, but also in other, especially in healthcare, thinking about what AI

37:48.720 --> 37:54.400
can do among many other disciplines that are, you know, putting their brains together. What can we

37:54.400 --> 38:01.920
do to help fight this COVID-19 pandemics? So I'm currently involved in a number of projects.

38:01.920 --> 38:10.160
One that is taking a lot of my time these days is the design of a tracing app. So one of the things

38:10.160 --> 38:18.000
that we can do to really find a good balance between saving lives and allowing people to go out of

38:18.000 --> 38:26.640
their homes is keeping track of where people go and who they meet so as to estimate their risks of

38:26.640 --> 38:32.960
being infected and reduce those risks. And we want to do it in a way that's very mindful of privacy,

38:33.600 --> 38:38.560
maybe unlike some of the apps that have already been put out there. This has come up quite a bit

38:38.560 --> 38:43.120
recently. Yeah, it's very important because you just use Bluetooth and make the information available

38:43.120 --> 38:49.840
to, you know, the benevolent government agencies. That's right. So I don't think it would pass

38:50.560 --> 38:55.280
in North America. And I think there are good reasons for this. But the good news is there are

38:55.280 --> 39:00.080
technical solutions to this. And we're working on that. And we're working on machine learning to

39:00.080 --> 39:05.520
help predict your risk level based on the encounters you had before. So, you know, maybe you didn't

39:05.520 --> 39:11.280
meet somebody, you didn't meet somebody who we knew was infected. But maybe you met somebody,

39:11.280 --> 39:15.120
who met somebody, who met somebody who was infected. So then what the probability that you are

39:15.120 --> 39:20.320
infected, right? And you've made 20 of these encounters. So maybe the risks accumulate. So how do you

39:20.320 --> 39:27.520
aggregate all that information and help people know what is their risk? Another thing we're working

39:27.520 --> 39:32.800
on is the design of new drugs. So machine learning and especially deep learning has been used

39:32.800 --> 39:38.560
in the last couple of years. There's been a flurry of papers using these systems to propose new

39:38.560 --> 39:44.000
candidate drugs. In particular, we're working on antivirals. In other words, drugs that you would

39:44.000 --> 39:48.320
give to somebody who's already sick. And maybe who's very sick and we're, you know, concerned that

39:48.320 --> 39:55.280
they might die. And so we could give them these experimental drugs because because these are

39:55.280 --> 40:01.600
new things. And so there's a problem where machine learning can come handy is that the normal,

40:01.600 --> 40:07.760
usual development of new drugs could take many years, sometimes a decade. But it turns out that

40:07.760 --> 40:13.520
machine learning can greatly accelerate the search for good molecules. When you're talking to clinicians

40:13.520 --> 40:19.440
and practitioners, folks from the healthcare, side of things, as well as, um,

40:20.160 --> 40:27.600
virologists and epidemiologists and, uh, kind of the end users of systems like these.

40:28.320 --> 40:34.000
What are the things that they say they need from, you know, us as a community of, um,

40:34.800 --> 40:39.280
you know, data scientists, machine learning, uh, researchers and the like.

40:39.280 --> 40:47.120
Well, it's not always easy to get that answer. I can tell you, I've been learning a lot,

40:47.120 --> 40:52.560
uh, in, in the last few weeks and, uh, I'm sure a lot of people have been learning a lot because

40:52.560 --> 40:59.120
we all need to understand what is going on. So, uh, there are many questions. On the healthcare

40:59.120 --> 41:04.720
and clinical side, they, they would like to be able to predict just monitor what is going on.

41:04.720 --> 41:11.920
Is, is already difficult. And, uh, predict, uh, where there might be greater need for healthcare

41:11.920 --> 41:18.720
resources to allocate resources properly, um, predict what patient, given their history,

41:18.720 --> 41:26.320
would be most likely to need like an ICU, uh, or a respirator, um, predict the risk level of people

41:26.320 --> 41:32.400
as, as I was mentioning earlier, uh, help, uh, epidemiologists model what, you know, what's

41:32.400 --> 41:38.240
going to be the likely, uh, scenarios that we're, we're going to be facing, even, even things like

41:38.240 --> 41:42.880
logistics. So a lot, there are a lot of problems right now where we're just not organized properly to

41:42.880 --> 41:49.440
deal with the massive number of people calling for help, right? So, uh, there's just lots and

41:49.440 --> 41:56.080
lots of areas where machine learning can be useful. And, uh, it takes a lot of time to understand

41:56.080 --> 42:03.120
those issues to talk to those people, uh, to get access to data as a big thing. But now the good

42:03.120 --> 42:10.080
news is, in our societies, access to healthcare data has been a huge problem, um, because we've put

42:10.080 --> 42:17.600
all of our, uh, weight on, uh, privacy. And, uh, it's, it has meant that it's been difficult to,

42:18.400 --> 42:22.080
for researchers, machine learning researchers to have access to this kind of data. But, but now what

42:22.080 --> 42:29.120
is happening with COVID is that the health authorities are seeing that, that we're missing a boat,

42:29.120 --> 42:35.040
like, or they're, you know, quickly changing the ways that we're doing things to allow researchers

42:35.040 --> 42:41.040
to get their hands on, on the proper data sets, or we're going to lose lives where, you know,

42:41.040 --> 42:47.600
we could have saved those lives. So I think it's an important moment to demonstrate the need for

42:47.600 --> 42:54.720
a more agile, uh, data infrastructure, uh, for healthcare. With regards to the app that you mentioned,

42:54.720 --> 42:59.520
we've talked quite a bit on the podcast about differential privacy. Does that come into play,

43:00.320 --> 43:05.200
uh, in allowing you to use this location data in a, in a private way?

43:06.160 --> 43:11.520
We're looking at different options. Right now, I don't think we need differential privacy. So we,

43:11.520 --> 43:15.920
we may need to blur some of some pieces of evidence that would make it too easy to reach race

43:15.920 --> 43:21.200
people. But if you want to predict the risk level of a person, otherwise predict the probability

43:21.200 --> 43:28.800
that you are currently infected, um, you don't need to know where I was. Um, you all need to know that,

43:29.840 --> 43:36.320
uh, you know, maybe yesterday I, I was close to somebody who had risk level six and the day before

43:36.320 --> 43:43.520
I was close to somebody who had risk level seven and, and so on. And you don't need to know, uh,

43:43.520 --> 43:48.720
the kind of trace of where everyone was, that can be computed in each person's phone.

43:50.320 --> 43:57.840
The only thing you need to share as data for training the, the risk predictor is what were the

43:57.840 --> 44:02.320
encounters, like in the sense of what were the risk level of people you're encountered and when,

44:03.600 --> 44:08.400
and that, from that information is very difficult to trace who met with whom because you don't,

44:08.400 --> 44:15.760
you don't have a handle on who was where when. So, so I think that the, you can, you, we could

44:15.760 --> 44:22.000
globally share the level of the planet, that kind of data and deal very good models of your, um,

44:22.000 --> 44:28.800
risk level. You know, when you look across all of the things that are happening, you know,

44:28.800 --> 44:37.440
your research and, uh, elsewhere, what are the things that, um, you see as most exciting in terms

44:37.440 --> 44:48.240
of AI's contributions of this fight? Wow, I guess I'm very biased. So, uh, I'm, I'm most invested

44:48.240 --> 44:56.160
right now in this tracing thing because, um, I think that, uh, medical treatments are going to take

44:56.160 --> 45:03.600
months, probably, you know, your ish two years, for some cases to converge to something everyone

45:03.600 --> 45:09.440
can have, especially vaccines. You have to understand that before we release a vaccine, uh,

45:09.440 --> 45:13.040
we need to make sure this drug is really, really harmless because we're going to give it to

45:13.040 --> 45:18.960
everybody, right? So, right, right. And that process takes a while, even if it's, even if it's

45:18.960 --> 45:23.200
fast track, it takes a while. No, but like, if one percent of the people we give a vaccine to die

45:23.200 --> 45:31.120
because of the vaccine, that is not good, okay? Right. But a antiviral is a drug you could give

45:31.120 --> 45:36.160
to somebody who's already close to dying anyways. And so, we can take a bigger chance. And so,

45:36.160 --> 45:42.160
we don't need to wait a year or two, uh, to know that it's okay. And so, I'm, I'm invested in

45:42.160 --> 45:49.840
the, uh, project around, uh, the development of new antivirals. Um, I mean, uh, before that,

45:49.840 --> 45:54.960
there are already been work in using machine learning to test existing drugs. This is like the

45:54.960 --> 45:59.840
first line is there a drug that has already been approved. So, we don't need clinical trials.

45:59.840 --> 46:04.720
Uh, that we can just use tomorrow morning. Okay. And that's what we're going on. There's a bunch

46:04.720 --> 46:08.960
of clinical trials going on with promising leads and machine learning has already been used to

46:08.960 --> 46:14.480
suggest candidates. But the next step, if, you know, if none of these things work is to develop, uh,

46:14.480 --> 46:21.840
uh, a new molecule that didn't exist. Right. So, so the tracing thing is very important because it,

46:21.840 --> 46:27.440
it's, uh, it's something we can do even before all this, right? Even before we find which,

46:27.440 --> 46:34.000
uh, those, those drugs and, and, and do clinical trials for them, we should be able, in a matter of

46:34.800 --> 46:44.000
days and weeks, to all have on our phone, an app that will help trace our contacts, uh, in a,

46:44.000 --> 46:50.400
in a private sea respecting way, um, and make it possible for us to meet, for example, other people

46:50.400 --> 46:54.560
and know that they're unlikely to be infected. And so it's okay. We can be close to each other.

46:54.560 --> 46:59.040
We can work together. Uh, we can be in the same bus together. This is very important. A lot of

46:59.040 --> 47:03.840
people right now don't have any transportation because if you don't have a car or if you don't drive

47:04.560 --> 47:09.280
and you're infected, for example, you, you can't have transportation. So like, we need to know

47:10.320 --> 47:16.720
who is likely to be infected and not or at one degree in order to organize society around this

47:16.720 --> 47:23.840
for the next probably one or two years. Well, super important work. Thank you so much for

47:23.840 --> 47:30.160
taking a time to chat with us about what you're up to, both the work that you've been focusing on

47:30.160 --> 47:36.000
broadly, the consciousness work, as well as the more recent work you've been doing, uh, in this

47:36.000 --> 47:41.200
fight against COVID. My pleasure. It's great to speak with you. Thank you. Bye.

47:45.600 --> 47:50.640
All right, everyone. That's our show for today. For more information on today's show,

47:50.640 --> 47:58.400
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

