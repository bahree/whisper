1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,520
I'm your host Sam Charrington.

4
00:00:23,520 --> 00:00:28,320
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

5
00:00:28,320 --> 00:00:31,040
to Kevin T from SIGUP for presenting.

6
00:00:31,040 --> 00:00:35,360
You can find the slides for his presentation in the Meetup Slack channel, as well as in

7
00:00:35,360 --> 00:00:37,240
this week's show notes.

8
00:00:37,240 --> 00:00:41,640
Our final Meetup of the Year will be held on Wednesday, December 13th.

9
00:00:41,640 --> 00:00:46,840
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

10
00:00:46,840 --> 00:00:49,000
for our discussion segment.

11
00:00:49,000 --> 00:00:54,640
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

12
00:00:54,640 --> 00:01:01,120
the paper understanding deep learning requires rethinking generalization, by Shi Huang Zhang

13
00:01:01,120 --> 00:01:04,480
from MIT and Google Brain and others.

14
00:01:04,480 --> 00:01:09,600
You can find more details and register at twimbleia.com slash Meetup.

15
00:01:09,600 --> 00:01:13,920
If you receive my newsletter, you already know this, but Twimble is growing and we're

16
00:01:13,920 --> 00:01:19,640
looking for an energetic and passionate community manager to help expand our programs.

17
00:01:19,640 --> 00:01:24,040
This position can be remote, but if you happen to be in St. Louis, all the better.

18
00:01:24,040 --> 00:01:27,760
If you're interested, please reach out to me for additional details.

19
00:01:27,760 --> 00:01:31,720
I should mention that if you don't already get my newsletter, you are really missing

20
00:01:31,720 --> 00:01:36,760
out and should visit twimbleia.com slash newsletter to sign up.

21
00:01:36,760 --> 00:01:42,680
Now, the show you're about to hear is part of our Strange Loop 2017 series, brought to

22
00:01:42,680 --> 00:01:45,520
you by our friends at Nexusos.

23
00:01:45,520 --> 00:01:50,440
Nexusos is a company focused on making machine learning more easily accessible to enterprise

24
00:01:50,440 --> 00:01:51,440
developers.

25
00:01:51,440 --> 00:01:56,000
Their machine learning API meets developers where they're at, regardless of their mastery

26
00:01:56,000 --> 00:02:01,200
of data science, so they can start cutting up predictive applications immediately and

27
00:02:01,200 --> 00:02:04,080
in their preferred programming language.

28
00:02:04,080 --> 00:02:08,440
It's as simple as loading your data and selecting the type of problem you want to solve.

29
00:02:08,440 --> 00:02:13,320
Their automated platform trains and selects the best model fit for your data and then outputs

30
00:02:13,320 --> 00:02:14,880
predictions.

31
00:02:14,880 --> 00:02:19,200
To learn more about Nexusos, be sure to check out the first episode in this series at

32
00:02:19,200 --> 00:02:27,480
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

33
00:02:27,480 --> 00:02:32,920
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

34
00:02:32,920 --> 00:02:38,400
learning in your next project at nexosos.com slash twimble.

35
00:02:38,400 --> 00:02:43,400
In this episode, I speak with Sam Richie, a software engineer at Stripe.

36
00:02:43,400 --> 00:02:47,720
I caught up with Sam right after his talk at the conference where he covered his team's

37
00:02:47,720 --> 00:02:51,120
work on explaining black box predictions.

38
00:02:51,120 --> 00:02:56,440
In our conversation, we discuss how Stripe uses these predictions for fraud detection and

39
00:02:56,440 --> 00:02:59,400
he gives us a few use case examples.

40
00:02:59,400 --> 00:03:03,960
We then discuss Stripe's approach for explaining those predictions and briefly mention Carlos

41
00:03:03,960 --> 00:03:18,480
Guestren's work on the line paper, which he and I discuss in twimble talk number 7.

42
00:03:18,480 --> 00:03:23,360
Well hey everyone, I am here at the Strange Loop Conference in St. Louis and I've got the

43
00:03:23,360 --> 00:03:28,360
pleasure of being seated across from Sam Richie, who is a software engineer at Stripe.

44
00:03:28,360 --> 00:03:36,040
And Sam was in to talk about just so stories for AI, explaining black box predictions.

45
00:03:36,040 --> 00:03:40,520
And in fact, he just jogged over here from delivering his talk and it's like five blocks away

46
00:03:40,520 --> 00:03:42,280
at hustle.

47
00:03:42,280 --> 00:03:43,280
So welcome to the show, Sam.

48
00:03:43,280 --> 00:03:44,280
Yeah, thank you.

49
00:03:44,280 --> 00:03:45,280
Thanks for having me.

50
00:03:45,280 --> 00:03:48,080
Why don't we start by having you tell us a little bit about your background and how

51
00:03:48,080 --> 00:03:51,280
you got started working in AI and machine learning?

52
00:03:51,280 --> 00:03:52,280
Sure.

53
00:03:52,280 --> 00:03:54,680
Yeah, you want the just so story of exactly.

54
00:03:54,680 --> 00:03:59,400
So this is fairly new, I guess less year and a half I've been at Stripe, as I said, working

55
00:03:59,400 --> 00:04:02,080
on machine learning infrastructure.

56
00:04:02,080 --> 00:04:03,440
How did I get here?

57
00:04:03,440 --> 00:04:08,240
We were talking before this started about, you know, my kind of initial coding background

58
00:04:08,240 --> 00:04:12,880
is coming up in the functional programming world years ago, I guess the first thing that

59
00:04:12,880 --> 00:04:18,440
led to this was this large scale like deforestation monitoring system I worked on.

60
00:04:18,440 --> 00:04:22,600
This got me into like Hadoop and closure and this is sort of it was a machine learning

61
00:04:22,600 --> 00:04:26,160
application, I had no idea what I was doing when I was on this, right?

62
00:04:26,160 --> 00:04:27,560
And where were you working on that?

63
00:04:27,560 --> 00:04:32,800
So this was just a freelance thing with a guy I used to race like flat water kayaks

64
00:04:32,800 --> 00:04:33,800
with.

65
00:04:33,800 --> 00:04:36,200
And he moved off to the data science world.

66
00:04:36,200 --> 00:04:40,760
I was making apps like in my apartment in New York, I had a year in New York city.

67
00:04:40,760 --> 00:04:46,760
And yeah, just wanted to, well, the real, okay, so the reason I, it's like hard to put

68
00:04:46,760 --> 00:04:48,880
a narrative together in my life, man.

69
00:04:48,880 --> 00:04:53,160
The deforestation monitoring system was just this collaboration with a buddy who was doing

70
00:04:53,160 --> 00:04:54,760
work for the World Bank though, right?

71
00:04:54,760 --> 00:04:55,760
Okay.

72
00:04:55,760 --> 00:04:59,720
So the idea was to build like a model that could, on the basis of these studies that happen

73
00:04:59,720 --> 00:05:05,120
every like five years or so, build a logistic regression that could predict for certain pixels

74
00:05:05,120 --> 00:05:06,120
in the tropics.

75
00:05:06,120 --> 00:05:09,640
The idea was like any spot you looked at, the goal was to get out some prediction of the

76
00:05:09,640 --> 00:05:13,520
chance that that piece of forest would be deforested in the next month, right?

77
00:05:13,520 --> 00:05:14,520
Okay.

78
00:05:14,520 --> 00:05:15,520
And you're starting with satellite imagery?

79
00:05:15,520 --> 00:05:16,520
Exactly.

80
00:05:16,520 --> 00:05:17,520
Exactly.

81
00:05:17,520 --> 00:05:18,520
Okay.

82
00:05:18,520 --> 00:05:19,880
We came out every two weeks.

83
00:05:19,880 --> 00:05:23,320
The training data for this, again, I didn't know what I was doing at the time.

84
00:05:23,320 --> 00:05:27,880
The training data for this was a study that this guy Matt Hansen had done, which looked

85
00:05:27,880 --> 00:05:32,240
at, modus data from the year 2000, same data again in the year 2005.

86
00:05:32,240 --> 00:05:33,240
Okay.

87
00:05:33,240 --> 00:05:36,640
And then he just went through a manually like selected which pixels they thought were deforested

88
00:05:36,640 --> 00:05:37,640
in which work.

89
00:05:37,640 --> 00:05:41,960
So this is useful in the back, like a lot of policy, and there's a lot of work on, you

90
00:05:41,960 --> 00:05:46,880
know, reforestation and things like this, a lot of money that went out from this study,

91
00:05:46,880 --> 00:05:49,120
but the plan was to do it every five years.

92
00:05:49,120 --> 00:05:53,040
And so a lot of money went into Indonesia, for example, to like work on deforestation

93
00:05:53,040 --> 00:05:54,040
there.

94
00:05:54,040 --> 00:05:58,040
The next time he did the study or when our model came online, it was clear that like Indonesia

95
00:05:58,040 --> 00:05:59,040
was actually looking pretty good.

96
00:05:59,040 --> 00:06:00,880
It was not talking to charts anymore.

97
00:06:00,880 --> 00:06:01,880
Okay.

98
00:06:01,880 --> 00:06:03,320
And like me and Mar was now the issue.

99
00:06:03,320 --> 00:06:04,320
Okay.

100
00:06:04,320 --> 00:06:06,800
So the goal was to get new data every two weeks.

101
00:06:06,800 --> 00:06:11,360
So what we did is this is where the, you know, big data, functional programming stuff came

102
00:06:11,360 --> 00:06:12,360
in.

103
00:06:12,360 --> 00:06:13,360
Right.

104
00:06:13,360 --> 00:06:17,840
And then from satellite data, like time series of what was happening during that five

105
00:06:17,840 --> 00:06:19,240
year period, right?

106
00:06:19,240 --> 00:06:20,240
Okay.

107
00:06:20,240 --> 00:06:21,440
So use that as the training data.

108
00:06:21,440 --> 00:06:25,880
The prediction variable was what this guy Matt Hansen had said deforestation was.

109
00:06:25,880 --> 00:06:26,880
Right.

110
00:06:26,880 --> 00:06:31,280
And then we just marched the time series forward and predict, does this now look like deforestation?

111
00:06:31,280 --> 00:06:32,440
Does this now?

112
00:06:32,440 --> 00:06:36,400
You end up with a map that you can scroll through that just shows the trends of deforestation

113
00:06:36,400 --> 00:06:37,400
moving through the world.

114
00:06:37,400 --> 00:06:38,400
Oh wow.

115
00:06:38,400 --> 00:06:39,400
It's really amazing.

116
00:06:39,400 --> 00:06:40,400
Interesting.

117
00:06:40,400 --> 00:06:43,760
I don't know what's happened with that, but I think it's now the World Resources Institute

118
00:06:43,760 --> 00:06:47,000
was the organization that was sponsoring us.

119
00:06:47,000 --> 00:06:51,800
I think this is out as I can open data set now, but that's kind of my first taste of.

120
00:06:51,800 --> 00:06:55,840
And now I came at machine learning from the statistics side a little bit.

121
00:06:55,840 --> 00:06:58,320
And it turns out that these are like the same field.

122
00:06:58,320 --> 00:06:59,920
They just have different words for things.

123
00:06:59,920 --> 00:07:00,920
Right.

124
00:07:00,920 --> 00:07:01,920
So you move from one of the other.

125
00:07:01,920 --> 00:07:04,040
You don't really know if you can bring anything over.

126
00:07:04,040 --> 00:07:05,960
But yeah, that was the first thing.

127
00:07:05,960 --> 00:07:11,080
That took me, you know, I took all this like wonderful, non-profit, amazing work I was

128
00:07:11,080 --> 00:07:12,080
doing.

129
00:07:12,080 --> 00:07:16,640
And I let Twitter like recruit me to go do the same stuff on ads.

130
00:07:16,640 --> 00:07:19,120
The best minds of our world, that people put ads, right?

131
00:07:19,120 --> 00:07:20,120
That's it, man.

132
00:07:20,120 --> 00:07:21,120
Yeah.

133
00:07:21,120 --> 00:07:22,120
So it sold the soul for a little bit.

134
00:07:22,120 --> 00:07:27,240
But, you know, had a few years there building open source tech to do, again, like effectively

135
00:07:27,240 --> 00:07:31,440
the same sort of Hadoop based stuff that we were working on before.

136
00:07:31,440 --> 00:07:35,440
We ended up open sourcing a lot of that work as this library called Summingbird, which

137
00:07:35,440 --> 00:07:40,320
is, again, as we were saying, it's like all monads all the time.

138
00:07:40,320 --> 00:07:44,760
It's a library that lets you write these like big streaming data computations and then

139
00:07:44,760 --> 00:07:49,920
run them on Hadoop or on like a real-time streaming system or on both.

140
00:07:49,920 --> 00:07:54,520
And basically, like, separates what you want to compute from where you want to compute

141
00:07:54,520 --> 00:07:55,520
it.

142
00:07:55,520 --> 00:08:00,000
And it's only relevant really because the final piece of this puzzle is what drew me

143
00:08:00,000 --> 00:08:02,520
to stripe is that I was interested in machine learning.

144
00:08:02,520 --> 00:08:07,440
I was doing a lot of work on my own and studying and just trying to get up to speed and like,

145
00:08:07,440 --> 00:08:12,040
stripe had pulled this library in and was using it for their feature generation pipeline

146
00:08:12,040 --> 00:08:13,600
for a lot of their models internally.

147
00:08:13,600 --> 00:08:14,600
Okay.

148
00:08:14,600 --> 00:08:18,720
So it was like, I'm not the most qualified, like, I don't have this amazing like data science

149
00:08:18,720 --> 00:08:19,720
background.

150
00:08:19,720 --> 00:08:22,080
I did have a hook in the infrastructure side.

151
00:08:22,080 --> 00:08:27,080
And so a lot of the work I do now at this intersection between how do you make features?

152
00:08:27,080 --> 00:08:29,160
How do you run the stuff at production scale?

153
00:08:29,160 --> 00:08:30,480
How do you ship models?

154
00:08:30,480 --> 00:08:33,680
The intersection between that and like, what models are even worshipping?

155
00:08:33,680 --> 00:08:34,680
What should you care about?

156
00:08:34,680 --> 00:08:36,360
What should you put in the product?

157
00:08:36,360 --> 00:08:40,160
Super interesting for me and it's been a really fun, like, year and a half or so.

158
00:08:40,160 --> 00:08:42,680
Leading now to some work on this stuff we were talking about today.

159
00:08:42,680 --> 00:08:43,680
Okay.

160
00:08:43,680 --> 00:08:44,680
Awesome.

161
00:08:44,680 --> 00:08:45,680
Awesome.

162
00:08:45,680 --> 00:08:47,920
So why don't you tell us what you were talking about today?

163
00:08:47,920 --> 00:08:48,920
Yeah.

164
00:08:48,920 --> 00:08:54,520
So the talk today was on, in general, it was on this idea of how to explain the predictions

165
00:08:54,520 --> 00:08:55,920
that black box models make.

166
00:08:55,920 --> 00:08:56,920
Right.

167
00:08:56,920 --> 00:09:01,560
This is like a term that's tossed around like a black box model is a model, like, a neural

168
00:09:01,560 --> 00:09:05,120
network is a black box model, a random force is a black box model, right?

169
00:09:05,120 --> 00:09:09,720
Like, it's just kind of this term meant to express how, like, powerful and complicated

170
00:09:09,720 --> 00:09:11,440
these things are internally.

171
00:09:11,440 --> 00:09:14,160
The internal structure is very rich and varied.

172
00:09:14,160 --> 00:09:19,920
You know, a black box model is typically seen as difficult to understand or hard to explain.

173
00:09:19,920 --> 00:09:22,160
That's like, it's hard to know what that really means, right?

174
00:09:22,160 --> 00:09:25,280
Like, what does it mean to explain a black box model?

175
00:09:25,280 --> 00:09:27,320
So that was the general theme.

176
00:09:27,320 --> 00:09:31,360
The specific things I started with were the work we do at Stripe on fraud detection.

177
00:09:31,360 --> 00:09:35,120
You know, we use models that are as accurate as possible to try to catch fraud for merchants

178
00:09:35,120 --> 00:09:36,440
who sign up for Stripe.

179
00:09:36,440 --> 00:09:42,040
At the same time, this black box property of, we're just going to block charges and, you

180
00:09:42,040 --> 00:09:46,440
know, you as a merchant, like, you might know more about your business.

181
00:09:46,440 --> 00:09:49,480
We might block a charge that you think is a legit customer.

182
00:09:49,480 --> 00:09:53,480
And if we're just sort of telling you like, look, the world is a better place if you accept

183
00:09:53,480 --> 00:09:54,480
our decisions.

184
00:09:54,480 --> 00:09:56,360
It's not totally appropriate.

185
00:09:56,360 --> 00:10:01,480
And people don't trust us with this and often what they've done is not used our product

186
00:10:01,480 --> 00:10:05,440
and gone to the system of very manual rules.

187
00:10:05,440 --> 00:10:08,440
You know, you go in, you say what you think fraud looks like.

188
00:10:08,440 --> 00:10:12,440
You're probably like cutting a lot more tissue out than you should.

189
00:10:12,440 --> 00:10:15,560
But you know, you feel good about it because you made the decision.

190
00:10:15,560 --> 00:10:19,800
So there's a tension here between a black box model making decisions for you that you

191
00:10:19,800 --> 00:10:21,480
don't understand.

192
00:10:21,480 --> 00:10:26,440
And something is not as effective, just strictly, like, a merchant does not know as much about

193
00:10:26,440 --> 00:10:32,160
fraud as some like a big company that is dealing with tens of thousands of merchants can.

194
00:10:32,160 --> 00:10:37,480
So the talk was about some tech we developed at Stripe to give people alongside our internal

195
00:10:37,480 --> 00:10:40,840
sort of suggestion or decision about what we're going to block, an explanation of why we

196
00:10:40,840 --> 00:10:41,840
did that.

197
00:10:41,840 --> 00:10:42,840
Okay.

198
00:10:42,840 --> 00:10:43,840
So this is a subtle problem.

199
00:10:43,840 --> 00:10:46,680
It was about the solution we came up with for that.

200
00:10:46,680 --> 00:10:51,960
And then about why that solution is kind of not great and why it's not a real explanation,

201
00:10:51,960 --> 00:10:52,960
is it not?

202
00:10:52,960 --> 00:10:56,920
And then we talked about a bunch of other techniques that the goal was to build up in

203
00:10:56,920 --> 00:11:01,960
the mind of, I mean, I guess we'll talk about it here, like, there's a bunch of techniques

204
00:11:01,960 --> 00:11:02,960
to do this.

205
00:11:02,960 --> 00:11:07,040
And it turns out that black box models often are like the most explainable models.

206
00:11:07,040 --> 00:11:11,440
They have such a rich structure that you can ask so many questions that really tease

207
00:11:11,440 --> 00:11:15,880
apart subtleties of an individual example in a way that you absolutely cannot with something

208
00:11:15,880 --> 00:11:19,640
like a logistic regression or a very simple model that you can just simulate in your

209
00:11:19,640 --> 00:11:20,640
mind.

210
00:11:20,640 --> 00:11:21,640
As a human.

211
00:11:21,640 --> 00:11:22,640
That's counterintuitive.

212
00:11:22,640 --> 00:11:23,640
It is.

213
00:11:23,640 --> 00:11:27,440
Then a black box model, you'd consider black box models be the most explainable model.

214
00:11:27,440 --> 00:11:28,600
Yeah.

215
00:11:28,600 --> 00:11:33,600
And I mean, I do it by just taking the definition and twisting it a little, but I would argue

216
00:11:33,600 --> 00:11:36,800
that it's, it's fuzzy what people mean when they talk about this.

217
00:11:36,800 --> 00:11:37,800
Right.

218
00:11:37,800 --> 00:11:41,840
There's a number of ways to frame this problem of is a model explainable or not, right?

219
00:11:41,840 --> 00:11:47,680
And so when you, how are you defining it leading up to the conclusion you've drawn?

220
00:11:47,680 --> 00:11:51,680
So I think the way in which people think a black box model is not understandable, you

221
00:11:51,680 --> 00:11:58,000
know, is it's the same way that like, you could say, if you asked me why I decided to

222
00:11:58,000 --> 00:11:59,560
go or get straight for something else.

223
00:11:59,560 --> 00:12:03,720
And I just like printed out the contents of my brain and showed you the state of every

224
00:12:03,720 --> 00:12:05,120
neuron and every connection.

225
00:12:05,120 --> 00:12:06,880
This is not understandable, right?

226
00:12:06,880 --> 00:12:08,560
Like, it's the truth.

227
00:12:08,560 --> 00:12:14,040
This is why I did it, like the physical state of my mind just, I couldn't do otherwise if

228
00:12:14,040 --> 00:12:16,680
you buy a sort of free will argument.

229
00:12:16,680 --> 00:12:20,360
But that's not what people mean really when they talk about human explanations, right?

230
00:12:20,360 --> 00:12:24,120
Like, when you talk about a human explanation, you want to know, okay, well, give me some

231
00:12:24,120 --> 00:12:29,280
narrative, give me some plausible reason why in this case, like not the entire, your brain

232
00:12:29,280 --> 00:12:31,800
has information from everything you've ever done.

233
00:12:31,800 --> 00:12:37,880
Give me for this example, you know, maybe, maybe one way to explain the decision is what

234
00:12:37,880 --> 00:12:40,640
would have had to change to make you change your mind, right?

235
00:12:40,640 --> 00:12:42,000
So that's a form of explanation.

236
00:12:42,000 --> 00:12:47,440
That's a kind of question that you can sort of ask of a logistic regression.

237
00:12:47,440 --> 00:12:52,640
You can ask of simple models when you get to things like image processing, where the

238
00:12:52,640 --> 00:12:57,120
features, the inputs to the model are tens of thousands of different pixels, each of which

239
00:12:57,120 --> 00:12:59,880
has individual weights in say a logistic regression.

240
00:12:59,880 --> 00:13:04,640
Like, that ability to look at a feature and see how much it affected the output, that

241
00:13:04,640 --> 00:13:08,040
kind of stops being helpful, right?

242
00:13:08,040 --> 00:13:12,600
Whereas with a neural network or something like this, you can, for an individual example,

243
00:13:12,600 --> 00:13:15,720
you can start one technique I talked about in the talk, it's called line.

244
00:13:15,720 --> 00:13:16,920
And the idea here is that-

245
00:13:16,920 --> 00:13:18,600
That's Carlos Gastron's work.

246
00:13:18,600 --> 00:13:20,600
Yeah, yeah, it's so good.

247
00:13:20,600 --> 00:13:26,160
So the idea here is that for an individual example, you can probe the model and see what

248
00:13:26,160 --> 00:13:29,240
would have happened had any individual thing changed, right?

249
00:13:29,240 --> 00:13:34,920
So you can build effectively like a little local linear model inside of this wild space that

250
00:13:34,920 --> 00:13:37,400
the neural network is trained, right?

251
00:13:37,400 --> 00:13:42,760
So because of this rich internal representation, overall, totally with you, like, it's hard

252
00:13:42,760 --> 00:13:46,520
to explain why the model is doing what is doing in aggregate, just like it's hard to say

253
00:13:46,520 --> 00:13:51,360
why like, you know, the entire population of a country is doing something more.

254
00:13:51,360 --> 00:13:56,720
But for any individual case, you can, in fact, build a story or build more technically

255
00:13:56,720 --> 00:14:00,560
like a local linear model, which will tell you what the most important aspects of that

256
00:14:00,560 --> 00:14:01,560
particular example were.

257
00:14:01,560 --> 00:14:05,640
And it's been a while since I've talked to Carlos about this.

258
00:14:05,640 --> 00:14:08,560
We'll put a link to the podcast I did with him in the show notes.

259
00:14:08,560 --> 00:14:14,040
But I wasn't even impressed in that the line work, at least at that time, which was a year

260
00:14:14,040 --> 00:14:18,680
and change ago, like wasn't really being applied to neural nets.

261
00:14:18,680 --> 00:14:22,120
It was for, you know, other types of models.

262
00:14:22,120 --> 00:14:25,640
The stuff that we, that I mentioned in the talk is we're not using neural nets at strike.

263
00:14:25,640 --> 00:14:31,000
But the examples he's got are, like, his techniques don't really depend on, my understanding,

264
00:14:31,000 --> 00:14:32,000
the underlying model.

265
00:14:32,000 --> 00:14:33,000
The models themselves.

266
00:14:33,000 --> 00:14:34,000
Okay.

267
00:14:34,000 --> 00:14:35,000
Exactly.

268
00:14:35,000 --> 00:14:37,160
So you can sort of probe a neural net in that if you train a neural net, the example I

269
00:14:37,160 --> 00:14:42,000
showed in the talk was that he has actually in his paper, you have a neural net that recognizes

270
00:14:42,000 --> 00:14:44,760
Huskies, say, versus wolves.

271
00:14:44,760 --> 00:14:49,040
And so he had a beautiful example of a husky that was misclassified as a wolf.

272
00:14:49,040 --> 00:14:50,040
Right.

273
00:14:50,040 --> 00:14:52,680
And so you look at it and you go, yeah, they look kind of the same, like, I get why this

274
00:14:52,680 --> 00:14:53,680
is happening.

275
00:14:53,680 --> 00:14:54,680
Yeah.

276
00:14:54,680 --> 00:14:59,840
So the explanation that Lyme produces shows you the most relevant pixels to the decision.

277
00:14:59,840 --> 00:15:02,840
So like, what pixels that they change to have the most input?

278
00:15:02,840 --> 00:15:06,880
And in this pathological example, it's like the snow underneath the husky.

279
00:15:06,880 --> 00:15:07,880
Right.

280
00:15:07,880 --> 00:15:08,880
Interesting.

281
00:15:08,880 --> 00:15:09,880
Yeah.

282
00:15:09,880 --> 00:15:13,880
One example, and you can, it's clear that like the training set you used clearly just

283
00:15:13,880 --> 00:15:17,240
had every wolf associated with snow marked up on the wrong pattern.

284
00:15:17,240 --> 00:15:22,600
So you, you've explained this deep property of the neural net through one explanation, like

285
00:15:22,600 --> 00:15:25,960
just one image, right, which exposed this rich structure.

286
00:15:25,960 --> 00:15:26,960
Okay.

287
00:15:26,960 --> 00:15:27,960
One last thing.

288
00:15:27,960 --> 00:15:31,440
I'm not trying to make like a, this is this controversial claim that black boxes are actually

289
00:15:31,440 --> 00:15:32,640
easy to explain.

290
00:15:32,640 --> 00:15:37,520
It's more just for the things that we would care about at Stripe, which is how, like, that

291
00:15:37,520 --> 00:15:41,040
you care about with an image, like, why is this particular image misclassified?

292
00:15:41,040 --> 00:15:42,040
Right.

293
00:15:42,040 --> 00:15:45,800
Or why is this particular merchant getting blocked or why is this particular charge?

294
00:15:45,800 --> 00:15:49,000
You can come up with a much richer story about why that's true.

295
00:15:49,000 --> 00:15:52,840
And that seems to often have the property that if you look at enough of these and they're

296
00:15:52,840 --> 00:15:59,560
different enough, you can leach out some understanding of what the model is doing without

297
00:15:59,560 --> 00:16:04,120
actually having to go, you know, look at every path through every tree or the weights

298
00:16:04,120 --> 00:16:05,120
of every neuron.

299
00:16:05,120 --> 00:16:09,240
So you can treat the thing as a black box and get some insight into what patterns it

300
00:16:09,240 --> 00:16:11,920
extracted from the data set you gave it.

301
00:16:11,920 --> 00:16:12,920
Okay.

302
00:16:12,920 --> 00:16:17,840
So the, the line approach is doing what, I guess, sounds like almost like sensitivity

303
00:16:17,840 --> 00:16:19,600
analysis for the inputs, right?

304
00:16:19,600 --> 00:16:21,400
Is that maybe what I think about it?

305
00:16:21,400 --> 00:16:25,720
And then there are also approaches that are based on, you know, kind of introspecting the

306
00:16:25,720 --> 00:16:29,960
layers of a neural network, like deep dream and these other things trying to get, well,

307
00:16:29,960 --> 00:16:31,800
this, this network's looking at edges.

308
00:16:31,800 --> 00:16:35,720
And so if it's misclassifying something, it's because it sees an edge wrong or something

309
00:16:35,720 --> 00:16:37,520
like that.

310
00:16:37,520 --> 00:16:43,840
Did you develop your own technique or, okay, so tell me a little bit about that.

311
00:16:43,840 --> 00:16:44,840
Yeah.

312
00:16:44,840 --> 00:16:47,760
So the technique that we developed, the ones we were just talking about are really

313
00:16:47,760 --> 00:16:54,120
good for actually understanding, for some particular example of fraud or some merchant,

314
00:16:54,120 --> 00:16:56,040
why the model did what it did.

315
00:16:56,040 --> 00:16:58,240
So, or what would have to change to change the decision?

316
00:16:58,240 --> 00:16:59,240
Yeah.

317
00:16:59,240 --> 00:17:05,920
Now, you know, building a product that is meant to block fraud, you know, unfortunately,

318
00:17:05,920 --> 00:17:09,840
some of the merchants that sign up for Stripe, sign up to go just run stolen credit cards

319
00:17:09,840 --> 00:17:11,440
through the system and collect money.

320
00:17:11,440 --> 00:17:15,800
The whole issue is that there are people who are testing cards who are sort of committing

321
00:17:15,800 --> 00:17:20,480
fraud in this network, and because we're one level behind merchants, we have to be a

322
00:17:20,480 --> 00:17:24,680
bit careful about what we expose in terms of our explanations of why the models are doing

323
00:17:24,680 --> 00:17:25,880
what they're doing.

324
00:17:25,880 --> 00:17:31,080
If we were just to give, for every charge, our best sort of most beautiful explanation

325
00:17:31,080 --> 00:17:36,120
of what would have to be different to not get blocked, you know, there's, you could probably

326
00:17:36,120 --> 00:17:40,000
wait this, per merchant by trust and start to expose more and more as they're with you

327
00:17:40,000 --> 00:17:41,000
longer.

328
00:17:41,000 --> 00:17:45,960
You do that through, you know, individual like customer relationships, but as far as

329
00:17:45,960 --> 00:17:49,760
a product, like, what do you want to see on the screen when your card gets blocked?

330
00:17:49,760 --> 00:17:54,400
Like, the explanation that line produces is almost too good.

331
00:17:54,400 --> 00:17:55,840
It's like easy to game, right?

332
00:17:55,840 --> 00:17:56,840
Okay.

333
00:17:56,840 --> 00:18:00,280
So, the system we developed internally, I'll develop it here.

334
00:18:00,280 --> 00:18:05,880
It's a way that you can come up with an explanation for basically a post-hoc explanation

335
00:18:05,880 --> 00:18:11,440
of that looks like a rule that the merchant could have implemented themselves, where if

336
00:18:11,440 --> 00:18:15,280
they deployed that rule, like, it would have caught the charge and it would have agreed

337
00:18:15,280 --> 00:18:18,960
with the model in some high percentage of cases, right?

338
00:18:18,960 --> 00:18:23,160
But it's not necessarily tied to anything internal about the model structure.

339
00:18:23,160 --> 00:18:28,120
So I guess the subtlety here versus the other system is, and this is the title of the talk

340
00:18:28,120 --> 00:18:31,120
is just so stories for AI.

341
00:18:31,120 --> 00:18:35,880
The idea here is that you can develop a sort of, you know, ahead of time, it's hard to make

342
00:18:35,880 --> 00:18:36,880
predictions.

343
00:18:36,880 --> 00:18:39,320
That's why we have to use these models with rich internal structure.

344
00:18:39,320 --> 00:18:42,880
Once you know what's happened, once you know that the model thinks that this charge is

345
00:18:42,880 --> 00:18:48,880
fraudulent, it's much easier to go back and say, okay, given that I know that this is

346
00:18:48,880 --> 00:18:53,160
what the model said, I can go come up with an explanation for why the model did what it

347
00:18:53,160 --> 00:18:54,160
did.

348
00:18:54,160 --> 00:18:57,320
So, like, conspiracy theories are kind of an example of this, right?

349
00:18:57,320 --> 00:19:01,560
Once you know that the model went off in this particular building, you can come up with

350
00:19:01,560 --> 00:19:06,560
some crazy tale about how it got there and it's like a much easier problem.

351
00:19:06,560 --> 00:19:10,280
There's also different, many explanations that will fit some are more credible, some

352
00:19:10,280 --> 00:19:14,720
are less credible, but it's a much easier task than actually predicting a decision.

353
00:19:14,720 --> 00:19:20,560
Is what you're ultimately doing some form of, like, dimensionality reduction or dimension

354
00:19:20,560 --> 00:19:21,560
compression?

355
00:19:21,560 --> 00:19:26,080
Like, you've got all these internal variables in your model, but you're trying to, it sounds

356
00:19:26,080 --> 00:19:30,560
like you're trying to map your story down to, like, these four public facing, what aggregate

357
00:19:30,560 --> 00:19:31,560
thing.

358
00:19:31,560 --> 00:19:36,160
I wish it was, I'll tell you the algorithm, it's, yeah, I think it's a little as disciplined.

359
00:19:36,160 --> 00:19:40,520
Okay, so the simple way to say it, or the first pass of it, is that what we're trying

360
00:19:40,520 --> 00:19:45,720
to do is train, like, a decision tree on, it's basically a bad version of the model that's

361
00:19:45,720 --> 00:19:48,680
meant to predict what the model is going to do, right?

362
00:19:48,680 --> 00:19:54,200
So, we want it to look like a decision tree because we want to be able to present some,

363
00:19:54,200 --> 00:19:58,920
like, the form, the, the output, I guess, of this model, we want to be some set or, some

364
00:19:58,920 --> 00:20:03,560
rule that the user could, could see, or any, of course, using a decision tree because

365
00:20:03,560 --> 00:20:04,560
it's explainable.

366
00:20:04,560 --> 00:20:05,560
It's explainable, right?

367
00:20:05,560 --> 00:20:06,560
Exactly, exactly.

368
00:20:06,560 --> 00:20:12,120
It's not very good, but so, that's the shape of what we want and what the model looks

369
00:20:12,120 --> 00:20:19,240
like is basically a can set of potential explanations, any one of which, if it applies to some

370
00:20:19,240 --> 00:20:24,960
charge you give me, like, any one of which has a high precision with respect to what the

371
00:20:24,960 --> 00:20:26,440
model would have chosen, right?

372
00:20:26,440 --> 00:20:32,360
So, if my rule, if my explanation applies to some charge of the model calls fraudulent

373
00:20:32,360 --> 00:20:36,600
in the test set, I want to be sure that it also would have applied to many other charges,

374
00:20:36,600 --> 00:20:37,600
right?

375
00:20:37,600 --> 00:20:41,480
So, if you just tell me, like, if the explanation is, well, there was a charge on the card,

376
00:20:41,480 --> 00:20:44,960
this is very high recall, it catches a lot of fraud, it has incredibly low precision,

377
00:20:44,960 --> 00:20:45,960
right?

378
00:20:45,960 --> 00:20:48,400
There's no sort of information contained in that statement.

379
00:20:48,400 --> 00:20:53,920
So, again, the explanation model is a list of possible explanations sorted by their precision

380
00:20:53,920 --> 00:20:58,320
in descending order compared to the models, like, what the model claims.

381
00:20:58,320 --> 00:21:04,160
What we do then is, when a charge comes in, if the model claims that it's fraud, we then

382
00:21:04,160 --> 00:21:08,080
go to our explanation model and we just start looking through the explanations, and we find

383
00:21:08,080 --> 00:21:11,320
the first one that applies to the charge that has come in.

384
00:21:11,320 --> 00:21:14,240
So basically, like, what's a path through the decision tree?

385
00:21:14,240 --> 00:21:20,080
Looking at all the paths, not looking necessarily first at the features, just taking the outcomes

386
00:21:20,080 --> 00:21:24,080
and, like, trimming out anything that doesn't apply, which is the explanation that, as you

387
00:21:24,080 --> 00:21:27,880
go down the list, has the highest precision, but also applies to this one.

388
00:21:27,880 --> 00:21:30,040
That's the thing we exposed to the user, right?

389
00:21:30,040 --> 00:21:34,040
So, it's sort of like a decision tree, and then it has, it has the same structure as

390
00:21:34,040 --> 00:21:39,360
a decision tree, but you don't independently ask the explanation model and the forests

391
00:21:39,360 --> 00:21:40,960
and then present both of these.

392
00:21:40,960 --> 00:21:47,880
This probably wouldn't match, right? Because a lot of these explanations won't necessarily

393
00:21:47,880 --> 00:21:53,520
match up, but if you know what the model did, you can take your kind of bad model, your

394
00:21:53,520 --> 00:21:57,600
decision tree, you can trim all the paths that don't agree with the model and then show

395
00:21:57,600 --> 00:22:00,920
the highest precision path down to some leaf.

396
00:22:00,920 --> 00:22:07,480
Okay, so you've got your high fidelity model and your low fidelity model, the low fidelity

397
00:22:07,480 --> 00:22:13,440
model, the decision tree, you are, I mean, it sounds like you're running both of them parallel

398
00:22:13,440 --> 00:22:14,800
along your input data.

399
00:22:14,800 --> 00:22:20,160
Yeah, you run the first one and then the second one gets features as input, it runs its

400
00:22:20,160 --> 00:22:23,400
thing, and again, like, exactly like you're saying.

401
00:22:23,400 --> 00:22:28,760
So I'm not super clear on the part where you're trimming off the, like, you're pruning this

402
00:22:28,760 --> 00:22:30,560
decision tree in some way or...

403
00:22:30,560 --> 00:22:33,560
Okay, so let's see if I can, let's see if I can elaborate on that.

404
00:22:33,560 --> 00:22:38,760
Let's take a step back. So the input to this, the second model decision tree is, it's

405
00:22:38,760 --> 00:22:43,120
not your input data and it's not the output of the first models, the features of the first

406
00:22:43,120 --> 00:22:44,120
model.

407
00:22:44,120 --> 00:22:45,720
Are you talking about the training or the prediction process?

408
00:22:45,720 --> 00:22:46,920
The prediction process.

409
00:22:46,920 --> 00:22:54,600
So for prediction, yeah, it's, yeah, so the input is in fact, yeah, okay, let's step back

410
00:22:54,600 --> 00:22:55,760
and frame this.

411
00:22:55,760 --> 00:22:58,640
So your first, like you said, first model is the black box, right?

412
00:22:58,640 --> 00:23:03,320
Second model is the decision tree or another way to look at this list of predicates, right?

413
00:23:03,320 --> 00:23:08,440
So what you need to evaluate the second model is the output of your first model plus all

414
00:23:08,440 --> 00:23:10,160
the features, right?

415
00:23:10,160 --> 00:23:11,160
So the output of the...

416
00:23:11,160 --> 00:23:14,960
And my features are we referring to the inputs or, like, weights or some internal...

417
00:23:14,960 --> 00:23:15,960
Sorry, good call.

418
00:23:15,960 --> 00:23:20,720
See, again, the statistics machine learning, crazy terms, by features here, I mean, like,

419
00:23:20,720 --> 00:23:26,040
the things we know about the charge, like the dictionary, the inputs to the black box.

420
00:23:26,040 --> 00:23:27,040
Got it.

421
00:23:27,040 --> 00:23:28,040
Okay.

422
00:23:28,040 --> 00:23:29,040
So forget what's going on in the black box.

423
00:23:29,040 --> 00:23:30,040
You're totally right.

424
00:23:30,040 --> 00:23:31,040
Got your features as well.

425
00:23:31,040 --> 00:23:32,040
Okay.

426
00:23:32,040 --> 00:23:34,360
We're going to evaluate the charge, the variables.

427
00:23:34,360 --> 00:23:37,960
We first send those into the black box, we get out some decisions from the black box.

428
00:23:37,960 --> 00:23:38,960
Right.

429
00:23:38,960 --> 00:23:42,080
And we've chosen a threshold in advance, so all we care about is does the model think

430
00:23:42,080 --> 00:23:43,080
it's fraud or not?

431
00:23:43,080 --> 00:23:44,080
Right.

432
00:23:44,080 --> 00:23:45,080
Okay.

433
00:23:45,080 --> 00:23:49,920
Now, we take that, we go to the second model, we take the same inputs to the black box,

434
00:23:49,920 --> 00:23:55,480
and we go through this list, which is ordered from high precision to low precision, and

435
00:23:55,480 --> 00:24:00,160
we find the first path through the tree, again, like, it's important that they're ordered.

436
00:24:00,160 --> 00:24:05,280
We find the first path through the tree that one evaluates the true as well, so agrees

437
00:24:05,280 --> 00:24:06,600
with the black box model.

438
00:24:06,600 --> 00:24:07,600
Well, that's it.

439
00:24:07,600 --> 00:24:09,000
That agrees with the black box model.

440
00:24:09,000 --> 00:24:10,000
Okay.

441
00:24:10,000 --> 00:24:12,760
There might be multiple paths that agree with the black box model, right?

442
00:24:12,760 --> 00:24:17,040
So this is the difference between just evaluating the decision tree.

443
00:24:17,040 --> 00:24:21,200
So we find the one that has the highest precision that does agree, and we return that as the

444
00:24:21,200 --> 00:24:24,120
explanation model, or as the explanation user.

445
00:24:24,120 --> 00:24:25,120
Okay.

446
00:24:25,120 --> 00:24:31,160
How do you even measure the results from, I mean, if we're talking about, if we're talking

447
00:24:31,160 --> 00:24:36,360
about, you know, whether the fraud is the transaction as fraudulent or not, the easy to measure

448
00:24:36,360 --> 00:24:37,360
the results are.

449
00:24:37,360 --> 00:24:39,760
How do you measure like the explainability of the result?

450
00:24:39,760 --> 00:24:41,280
There's a couple answers to this.

451
00:24:41,280 --> 00:24:43,000
One is, I think, a good answer.

452
00:24:43,000 --> 00:24:44,840
One is kind of unfortunate.

453
00:24:44,840 --> 00:24:49,440
The first answer is the way you can evaluate your explanation model is you want each individual

454
00:24:49,440 --> 00:24:51,720
explanation to have high precision.

455
00:24:51,720 --> 00:24:55,800
You want overall the entire set of explanations to have I recall, right?

456
00:24:55,800 --> 00:24:59,680
So you want for any charge that comes in, you want a very high probability of having an

457
00:24:59,680 --> 00:25:01,560
explanation available, right?

458
00:25:01,560 --> 00:25:05,280
And then for each explanation that you give, you want that to be as high precision as possible.

459
00:25:05,280 --> 00:25:08,320
So internally, like, that's one way you can evaluate this.

460
00:25:08,320 --> 00:25:11,680
I think what you're actually getting at is like, what's the point of this?

461
00:25:11,680 --> 00:25:15,960
Like how can you evaluate the effect of giving these to merchants and seeing if they actually

462
00:25:15,960 --> 00:25:16,960
mean anything?

463
00:25:16,960 --> 00:25:19,880
That we were just kind of at the early days.

464
00:25:19,880 --> 00:25:21,600
We don't do a great job at this now.

465
00:25:21,600 --> 00:25:27,520
We don't have, I believe, much built into the product now to evaluate the actual thing

466
00:25:27,520 --> 00:25:33,480
we're trying to do, which is to take something that feels like a black box decision and give

467
00:25:33,480 --> 00:25:37,760
people some way to kind of relate or build a model in their mind for what this black box

468
00:25:37,760 --> 00:25:38,960
model is doing.

469
00:25:38,960 --> 00:25:44,360
Because again, each individual instance of fraud, they might be in a domain where they understand

470
00:25:44,360 --> 00:25:45,360
what's happening, right?

471
00:25:45,360 --> 00:25:47,800
They might be a charity site that is dealing with card testing.

472
00:25:47,800 --> 00:25:51,920
And if they see an instance of this and then the model gives them an explanation that's

473
00:25:51,920 --> 00:25:58,200
a rule that, you know, maybe it's not why the model did what it did, but it actually

474
00:25:58,200 --> 00:26:02,160
happens to match up in a lot of ways with the decisions the model would make for charges

475
00:26:02,160 --> 00:26:03,640
that look like this.

476
00:26:03,640 --> 00:26:08,040
Like that's a valuable piece of information that a merchant could then take that would

477
00:26:08,040 --> 00:26:14,040
one give them some insight into what fraud looks like and two, and this is probably the

478
00:26:14,040 --> 00:26:18,360
real thing you want down the road is to have people just trust what the model is doing

479
00:26:18,360 --> 00:26:20,360
in decision they understand less, right?

480
00:26:20,360 --> 00:26:21,360
Right.

481
00:26:21,360 --> 00:26:25,320
Now, a little bit of what I talked about in the talk is that this is a kind of pathological

482
00:26:25,320 --> 00:26:31,000
by itself, like you have to know, the explanation is there to get you to trust the black box.

483
00:26:31,000 --> 00:26:32,000
Right.

484
00:26:32,000 --> 00:26:35,400
But you shouldn't trust the explanation unless you have some reason already to trust

485
00:26:35,400 --> 00:26:36,960
the black box.

486
00:26:36,960 --> 00:26:42,200
So it's, it's like when Twitter says, oh, you should follow this person because, you

487
00:26:42,200 --> 00:26:44,680
know, Sam follows him and Oscar does too.

488
00:26:44,680 --> 00:26:45,680
Right.

489
00:26:45,680 --> 00:26:46,680
That's not a reason.

490
00:26:46,680 --> 00:26:47,680
Right.

491
00:26:47,680 --> 00:26:51,000
It's just, there's sort of an implied reason, but it's not what's actually happening behind

492
00:26:51,000 --> 00:26:52,000
the scenes.

493
00:26:52,000 --> 00:26:56,520
Well, I mean, part of what I was getting at was, you know, there's all kinds of questions

494
00:26:56,520 --> 00:27:00,960
that it raises, like how granular you want your explanations to be.

495
00:27:00,960 --> 00:27:05,880
The degree to which people actually care about the explanations, like there's, you know,

496
00:27:05,880 --> 00:27:11,600
the book influence, Robert Chaldeini, you know, basically we rejected this charge because

497
00:27:11,600 --> 00:27:14,720
we rejected this charge, like it's putting the word reject there.

498
00:27:14,720 --> 00:27:15,720
Yep.

499
00:27:15,720 --> 00:27:21,320
Just has a huge, you know, provides a huge degree of acceptance, just based on, you know,

500
00:27:21,320 --> 00:27:27,920
human biology and so how do you, how do you wrap your head around, like how granular

501
00:27:27,920 --> 00:27:31,680
you need these explanations to be or is it just like you started somewhere and that's

502
00:27:31,680 --> 00:27:35,240
kind of how much are you investing in that process, I guess?

503
00:27:35,240 --> 00:27:40,520
So internally, we care, well, we care internally and externally, but like the initial reason

504
00:27:40,520 --> 00:27:45,320
we created these, these kinds of models was to try to give our risk analysts and our,

505
00:27:45,320 --> 00:27:50,600
I guess account managers, some way to understand as deeply as they can, like what's happening

506
00:27:50,600 --> 00:27:54,880
with some decision with, you know, a charge that a merchant called to ask about.

507
00:27:54,880 --> 00:27:59,800
So this is a case where you're not really trying to persuade the risk analysts, they,

508
00:27:59,800 --> 00:28:03,880
they have a vested interest in learning as much as possible about this so that they can,

509
00:28:03,880 --> 00:28:07,360
you know, sound knowledgeable or be knowledgeable like when they're talking.

510
00:28:07,360 --> 00:28:12,120
But you still don't want to give them the internal representation of the black boss model.

511
00:28:12,120 --> 00:28:13,120
That's right.

512
00:28:13,120 --> 00:28:16,360
The case I've just described leaves it up to the, you know, the person on the phone to

513
00:28:16,360 --> 00:28:18,600
like hide information that seems sensitive.

514
00:28:18,600 --> 00:28:19,600
Right.

515
00:28:19,600 --> 00:28:25,440
Whereas when you have the model do it, you know, I don't have a great answer for this.

516
00:28:25,440 --> 00:28:29,080
We're not like, it's a, it's a really interesting question.

517
00:28:29,080 --> 00:28:33,800
What level of granularity, you know, what you're really trying to do for a merchant?

518
00:28:33,800 --> 00:28:38,000
How you, how you have them, how you give them the ability to like let you know what they

519
00:28:38,000 --> 00:28:43,280
care about or maybe you give them the ability to, you know, sort of request more information

520
00:28:43,280 --> 00:28:44,560
that you can expand this thing.

521
00:28:44,560 --> 00:28:45,560
Right.

522
00:28:45,560 --> 00:28:47,760
How many explanations do you have?

523
00:28:47,760 --> 00:28:50,680
Each model has roughly like 80 possible explanations.

524
00:28:50,680 --> 00:28:52,400
That usually gets like full coverage on this thing.

525
00:28:52,400 --> 00:28:53,400
Okay.

526
00:28:53,400 --> 00:28:54,840
There's like 80 possible explanations that could apply.

527
00:28:54,840 --> 00:28:55,840
Okay.

528
00:28:55,840 --> 00:28:57,840
I'm comparing it to like credit reporting.

529
00:28:57,840 --> 00:28:58,840
Yeah.

530
00:28:58,840 --> 00:29:02,280
There's like what all of eight explanations and these are ones that you commonly see.

531
00:29:02,280 --> 00:29:03,280
Yeah.

532
00:29:03,280 --> 00:29:06,480
And this, this might help us out that like the rest of the world is just like, there's

533
00:29:06,480 --> 00:29:10,120
a sort of argument by authority for like scary thing is happening in the world.

534
00:29:10,120 --> 00:29:11,120
Yeah.

535
00:29:11,120 --> 00:29:12,120
Go ahead and deal with it.

536
00:29:12,120 --> 00:29:17,480
So this is, I mean, the reason I kind of, I was interested in giving this talk and

537
00:29:17,480 --> 00:29:21,560
doing this work is, these are a lot of the open questions in this, in this field, right?

538
00:29:21,560 --> 00:29:26,760
Like, there's the goal here of getting a customer to trust us.

539
00:29:26,760 --> 00:29:32,040
That doesn't mean anything without some internal, you know, sense of accuracy or reason to

540
00:29:32,040 --> 00:29:33,040
actually trust the model.

541
00:29:33,040 --> 00:29:36,800
If you've got the Husky snow dog problem going on internally, but then you're talking

542
00:29:36,800 --> 00:29:39,960
people into accepting the decisions, that's kind of pathological.

543
00:29:39,960 --> 00:29:44,240
And so with the talk, what I, what I was interested in after this that kind of expands

544
00:29:44,240 --> 00:29:48,800
beyond stripe is that, though we probably will have to deal with this, or I probably will

545
00:29:48,800 --> 00:29:53,200
personally in the next year or so, you know, the EU has this, what's it, the general data

546
00:29:53,200 --> 00:29:54,200
GDPR?

547
00:29:54,200 --> 00:29:55,200
GDPR.

548
00:29:55,200 --> 00:29:56,200
Exactly.

549
00:29:56,200 --> 00:29:58,640
The GDPR, the GDPR gives you this, you know, right to an explanation.

550
00:29:58,640 --> 00:30:00,400
This is one of the clauses.

551
00:30:00,400 --> 00:30:06,240
Now, after working up to this talk and giving it one of the messages I have is, it's, it's

552
00:30:06,240 --> 00:30:08,040
not clear what this means, right?

553
00:30:08,040 --> 00:30:12,520
There's two forms of explanations, there's many forms, but of these two, if people are having

554
00:30:12,520 --> 00:30:16,880
a right to this sort of explanation, like you said, to just have some emotionally triggering

555
00:30:16,880 --> 00:30:21,760
word like rejected in their explanation that's meant to convince them of something that's

556
00:30:21,760 --> 00:30:26,320
going on, this is actually not good, like, I don't think this is the intent of what's

557
00:30:26,320 --> 00:30:27,320
in that law.

558
00:30:27,320 --> 00:30:28,320
Absolutely.

559
00:30:28,320 --> 00:30:33,560
And the other hand, on the other hand, if you're giving people like incredibly detailed

560
00:30:33,560 --> 00:30:39,760
explanations about why the model did what it did, well, this is good, but it's potentially

561
00:30:39,760 --> 00:30:44,400
gameable, you know, like, I guess the core issue here is that you started.

562
00:30:44,400 --> 00:30:51,280
There's an element of, I mean, like domain appropriateness for lack of a better term.

563
00:30:51,280 --> 00:30:59,520
If you, you know, you could, you know, one way to, you describe one way of kind of scurrying

564
00:30:59,520 --> 00:31:01,960
GDPR is by, by, you know, having just this, you know, this, you were rejected.

565
00:31:01,960 --> 00:31:02,960
That's the explanation, right?

566
00:31:02,960 --> 00:31:04,960
That's certainly not in the spirit of the law.

567
00:31:04,960 --> 00:31:09,760
You know, but on the other side, if you say, well, the, you know, the third neuron and,

568
00:31:09,760 --> 00:31:15,040
you know, layer five of our neural network, you know, spit out a 0.6, you know, that's

569
00:31:15,040 --> 00:31:16,040
why you're rejected.

570
00:31:16,040 --> 00:31:21,080
That's also inappropriate, even though it's way more detailed and way more granular.

571
00:31:21,080 --> 00:31:25,880
So I didn't really mean, though, it's, it's a good characterization of it.

572
00:31:25,880 --> 00:31:29,600
I wasn't necessarily saying that that would be a way to skirt the rule, though it certainly

573
00:31:29,600 --> 00:31:30,600
would, right?

574
00:31:30,600 --> 00:31:34,920
If you give people these, okay, you've been rejected, you know, that character of explanation

575
00:31:34,920 --> 00:31:39,360
though about, hey, we don't know why the model worked, but here's a plausible explanation.

576
00:31:39,360 --> 00:31:43,920
I guess in, in thinking about this, I think the problem with that, if you did, if that

577
00:31:43,920 --> 00:31:48,600
was what was adopted by say the courts or, you know, a car, for example, having to spit

578
00:31:48,600 --> 00:31:51,920
out an explanation of why it did, but it did the cost and damage.

579
00:31:51,920 --> 00:31:56,360
This doesn't teach you much about the core problem of, that I think we're trying to solve

580
00:31:56,360 --> 00:31:59,760
here, which is there are things we care about in the world.

581
00:31:59,760 --> 00:32:04,440
And then there are the goals that we give to our models when they get trained, right?

582
00:32:04,440 --> 00:32:06,200
You have some prediction target.

583
00:32:06,200 --> 00:32:09,840
And it's just the fact that we don't, we don't know enough about ourselves about what we

584
00:32:09,840 --> 00:32:16,040
care about, about how we work to encode all of our ethics, all of these things in the

585
00:32:16,040 --> 00:32:17,880
goals of a model.

586
00:32:17,880 --> 00:32:22,520
And so when you can ask a model, like, I think my understanding here, my interpretation

587
00:32:22,520 --> 00:32:26,640
of this is, when you can ask a model for an explanation of why it's doing what it's doing,

588
00:32:26,640 --> 00:32:30,480
what you're doing is you're monitoring whether or not it picked up the things you actually

589
00:32:30,480 --> 00:32:34,120
care about from the data that was easy to encode, right?

590
00:32:34,120 --> 00:32:36,560
And what do you do then?

591
00:32:36,560 --> 00:32:42,240
What you do if you find a mismatch, the only thing that makes sense is to take the thing

592
00:32:42,240 --> 00:32:46,440
that wasn't quite there and to figure out how to encode that in the goal of your model

593
00:32:46,440 --> 00:32:48,040
that you're training, right?

594
00:32:48,040 --> 00:32:54,720
So you have, on the one hand, this, again, kind of pathological case where models start

595
00:32:54,720 --> 00:32:57,960
optimizing for whatever they're optimizing for, right?

596
00:32:57,960 --> 00:33:02,320
Once you start feeding data back, like, model decisions back into its own training set,

597
00:33:02,320 --> 00:33:06,840
you start to get this odd effect of, like, putting a copy on the copy over and over.

598
00:33:06,840 --> 00:33:10,960
And then you've got this other parasitic model that's, like, just justifying whatever's

599
00:33:10,960 --> 00:33:11,960
happening under there.

600
00:33:11,960 --> 00:33:12,960
This is not good.

601
00:33:12,960 --> 00:33:13,960
Right.

602
00:33:13,960 --> 00:33:14,960
Right.

603
00:33:14,960 --> 00:33:15,960
That's the downside.

604
00:33:15,960 --> 00:33:20,320
So we're forced to really clarify what we care about in technical terms, right?

605
00:33:20,320 --> 00:33:24,280
Like, you can take these ethical concerns, this is what I guess the law attempts to do

606
00:33:24,280 --> 00:33:31,280
through almost like your sidecar model is like a, almost like a unit test for your regular

607
00:33:31,280 --> 00:33:32,280
model.

608
00:33:32,280 --> 00:33:38,080
Is it encoding the kind of the way of thinking about the relationship between the transactions

609
00:33:38,080 --> 00:33:39,080
and the judgments?

610
00:33:39,080 --> 00:33:40,080
Yep.

611
00:33:40,080 --> 00:33:43,280
Like in a conversation, if you explain, you say something to someone, they repeat it back

612
00:33:43,280 --> 00:33:45,520
in a slightly different way, what's the point of this?

613
00:33:45,520 --> 00:33:49,760
Or you're just demonstrating that you've absorbed the content, right, stating it in a different

614
00:33:49,760 --> 00:33:50,760
way.

615
00:33:50,760 --> 00:33:55,080
And if you go, yeah, totally get it, you know, and just say something insane, right?

616
00:33:55,080 --> 00:33:57,120
We get to talking again, you know?

617
00:33:57,120 --> 00:34:03,480
So that I think is what's so interesting about this tension between the different kinds

618
00:34:03,480 --> 00:34:06,760
of explanations, the subtleties of what's going on inside the model.

619
00:34:06,760 --> 00:34:07,760
Right.

620
00:34:07,760 --> 00:34:12,760
So one example of, like, an ethical concern that people have been talking about, there's

621
00:34:12,760 --> 00:34:18,600
a couple of folks internally, it's right, where actually speaking of the GDPR, like one

622
00:34:18,600 --> 00:34:23,760
concern for a model is that it's going to pick up on some kind of implicit like pattern

623
00:34:23,760 --> 00:34:27,400
in the data and start treating like people of different genders, different race, like

624
00:34:27,400 --> 00:34:28,400
different groups.

625
00:34:28,400 --> 00:34:30,160
It's going to have different considerations for them.

626
00:34:30,160 --> 00:34:35,160
Now this first kind of model, there's a Peter Norvik quote I gave in the talk, can give

627
00:34:35,160 --> 00:34:39,160
you an explanation that just kind of ignores what's actually going on and gives you a plausible

628
00:34:39,160 --> 00:34:43,520
reason why, you know, somebody might have been rejected from a job, like say it's right,

629
00:34:43,520 --> 00:34:46,600
we start turning a model loose on hiring, right?

630
00:34:46,600 --> 00:34:52,000
If you can figure out what is different about what you care about, which is this sense of,

631
00:34:52,000 --> 00:34:56,960
this kind of vague sense maybe that everyone like deserves a fair shot, et cetera, you

632
00:34:56,960 --> 00:35:01,280
find that your model is not doing that, you're then forced to encode this more formally.

633
00:35:01,280 --> 00:35:06,360
So Google has a great post which says this in a nice way, which is like one way to encode

634
00:35:06,360 --> 00:35:11,080
this is to say that along any split you care about, along any group, along any like business

635
00:35:11,080 --> 00:35:15,400
type or something like this, let's make sure that the false positive rate is identical

636
00:35:15,400 --> 00:35:16,720
across all these things.

637
00:35:16,720 --> 00:35:21,680
And so this then can get fed back into the, like once you realize that if this is not happening,

638
00:35:21,680 --> 00:35:25,560
you can then feed this back into the training process for your black box models, they then

639
00:35:25,560 --> 00:35:30,360
will kind of formally encode the ethics you care about in the world and then you move

640
00:35:30,360 --> 00:35:32,760
on to the next problem, of course, right?

641
00:35:32,760 --> 00:35:38,840
Yeah, it's really interesting, even circling back to the kind of the beginning, the notion

642
00:35:38,840 --> 00:35:45,440
of explainability, the model you're describing, like the second model, you could argue it's

643
00:35:45,440 --> 00:35:51,240
not really explaining the first model, it's explaining, it's offering a justification,

644
00:35:51,240 --> 00:35:52,240
right?

645
00:35:52,240 --> 00:35:56,720
And you kind of said that, but we talk about this broadly as solving the explainability

646
00:35:56,720 --> 00:36:01,160
problem, but it doesn't necessarily offer any insight whatsoever to what's actually

647
00:36:01,160 --> 00:36:02,560
happening inside your model.

648
00:36:02,560 --> 00:36:07,840
That's right, which I guess, you know, we're kind of getting into semantic land here,

649
00:36:07,840 --> 00:36:11,520
but you know, explainability of the model versus explainability of the output.

650
00:36:11,520 --> 00:36:17,560
But I think they're, you know, at the very least, I think, you know, for folks that are,

651
00:36:17,560 --> 00:36:20,960
you know, thinking about this stuff, it makes sense to at least try to be clear on which

652
00:36:20,960 --> 00:36:22,600
one you're trying to achieve.

653
00:36:22,600 --> 00:36:28,480
I think that's a great takeaway, I mean, to wrap back to what you, when you were talking

654
00:36:28,480 --> 00:36:32,720
about like, if you give someone an explanation on the state of the neurons, you know, one

655
00:36:32,720 --> 00:36:37,040
thing that comes up a lot in these, in these papers, these papers that are trying to talk

656
00:36:37,040 --> 00:36:41,840
about how to formally encode explanations is this idea that, or these casual references

657
00:36:41,840 --> 00:36:45,320
to how humans reason and how humans explain their own decisions.

658
00:36:45,320 --> 00:36:49,560
And it's just kind of taken for granted at this point that the way we work is separate

659
00:36:49,560 --> 00:36:52,560
from the explanations we give for our actions, right?

660
00:36:52,560 --> 00:36:59,160
And so this is one thing that comes up when you read about the GDPR is that one critique,

661
00:36:59,160 --> 00:37:03,360
I guess, of this clause is that we're asking for things from our black box models that

662
00:37:03,360 --> 00:37:07,000
we don't ask from judges, because there's just no conceivable answer, right?

663
00:37:07,000 --> 00:37:11,680
So it's like you said, it's sort of getting into semantics is kind of getting into sort

664
00:37:11,680 --> 00:37:16,960
of whatever philosophy I can sort of pull out of a software engineering career.

665
00:37:16,960 --> 00:37:19,200
But I think it's really important to contrast the two, right?

666
00:37:19,200 --> 00:37:21,480
And say, you know, what do we mean?

667
00:37:21,480 --> 00:37:22,760
What are we asking for?

668
00:37:22,760 --> 00:37:26,800
There's this naive idea that because it's a model, because it's an algorithm, there's

669
00:37:26,800 --> 00:37:30,360
like a clear answer to why it did what it did, or because it was something you could turn

670
00:37:30,360 --> 00:37:31,360
the crank on.

671
00:37:31,360 --> 00:37:32,360
Right.

672
00:37:32,360 --> 00:37:33,360
But that's not right at all.

673
00:37:33,360 --> 00:37:34,960
It's much more subtle than that.

674
00:37:34,960 --> 00:37:35,960
Awesome.

675
00:37:35,960 --> 00:37:36,960
Awesome.

676
00:37:36,960 --> 00:37:39,960
So is any of the work that you've done in this area published?

677
00:37:39,960 --> 00:37:44,000
We're planning in the next few weeks on writing more about this, and I think publishing

678
00:37:44,000 --> 00:37:45,000
the algorithm.

679
00:37:45,000 --> 00:37:46,000
Okay.

680
00:37:46,000 --> 00:37:47,000
That'd be great.

681
00:37:47,000 --> 00:37:49,440
Yeah, I'm sure folks would be very interested in learning more about it.

682
00:37:49,440 --> 00:37:54,480
And once again, we'll drop the link to Lime and the conversation with Carlos.

683
00:37:54,480 --> 00:38:00,080
And are there any other efforts at explainability that figured heavily into the work that you

684
00:38:00,080 --> 00:38:01,080
did there?

685
00:38:01,080 --> 00:38:03,920
I'll give you some links to put in the show notes too, but we've got, I mean, there's

686
00:38:03,920 --> 00:38:04,920
so much stuff going on.

687
00:38:04,920 --> 00:38:09,960
There's a DARPA program now, an explainable AI, Stuart Russell at Berkeley has, I think

688
00:38:09,960 --> 00:38:13,920
it's called, I cannot remember the name, not human compatible, maybe human compatible

689
00:38:13,920 --> 00:38:14,920
AI.

690
00:38:14,920 --> 00:38:15,920
Okay.

691
00:38:15,920 --> 00:38:16,920
There's a track at MIPS about this issue.

692
00:38:16,920 --> 00:38:21,720
I mean, this is really coming up as, of course, we have to get into this, it's coming up

693
00:38:21,720 --> 00:38:27,280
as one of the core approaches toward this problem of like AI and algorithmic safety, right?

694
00:38:27,280 --> 00:38:30,160
Which can often be this fuzzy, scary conversation.

695
00:38:30,160 --> 00:38:34,240
For me, I think there's grounds it very heavily, like what are we worried about kind of one

696
00:38:34,240 --> 00:38:38,280
thing is the leaching of meaning from just letting things that are loose that are really,

697
00:38:38,280 --> 00:38:42,360
really accurate, but haven't encoded the things you care about in the world, right?

698
00:38:42,360 --> 00:38:44,240
And explanations are kind of our tether on this.

699
00:38:44,240 --> 00:38:45,240
Yeah.

700
00:38:45,240 --> 00:38:48,960
So I'll send more links and I'm going to do some writing on this next week that I'll

701
00:38:48,960 --> 00:38:49,960
send over as well.

702
00:38:49,960 --> 00:38:50,960
Okay.

703
00:38:50,960 --> 00:38:51,960
Just the full brain dump.

704
00:38:51,960 --> 00:38:52,960
Awesome.

705
00:38:52,960 --> 00:38:53,960
Looking forward to it.

706
00:38:53,960 --> 00:38:54,960
Well, thanks so much, Sam.

707
00:38:54,960 --> 00:38:55,960
Always very to have another Sam on the show.

708
00:38:55,960 --> 00:38:56,960
Yeah.

709
00:38:56,960 --> 00:38:57,960
Thank you, sir.

710
00:38:57,960 --> 00:38:58,960
Enjoy the rest of the conversation.

711
00:38:58,960 --> 00:38:59,960
Thank you, Sam.

712
00:38:59,960 --> 00:39:05,440
All right, everyone, that's our show for today.

713
00:39:05,440 --> 00:39:10,240
Thanks so much for listening and for your continued feedback and support.

714
00:39:10,240 --> 00:39:15,040
For more information on Sam or any of the topics covered in this show, head on over

715
00:39:15,040 --> 00:39:19,960
to twomolei.com slash talk slash 73.

716
00:39:19,960 --> 00:39:27,440
To follow along with our Strange Loop 2017 series, visit twomolei.com slash ST loop.

717
00:39:27,440 --> 00:39:33,200
Of course, you can send along your feedback or question via Twitter to at twomolei or

718
00:39:33,200 --> 00:39:38,080
at Sam Charrington or leave a comment right on the show notes page.

719
00:39:38,080 --> 00:39:41,360
Thanks again to Nick Sosis for their sponsorship of the show.

720
00:39:41,360 --> 00:39:48,600
Check out twomolei.com slash talk slash 69 to hear my interview with the company founders

721
00:39:48,600 --> 00:39:55,960
and visit NickSosis.com slash twimmel for more information and to try their API for free.

722
00:39:55,960 --> 00:40:14,160
Thanks again for listening and catch you next time.

