1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,360
I'm your host Sam Charrington. Hey everyone, Sam again with another quick Twimble

4
00:00:35,360 --> 00:00:41,440
Con update. One of the things that's been especially exciting to see is the number of organizations

5
00:00:41,440 --> 00:00:47,320
sending multiple people in some cases entire teams to Twimble Con to learn about scaling

6
00:00:47,320 --> 00:00:52,280
and operationalizing machine learning. A full third of the companies attending are sending

7
00:00:52,280 --> 00:00:59,400
groups in many cases three four and five people. This is awesome. Seeing so many teams attending

8
00:00:59,400 --> 00:01:03,680
is a great indicator that folks really see the opportunity associated with improving the

9
00:01:03,680 --> 00:01:08,040
efficiency of their data science and machine learning operations and are excited about

10
00:01:08,040 --> 00:01:12,520
the conversations we'll be curating at the event. If you'd like to attend Twimble

11
00:01:12,520 --> 00:01:18,920
Con with your team just reach out to me at Sam at Twimlai.com and let's make it happen.

12
00:01:18,920 --> 00:01:22,560
Of course you're welcome to reach out to me if you want to attend as an individual or

13
00:01:22,560 --> 00:01:29,240
just head over to twimblecon.com slash register to sign up.

14
00:01:29,240 --> 00:01:33,800
I hearty thanks to our friends at Qualcomm for sponsoring today's show. As you hear in

15
00:01:33,800 --> 00:01:38,160
my conversation with time in Qualcomm has been actively involved in AI research for

16
00:01:38,160 --> 00:01:43,400
well over a decade leading to advances in power efficient on device AI through efficient

17
00:01:43,400 --> 00:01:48,560
neural network quantization and compression and more. Of course Qualcomm power some of

18
00:01:48,560 --> 00:01:52,960
the latest and greatest Android devices with their Snapdragon chipset family. From this

19
00:01:52,960 --> 00:01:57,960
strong foundation in the mobile chipset space Qualcomm now has the goal of scaling AI across

20
00:01:57,960 --> 00:02:03,600
devices and making it ubiquitous. To learn more about what Qualcomm is up to including

21
00:02:03,600 --> 00:02:10,600
their AI research platforms and developer tools visit twimlai.com slash Qualcomm.

22
00:02:10,600 --> 00:02:16,760
All right everyone I am on the line with time in Blunk of War. Time in as a staff engineer

23
00:02:16,760 --> 00:02:22,600
at Qualcomm leading the compression and quantization research team. Time in welcome to this

24
00:02:22,600 --> 00:02:23,880
week in machine learning and AI.

25
00:02:23,880 --> 00:02:27,000
Hey, it's great to be here. Nice to meet you all.

26
00:02:27,000 --> 00:02:30,960
Great to have you on the show. I'm looking forward to our conversation and digging into

27
00:02:30,960 --> 00:02:36,200
the topic of quantization and compression. But before we do that, I'd love to hear

28
00:02:36,200 --> 00:02:38,800
a little bit about your background.

29
00:02:38,800 --> 00:02:45,640
Yeah, so I have a background to bachelor's degree in mathematics from Light University.

30
00:02:45,640 --> 00:02:50,600
And then I moved to Artificial Intelligence in Amsterdam and during my master's I started

31
00:02:50,600 --> 00:02:55,960
my own company together with the Professor Maxwelling which been on the show before I think.

32
00:02:55,960 --> 00:03:01,000
And after, yeah, and that was quite successful. So our AI started got acquired by Qualcomm.

33
00:03:01,000 --> 00:03:04,960
And now I'm here already since two years now working in Qualcomm.

34
00:03:04,960 --> 00:03:08,240
Awesome. What was your role with with Cypher?

35
00:03:08,240 --> 00:03:13,120
So at Cypher, we're basically co-founder officially called CTO. I did a lot of the technical

36
00:03:13,120 --> 00:03:19,480
stuff leading the technical team. But I also did a lot of like going to different companies

37
00:03:19,480 --> 00:03:23,000
talking to them about their problems that we could potentially solve a deep learning,

38
00:03:23,000 --> 00:03:27,440
not stage a lot, doing some sales. I mean, if you feel like a small starter, you have to

39
00:03:27,440 --> 00:03:29,200
do everything, right? So I did everything.

40
00:03:29,200 --> 00:03:31,320
We're a lot of hats. Absolutely.

41
00:03:31,320 --> 00:03:32,320
Yes, exactly.

42
00:03:32,320 --> 00:03:39,240
And so Qualcomm now your focus has it been on quantization and compression research the

43
00:03:39,240 --> 00:03:43,480
entire time or have you done a variety of things in a couple of years?

44
00:03:43,480 --> 00:03:48,760
Yeah, that's been it for me. So basically two years ago when I joined, I found this topic

45
00:03:48,760 --> 00:03:53,600
of neural network compression and quantization incredibly interesting. So I started working

46
00:03:53,600 --> 00:03:59,800
on that both on the research and in the application end. And then as I went, the team was growing

47
00:03:59,800 --> 00:04:03,920
and the topic became more and more important. So yeah, basically the last two years of my

48
00:04:03,920 --> 00:04:07,120
life have been making neural networks more efficient.

49
00:04:07,120 --> 00:04:15,400
Awesome. And so quantization and compression, what are defined for us this topic and are

50
00:04:15,400 --> 00:04:22,320
they the same thing or are they different things? What are those terms mean? Let's start

51
00:04:22,320 --> 00:04:23,320
from the top.

52
00:04:23,320 --> 00:04:29,360
Yeah, that's good. So in general, what we're talking about is deep learning in an efficient

53
00:04:29,360 --> 00:04:36,000
way, right? So instead of having very big large neural networks that's a convry run on,

54
00:04:36,000 --> 00:04:39,160
for example, a cell phone, we want to make them as small as possible as they're very

55
00:04:39,160 --> 00:04:43,720
efficient and through very, very power efficient that they are quick and fast, et cetera.

56
00:04:43,720 --> 00:04:47,800
So there's a couple of ways that you can do this, right? You can take a network and make

57
00:04:47,800 --> 00:04:53,720
the architecture more efficient, for example, or you can optimize the kernels that execute

58
00:04:53,720 --> 00:04:58,120
or you can develop like a specific hardware from making a neural network more efficient.

59
00:04:58,120 --> 00:05:02,320
And specifically what we're looking into is, well, also of course, those topics are

60
00:05:02,320 --> 00:05:05,440
quite common, looking into all of those things. And specifically, my team is looking

61
00:05:05,440 --> 00:05:10,800
compression and quantization, compression being, let's say, take any pre-trained network.

62
00:05:10,800 --> 00:05:14,840
So a network that somebody trained for a task, you spend a lot of time on this, right?

63
00:05:14,840 --> 00:05:18,920
And then comes the question, like, how can we run this more efficiently? And with that

64
00:05:18,920 --> 00:05:21,920
question, you want to make sure that that same network that you have, and that you've trained

65
00:05:21,920 --> 00:05:26,200
to put a lot of sweat in, and that you make that one as small as possible. So the idea

66
00:05:26,200 --> 00:05:31,280
with compression is to remove individual weights, or perhaps even better, remove complete

67
00:05:31,280 --> 00:05:35,280
feature maps, complete neurons, complete convolutional channels from your network to make

68
00:05:35,280 --> 00:05:40,360
it more efficient, right? And parallel to that, another way of making a network more efficient

69
00:05:40,360 --> 00:05:44,160
is quantization. And normally, when you train a neural network, it's done in like floating

70
00:05:44,160 --> 00:05:50,400
point 32. So that means that every number is like 32-bit value. Now, you can calculate

71
00:05:50,400 --> 00:05:54,560
a lot more effectively and efficiently. And you have a lot less memory transfer and energy

72
00:05:54,560 --> 00:05:59,120
consumption, if you would use less bits for each of the weights and the activations in your

73
00:05:59,120 --> 00:06:04,480
neural network. So the idea is that you can do calculations in 8 bits, quantize your

74
00:06:04,480 --> 00:06:09,360
whole network, both the weights and all the operations in between to 8-bit operations.

75
00:06:09,360 --> 00:06:13,600
And then you're a lot more efficient. So both work together, compression quantization,

76
00:06:13,600 --> 00:06:17,200
to kind of skill models down. And sometimes you can do this with like a factor 80 that your

77
00:06:17,200 --> 00:06:21,440
model becomes a lot and a lot smaller than the original model that you started with.

78
00:06:21,440 --> 00:06:28,840
And now, how dependent on that compression factor is the specific model that you're working

79
00:06:28,840 --> 00:06:33,040
with? So like, you mean how much you can compress it?

80
00:06:33,040 --> 00:06:36,960
Yeah, absolutely. Yeah. Yeah. Yeah. Yeah. Yeah. It's very important what Motivus

81
00:06:36,960 --> 00:06:41,120
start with. I think there's this running joke in the compression literature that everybody

82
00:06:41,120 --> 00:06:45,800
always tests their models on like VGG because the VGG architecture is so inefficient that

83
00:06:45,800 --> 00:06:50,520
you always get these huge numbers like, oh, I compress my models by a factor 20 or something

84
00:06:50,520 --> 00:06:57,000
like that. So those models can be really, really efficiently compressed. More recently,

85
00:06:57,000 --> 00:07:01,920
the smaller models are not as you can get those big compression numbers. But very frequently

86
00:07:01,920 --> 00:07:06,440
we've seen, for example, MobileNet V2, which is already a very efficient fast network

87
00:07:06,440 --> 00:07:11,560
can still be compressed by factor two sometimes. And that's not affected to less energy that

88
00:07:11,560 --> 00:07:14,960
you're using, right? Or speed up into your latency. So that's quite significant.

89
00:07:14,960 --> 00:07:24,280
And so a lot of the networks that I've heard you toss out are our CNNs. Is that the primary

90
00:07:24,280 --> 00:07:30,720
place that we're focused on quantization and compression? Or does it really span the

91
00:07:30,720 --> 00:07:36,840
gamut in terms of the types of models? Yeah. I think like any model that you want to run

92
00:07:36,840 --> 00:07:41,840
on a phone or you want to run efficiently the clouds, be it like neural network translation,

93
00:07:41,840 --> 00:07:46,560
image models, computer vision models, like cement limitation, like for all of these,

94
00:07:46,560 --> 00:07:50,840
it's very, very, very important to run them efficiently. I think there's like a general

95
00:07:50,840 --> 00:07:54,280
tendency in the research and compression quantization that they're focusing on computer

96
00:07:54,280 --> 00:07:59,160
vision architectures. And there's some literature from like RNNs and transformers, but it's

97
00:07:59,160 --> 00:08:07,120
a very large amount. But in general, what's driving that? Is that because they're starting

98
00:08:07,120 --> 00:08:15,000
out very inefficient or big or power hungry? Or is it just that's what we're using more

99
00:08:15,000 --> 00:08:20,480
of? I think it's also what we're using a lot of, right? Like if you look at deep learning

100
00:08:20,480 --> 00:08:25,000
algorithms on the phone, yes, there's some like neural machine translation models, etc.

101
00:08:25,000 --> 00:08:30,440
The large bill of data is starting in the cloud, but a lot of camera algorithms are already

102
00:08:30,440 --> 00:08:35,200
on phones nowadays. So I think most of your efficiency work because of that is in the

103
00:08:35,200 --> 00:08:39,440
computer vision area. And also if you look at conferences, right? Like the biggest conferences

104
00:08:39,440 --> 00:08:44,880
are computer vision conferences. And those people do a lot on these efficiency compression

105
00:08:44,880 --> 00:08:48,720
kind of algorithms. And so I think it's just because of the necessity of the market.

106
00:08:48,720 --> 00:08:53,720
But slowly bit steadily, once more algorithms make their way to efficient devices, I think

107
00:08:53,720 --> 00:08:59,800
we'll see like more work on like sequence models and other types of models.

108
00:08:59,800 --> 00:09:07,080
And so the impression that I'm getting from hearing you describe compression is that,

109
00:09:07,080 --> 00:09:16,840
you know, I may start with kind of an off-the-shelf model for computer vision like a VGG and then

110
00:09:16,840 --> 00:09:21,920
kind of train it to, you know, solve whatever problem I'm trying to solve and then put

111
00:09:21,920 --> 00:09:29,000
it through this compression process as opposed to starting with a compressed network and

112
00:09:29,000 --> 00:09:36,280
train it. Is that correct? Or is the way that we're trying to get to smaller computer

113
00:09:36,280 --> 00:09:42,120
vision networks starting by training big networks and then compressing them? And, you

114
00:09:42,120 --> 00:09:46,840
know, as an alternative, is there a path to like just starting with the smaller network

115
00:09:46,840 --> 00:09:50,920
architecture and training it? And if not, you know, why doesn't that work? What are the

116
00:09:50,920 --> 00:09:54,280
issues associated with the way we approach this problem?

117
00:09:54,280 --> 00:09:59,320
I think that's a great question, very fundamental to the kind of work we're doing and deep learning

118
00:09:59,320 --> 00:10:05,720
in general, perhaps. And so basically, if you want to do that like make a efficient network

119
00:10:05,720 --> 00:10:10,440
architecture, I would always start with just making your network smaller for something

120
00:10:10,440 --> 00:10:13,960
I got, right? You know, you don't want to have a network that's like 20 times too big

121
00:10:13,960 --> 00:10:17,520
and then say, okay, now we're going to compress it. I would rather say, well, if that's

122
00:10:17,520 --> 00:10:21,960
the case, just tweak your architecture in some kind of way and get a more efficient architecture

123
00:10:21,960 --> 00:10:27,080
and use that. But then afterwards, like if you have this efficient already efficient

124
00:10:27,080 --> 00:10:31,720
architecture like a mobile network, for example, or an efficient net, it still helps a lot

125
00:10:31,720 --> 00:10:36,880
to compress it afterwards. And there's this theory, like this theoretical paper got a

126
00:10:36,880 --> 00:10:42,680
lottery ticket hypothesis that was recently published. And this kind of positive also proves

127
00:10:42,680 --> 00:10:47,840
to a certain extent that you're better off like training a large neural network, something

128
00:10:47,840 --> 00:10:52,640
that's like over parameterized. There's a lot of filters and a lot of parameters at training

129
00:10:52,640 --> 00:10:57,440
that first and then making it smaller afterwards. And that's generally better than training

130
00:10:57,440 --> 00:11:03,480
that smaller resulting architecture from scratch. So I think currently literature or the

131
00:11:03,480 --> 00:11:06,600
quicker consensus is that it's better to do that. Just start with the big model and

132
00:11:06,600 --> 00:11:13,840
then make it smaller. And why is that? What is that paper at least suggests is the reasoning

133
00:11:13,840 --> 00:11:21,480
for that? The paper suggests that basically you want to find like a lottery ticket. So

134
00:11:21,480 --> 00:11:27,280
the lottery ticket is basically a network architecture that's very good for your problem.

135
00:11:27,280 --> 00:11:31,600
And every feature will have like a certain probability of being a good feature to use.

136
00:11:31,600 --> 00:11:36,000
So what the neural network is doing during training is that you want to find features that

137
00:11:36,000 --> 00:11:40,840
are initialized close to good features and those will be used. And then during training

138
00:11:40,840 --> 00:11:46,040
the rest will be like pruned away or like removed or set to zero because of your regularization.

139
00:11:46,040 --> 00:11:48,840
And now the more parameters you have, the more feature maps you have, the higher the

140
00:11:48,840 --> 00:11:54,280
probability you have of finding those optimal features that you can find. So because of

141
00:11:54,280 --> 00:11:57,280
that, you're better off training with a large network that is very of a parameterized.

142
00:11:57,280 --> 00:12:02,160
You have more shots that find the correct architecture and then removing or regularizing

143
00:12:02,160 --> 00:12:09,760
away the smaller ones. So each year parameters is buying a lottery ticket and a network that

144
00:12:09,760 --> 00:12:15,040
works well for your problem is a winning lottery ticket. And so they're theorizing that

145
00:12:15,040 --> 00:12:20,240
it's better to start with a winning lottery ticket and find a way to make it better than

146
00:12:20,240 --> 00:12:27,200
it is to start with a smaller network fewer shots at finding that winning lottery ticket.

147
00:12:27,200 --> 00:12:31,600
Yeah, exactly. It's like as if you were looking at, for example, you have a hole in

148
00:12:31,600 --> 00:12:36,480
the big, in the big table and you can, you can randomly distribute balls over this table.

149
00:12:36,480 --> 00:12:40,800
And if you want one ball in the table, you want to minimize the distance that takes you

150
00:12:40,800 --> 00:12:44,360
to roll the ball from one part of the table to the, to the, to the hole, then you're better

151
00:12:44,360 --> 00:12:49,440
off having 50 balls in the table to and then pick the smallest and the closest ball and

152
00:12:49,440 --> 00:12:53,200
putting that in the hole, then you might have to move a few centimeters rather than picking

153
00:12:53,200 --> 00:12:57,440
one ball, putting it randomly on the table and then you have like a very large distance.

154
00:12:57,440 --> 00:13:01,560
So you're basically, you're trying to minimize the distance between your optimal features

155
00:13:01,560 --> 00:13:03,440
and what you initialize.

156
00:13:03,440 --> 00:13:08,760
And so this, the lottery ticket hypothesis kind of flies in the face of your previous

157
00:13:08,760 --> 00:13:13,880
advice, which is to not start with something that's massive and, and start with something

158
00:13:13,880 --> 00:13:18,920
that's smaller. How do you rationalize those two perspectives?

159
00:13:18,920 --> 00:13:25,240
Yeah, so I think it's, it's a part of skill. Like, I think if you started the network

160
00:13:25,240 --> 00:13:30,520
that's like 20 times too big for what you're eventually want, then you get to this point

161
00:13:30,520 --> 00:13:33,800
where it's actually pretty difficult to create algorithms that compress these networks

162
00:13:33,800 --> 00:13:34,800
efficiently.

163
00:13:34,800 --> 00:13:38,320
At least on my experience, the current tools that we have to compress networks are not

164
00:13:38,320 --> 00:13:42,680
very well equipped to prune a network by such significant amounts, because you have to

165
00:13:42,680 --> 00:13:48,120
imagine that that removing parts of the network and, and then training it while removing parts

166
00:13:48,120 --> 00:13:51,800
is also a very difficult task for the network to do, a lot of discreet optimization going

167
00:13:51,800 --> 00:13:55,800
on there, it's very difficult to train networks and prune them at the same time.

168
00:13:55,800 --> 00:14:00,040
So those numbers are too big, but definitely if your network is two times or three times

169
00:14:00,040 --> 00:14:04,480
too big for the original size, then yeah, just train it on the larger size and then prune

170
00:14:04,480 --> 00:14:06,280
it down and make it smaller.

171
00:14:06,280 --> 00:14:11,200
And so how, how do you know if the network is two or three times too big or 20 times too

172
00:14:11,200 --> 00:14:12,200
big?

173
00:14:12,200 --> 00:14:18,200
I think that's trial and error. So, I mean, one of the most basic things that you can do

174
00:14:18,200 --> 00:14:22,320
is just stake your network that you've already designed and see what happens if you remove

175
00:14:22,320 --> 00:14:27,120
the amount of channels by a factor half, like you reduce everything by 50%.

176
00:14:27,120 --> 00:14:30,960
Then I remove and then reduce it to 25% or 12.5%, etc.

177
00:14:30,960 --> 00:14:34,080
And that's kind of already, if you're a pretty good idea of how over pyramids write

178
00:14:34,080 --> 00:14:35,080
your network is.

179
00:14:35,080 --> 00:14:36,080
Yeah, it's funny.

180
00:14:36,080 --> 00:14:42,560
I talk to a lot of people, particularly folks that are on the applied side of things,

181
00:14:42,560 --> 00:14:48,160
so they're not in research, they're not trying to develop new network architectures,

182
00:14:48,160 --> 00:14:55,280
and 90% of the time what they're doing is they're picking up some standard network architecture

183
00:14:55,280 --> 00:15:00,320
off the shelf, training it, and it kind of works.

184
00:15:00,320 --> 00:15:04,080
And they're like, thumbs up, hey, we've got something that works here.

185
00:15:04,080 --> 00:15:05,080
Yeah, let's go.

186
00:15:05,080 --> 00:15:06,080
Let's put it in a product.

187
00:15:06,080 --> 00:15:07,560
Let's put it in a product, right?

188
00:15:07,560 --> 00:15:12,400
And so, granted, most of the time, they're not working on putting it in a phone, but

189
00:15:12,400 --> 00:15:23,840
let's say that they are, they've developed something that can look at a medical image

190
00:15:23,840 --> 00:15:30,760
and identify some kind of feature in that medical image, fairly successfully on a computer,

191
00:15:30,760 --> 00:15:37,400
and they want to get that onto a phone, and they've trained it using something off the

192
00:15:37,400 --> 00:15:46,960
shelf like a VGG or a ResNet, what are the specific steps that they would take to get

193
00:15:46,960 --> 00:15:55,720
from there to a model that is more efficient running on that phone?

194
00:15:55,720 --> 00:16:01,040
Do they toss that out and start from scratch on something smaller and then optimize that,

195
00:16:01,040 --> 00:16:05,920
or do they kind of play with it in certain kind of ways, walk us through the way that

196
00:16:05,920 --> 00:16:07,840
you should approach this?

197
00:16:07,840 --> 00:16:12,800
I think generally those types of architectures are already quite, well, I'm part of VGG,

198
00:16:12,800 --> 00:16:16,880
of course, but the ResNet architectures are generally already quite efficient for the

199
00:16:16,880 --> 00:16:23,640
task that they're designed for, and the classification, or cement expectation, or object detection.

200
00:16:23,640 --> 00:16:27,560
For those of you who start with compression, just take your model that you've created,

201
00:16:27,560 --> 00:16:31,440
and then there's multiple algorithms that you can apply, things like tensor factorization

202
00:16:31,440 --> 00:16:36,800
algorithms that decompose like one layer into multiple layers, or channel pruning type of

203
00:16:36,800 --> 00:16:41,680
architectures where you're actually removing complete channels from your network, and there's

204
00:16:41,680 --> 00:16:45,600
some libraries out there, right, and a lot of papers on these topics that you can look

205
00:16:45,600 --> 00:16:46,600
into.

206
00:16:46,600 --> 00:16:51,560
So apply these two type algorithms, and you can make your model two or three times smaller

207
00:16:51,560 --> 00:16:52,560
than original.

208
00:16:52,560 --> 00:16:57,160
That's generally what we see for most applications.

209
00:16:57,160 --> 00:17:03,080
You mentioned a couple of these algorithms, tensor factorization, channel pruning, are

210
00:17:03,080 --> 00:17:05,080
there other major ones?

211
00:17:05,080 --> 00:17:07,640
I think those are the large categories.

212
00:17:07,640 --> 00:17:11,400
I mean, it kind of really depends on what you're compressing your network for, right?

213
00:17:11,400 --> 00:17:13,120
Are you compressing it for latency?

214
00:17:13,120 --> 00:17:16,960
Are you compressing it for power consumption reduction, or do you just want a smaller model

215
00:17:16,960 --> 00:17:18,960
to transfer to some other device?

216
00:17:18,960 --> 00:17:23,120
But generally, if you're looking at things like latency or power consumption reduction,

217
00:17:23,120 --> 00:17:26,840
then yeah, those are the major ones for compression.

218
00:17:26,840 --> 00:17:31,520
And afterwards, the quantization step is arguably even more impactful, and then even bigger

219
00:17:31,520 --> 00:17:33,040
numbers in terms of efficiency.

220
00:17:33,040 --> 00:17:38,960
Before we get to quantization, let's stick with compressing here, so you're compressing

221
00:17:38,960 --> 00:17:46,440
for latency or power, and you've got these couple of methods, tensor factorization, and channel

222
00:17:46,440 --> 00:17:53,680
pruning, are they, are these kind of blunt tools that you're just kind of throwing at

223
00:17:53,680 --> 00:17:59,520
your model, like you, you know, go to to GitHub or something and download a tensor factorization

224
00:17:59,520 --> 00:18:05,280
script, and you just run it and, you know, it spits something out, or is this a process

225
00:18:05,280 --> 00:18:09,000
that you're kind of exercising with some degree of care?

226
00:18:09,000 --> 00:18:13,240
Yeah, so one of the interesting things that we're working on is to make all of these

227
00:18:13,240 --> 00:18:17,840
kind of algorithms work very easily out of the box, so that you can just apply it to

228
00:18:17,840 --> 00:18:20,000
any, any type of algorithm, right?

229
00:18:20,000 --> 00:18:23,640
Any type of deep learning network, and then without any efforts actually doing this.

230
00:18:23,640 --> 00:18:29,120
But this is still not completely solved yet, there's not a really a lot of good libraries

231
00:18:29,120 --> 00:18:34,680
that you can just download and use, and then without any hassle compressing your network.

232
00:18:34,680 --> 00:18:39,320
The generally it's still a process of applying these techniques, either factorizing your

233
00:18:39,320 --> 00:18:43,080
rate matrix or removing some channels with some smart algorithm, and then doing a lot

234
00:18:43,080 --> 00:18:46,880
of fine tuning and a lot of handcraft tuning on top of it.

235
00:18:46,880 --> 00:18:50,200
So I wish it was as easy as just downloading a library and applying it, but generally it

236
00:18:50,200 --> 00:18:53,720
still takes a week or a few weeks to get this done properly.

237
00:18:53,720 --> 00:18:59,280
Okay, so you need to understand a little bit about these different techniques in order

238
00:18:59,280 --> 00:19:02,400
to actually apply them at this point.

239
00:19:02,400 --> 00:19:08,240
So maybe walk us through tensor factorization, you know, when you're starting this process,

240
00:19:08,240 --> 00:19:10,280
you know, what is it that you're actually doing?

241
00:19:10,280 --> 00:19:12,680
How do you need to think about the process?

242
00:19:12,680 --> 00:19:14,200
Yeah, sure.

243
00:19:14,200 --> 00:19:18,360
So the tensor factorization is basically a deal of decomposing your weight tensor into something

244
00:19:18,360 --> 00:19:20,080
smaller.

245
00:19:20,080 --> 00:19:24,440
And then in the end, what you end up with is instead of having one layer where you just

246
00:19:24,440 --> 00:19:30,160
do one convolution, for example, or in the case of RNN's one matrix multiplication, is

247
00:19:30,160 --> 00:19:34,680
that you do two smaller convolutions or two smaller matrix multiplication.

248
00:19:34,680 --> 00:19:36,960
That's the general gist of it.

249
00:19:36,960 --> 00:19:37,960
So what do you do?

250
00:19:37,960 --> 00:19:43,880
Well, you take a network, then you have to somehow decide for each of the layers in

251
00:19:43,880 --> 00:19:47,040
your network how much you want to prune them, so how much do you want to reduce them in

252
00:19:47,040 --> 00:19:48,040
size?

253
00:19:48,040 --> 00:19:51,040
There's some smart algorithms for choosing this setting.

254
00:19:51,040 --> 00:19:55,880
And I have to have done that for each of the layers, you compress it, basically by applying

255
00:19:55,880 --> 00:19:56,880
SVD on it.

256
00:19:56,880 --> 00:20:01,280
So SVD is like a very standard algorithm of doing tensor factorization, especially for

257
00:20:01,280 --> 00:20:02,280
convolutional neural networks.

258
00:20:02,280 --> 00:20:05,520
SVD leading singular value decomposition.

259
00:20:05,520 --> 00:20:06,520
Exactly, that's it.

260
00:20:06,520 --> 00:20:07,520
Yeah.

261
00:20:07,520 --> 00:20:11,600
So you basically apply a single value decomposition on it, gets two smaller sub matrices

262
00:20:11,600 --> 00:20:13,840
with a lower rank.

263
00:20:13,840 --> 00:20:18,080
And then you have two layers instead of one, and after you've done that for the whole network,

264
00:20:18,080 --> 00:20:22,680
you fine tune it for multiple epochs to get back to the original accuracy.

265
00:20:22,680 --> 00:20:23,880
That's generally how that's done.

266
00:20:23,880 --> 00:20:24,880
Okay.

267
00:20:24,880 --> 00:20:29,600
And you mentioned there are some algorithms to tell you which of the layers in your network

268
00:20:29,600 --> 00:20:32,280
you want to apply this to.

269
00:20:32,280 --> 00:20:38,240
What are those algorithms and what's the general kind of intuition behind where you want

270
00:20:38,240 --> 00:20:39,240
to apply this?

271
00:20:39,240 --> 00:20:40,240
Yeah.

272
00:20:40,240 --> 00:20:42,480
So I mean, there's a couple of them.

273
00:20:42,480 --> 00:20:47,480
I think one of the most recent ones that's interesting is called automatic model compression

274
00:20:47,480 --> 00:20:51,640
or ANC, which is from Songhound Group at MIT.

275
00:20:51,640 --> 00:20:56,480
And this algorithm basically applies a reinforcement learning algorithm on top of the network.

276
00:20:56,480 --> 00:21:03,240
So it tells you, based on reinforcement learning, every iteration gives you a value, namely

277
00:21:03,240 --> 00:21:06,440
what is the accuracy of your network after compression.

278
00:21:06,440 --> 00:21:09,840
And your reinforcement learning agent optimizes the primary shows for each layer.

279
00:21:09,840 --> 00:21:14,040
So how much am I compressing each layer, such that your accuracy still stays very high.

280
00:21:14,040 --> 00:21:16,520
So that's like one way to do it, for example.

281
00:21:16,520 --> 00:21:22,040
And is it is it deep reinforcement learning or is it more traditional RL?

282
00:21:22,040 --> 00:21:24,040
It's like an extra critic model.

283
00:21:24,040 --> 00:21:25,040
It's not very deep.

284
00:21:25,040 --> 00:21:26,040
Okay.

285
00:21:26,040 --> 00:21:27,040
Got it.

286
00:21:27,040 --> 00:21:28,040
Got it.

287
00:21:28,040 --> 00:21:29,040
Okay.

288
00:21:29,040 --> 00:21:30,040
Pretty standard, but still recent.

289
00:21:30,040 --> 00:21:31,040
Okay.

290
00:21:31,040 --> 00:21:32,040
Yeah.

291
00:21:32,040 --> 00:21:33,040
That's one way of doing that.

292
00:21:33,040 --> 00:21:34,040
Are there others that are worth knowing about?

293
00:21:34,040 --> 00:21:35,040
Yeah.

294
00:21:35,040 --> 00:21:37,680
So one of the things that works very well is just looking at the singular values of your

295
00:21:37,680 --> 00:21:42,000
layers, if you're doing tensor factorization, for example, that also works very well.

296
00:21:42,000 --> 00:21:44,800
You're basically looking at the residual energy.

297
00:21:44,800 --> 00:21:49,240
So I don't know how technical or in depth I should go here, but let's do it.

298
00:21:49,240 --> 00:21:51,680
So let's go deeper.

299
00:21:51,680 --> 00:21:56,200
Well, just look at the singular values, I think that that's as deep as we, we perhaps

300
00:21:56,200 --> 00:21:57,200
should be going.

301
00:21:57,200 --> 00:22:02,520
So the singular values are like a rough indication of the residual after doing the tensor

302
00:22:02,520 --> 00:22:03,520
factorization.

303
00:22:03,520 --> 00:22:08,120
So you're basically looking at the forbidden norm of your rate matrix or even better of

304
00:22:08,120 --> 00:22:11,440
your activation, set of activation.

305
00:22:11,440 --> 00:22:16,760
So generally you fit some data to your network, you look at the output tensor for that, and

306
00:22:16,760 --> 00:22:18,800
then you decompose that.

307
00:22:18,800 --> 00:22:23,720
And so you can look at the residual, basically residual energy, so that the singular values

308
00:22:23,720 --> 00:22:25,600
give you an indication of that.

309
00:22:25,600 --> 00:22:32,840
But now we're applying this to determine which of the layers of our network we want

310
00:22:32,840 --> 00:22:38,800
to apply SVD2, I thought it sounds circular here.

311
00:22:38,800 --> 00:22:43,000
Now it's not really a witch layer, so every layer you probably want to decompose, in

312
00:22:43,000 --> 00:22:49,000
terms of all layers should have some redundancy in them, but some more than others.

313
00:22:49,000 --> 00:22:51,440
Sometimes you have a neural network layer that is very redundant.

314
00:22:51,440 --> 00:22:54,480
It almost didn't learn any useful features.

315
00:22:54,480 --> 00:22:58,400
Sometimes you can even completely cut out layers out of the network, right?

316
00:22:58,400 --> 00:23:02,400
Especially residual networks that happen sometimes, if they're very deep.

317
00:23:02,400 --> 00:23:07,520
So it's really, what you want to do is try to figure out what the prune ratio is for

318
00:23:07,520 --> 00:23:08,520
each of the layers.

319
00:23:08,520 --> 00:23:10,840
So for each layer, how much should I reduce it?

320
00:23:10,840 --> 00:23:15,480
Yeah, and so you're applying this layer by layer, but in order to determine these prune

321
00:23:15,480 --> 00:23:20,960
ratios, you need to be looking across different layers to try to get at what the redundancy

322
00:23:20,960 --> 00:23:27,520
is of a given layer, as opposed to within layers independent of the other layers.

323
00:23:27,520 --> 00:23:28,520
Is that right?

324
00:23:28,520 --> 00:23:29,520
Yeah, sort of.

325
00:23:29,520 --> 00:23:30,520
Yeah.

326
00:23:30,520 --> 00:23:34,440
And sometimes you can look at the layers independently, but it's definitely better to look

327
00:23:34,440 --> 00:23:38,040
somehow at the data distributions of each of the layers.

328
00:23:38,040 --> 00:23:42,080
And you're most better off starting at the start of the network, putting that one first

329
00:23:42,080 --> 00:23:46,560
seeing what effect it has on the whole network, and then doing it iteratively from the start

330
00:23:46,560 --> 00:23:47,560
to the end of the network.

331
00:23:47,560 --> 00:23:48,560
Yeah, okay.

332
00:23:48,560 --> 00:23:49,560
Got it.

333
00:23:49,560 --> 00:23:50,560
Yeah.

334
00:23:50,560 --> 00:23:54,600
Okay, so that's center factorization, and then there's channel pruning.

335
00:23:54,600 --> 00:23:57,200
Yeah, that's a channel pruning is basically the same idea.

336
00:23:57,200 --> 00:24:03,560
So except for you don't do any factorization, would you say, hey, which channels from a

337
00:24:03,560 --> 00:24:06,080
network can I safely remove?

338
00:24:06,080 --> 00:24:09,680
So I mean, every network is going to have some redundant channels.

339
00:24:09,680 --> 00:24:15,720
If you look at a standard rest next architecture from PyTorch, for example, you'll find that

340
00:24:15,720 --> 00:24:20,920
some of the layers in there, you can remove half of the features and not decrease your performance

341
00:24:20,920 --> 00:24:21,920
at all.

342
00:24:21,920 --> 00:24:24,600
So networks generally have a lot of channels that are useless.

343
00:24:24,600 --> 00:24:29,160
And so when we're talking about channels, we're talking about the width of the network essentially

344
00:24:29,160 --> 00:24:30,160
at the demo.

345
00:24:30,160 --> 00:24:31,160
Yeah, yeah, exactly.

346
00:24:31,160 --> 00:24:33,160
Yeah, so input and output channels, right?

347
00:24:33,160 --> 00:24:39,040
So each residual block, for example, has like 64 and 28 channels, and then a lot of

348
00:24:39,040 --> 00:24:42,760
those can be removed without losing any accuracy or your network.

349
00:24:42,760 --> 00:24:50,240
And so what are the kind of practices for determining which of those can be removed?

350
00:24:50,240 --> 00:24:55,480
Is it kind of a most significantly significant ordering, and you can just kind of draw lines

351
00:24:55,480 --> 00:25:00,360
somewhere and see how the network performs, or do you have to look at it on a channel

352
00:25:00,360 --> 00:25:06,240
by channel basis to see what the importance of a given channel is?

353
00:25:06,240 --> 00:25:09,080
Yeah, there's a few different indications.

354
00:25:09,080 --> 00:25:13,560
So you can look at the weight magnitude of each of the, let's say you want to have

355
00:25:13,560 --> 00:25:18,600
prune an input channel and you want to prune, let's say, 10 out of 128, and you can just

356
00:25:18,600 --> 00:25:23,600
find the ones that in terms of magnitude, the weights of the input channels, the lowest

357
00:25:23,600 --> 00:25:29,000
value, you prune those, those are generally the ones that are less, less, less important.

358
00:25:29,000 --> 00:25:34,400
There's some ways of calculating the sensitivity of each of the input channels based on the

359
00:25:34,400 --> 00:25:40,200
Hessian, or like a recent paper called eigendamage, does this based on like a chronic

360
00:25:40,200 --> 00:25:41,200
factorization?

361
00:25:41,200 --> 00:25:42,200
eigendamage, yeah.

362
00:25:42,200 --> 00:25:45,440
That's a great paper name.

363
00:25:45,440 --> 00:25:51,040
Yeah, it's an ICML paper from this year, it's a really cool paper, it's definitely recommended

364
00:25:51,040 --> 00:25:52,040
to read.

365
00:25:52,040 --> 00:25:58,040
So this looks at the sensitivity function, so basically you calculate the Hessian of the

366
00:25:58,040 --> 00:26:01,640
illustration, and it says, okay, which ones are more important or which ones are less

367
00:26:01,640 --> 00:26:02,640
important, right?

368
00:26:02,640 --> 00:26:04,840
And then the least important ones you throw away.

369
00:26:04,840 --> 00:26:11,440
And so you're throwing away these various channels, you made a comment about if you want

370
00:26:11,440 --> 00:26:18,960
to throw away 10 out of your 128, are you generally starting with some idea of the number

371
00:26:18,960 --> 00:26:25,040
of channels you want to throw away, and then kind of testing the, so starting with an

372
00:26:25,040 --> 00:26:30,640
idea of the number, then determining of the, which do you throw away to get to that number

373
00:26:30,640 --> 00:26:35,320
and then testing performance, and then just kind of iterating that loop to determine

374
00:26:35,320 --> 00:26:39,880
what the best number is, or how many you can get away with throwing away.

375
00:26:39,880 --> 00:26:45,600
Yeah, basically like that, or you could take this AMC approach, for example, to automatically

376
00:26:45,600 --> 00:26:49,880
with reinforcement learning, figure out what the prune ratios are of the layer, you can

377
00:26:49,880 --> 00:26:55,200
make it dependent on the sensitivity metric that I just mentioned, there's some different

378
00:26:55,200 --> 00:26:56,200
ways.

379
00:26:56,200 --> 00:26:58,720
So generally, how you do it, as you see now, let's test this out.

380
00:26:58,720 --> 00:27:01,840
I want to compress my model by a factor two, right?

381
00:27:01,840 --> 00:27:06,000
And then the algorithm automatically finds, or finds in some way how to set the prune ratios

382
00:27:06,000 --> 00:27:11,000
for each of the layers, such that I guess to a latency reduction of save effect of two.

383
00:27:11,000 --> 00:27:16,000
And then you take that, and then you remove those channels, make a model smaller, and it's

384
00:27:16,000 --> 00:27:17,320
going to be two times smaller.

385
00:27:17,320 --> 00:27:19,640
Yeah, yeah.

386
00:27:19,640 --> 00:27:24,600
And so you mentioned that we're not quite at the point where, you know, we've got kind

387
00:27:24,600 --> 00:27:33,880
of auto compression for these networks, and yet it sounds like a very kind of iterative

388
00:27:33,880 --> 00:27:38,840
process that we could, you know, apply automatically to networks.

389
00:27:38,840 --> 00:27:42,440
What's really in the way of doing that?

390
00:27:42,440 --> 00:27:44,800
Yeah, that's a great question.

391
00:27:44,800 --> 00:27:51,080
I think just the literature in compression is just very recent and very new.

392
00:27:51,080 --> 00:27:56,920
We have to figure out like a proper relationship between, for example, what type of error do

393
00:27:56,920 --> 00:28:01,040
we introduce in the network when we compress a layer?

394
00:28:01,040 --> 00:28:04,000
And if we look at that error, how does it propagate over the rest of the network, right?

395
00:28:04,000 --> 00:28:07,000
How does it leads to a loss at the end of the network?

396
00:28:07,000 --> 00:28:12,160
Because generally, for example, you can compress a layer and exactly figure out how to compress

397
00:28:12,160 --> 00:28:17,360
the layer and minimize the residual error that you get when doing so.

398
00:28:17,360 --> 00:28:20,920
But sometimes errors are very impactful at the end of the network, right?

399
00:28:20,920 --> 00:28:22,680
And sometimes errors are not.

400
00:28:22,680 --> 00:28:27,000
And trying to figure these things out, I don't think that has been sensibly done yet to

401
00:28:27,000 --> 00:28:29,640
really solve this issue of how to do this automatically.

402
00:28:29,640 --> 00:28:36,640
And so maybe asking you a different way, if, you know, let's say compute wasn't an issue

403
00:28:36,640 --> 00:28:41,440
and obviously it always is and that's a big part of the challenge, but let's imagine

404
00:28:41,440 --> 00:28:45,880
a world where compute was free, you know, it sounds like what we're doing here is just

405
00:28:45,880 --> 00:28:50,960
kind of, you know, a big loop of loop of loops kind of thing, it could be done iteratively

406
00:28:50,960 --> 00:28:53,280
and you could reinforce it.

407
00:28:53,280 --> 00:28:58,160
And so do we have any kind of intuition for, you know, if we started with some kind of

408
00:28:58,160 --> 00:29:05,800
reasonable, you know, reasonable defaults or something for a given class of network,

409
00:29:05,800 --> 00:29:12,240
like what the, you know, the ratio between the brute force compute that we would use versus

410
00:29:12,240 --> 00:29:16,200
the compute we would use if we take a more reasoned approach.

411
00:29:16,200 --> 00:29:21,800
Yeah, yeah, I mean, a completely brute force, which is be a prune every channel, a fine tune

412
00:29:21,800 --> 00:29:24,640
for complete epoch for every pruning that you do.

413
00:29:24,640 --> 00:29:29,160
And yeah, I mean, what skill do you want it? I think you can, I can create an algorithm

414
00:29:29,160 --> 00:29:36,600
for you that's going to take like a hundred million years to do this and or cost you a million

415
00:29:36,600 --> 00:29:37,600
euros.

416
00:29:37,600 --> 00:29:43,840
But hey, if I can have this thing running for a hundred million years and it only costs

417
00:29:43,840 --> 00:29:46,360
me a million euros, that sounds like a pretty good deal.

418
00:29:46,360 --> 00:29:51,720
We're going to do it on a very efficient Qualcomm chip there.

419
00:29:51,720 --> 00:29:56,600
Yeah, I have no idea about this question, sorry.

420
00:29:56,600 --> 00:29:57,600
Yeah, yeah, yeah.

421
00:29:57,600 --> 00:29:58,600
Okay.

422
00:29:58,600 --> 00:29:59,600
No good intuition here.

423
00:29:59,600 --> 00:30:04,720
Yeah, I guess, you know, I'm trying to get at like where the, you know, the intuition

424
00:30:04,720 --> 00:30:13,280
is in this process that we just don't know how to encode versus, you know, what is just

425
00:30:13,280 --> 00:30:19,520
kind of researchers and data scientists applying, you know, wrote lead that the computer

426
00:30:19,520 --> 00:30:24,720
could do itself. And I don't feel like I've fully captured like what the intuitive process

427
00:30:24,720 --> 00:30:29,800
is here that, you know, requires us to manually do this.

428
00:30:29,800 --> 00:30:34,560
Yeah, I also don't know yet to be 100% honest.

429
00:30:34,560 --> 00:30:37,680
So I think parts of this can definitely be be automated.

430
00:30:37,680 --> 00:30:40,640
And they're like a set, for example, this, this aim sees automatic reinforcement learning

431
00:30:40,640 --> 00:30:43,000
based algorithm is marshy automating it, right?

432
00:30:43,000 --> 00:30:44,000
Right.

433
00:30:44,000 --> 00:30:47,320
I'll be it with a reinforcement learning algorithm that that's just trying a lot of things

434
00:30:47,320 --> 00:30:52,920
in a smart way, rather than as having a proper understanding of what it means for a layer

435
00:30:52,920 --> 00:30:58,720
to, to add a certain amount of information to your, to your, to your presentation, for

436
00:30:58,720 --> 00:31:04,800
example, or how much expressive power that doesn't, doesn't neural network layer or like

437
00:31:04,800 --> 00:31:10,600
group of layers need to have to really do the job that it needs to do, right?

438
00:31:10,600 --> 00:31:11,600
Mm-hmm.

439
00:31:11,600 --> 00:31:14,800
These are all very, very good sport questions and deep learning at the moment I feel.

440
00:31:14,800 --> 00:31:15,800
Yeah, yeah.

441
00:31:15,800 --> 00:31:21,920
Well, I feel like deep learning there, you know, even before we get to compression, deep

442
00:31:21,920 --> 00:31:25,680
learning has a lot of unexplored issues and we're, we're managing to figure it out and

443
00:31:25,680 --> 00:31:27,440
automate a lot of it.

444
00:31:27,440 --> 00:31:32,360
Anyway, so those are, that's kind of the compression side of things.

445
00:31:32,360 --> 00:31:38,160
And we, you know, let's, let's walk through quantization in the same way.

446
00:31:38,160 --> 00:31:46,960
So quantization is, what, you gave us a summary earlier, essentially trying to reduce the

447
00:31:46,960 --> 00:31:51,480
number of bits we're using in each of these channels, right, as opposed to removing channels,

448
00:31:51,480 --> 00:31:53,080
now we're moving bits per channels.

449
00:31:53,080 --> 00:31:55,160
Is that a fair way to look at it?

450
00:31:55,160 --> 00:32:01,520
Yeah, in essence, you're, you're reducing the amount of bits of your weights, no network,

451
00:32:01,520 --> 00:32:03,160
and you're doing it for the calculations.

452
00:32:03,160 --> 00:32:08,240
So instead of doing calculations or floating points, 32, you could do your calculations

453
00:32:08,240 --> 00:32:09,640
in int8, right?

454
00:32:09,640 --> 00:32:15,960
And then you're like four times more energy efficient and it comes to memory transfer

455
00:32:15,960 --> 00:32:19,840
and you're going to be 16 times more efficient when it comes to doing the actual calculations

456
00:32:19,840 --> 00:32:20,840
itself.

457
00:32:20,840 --> 00:32:25,760
So just by going from floating point 32, where you're doing all the calculations, if

458
00:32:25,760 --> 00:32:32,400
you have some kind of MAC array, let's eat it, like unscience or science or whatever,

459
00:32:32,400 --> 00:32:35,520
then they're going to build up more efficient, which is one of the most impactful things

460
00:32:35,520 --> 00:32:39,640
that you can do to make a network run more efficiently on devices.

461
00:32:39,640 --> 00:32:41,320
And that might be easier.

462
00:32:41,320 --> 00:32:42,320
Okay, yeah.

463
00:32:42,320 --> 00:32:50,120
And at my sense of where things are with regard to quantization is that a few years ago,

464
00:32:50,120 --> 00:32:58,960
when I talked to folks, I think my interview with Shibos and Gupta, a very early one,

465
00:32:58,960 --> 00:33:06,440
he was talking about his experiences at Baidu building their neural machine translation

466
00:33:06,440 --> 00:33:13,280
system and they were experimenting with quantization and it was a very manual process, right?

467
00:33:13,280 --> 00:33:18,280
They had to spend a lot of time figuring out where in their networks, they wanted to

468
00:33:18,280 --> 00:33:23,760
apply quantization, how to do it without introducing too much noise, all that kind of thing.

469
00:33:23,760 --> 00:33:29,800
My impression is that over the span of a couple of years, we've come a long way from

470
00:33:29,800 --> 00:33:41,000
that and a lot of cases, the infrastructure and the libraries, on top of that infrastructure,

471
00:33:41,000 --> 00:33:46,560
TensorFlow, for example, gives us some of that stuff for free.

472
00:33:46,560 --> 00:33:53,920
We can flip a bit and quantize our network and achieve reasonable performance, is that

473
00:33:53,920 --> 00:33:54,920
the case?

474
00:33:54,920 --> 00:34:01,720
And if so, in a qualified way, what's the envelope around that qualification?

475
00:34:01,720 --> 00:34:02,720
What does that work?

476
00:34:02,720 --> 00:34:03,720
When doesn't it work?

477
00:34:03,720 --> 00:34:09,520
Yeah, so that's actually a very, very interesting question, something that we've been really deeply

478
00:34:09,520 --> 00:34:10,520
looking into.

479
00:34:10,520 --> 00:34:13,880
Yes, some networks can be very easily quantized.

480
00:34:13,880 --> 00:34:20,040
If you pick almost all of the ImageNet train networks, even though ones that are like

481
00:34:20,040 --> 00:34:24,360
an okay detection or semantic segmentation on top of the base network, many of those

482
00:34:24,360 --> 00:34:28,680
can just be a bit quantized with your standard TensorFlow tools, sometimes you have to find

483
00:34:28,680 --> 00:34:32,440
you in a little bit with your operations in place and train the network, but generally

484
00:34:32,440 --> 00:34:34,280
those, those are okay.

485
00:34:34,280 --> 00:34:37,440
And some networks seem to have a lot of issues.

486
00:34:37,440 --> 00:34:41,520
You can look at mobile nets, for example, so most of the mobile nets out there, like

487
00:34:41,520 --> 00:34:46,640
mobile nets V2, even if V3 version, if you just not easily quantize it with the TensorFlow

488
00:34:46,640 --> 00:34:48,800
operations, everything breaks.

489
00:34:48,800 --> 00:34:52,280
You could like 0% accuracy.

490
00:34:52,280 --> 00:34:57,800
And sometimes, so we've also looked at things like ImageToImageNetworks, networks that

491
00:34:57,800 --> 00:35:04,640
do, for example, like things like style, transfer, or super resolution, or have these

492
00:35:04,640 --> 00:35:05,640
types of networks.

493
00:35:05,640 --> 00:35:10,240
And then sometimes in network, these networks, you can't just eat a bit quantize, no problem

494
00:35:10,240 --> 00:35:11,240
with all.

495
00:35:11,240 --> 00:35:15,240
And some networks don't, at all.

496
00:35:15,240 --> 00:35:18,680
And it's very difficult to put our finger on why exactly that is.

497
00:35:18,680 --> 00:35:26,040
And so do we have any kind of intuition at all, or are we still, you know, we recognize

498
00:35:26,040 --> 00:35:32,280
that there's this distinction between the different types of networks, but really have no idea

499
00:35:32,280 --> 00:35:34,240
what or why it's happening.

500
00:35:34,240 --> 00:35:40,120
Yeah, so in our recent paper called Data Frequentization, we've investigated a lot of these issues

501
00:35:40,120 --> 00:35:44,240
on what, why some networks are very difficult to quantize.

502
00:35:44,240 --> 00:35:49,160
One of the biggest issues generally has to do with the ranges that you send.

503
00:35:49,160 --> 00:35:53,480
So you have to create some kind of quantization grids where you say, these are the values

504
00:35:53,480 --> 00:35:57,680
I'm going to represent with the 8 bits that I have, let's say.

505
00:35:57,680 --> 00:36:01,360
Now for this grid, you have to choose like a minimum and a maximum, right?

506
00:36:01,360 --> 00:36:05,680
And generally everything, all of the points in between are equidistantly spaced, and those

507
00:36:05,680 --> 00:36:10,080
are the values that you can represent, both for your weight center activations.

508
00:36:10,080 --> 00:36:16,200
Now, how you choose that range, the minimum and the maximum is incredibly important.

509
00:36:16,200 --> 00:36:20,920
For example, you could have some outlier with weights, and those weights then determine

510
00:36:20,920 --> 00:36:24,600
what your minimum max could be, if you just set your minimum max of the quantization

511
00:36:24,600 --> 00:36:28,960
range to the exact minimum of your weight tensor, there could be one big outlier somewhere,

512
00:36:28,960 --> 00:36:32,200
and that could ruin the representative power for the rest of your weights.

513
00:36:32,200 --> 00:36:37,160
So a lot of, you can only guy with a few points that are representable in the weights that

514
00:36:37,160 --> 00:36:41,960
are small, and then the big one is taking a lot of the bandwidth.

515
00:36:41,960 --> 00:36:46,880
So a lot of issues in quantization come from that, the fact that you set your ranges

516
00:36:46,880 --> 00:36:49,720
in properly.

517
00:36:49,720 --> 00:36:54,600
And the second one is that while you're quantizing, is that you can sometimes quantize

518
00:36:54,600 --> 00:36:57,120
in a not-so-smart way.

519
00:36:57,120 --> 00:37:01,040
For example, you could have a bias in your quantization, especially in mobile nets, if you have

520
00:37:01,040 --> 00:37:06,040
like a three by three filter that maps an input to the output.

521
00:37:06,040 --> 00:37:11,560
So by accident, you quantize all the values of your three by three filter down, then your

522
00:37:11,560 --> 00:37:15,480
average output of that channel is going to be lower, and you're going to incur a lot

523
00:37:15,480 --> 00:37:19,480
more loss than if everything was like randomly flipped.

524
00:37:19,480 --> 00:37:22,360
So those are like two major issues when it comes to quantization.

525
00:37:22,360 --> 00:37:27,440
Does the data free quantization paper that you referenced, does it propose an approach

526
00:37:27,440 --> 00:37:29,320
for addressing these issues?

527
00:37:29,320 --> 00:37:30,320
Yeah, definitely.

528
00:37:30,320 --> 00:37:34,080
Yeah, so there's multiple things in there that we've looked into.

529
00:37:34,080 --> 00:37:40,800
Basically, the data free quantization paper was born out of this idea that many methods

530
00:37:40,800 --> 00:37:44,960
that help with quantization require a lot of effort, right?

531
00:37:44,960 --> 00:37:49,400
So generally, how this works is that somebody tells you, okay, change this neural network

532
00:37:49,400 --> 00:37:53,040
architecture to this architecture, and then it's easier to quantize.

533
00:37:53,040 --> 00:37:57,200
Or do a lot of fine tuning on top of your architecture and a lot of training with a lot

534
00:37:57,200 --> 00:37:59,760
of hyperparameters, et cetera.

535
00:37:59,760 --> 00:38:05,560
Now if you want to make sure that like 8-bit networks can be run everywhere, it's very important

536
00:38:05,560 --> 00:38:07,640
that this is as hassle-free as possible, right?

537
00:38:07,640 --> 00:38:12,080
If I'm a cloud provider and I want to run my customers' models in 8 bits, I don't want

538
00:38:12,080 --> 00:38:15,920
to require them that they're like, they got their PhDs in quantization just so that they

539
00:38:15,920 --> 00:38:16,920
can work more efficient models.

540
00:38:16,920 --> 00:38:21,520
You just want to make something that's basically you snap your finger and then have your

541
00:38:21,520 --> 00:38:23,320
networks gone, right?

542
00:38:23,320 --> 00:38:25,480
And it's like two times smaller.

543
00:38:25,480 --> 00:38:29,080
So that's kind of the idea of the paper, and then we said like, what can we do to make

544
00:38:29,080 --> 00:38:30,080
that even further, right?

545
00:38:30,080 --> 00:38:32,400
Can we do this without using any data at all?

546
00:38:32,400 --> 00:38:36,800
So many methods use data to set the ranges, et cetera.

547
00:38:36,800 --> 00:38:40,240
And then the idea of our paper is like, okay, what can we do to make sure that this is

548
00:38:40,240 --> 00:38:41,240
as easy as possible?

549
00:38:41,240 --> 00:38:44,800
Because you don't even have to give data to the algorithm to quantize it.

550
00:38:44,800 --> 00:38:50,400
And when you say give data to it, meaning the quantization happens as part of a training

551
00:38:50,400 --> 00:38:54,800
loop or process, or is there some other process that is being applied?

552
00:38:54,800 --> 00:38:56,640
Yeah, it's very often that, right?

553
00:38:56,640 --> 00:39:00,440
So you take like a hundred batches, you run them through the network, you get some idea

554
00:39:00,440 --> 00:39:04,880
of the activation ranges, right, that you get, that you get, and the activation ranges

555
00:39:04,880 --> 00:39:10,040
help you inform what your mini or max setting is for the tenser that you're quantizing.

556
00:39:10,040 --> 00:39:11,640
It's basically that idea.

557
00:39:11,640 --> 00:39:15,160
Yeah, so give a couple of hundred batches to the algorithm to figure out those ranges.

558
00:39:15,160 --> 00:39:17,320
Or to even find you in a little bit, right?

559
00:39:17,320 --> 00:39:22,520
And so this paper proposes a way to allow you to skip that.

560
00:39:22,520 --> 00:39:27,320
And avoid an approach that requires that you do this with regard to data.

561
00:39:27,320 --> 00:39:29,120
Exactly, yeah.

562
00:39:29,120 --> 00:39:32,880
And so the complete data free part is more of like a mental exercise.

563
00:39:32,880 --> 00:39:36,720
It's very interesting for the, for, for, for like a paper, and we were able to get really

564
00:39:36,720 --> 00:39:40,920
good results like state-of-the-art results on compressing things like mobile nets or some

565
00:39:40,920 --> 00:39:45,320
object detection networks with even that hardware strength in place.

566
00:39:45,320 --> 00:39:50,040
Now in practice, you can often just use a couple of batches and improve, improve your quantization

567
00:39:50,040 --> 00:39:51,040
performance.

568
00:39:51,040 --> 00:39:56,720
But basically the algorithms we, we put in the paper can be used for either case.

569
00:39:56,720 --> 00:39:59,640
The most important part is that you don't have to do any fine tuning.

570
00:39:59,640 --> 00:40:04,480
So this is the difference between like getting a network quantized within an hour or having

571
00:40:04,480 --> 00:40:08,680
to spend like a couple of weeks with like an expert in the field to make sure that your

572
00:40:08,680 --> 00:40:11,400
algorithms quantize properly.

573
00:40:11,400 --> 00:40:13,360
And so how do the algorithms work?

574
00:40:13,360 --> 00:40:18,560
Is it multiple algorithms or is there one algorithm?

575
00:40:18,560 --> 00:40:24,360
Yeah, there's a couple that that we kind of stitch together to form like a complete quantization

576
00:40:24,360 --> 00:40:25,360
pipeline.

577
00:40:25,360 --> 00:40:30,720
So for this paper, one of the things we do is equalization.

578
00:40:30,720 --> 00:40:33,400
So remember, remember these ranges that I talked about, right?

579
00:40:33,400 --> 00:40:36,200
That sometimes these outliers can cause some of the issues.

580
00:40:36,200 --> 00:40:37,200
Right.

581
00:40:37,200 --> 00:40:42,520
Now, for networks that are Raylu or Kraylu-based, what you can do, you can actually move scaling

582
00:40:42,520 --> 00:40:44,080
factors around the network.

583
00:40:44,080 --> 00:40:49,280
So for layer one, for layer one, if you have, if you look at the output, you can multiply

584
00:40:49,280 --> 00:40:51,200
that with a certain scaling factor.

585
00:40:51,200 --> 00:40:55,120
And then for the next layer, which is the, which takes the, as input, the output of a previous

586
00:40:55,120 --> 00:40:59,400
layer, you can multiply it with one divided by that scaling factor and you get mathematically

587
00:40:59,400 --> 00:41:00,920
exactly the same network.

588
00:41:00,920 --> 00:41:06,400
And by moving these scaling factors around, we can make the general min-max ranges.

589
00:41:06,400 --> 00:41:10,800
So these, these big ranges, we can make them smaller, such that the network is easier

590
00:41:10,800 --> 00:41:11,800
to quantize.

591
00:41:11,800 --> 00:41:20,800
And so these min-max ranges, are you applying them on a, a tensor by tensor basis, or is it

592
00:41:20,800 --> 00:41:22,520
across the entire network?

593
00:41:22,520 --> 00:41:23,520
Yeah.

594
00:41:23,520 --> 00:41:26,640
So there's multiple ways of doing this.

595
00:41:26,640 --> 00:41:30,880
So depending on the hardware that you're looking at, you can do it in different ways.

596
00:41:30,880 --> 00:41:34,720
But generally, there's two major categories that you do for quantization.

597
00:41:34,720 --> 00:41:36,360
One is that you do it per tensor.

598
00:41:36,360 --> 00:41:40,640
So each layer has its own set of min-max ranges.

599
00:41:40,640 --> 00:41:44,760
So that that in turn translates to its own set of, offset vectors when you do all the

600
00:41:44,760 --> 00:41:45,760
calculations.

601
00:41:45,760 --> 00:41:51,520
But setting that aside, you can do it per tensor or something that was more recently introduced

602
00:41:51,520 --> 00:41:55,120
by a guy called Ragooh-Krishnamurti.

603
00:41:55,120 --> 00:42:00,280
And he introduced, sorry, he's from Google, he introduced per channel quantization.

604
00:42:00,280 --> 00:42:03,280
So each individual channel has its own min-max range.

605
00:42:03,280 --> 00:42:04,800
And that's all some of the issues.

606
00:42:04,800 --> 00:42:06,280
Okay.

607
00:42:06,280 --> 00:42:11,720
So presumably requiring a more complex training process because you have a lot more of these

608
00:42:11,720 --> 00:42:15,880
offsets to keep track of.

609
00:42:15,880 --> 00:42:20,800
I think the larger drawback of it, is that not all hardware supports it?

610
00:42:20,800 --> 00:42:25,200
So you have to make sure that your hardware supports, and the kernels for that as well,

611
00:42:25,200 --> 00:42:29,840
supports the calculation of per channel min-max ranges.

612
00:42:29,840 --> 00:42:31,640
And not all hardware does that.

613
00:42:31,640 --> 00:42:32,640
Okay.

614
00:42:32,640 --> 00:42:38,960
So that dependent on hardware as opposed to the training process or something else that

615
00:42:38,960 --> 00:42:42,800
can be taking care of and software.

616
00:42:42,800 --> 00:42:47,040
I mean, if you run things on like a CPU, of course, you can see me at everything on the

617
00:42:47,040 --> 00:42:48,560
CPU and we're in a efficient lane.

618
00:42:48,560 --> 00:42:54,600
But if you have like dedicated hardware for inference, you're doing low-level operations

619
00:42:54,600 --> 00:43:02,400
and pushing everything down to those low-level hardware modules for speed, then you're

620
00:43:02,400 --> 00:43:04,880
kind of stuck with what they're able to do.

621
00:43:04,880 --> 00:43:05,880
Exactly.

622
00:43:05,880 --> 00:43:06,880
Yes.

623
00:43:06,880 --> 00:43:07,880
Okay.

624
00:43:07,880 --> 00:43:08,880
So that's one possibility.

625
00:43:08,880 --> 00:43:14,240
And another thing we do in the paper is that we correct for this bias that I mentioned

626
00:43:14,240 --> 00:43:15,240
before.

627
00:43:15,240 --> 00:43:20,800
So sometimes because of rounding issues, you have to choose either rounding up or am I rounding

628
00:43:20,800 --> 00:43:24,680
down, each of the weight factors.

629
00:43:24,680 --> 00:43:29,080
You can kind of correct for some bias that you might introduce in doing that.

630
00:43:29,080 --> 00:43:32,120
And that's an algorithm we dubbed bias correction.

631
00:43:32,120 --> 00:43:38,200
Those two together really make most of the networks that we've tried make them very easy

632
00:43:38,200 --> 00:43:40,200
to quantize.

633
00:43:40,200 --> 00:43:41,200
Okay.

634
00:43:41,200 --> 00:43:50,160
And are there any kind of dependencies on the training approach you're taking in terms

635
00:43:50,160 --> 00:43:56,760
of, you know, things like, you know, whether you're applying dropout or like you're learning

636
00:43:56,760 --> 00:44:00,800
rate, you know, if you're doing cyclical learning rates or anything like that, or is

637
00:44:00,800 --> 00:44:03,560
this all independent of the way you train your network?

638
00:44:03,560 --> 00:44:04,560
Yeah.

639
00:44:04,560 --> 00:44:05,760
I think it's completely independent.

640
00:44:05,760 --> 00:44:09,280
So this approach is a state of free quantization approach.

641
00:44:09,280 --> 00:44:12,720
In essence, you can just take any network.

642
00:44:12,720 --> 00:44:16,360
You can equalize it if there's like really impregnated in it.

643
00:44:16,360 --> 00:44:19,880
And then the bias correction you can always apply.

644
00:44:19,880 --> 00:44:21,880
And that should give you better quantization performance.

645
00:44:21,880 --> 00:44:23,200
And we do it without any fine tuning.

646
00:44:23,200 --> 00:44:24,480
So there's no hyperparameters.

647
00:44:24,480 --> 00:44:27,120
You just, it's like a simple API call.

648
00:44:27,120 --> 00:44:29,200
You just throw your model to like an API call.

649
00:44:29,200 --> 00:44:32,520
You optimize it for quantization and then you get a quantized network.

650
00:44:32,520 --> 00:44:37,520
It is hardware independent or are there dependencies on the hardware with this method?

651
00:44:37,520 --> 00:44:39,480
Nope, not at all.

652
00:44:39,480 --> 00:44:42,160
So it's all for a tensor quantization that we do in a paper.

653
00:44:42,160 --> 00:44:46,360
So that's implemented in every hardware that I know of.

654
00:44:46,360 --> 00:44:51,400
So yeah, that's that's optimizes it for any any architecture.

655
00:44:51,400 --> 00:44:57,240
And so what kind of results did you see relative to other ways that you might quantize?

656
00:44:57,240 --> 00:45:02,120
Yeah, so one of the problems that you have, for example, with mobile SV2 is that you

657
00:45:02,120 --> 00:45:08,080
actually get no performance after naive quantization with like T of light, for example.

658
00:45:08,080 --> 00:45:12,160
And we gain in our paper almost all of the performance.

659
00:45:12,160 --> 00:45:16,720
So usually go from like 71.7% down to zero.

660
00:45:16,720 --> 00:45:17,720
We go to 71.2%.

661
00:45:17,720 --> 00:45:25,040
So that's only like half the percent loss when going from flow 32 to int 8.

662
00:45:25,040 --> 00:45:29,360
And that's of course like going from flow 32 to int 8 is tremendous gain in terms of power

663
00:45:29,360 --> 00:45:32,520
efficiency and in terms of latency.

664
00:45:32,520 --> 00:45:42,960
And to what degree it is that result generalize to other models or is it just you know, just

665
00:45:42,960 --> 00:45:49,560
working for mobile net V2, you know, sufficiently applicable that you think it's important.

666
00:45:49,560 --> 00:45:54,840
I think any so we we we've also not papers in gains and residual net architectures.

667
00:45:54,840 --> 00:46:00,320
We've seen it on multiple tasks like object detection, semantic segmentation.

668
00:46:00,320 --> 00:46:06,200
So we really because when you're doing equalization, your network stays mathematically equivalent.

669
00:46:06,200 --> 00:46:08,400
So there's definitely no loss.

670
00:46:08,400 --> 00:46:11,600
And it can only help for quantization.

671
00:46:11,600 --> 00:46:15,760
Similarly for for the bias correction, there's no negative drawbacks to this.

672
00:46:15,760 --> 00:46:21,160
So in most settings, it will help you rather than there's like no real probability that

673
00:46:21,160 --> 00:46:22,160
it's going to hurt you.

674
00:46:22,160 --> 00:46:26,600
And so you can always test it and I think in many of the cases we've seen this far, this

675
00:46:26,600 --> 00:46:27,600
re-helps a lot.

676
00:46:27,600 --> 00:46:28,600
Awesome.

677
00:46:28,600 --> 00:46:33,240
So, you know, given this paper, which is relatively recently and some of the other things that

678
00:46:33,240 --> 00:46:37,840
we've discussed, you know, in terms of where the field is overall, what are you most excited

679
00:46:37,840 --> 00:46:38,840
about?

680
00:46:38,840 --> 00:46:41,040
Where do you see this all going?

681
00:46:41,040 --> 00:46:43,920
The whole model efficiency at point you think, you mean?

682
00:46:43,920 --> 00:46:48,320
Yeah, yeah, just in terms of our ability to, you know, take these models which are getting

683
00:46:48,320 --> 00:46:54,280
bigger and bigger and have them work on devices which we want to last longer and longer from

684
00:46:54,280 --> 00:46:58,560
the battery perspective and have user experiences that are faster and faster.

685
00:46:58,560 --> 00:47:03,680
Yeah, I think all of this stuff on model efficiency is like key for the adaptation of neural

686
00:47:03,680 --> 00:47:07,560
networks in many practical applications, right?

687
00:47:07,560 --> 00:47:13,240
I think on the cell phone, we're always bound by power and amount of power that you can use.

688
00:47:13,240 --> 00:47:15,840
Nobody wants to have their battery ranges in half an hour, right?

689
00:47:15,840 --> 00:47:23,600
We all expect our phones to last the whole day and the same goes for robotics and even

690
00:47:23,600 --> 00:47:30,120
for cloud players, if you could reduce your amount of energy that you're using by factored

691
00:47:30,120 --> 00:47:33,880
too, that would be a tremendous decrease in the energy bill that you're paying.

692
00:47:33,880 --> 00:47:40,560
So, I think a lot of our performance of algorithms is very restrained by, I mean, in practical

693
00:47:40,560 --> 00:47:43,880
settings, it's constrained by how efficient our algorithms are.

694
00:47:43,880 --> 00:47:47,440
So, the more efficient we can make our algorithms, from a compression point of view and a

695
00:47:47,440 --> 00:47:51,640
quantization point of view, the higher the adaptation of these algorithms will be.

696
00:47:51,640 --> 00:47:56,840
We all know these numbers also about how the training and network, for example, how training

697
00:47:56,840 --> 00:48:03,200
and network is like neural architecture search costs like the same CO2 output of 10 jumbo

698
00:48:03,200 --> 00:48:07,680
jets flying back and forth, like 10 or two trips or something like that.

699
00:48:07,680 --> 00:48:12,080
If you can do quantized training, like training in 8 bits, we can reduce that power consumption

700
00:48:12,080 --> 00:48:13,080
by a factor 4.

701
00:48:13,080 --> 00:48:17,680
So, I think all these things are crucial for the adaptation of deep learning.

702
00:48:17,680 --> 00:48:23,000
And are there specific directions that you're excited about, either in your research or

703
00:48:23,000 --> 00:48:28,320
in the research that other folks are doing that you think are particularly promising in

704
00:48:28,320 --> 00:48:29,840
helping us get there?

705
00:48:29,840 --> 00:48:30,840
Yeah, definitely.

706
00:48:30,840 --> 00:48:35,720
One of the important trends I've been seeing in the compression literature, some things

707
00:48:35,720 --> 00:48:45,440
from Songhands lab or Vivian says lab at MIT, is coupling the hardware and the compression

708
00:48:45,440 --> 00:48:48,360
and quantization and model efficiency together.

709
00:48:48,360 --> 00:48:54,360
So that you can design algorithms, like deep learning algorithms specifically for hardware.

710
00:48:54,360 --> 00:48:59,000
And then you can totally different results if you optimize for the CPU versus if you optimize

711
00:48:59,000 --> 00:49:03,200
them for your GPU versus if you optimize them for like a DSP and a quantum device, for

712
00:49:03,200 --> 00:49:04,200
example.

713
00:49:04,200 --> 00:49:11,520
So, if you're looking at a CPU algorithm, because all of the calculations are that sequentially,

714
00:49:11,520 --> 00:49:15,520
these architectures are usually very deep and have like three by three convolutions.

715
00:49:15,520 --> 00:49:20,880
But if you create an algorithm that's optimized for the GPU, you generally get larger, larger

716
00:49:20,880 --> 00:49:24,680
layers, like five by five, seven by seven convolutions, for example.

717
00:49:24,680 --> 00:49:29,360
So, I think it's a really interesting trend where given the hardware that you're going

718
00:49:29,360 --> 00:49:32,520
to run it on, can we optimize the algorithm to run efficiently on that?

719
00:49:32,520 --> 00:49:34,360
I think that's really cool.

720
00:49:34,360 --> 00:49:35,360
Cool.

721
00:49:35,360 --> 00:49:39,640
So, it's a better marriage between the hardware and the algorithms that we have.

722
00:49:39,640 --> 00:49:40,640
Uh-huh.

723
00:49:40,640 --> 00:49:48,400
And hopefully, ultimately, this gets us to a point where as a model developer, I have

724
00:49:48,400 --> 00:49:52,640
to think less about, you know, these kinds of issues and it all just kind of works for

725
00:49:52,640 --> 00:49:53,640
me.

726
00:49:53,640 --> 00:49:58,000
Yeah, especially for quantization, I think that's very, very much in reach.

727
00:49:58,000 --> 00:50:02,400
So, some of my vision is that literally every neural network architecture can be run

728
00:50:02,400 --> 00:50:03,400
in 8 bits.

729
00:50:03,400 --> 00:50:04,400
Uh-huh.

730
00:50:04,400 --> 00:50:05,400
I think that that's very possible.

731
00:50:05,400 --> 00:50:06,400
Uh-huh.

732
00:50:06,400 --> 00:50:11,320
And currently, we're seeing almost all algorithms are still being run in 16 bits, right?

733
00:50:11,320 --> 00:50:12,320
Uh-huh.

734
00:50:12,320 --> 00:50:16,120
So, getting that to 8 bits would be like two times more power efficient than many cases.

735
00:50:16,120 --> 00:50:19,440
And then we can run a lot more AI in our phones.

736
00:50:19,440 --> 00:50:20,440
Awesome.

737
00:50:20,440 --> 00:50:21,440
Awesome.

738
00:50:21,440 --> 00:50:25,440
Well, Simon, thank you so much for taking the time to chat with us about what you're

739
00:50:25,440 --> 00:50:26,440
up to.

740
00:50:26,440 --> 00:50:27,440
Yeah.

741
00:50:27,440 --> 00:50:28,440
Thanks for having me.

742
00:50:28,440 --> 00:50:30,720
It was great explaining all the compression and quantization stuff we've been working

743
00:50:30,720 --> 00:50:31,720
on.

744
00:50:31,720 --> 00:50:32,720
Absolutely.

745
00:50:32,720 --> 00:50:33,720
Absolutely.

746
00:50:33,720 --> 00:50:34,720
Thank you.

747
00:50:34,720 --> 00:50:35,720
All right, everyone.

748
00:50:35,720 --> 00:50:38,600
That's our show for today.

749
00:50:38,600 --> 00:50:43,120
If you like what you've heard, please do us a favor and tell your friends about the show.

750
00:50:43,120 --> 00:50:47,640
And if you haven't already hit that subscribe button yourself, make sure you do so you don't

751
00:50:47,640 --> 00:50:50,840
miss any of the great episodes we've got in store for you.

752
00:50:50,840 --> 00:50:54,600
Thanks once again to Qualcomm for their sponsorship of today's episode.

753
00:50:54,600 --> 00:50:58,880
Check them out at twomolei.com slash Qualcomm.

754
00:50:58,880 --> 00:51:02,160
As always, thanks so much for listening and catch you next time.

