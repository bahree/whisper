Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
This is Sam.
A quick reminder that we've got a bunch of newly formed or forming study groups, including
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders
part one courses.
It's not too late to join us, which you can do by visiting TumelAI.com slash community.
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.
If you're at either event, please reach out.
I'd love to connect.
Alright, this week on the podcast I'm excited to share a series of shows recorded in
Orlando during the Microsoft Ignite Conference.
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship
of this series.
Thanks to decades of breakthrough research and technology, Microsoft is making AI real
for businesses with Azure AI, a set of services that span vision, speech, language processing,
custom machine learning, and more.
Millions of developers and data scientists around the world are using Azure AI to build
innovative applications and machine learning models for their organizations, including
85% of the Fortune 100.
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven
enterprise grade capabilities and innovations, wide range of developer tools and services
and trusted approach.
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS
and DevOps professionals across all skill levels to increase productivity, operationalize
models at scale, and innovate faster and more responsibly with Azure machine learning.
Learn more at aka.ms slash Azure ML.
Alright, onto the show.
Alright, everyone.
I am here in sunny Orlando, Florida at Microsoft Ignite.
I've got the pleasure of being seated across from Jordan Edwards.
Jordan is a principal program manager for the Azure machine learning platform.
Jordan, welcome to the Twinwall AI podcast.
Oh, thanks.
So, I'm really looking forward to talking with you about our subject for the day, MLOPS
and related topics, but before we do that, I'd love to hear a little bit about your background.
It sounds like you got started off at Microsoft where a bunch of folks that are now working
on ML and AI, got started in the Bing group.
Right.
So, I started Microsoft a little over seven years ago, started off working on the big data
platforms and related machine learning platforms, then ended up working on engineering systems
for those platforms, then we decided to take those engineering systems and apply them to
machine learning as well, hence the internal machine learning platform was born.
As you mentioned, like a bunch of other folks who used to work on Bing, we all got moved
into, hey, let's take the cool stuff we built for Bing's internal engineering platform
and bring it to external customers on Azure.
And so, I've been in the Azure machine learning team a little bit over a year now.
Nice.
Nice.
And your role here on the team?
Yes.
I'm the product area lead for what we call MLOPS, which is really all about how do you bring
your machine learning workflows to production?
A topic that we've spent a lot of time talking about here on the podcast, as well as our
recent Twomelcon AI Platforms event, you know, maybe starting, kind of directly connecting
to your background, I'm curious the transition from, you know, a team that largely came
out of this internal product or project Bing and is now trying to generalize those, you
know, systems, but broader knowledge and learnings to the market, kind of, what are the commonalities
and differences, I guess, that you encounter in trying to do that?
So there's actually a lot of commonalities when you double click on it, but the biggest
thing is, you know, Bing and Office 365 internal Microsoft teams have been doing AI and ML
for a long time.
And so they built up a lot of habits and tools and technologies, but also a lot of things
that don't necessarily map to how we see enterprises getting started, right?
So there's the, you know, most of our external customers today are coming in wanting to
do Python-based development, and we have some of that internally, but we also have, you
know, languages that predate the popularity of Python as a data science platform.
So we have, you know, engineers doing machine learning work in.NET and C++, and so those
workflows are a bit different.
Also a lot of the machine learning platforms at Microsoft, as you would imagine, were previously
Windows-based, whereas the new customers coming in wanted to do things using Linux and containers
and their newer techniques that are being applied as well.
So there's similarities in there, the ways they want to solve the problem, but just different
tools that they're using, and also just different amounts of context that it builds up.
There's also the matter of scale.
So when you look at teams like Bing, they've got, you know, a thousand data scientists
that are collaborating together to train these huge models.
Most of the enterprise customers that we're talking to, they have small teams scattered
all over the place, or they're trying to staff a team, or they have a team, and they're
not sure how to make best use of their time.
Also the most common problem that we're seeing that they come to us with is, hey, we have
all these data scientists who are doing work in Jupyter notebooks, whatever, the work
is happening on their local machines.
We have no idea where the code is, if the code's even checked in, and they're doing all
this work, but we can't leverage any of it on the business side.
There's so many, so many problems in that problem statement, right?
There's kind of a reproducibility problem.
There's a business value path to production problem.
There's kind of an accountability problem.
Can you unpack that?
How do you do prioritize those?
We try to put it in terms of a process maturity model.
It's exactly how you framed it.
There's the reproducibility of the work.
Another data scientist and the team could reproduce the same work that one person did, and
then an automated system could also reproduce that work, which means you need clean modeling
around the code and data and configuration that you're using in your model development
process.
Then there's the, how do you transition this model, this thing you've created, to production,
so how do you package it, how do you certify it, and how do you roll it out in a controlled
fashion, and then at the end, how do you determine the business value of your model?
Is it making your business more effective?
From a cost point of view, is it worth the amount of compute hours you're spending and the
amount of man hours you're spending training these models?
Then on the absolute end of the process maturity model is, okay, I've got this
model.
It's reproducible.
I've got it deployed out.
I'm using it for a production scenario.
How do I know when I might need to retrain it, so completing the circle, and that's always
the question that customers will come and start with is, how do we do automated retrain?
It's like, let's walk back and begin it.
How do you reproduce these models in the first place?
That strikes me as a mature customer that's asking about automated retraining, right?
Correct.
Those people are trying to get the model into production in the first place, or many.
They see the marketing hype where they read all the things like, oh, look at this company
doing cool automated retraining stuff, and realistically, it takes a long time to get
to that degree of maturity where you can trust that you have the high-quality data coming
into your production systems to be able to analyze and compare and figure out I do need
to retrain.
Even in the case of being your office ML development teams, there's never a fully automated
retraining loop. It's always there's a scorecard that gets generated in humans go and do
some sort of review process prior to your new larger models going out, especially when
they deal with things like how do you monetize ads, for instance?
There's a lot there to dig into, but before we do that, one of the questions that I had
for you is you've got ML ops in your title.
What does that mean to you?
That means to me that it's all about how do you take the work that data scientists are
doing and make their lives easier, but also make it easier for others, other personas
to come into the fold and take advantage of data science.
The three personas I like to talk about is you have your data engineer who's got this
giant lake of data. They want to figure out what value they can derive from it.
You've got your data scientist who's tasked with finding interesting features in that data
and training models on top.
You've got this new emerging persona called the ML engineer whose responsibility it is
to take the work the data scientist is doing and bring it to production.
My job is to help the ML engineer be able to be successful and help the ML engineer be
able to interact well with the data engineering and data science personas that are required
to complete that circle.
Of course, you also have the hub in the center of it, your IT ops persona, who's giving
them all of the raw compute and storage resources to get started, making sure everybody plays
nicely together and actually connects things and to end.
There's an obvious echo to DevOps, to what extent is it inspirational, is it directly
applicable, or is it counter applicable, meaning just don't try to do exactly what you're
doing in DevOps because that's the...
I think it's sort of all three of the things that you mentioned shocking, right, to me up.
As far as how it's inspirational, definitely the practices that have been developed in
the DevOps field over the past 20 years or so are useful.
However, data scientists are not software engineers and they're not even engineers.
A lot of them are scientists, so talking to them they need to care about things related
to the infrastructure and package version management and dealing with all of the intricacies
of how to run a production infrastructure, that's just not something that they're interested
in at all.
Trying to force these habits onto them, we've seen this even trying to get them to write
tests for their code.
It takes a lot of education on the net value add they're going to get from it before
they're willing to onboard, so definitely inspirational from a process point of view.
A lot of the same tools are applicable, but then you also need new tools that are domain-specific
too.
How do you do data versioning, how do you do model versioning, how do you validate and run
integration testing on models, how do you release and do AB comparison on a model as
opposed to a normal software application, and know if it's better or not, so yeah.
You know, applicable and you'll get hit in the face by a data scientist if you tell
them to go and implement all these things themselves.
One of the things you mentioned earlier was testing.
What's the role of testing in an MLOPS process and what kind of experiences if you had
working with real customers to implement testing procedures that make sense for you?
The place we try to start is by integrating some sort of tests on the data itself, so ensuring
that your data is of the same schema, you have high quality data, like a columnized feature
hasn't just been dropped or the distribution of values in that feature haven't changed
dramatically.
A lot of the stuff that we've built into the machine learning platform, especially
on the data set profiling side, are designed to help you with that, to help you with skewed
testing and analyzing, is your data too different to the point where you shouldn't need to be
training it, or is it your data too similar, or is it in that sweet spot where the same
training pipeline is actually applicable to go and solve the problem.
That's on the profiling side, and then we also have some advanced capabilities on the
drift side, so analyzing the over time, how are the inputs or features into your model
changing, whether that's training versus scoring, or day every day we cover week and month
every month of the data coming into your model, when it's making predictions, does that
data, has the shape of that data changed over time, do you still trust the model based
on the input values, and then of course you have the other end of it too, which is looking
at the predictions, the model is making, whether it's from a business application, so say
I'm using the Outlook app on my phone, and I've got the smart reply model running there.
Now either they didn't click on any of my suggestions, they clicked on a different suggestion
from the one I did, they clicked on the top suggestion that I had, or they said I didn't
like any of these suggestions, all those types of feedback come into telling you, is the
quality of data that you've trained your model on, giving you a useful model on the prediction
side.
So, yeah, skew testing, validating your data's quality, correctness, consistency between
training and inference, it's all those things.
Okay, okay.
Yeah.
So I'm kind of pulling your threads here, I mean, you're taking a step back, you talked
a little bit about a maturity model that you, when you look at customers, they kind of
fall in these end buckets, is there a prerequisite for, you know, starting to think about
ML ops?
So I think the prerequisite is you have to have a desire to apply a model to a business
need.
If your only goal is to write a model to say, you know, publish a paper, like, hey, I have
this model to solve this school problem, then you don't really need any of the ML ops
stuff.
And if you're still, you know, if you're just mucking around in a Jupyter notebook,
trying some different things by yourself, it's also a stretch to say, like, oh, you need
these ML ops practices now, but the second you go beyond, you know, I'm keeping all my
notes in Jupyter or I'm dumping them into one note somewhere and just keeping track of
all my experiments on my own, the second you want collaboration or reproducibility or
the ability to scale up and scale out to run your jobs in the cloud, that's where ML
ops starts coming into play.
I agree that kind of collaboration is a big driver, but even an individual researcher,
you know, that's tracking hyperparameters and file names or on posted notes or some
equipment worse can benefit from some elements of the tooling that we kind of refer to as
ML ops.
Would you agree with that?
I would.
Yeah.
But just, you know, trying to sell them on using everything from the very beginning is
the tougher sell.
So we start by saying, you know, start by tracking your work, so it's the whole process
maturity flows.
You start with work tracking, then making sure everything's in a reproducible pipeline.
And then making sure that others can go and take advantage of that pipeline.
And then you actually have the model that you can go and use in other places.
Yeah.
Yeah, which, which I like the way you pull that together because in a lot of ways, one
of the questions that I've been kind of noodling around for a while now is the, you know,
where does ML ops, you know, start and end relative to kind of platforms and tooling and
the things that enable and support ML ops?
And it's, you know, very much like the conversation we were having around DevOps, like DevOps
isn't, you know, containers and Kubernetes and things like that DevOps is a set of practices.
And it's very much to your point, kind of a, you know, that end and process.
So you might need, you know, any one of a number of the tools that someone might use to
enable ML ops, but that doesn't necessarily mean that you need ML ops.
Right.
So when, sure, I work on Azure machine learning, when I'm talking to customers about how does
ML ops actually work, you're going to have at least like three different tools and technologies
being used, right?
Because you have three different personas.
You have data engineering, data science and DevOps, ML engineering, whatever it means,
you're going to have some sort of data pipelining tool, something like data factory or, you know,
airflow in the open source world, something to help with managing your training pipelines,
whether it's, you know, Azure ML is a managed service or something like cube flow.
If you're in the open source community and then same thing on the release management side,
whether it's using Azure DevOps or get up actions or you're running your own like Jenkins
server, either way, there's going to be at least those three different types of tools
with different personas and they all need to work together and interoperate.
So that's another key part of our pitch is like, make sure that you're being flexible
in how you're producing and consuming events because ML ops is more than just model ops
and you need to make sure it fits into your data and dev side of the house.
Yeah.
Yeah.
Awesome.
You mentioned Azure DevOps playing a role in here and Jenkins on the open source side.
Yeah.
These are tools that, you know, from the DevOps perspective, you associate with CICD,
continuous integration and continuous delivery.
The idea being that there's a parallel on the model deployment side, can you elaborate
a little bit on how those tools are used?
Yeah.
So the way we like to look at it from a DevOps point of view is we want to treat a model
as a packaged artifact that can be deployed and used in a variety of places.
So your model is sure you have like, you know, your pickle file or whatever, but you also
have the execution context for, you know, I can instantiate this model into a class and
Python or I can embed it into like my Spark processing pipeline or I can deploy it as an
API and a container onto, you know, Kubernetes cluster, something like that.
So it's all about how do you bring the model artifact in as another thing that can be
used in your release management process flow?
It does not have to be a pickle file.
It could be anything.
It could be anything.
Exactly.
Yeah.
Yeah.
It's, this is my, you know, serialized graph representation.
Here's my code file, my config that I'm feeding in.
So it's a model is just like any other type of application.
It just happens to come from some, or have some sort of association to like a machine
learning framework or to have come from some data, which is actually, you know, another
important part of the, the ML ops story is what's the, what's the end to end lineage look
like, right?
So ideally, you should be able to go from, I have this application that's using this model.
Here's the code and config that was used to train it and here is the data set that this
model came from, especially when we're talking to customers in more of the highly regulated
industries.
So, you know, healthcare, financial services, say you have a model deployed that's determining
if it's going to approve or reject somebody for a loan.
You need to be very careful that you've maintained your full audit trail of exactly, you know,
where that model came from in case somebody decides to come in and ask further.
This also becomes more complicated than more of a black box that your model is, but in
general, the goal of having all these different technologies work together and interoperate
is so that you can track sort of your correlation, ID or correlation vector across your entire
data and software and modeling landscape.
Can we talk about that kind of end-to-end lineage, is that a feature like you use tool X,
use Azure ML and create a button and you have that or is it more than that and a set
of, you know, disciplines that you have to follow as you're developing the model?
So, yeah, it's kind of the ladder leads to an element of the former.
Okay.
So, assuming that you use the, I think you're the all of the above guy.
Yeah.
You're seeing it up right now.
So yeah, when it comes to using the tools the right way is like, sure, you could just,
you know, have a random CSV file that you're running locally to train a model on, but if
you want to assert you have proper lineage of your end-to-end ML workflow, like that CSV
file should be uploaded into blob storage and locked down and accessed from there to guarantee
that you can come back, you know, a year later and reproduce where this model came from.
Same thing on the code and packaging and the base container images that you're using
when you're training the model.
All that collateral needs to get, needs to be kept around.
And what it's that allow you to do is, you know, we have the inside of the machine learning
service, internal metastore that keeps track of all the different entities and the edges
that connect them together.
And right now we have sort of a one hop exposure of that, but one of the things we're working
on is more of a comprehensive way to to peruse the graph.
So it's like, hey, across my enterprise, show me every single model that's been trained
using this data set, not scope to, you know, a single, a single Azure or a single project
that my team is doing.
But across the entire canvas, show me everybody using this data set, what types of features
are they extracting from it, is somebody doing work that's similar to mine?
Can I just fork their training pipeline and build on top of it?
And you know, going back to how has this work we've done for internal teams inspired the
work we're doing on Azure, that's probably the most powerful part of our platform for internal
Microsoft teams is the discovery, the collaboration, the sharing.
That's what allows you to do ML at high scale, at high velocity.
So we want to make sure as much as we can that the tools and technologies that we have
on Azure provide that same capability with all of the enterprise ready features that you
would come to expect from Microsoft and Azure.
So in that scenario, you outlined the starting place is a data set that's uploaded to blob
storage.
Even with that starting place, you've kind of disconnected your ability to do lineage
from the kind of the source data set, which may be in a data warehouse or something like
that.
Yeah.
Is there also the ability to kind of point back to those original sources?
Oh, yes.
So you know, sometimes you'll have a CSV there, but you can also connect to a SQL database
or to your raw data lake and have a tracking of, okay, this is the raw data, here's say
the data factory job that did all these transformations, here's my curated data set, here's all the
derivations of that data set, here's the one I ended up using for training this model.
Here's, I took this model and you know, transfer learned on top of it to produce this new model
and then I deployed this model as this API and you can trace things all the way back to
there and then going the other way, you know, when this model is now running, I can be collecting
the inputs coming into my model and the predictions my model is making.
I log those into Azure monitor and then my data engineer can set up a simple job to take
that data coming in and put it back into the lake or put it back into a curated data set
that my data scientist can now go and experiment on and say, well, you know, how is my, how's
it coming into my model that's deployed compared to the one I trained it, that's completing
the circle back to the beginning, yeah.
Nice, nice.
Which conceivably, you could, as opposed to talking about a data set, which, you know,
this data set has produced what models you could point to a particular, you know, row
in a data warehouse or something like that or a value and say, you know, what's been
impacted by this particular data point.
Exactly.
And that's, that's, you know, the value that we're trying to get out of the new generation
of Azure data lake store and some of the work we're doing on the Azure data catalog side
is to give you exposure into like, what's all the cool stuff that's being done or not
being done with this data, it goes back to letting your decision makers know, am I occurring
business value from these, you know, these ETL pipelines that I'm spinning all these
compute dollars to go and cook these curated data sets in.
And that's a large part of what are the larger ML platform to team did before as well
was we helped with creating curated data sets for, for being in office to go and build
models on top of.
So we had the data engineering pipelines and the machine learning pipelines and the
release management pipelines all like in under the same umbrella which helped to inform
the way we're designing the system now to be designed to meet enterprises where they
are and help them scale up and out as they go.
Yeah.
I'm curious what are some of the key things that you're learning from customers kind of
on the ground who are working to implement this type of stuff, kind of how would you characterize
you know where folks are if you can generalize and what are the key stumbling blocks that
kind of thing.
So there if we're to think about it in terms of like four phases where phase one is kicking
the tires, phase two is models reproducible, phase three is models deployed and being
used in phase four is I've all the magical automated retraining wizardry.
They're mostly between phase one and phase two right now, like very few of them have
actually gotten a model deployed into the wild.
If they have it deployed, it's only deployed as like a dev test API, they don't trust
it yet.
So that's you know one learning is customers were a lot earlier in the journey than we've
been expecting coming from doing this for internal Microsoft teams.
Another one is that the for the customers we're talking to their internal organizations
are not always structured to let them innovate most effectively, elaborate on that.
Yeah, so they'll have part of their org, you know, their data team and their IT department
and their research teams are totally disconnected, disjointed, don't communicate to each other,
don't understand each other.
And so IT just sees what the researchers are doing and says there's no way you're doing
any of this in production.
The data engineers are unsure what data the data scientists are using, like they might
data scientists might be off running SQL cores in the side, but they have no idea from
which tables, tables will disappear under the data scientist.
So we're just instead of doing a pure like here's how to use the platform.
It's more, hey, let's get all the right people in the room together from IT and research
and your data platform and your software development platforms and start a conversation
and build up the domain expertise and the relationships on the people side before you get started
with the process or the platform.
That's been yeah, one big learning is to step back and focus on getting the right people
involved first and then they can figure out the process that's going to work well for
their business and then they can adopt the platform tools that we've been building to
help them be more efficient at doing end-to-end ML.
Are you finding that there's a pattern in organization that allows organizations to
move more quickly like centralized versus decentralized or a quote unquote center of excellence
or embedded into business units that is it one of the other of those works best?
I think what we've seen work best is to have one business unit sort of act as the incubator
to vet the end-to-end flow and actually get a model working in production, but then
have the overall center of excellence centralized team observe what they're doing and take
notes and let them flesh out what the canonical reference ML ops architecture pipeline should
look like.
So I think that's seem out of all the patterns we've seen a lot of patterns being applied.
That one seems to be the best so far though is let a small team give them some flexibility
to go and build a model, take it to production with some light guard rails and they can build
out the reference architecture, get repository and CICD pipeline templates that the rest of
the teams and the company can use.
Is the salient point there that the end business unit that has the problem owns the deployment
of the model as opposed to the centralized but somewhat disconnected data science or
AI.
Correct.
Yes.
So your DevOps team for your business unit needs to know and understand the fact that
a model is going to be entering their ecosystem and needs to be able to manage it with the
same tools they manage their other application releases with hence the integration with Azure
DevOps to make sure that all your pipelines are tracked and managed in one place and there's
not this one row release pipeline that's coming in and causing issues and havoc with
the rest of your production system.
And generally when you look at these production pipelines did the pipelines and the tooling resonate
with the DevOps teams or are they like this strange beast that they take a long time
for them to run around.
So they freak out until they see the Azure DevOps integration and then they'll go,
OK, I understand this, yeah, hence where I'm like, you need to have the tools that your
audience can understand.
You show them a Jupyter notebook, they'll jump out of their seats and run away scared.
Whereas you show them like, oh, here's a managed multi-phase release pipeline with like
clearly defined declarative ammo for the different steps like that that resonates well
with them.
Whereas data scientists, you show them a big complex approval flow and they're going
to be like, I'm never using any of this.
You show them a Jupyter notebook, they're happy or you know an ID with like low friction
and Python and then your data engineers, again, you show them a, you know, a confusing notebook
process flow.
They're not going to like that as much, but you show them a clean like ETL where they
can drag and drop and run their SQL queries and understand are there pipelines running
in a stable fashion like that resonates with them say it's yeah.
Different personas, different tools, they need to work together and figure out what
process is going to work for their business needs.
Because I've kind of looked at primarily this machine learning engineer role that has
been emerging over the past few years and now we're talking about the DevOps engineers
like a separate thing that the line is kind of a great movie and blurred line, right?
What we've seen in terms of, we have customers to ask us, well, how do we hire these ML
engineers?
It's like, basically, you need a person who understands DevOps, but also can talk to
your data scientists or can, can figure out the work they're doing help them get their
work into a reproducible pipeline on the training side and help with deploying the model and
integrating it into the rest of your application lifecycle management tools.
So yeah, your ML engineer needs to be a DevOps person with some understanding of ML.
And is a DevOps person necessarily a software engineer that is coding a model based on
not necessarily, they just need to be really good at operational excellence.
So do they understand how to write things declaratively, how to set up process control
flows so that things work nicely and like, you don't need to understand the ML, the data
scientists is doing, you need to understand the process they're going through to produce
that model.
So they have a bunch of code and a Jupyter Notebook, like help them factor right into modules
that you can stitch together, but you don't need to understand like, you know, the machine
learning framework that they're using specifically in that context.
You've mentioned Jupyter Notebooks a few times, you know, one of the things that I see
that folks are trying to figure out is like, should we do ML in Notebooks or should we
do ML in IDEs, Microsoft, you know, has a huge investment in IDEs, but you've also been
like in Visual Studio Code, making it more kind of interactive, integrated kind of real
time to incorporate some of the Notebook Eskys, so interaction.
We wanted to be fluid to go from one of the other.
We've seen the value and the interactive canvases for doing, you know, rapid fire experimentation.
We also talked to large companies like Netflix to learn how they use Notebooks in automation
at scale and some of the ML project, for example, exactly.
So we've actually integrated paper mill into our platform as well.
So you can, if you're designing your training pipeline, you can stitch together a mix of scripts
and Notebooks and, you know, data processing steps together, and we try to be as fluid
as we can.
And we're working with the developer division as well to figure out how to more cleanly
integrate Notebooks into our ID experiences.
And you saw some of that in the BS code side and there's more stuff coming to help with
that.
We've talked a little bit about kind of this automated retraining aspect of managing
model life cycles.
Are there other aspects of managing model life cycles that you find important for folks
to think about?
So yeah, knowing when to retrain the model is one thing, knowing when to deprecate the
model is nothing to say that the data the model is trained with is stale or can't be
used anymore.
We've got removed for GDPR reasons, this is why having the whole lineage graph is so important
to be able to figure out exactly what data was used to train the model.
Other things around model life cycle management, yeah, I really know who's using it, know where
the model's running, know if the model's adding business value, know if the data coming into
the model has changed a lot since you trained it, know if the model is dealing with some
type of seasonal data and needs to be retrained on a seasonal basis there.
And then also know the resource requirements for your model.
So another big thing we see a trip a lot of our customers up is they train the model
on these big beefy VMs with massive GPUs and go to deploy and it's like, hey, my model
is crashing.
What do I do?
Right.
And so we've tried to build tooling in to help with that as well.
So profiling your model, running sample queries into it, different sizes of sample queries
to not always the same thing and making sure you know, does your model have enough CPU
and memory and the right size GPU to perform effectively.
And we're also doing some work on the on X framework to help with taking those models
and quantizing them or optimizing them for a specific business use case on the hardware
side, which is really slowly coming in, especially we have customers in the manufacturer, manufacturing
sector who want to run models quickly on the edge on small hardware.
So it's like, how do you, how do you manage that transition from this model I train on
this beefy machine to this model running on this tiny device?
You finding that that most customers are deploying models or even thinking about an individual,
I've got this, you know, this model that I've created and I'm going to think about the
way I deploy this model versus, I've got a model I've built it to a standard, it's
just like any other model that I'm going to just going to throw it into my model deployment
thing.
Like, are they there yet?
Some of them are there.
The ones that have been doing this for a while longer and develop like their template
for their model deployment flow.
Right.
We try to provide as much tooling as we can, you know, in the platform and in the registry
for you to track all the relevant things about it.
But really just getting, getting the model deployed into your existing apico system, making
sure that you have the ability to do controlled rollout and A-B testing because you don't
want to just always pave over the previous model.
So that's the most advanced customers are just getting to that point now where they're
ready to start doing A-B testing and looking for our help to go and do that.
Yeah.
So, A-B, along the lines of testing, we've talked about this a little bit.
There's both kind of the, you know, online testing of your models, freshness, but then
also all kinds of deployment scenarios that have been developed in the context of DevOps,
like Canary, then Red, Green, Blue kind of stuff, like all the colors.
Yeah.
Yeah, all the colors, right?
Do you see all of that stuff out in the wild?
Yes.
The main difference we've seen with models compared to normal software being rolled out is oftentimes
they'll develop a model and test it offline and batch for a while before using it.
So they won't need to necessarily deploy it to receive real traffic right away.
They'll let the model, you know, get the new model, they'll wait a week, run the model
and batch against the past weeks worth of data, and then compare how different it is to it.
So it's just the fact that you can test the model offline as opposed to having to do everything
in an online fashion.
Yeah.
Yeah.
That's probably the biggest Delta, but otherwise we see all the same patterns as with normal
software.
Right.
Right.
Because you're testing two things, right?
You're testing the model's statistical ability to predict something, but then it's also
software.
Right.
And you don't necessarily want to put a software, work in pieces of software out there.
Correct.
So it's a software with uncertain behavior, or more uncertain behavior than any normal
software application you threw out there, yeah.
Yeah.
What can we look forward to in this space from your perspective?
So as far as things to look forward to, there's lots of investments coming in improving
our story around enterprise readiness, making it easier for customers to do, and secure
data science and ML workloads, work to help improve collaboration and sharing across
the enterprise, how do I figure out which other teams have been doing modeling work similar
to mine?
How do I take advantage of that?
So accelerating collaboration, velocity, more work on the enterprise readiness front,
and then a tighter-knit integration with the rest of the big data platform stuff.
So integration with data lake, data catalog, data factory, DevOps get up, and it's all
about helping customers get to production ML faster.
Well Jordan, thanks so much for chatting with me.
Thanks for having me.
Yeah.
Appreciate it.
All right everyone, that's our show for today to learn more about this episode, visit
Twomlai.com.
As always, thanks so much for listening and catch you next time.
