Hello and welcome to another episode of Twomble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington, a few announcements before we get to the show.
First, I'd like to take a second to give a virtual high five to everyone who entered
our latest giveaway, in which two lucky listeners will get a chance to attend the recently
rebranded AI conference in San Francisco, compliments of this week in machine learning
in AI, and our friends over at O'Reilly.
This has been our most active giveaway to date, with over 100 of you submitting entries,
just wow.
We haven't picked the winners yet, as of the time of this recording, but by the time you
hear this, winners will be posted over at Twombleai.com slash AISF, and we'll give them a proper
announcement on next week's show.
Next up, about a month ago during my conversation with Chelsea Finn, I thought out loud about
starting a virtual paper reading group.
After receiving lots of positive support for the idea, we finally have a meetup to call
our own.
On August 16, we'll kick off the inaugural Twomble online meetup, where our first presenter,
community member Joshua Manella, will lead a discussion on Apple's GANS paper, learning
from simulated and unsupervised images through adversarial training, which is one of the
best paper award winners from this year's CVPR conference.
If you've already signed up, great, we look forward to seeing you there.
If not, head over to twombleai.com slash meetup to get registered.
Finally, if you've been paying attention, you know that after almost a year of procrastinating,
I've finally launched my email newsletter.
I've been having a blast with it, and you definitely want to subscribe.
We've got some fun stuff in store exclusively for newsletter subscribers, so make sure
you bounce on over to twombleai.com slash newsletter to sign up.
Alright, as you all might know, a few weeks ago, I was in San Francisco for Wrangle, a great
conference brought to you by our friends over at Claudeira.
Wrangle is a super fun event.
Each year, it brings an interesting and diverse community of data scientists to an intimate
and informal setting, this year a music venue in SF's Mission District, for great talks
on real data science projects and issues, not to mention cowboy hats and barbecue.
While I was there, I had a chance to sit down with a few of the event's great speakers,
including Drew Conway, founder and CEO of Aluvium, and a former data scientist with the CIA,
Sherrath Rao, a listener and fan of this podcast and an engineering manager over at Instacart,
and Aaron Schelman, a statistician and data science manager with Zimergen, a company
using robots and machine learning to engineer better microbes.
This show features my interview with Drew, whose Wrangle keynote could have been called
Confessions of a CIA Data Scientist.
The focus of our interview and the focus of Drew's presentation is an interesting set
of observations he makes about the role of cognitive biases in data science.
If your work involves making decisions or influencing behavior based on data driven
analyses, and it probably does or will, you'll want to hear what he has to say.
A quick note before we dive in, as is the case with my other field recordings, there
is a bit of unavoidable background noise in this interview, sorry about that.
And now on to the show.
Alright everyone, I am here at the Wrangle conference, the guest of Claude Ara, who's sponsoring
our series here, and I am with Drew Conway, who is the founder and CEO of Aluvium.
It's going to be with you, Sam.
It's great to have you on the show.
So you just did a really interesting presentation that I tweeted a little bit about.
Why don't we take a minute to have you introduce yourself to the audience?
Sure, so I am the founder and CEO of Aluvium, we're a New York based company that builds
what we call human-centered AI for the industrial industry, and what that means for us is we
build software products that exist at the intersection of complex machine data, so
think streaming data from an olive refinery, data from automated robotic systems, and human
knowledge.
One of the things that I can even talk a little bit about in the context of what I just
presented here at Wrangle is me, my career has really been one that has kind of moved
me through a path of working alongside and building software tools for people who need
to make decisions from data, and Aluvium in many ways is a kind of culmination of a lot
of thinking I've had over my career as to how best to extract maximum value from these
complex streams of data and present a human being, you know, a man or woman who's working
inside an industrial operation with the right information at the right time to make the
best decision.
So the company is just about two years old.
We work primarily in what we call process manufacturing, so sort of distinct from the
screen manufacturing and the way that I just grab is like screen manufacturing is screw
in bolts and rivets and process manufacturing is typically, you know, pipes and boilers
and things like that.
And the reason that we focus on that second half is our approach to learning is really
one where this continuous nature of information is more well suited for what we're doing.
Interesting, particularly interesting because I've been spending a lot of time researching
industrial AI or industrial applications of AI, and we're in the midst of a series of
podcasts on industrial AI, although we're connecting here in a totally different context.
And so why don't we jump into that?
Tell us about your presentation here at Wrangle.
Sure.
So, yes.
So the early part of my career when I first started a career was actually as a what was
called then a computational social scientist inside the US intelligence community.
And so this was a few years before data science was a profession, a sort of well-known profession
and I think probably the people who were doing the work that I was doing then now are probably
all data scientists.
But what my job primarily was to think about how to build statistical programming and
statistical software tools to support decision-making inside the intelligence loop, and basically
that meant my work was split primarily into two big halves.
One half was what I would call principally basic research.
So things that are more academic in nature, I spent a lot of time thinking about graph
theoretic models of network change and network moving over time.
The other half of my work was sort of much more tactical in nature, so it was really building
custom software and even kind of documentation systems for taking in a very wide breath
of different kinds of data.
So all the way from space-based assets and satellite imagery to signals intelligence,
to ground sensing, to an unstructured written report from some PFC in the field and being
able to distill all that information down in a reasonably timely way to help a sergeant
major who is in the field and needs to know whether they go knock on the store as the
person they're looking for going to be there, if they go inspect this shack, will they
find the weapons that they're looking for.
So it was a fascinating place to start my career and when I spoke about this morning
was really part of the lessons that I learned from that with respect to how data science
as a sort of profession and as industry can kind of get wrapped around the axle on bias
and how to overcome that and how to help yourself as a professional data science as it really
had to help the folks around you better leverage and use the tools that you have because everybody
brings bias to their work and I think my experience in the intel community was one where I was
sort of acutely aware of that because the stakes were relatively high.
Right, interesting.
So one of the silly questions that I had for you was, did you have to get your slides approved
by me?
No, no, no, so yeah, all things in my slides were sufficiently generalized that there's
no need to do that but one thing that I did mention in the talk which is sort of interesting
context for being a data science inside the intel community is sort of your access to
tools.
You know, so we, you know, we, the sort of collective community of people doing this work,
we think about just this ready access to the latest and greatest open source tools and
that as soon as Google or Facebook or whomever kind of open source this great new thing,
well, let's figure out a way to play with it and we're going to get it into our workflow.
That is absolutely not the case when the equipment that you're working, you know, literally
the computers that you're doing your work on are classified pieces of equipment.
And so one of the stories that I told in my talk was one where I was trying to overcome
this motivated reasoning as an example of how that problem-motivated reasoning can actually
be addressed through a kind of deliberate technical approach to analyzing data.
And in this case, we were, we were looking at satellite imagery, the context here was the
sort of never-ending search for weapons of mass destruction in Iraq, you know, the timing
here is sort of mid-2000s, 2000s, 2006.
And as I mentioned in my talk, there, I had the opportunity to work with these two, we
would call image intelligence analysts who had been working for, I mean, by the time
I got to know them, I think they've been passed their 30-year mark.
And so these were, these were people with a tremendous amount of expertise and they were
tasked with analyzing images, satellite images taken of Iraq and try to identify any places
where you might see weapons of mass destruction.
And one of the projects that we worked on, which they themselves asked for, was their
intuition was that there were no, they were never going to find so.
And unfortunately, because of this issue of motivated reasoning, they were continuing
to be asked to just look and look and look at no end.
And so we basically came up with this novel idea is like, well, why don't we try to automate
this process so that you can, in aggregate show my automatically analyzing images and
using very simple classification to try to identify where, oh, if there are any indications
of suspicious activity, if that actually exists, because, you know, two men doing this on
their own, they could be at this for the rest of their careers.
And so the reason I bring this up, because it ultimately became a really interesting exercise
in trying to get new tools in the building.
So, you know, one of my greatest achievements, I think, as in this job was actually not technical
but bureaucratic in that I was able to actually get early versions of scientific Python and
OpenCV installed on a classified machine so that we could write a simple classifier to
try to help these guys out.
And ultimately, the success was that A, we were able to do that, but B, we were able
to show an aggregate that there was really nothing there.
And ultimately, we were able to use that as a way to dislodge these guys from having
to continue to pursue something that ultimately they knew they were not going to find.
And do you think that's changed at all the ability to get new technologies open source
into these environments of a, you know, since you left that out?
I think it's improved quite a bit, you know, and it's funny to think, I mean, 10 years
is a long time in this sort of tech timeline, and they're, you know, large bureaucratic
institutions like the military and the Department of Cancer are always trailing indicators,
but one of the things that I've been very impressed by with the folks that I know that
continue to work in this space is their sort of progression to being able to bring new
tools in.
But there, there's part of this, which is there I think there are better tools available
commercially, and so that's always been an easy way to acquire new stuff, but having
it not only in appetite, but a sort of avenue for bringing in open source tools, I mean,
I know even, even AWS works with the Intel community now and creates, you know, distributed
compute for them to actually use some of these tools.
I mean, I think it's been obviously a real boon for their work, but I think more importantly
for recruiting and retention for, you know, very talented people.
Yeah.
So you mentioned motivated reasoning in the context of this story of pulling technology
into one of the agencies, tell us, you know, what that means, and spend some time talking
about the other biases that you talked about in the year, because that was the bulk
year.
Yeah.
Yeah.
Yeah.
So we see it all the time.
It's basically, I ask you a question because I want you to give me the answer that I
already know.
Right.
And motivated reasoning is a particularly sticky wicket when it comes to intelligence
gathering, because policymakers will oftentimes have free-determined policy biases.
And so your job, and as I mentioned in my talk, you know, particularly if you're working
in civilian intelligence, you know, principally the CIA, you have one customer.
Your customer is the president and by extension the White House.
And so, you know, motivated reasoning can be very problematic if you have a customer
with high levels of motivated reasoning.
And there are a number of examples of that throughout history, you know, I spoke about
the sort of search for weapons and mass destruction in the early part of the 2000s, but there
are lots of other biases.
Right.
And I think the next stop and the one that I mentioned during the talk is this idea of
confirmation bias.
Which in many ways I think is sort of the, you know, the cousin or the direct relative
of motivated reasoning, which is maybe I don't already have a predetermined policy outcome
that I'd like to see.
But I sure have a lot of bias in accepting, you know, analysis that confirms the thing
that I already think is true.
Right.
And that is, again, this incredibly problematic lens through which to observe information
that in and of itself is very hard to collect.
You know, I think in the context of data science, people often talk about bias in the data
generating process.
And it's true.
It's everywhere.
You know, if you work at a big social media company, you have access to a tremendous amount
of data.
But some engineer along the way shows to collect that click stream.
You know, that wasn't, that was done by a product manager who decided that they wanted
to track a specific kind of action.
Right.
And that's bias.
Right.
So kind of intel context where there's policy decisions that need to be made off of it.
You have an extremely limited set of kinds of information that you can get.
And oftentimes that information is brought to you in sort of opportunistic way.
You know, one of the, this is well after my time in the intel community, but of course,
after, you know, after the raid, had a Salomon Bin Laden's base camp, a lot of the most valuable
stuff that came out of that were all the laptop computers and information that they
were like, lean there, you know, obviously that data is highly biased.
And that was a, that was a collection of opportunity.
There was no, there was no, like, a B test and no experiment to try to identify which
would be the best pathway to do that.
And so confirmation bias becomes extremely dangerous if you have sudden access to a new
data set that you didn't already have.
And without taking a delivered approach to analyzing it may reinforce that bias in a very
dangerous way.
Yeah.
The last one that I mentioned during the talk and this is sort of where I left the talk
and I think is something that we as a community really need to think about in the context
of our work today now is sort of flat out denial that we, we as data scientists and we really
as sort of technicians because I wouldn't bucket this only for people who, you know, writes
just code, right, all folks who build tools with software have this attachment to both
information that is biased and imperfect and tools that are, you know, sort of approximations
of good ways of estimating what we think is occurring in the real world, the real world
is really hard to measure.
And so this kind of trifecta of imperfection means that everything that we do should be,
you know, fraught with caveats and considerations for all of that stuff.
But what that means is that we are, we accept the fact that we open the door to people who
will try to poke holes and deny that that's true.
Even if we can make a very confident assessment of something and we can show it to be true,
but because of good hygiene around doing data science, it's very easy to poke holes in
that.
And I think there's, I think part of what we need to think about as a community is, well,
how do we prepare ourselves for that?
How do we get better at communicating this kind of, you know, these foibles in our work
that we cannot pull away?
But also, how do we get the consumers out of that information to be more willing to
accept and more educated to a certain extent about that reality?
I mean, one of the things that I mentioned during my talk was, you know, this, the election
in 2016, the US election in 2016, I think it was a great example where people just had
this expectation that, you know, you get, whoever has the higher percentage of winning
is the winner, right?
Because we have this kind of horse race mentality.
But anybody who's ever been to a casino would know that if you, if there was a game at
the casino that gave you, you know, 25% out of winning, you would never leave that game.
Yeah.
And that's essentially the game that we ended up playing and we just got the one in four
chance that we didn't expect to see and I think part of that is this, you know, people
just need to get a better understanding of, I mean, it's part of it, I think, is sort
of becoming more numerate, but I think part of it is just we, we're responsible for that.
The data community is responsible for kind of conditioning folks to understand that,
but I think folks like Nate Silver do a great job and they're saying that I think there's
a lot of success in the world and that, but we need more of it.
Yeah.
And it's, it's struck me that it's a bit of a fine line between, you know, denial and
maybe it's not a fine, maybe it's a golf, but, you know, there's denial on one hand and
then there's, you know, challenging the results.
And I think, you know, we need to be, as a community, be open to having our results being
challenged because the flip side of that is this idea that, you know, data and quote unquote
AI is magic.
Right.
And whatever it says, that's what we got to do, right?
And that's a whole different, just, you know, that poses a whole different set of challenges.
I totally agree.
And I think, you know, we are, we are right at the beginning of what I think will be a
very interesting, you know, future, immediate future for, you know, sort of general consumer
because it seems like the technology trend is to move in the direction of effectively
using black boxes or solutions.
And that means that we accept the fact that it will be very difficult to understand why
a system makes a choice.
And I think we are opening ourselves up to a really difficult set of circumstances that
will very likely come sooner or later where, you know, intelligent software systems, whether
you want to call it AI or sort of a general class of just not decision support systems
but decision making systems that we don't understand.
And, you know, I think the danger now is that in some sense, we've moved so quickly in
the direction of having access to tools like this that the education component is just
not kept up.
And I think the flip side of this, which is in many ways a compliment to it, is I also
think that there's an unnecessary amount of fear, right?
I think now we have this pendulum swinging back the other direction where you have folks
creating a kind of anxiousness around the arrival of tools like this and systems like
this that doesn't really meet with the reality of how that technology trend is doing.
So consumers are being conditioned now to not trust ATM machines and to not trust subway
doors that open and close on their own.
And I think this friction now is really, I mean, literally starting to heat up in a sense
that, again, we're the folks that build these systems.
And I think because of that, it's partly our responsibility to be able to go out and
try to talk to folks and say, you know, this is how this works, you know, fine.
We don't, I think we lack in sometimes good champions for this stuff, right?
We have lots of really good entrepreneurs and technologists who can build this stuff.
We need to find our champions who can actually kind of go out there and try to help folks
understand how this is going to change their lives, you know, for good and potentially
in ways that we don't even understand yet.
Yeah.
Yeah.
I've got to ask the ATM and subway doors, a specific exam.
No, no, I'm just thinking that you know, you hear, you hear all these, you know, a lot
of kind of fear and certainty and doubt or stuff.
And so, you know, actually, these would be more examples for my personal life where I'm,
you know, asked by family members, you know, when am I going to show up to the ATM and
it's going to be hacked or when am I going to show up, you know, autonomous vehicles going
to turn on their own or use, you know, everybody, you know, people have seen, you know, fast and
furious movies and they're like, is this how things are going to work?
And I don't know.
But I also know that that future is a little bit further away than Hollywood is presenting
right.
And there's a long way to go before we actually have to start thinking about that.
I think because there is time, part of our job as a community is to try to fill that information
gap in a little bit.
The speaker after he said that in kind of running through similar sets of issues, more
applied, societally, gave the audience the advice that, you know, as you're thinking about
these systems you're building, think about the degree to which it resembles an episode
of Black Mirror.
Right.
Yeah.
I think that's, you know, if there's a useful rubric, that's probably a really good one.
Like if you start to bleed into Black Mirror territory, and then you probably have made
some choice that you might want to weakless enter.
Yeah.
And for those who don't know Black Mirror, probably the best analogy is like a British
version of the Twilight Zone.
Yeah.
Yeah.
It's a group heard of the Twilight Zone, but is, you know, the Twilight Zone.
Very modern.
Yeah, dealt with lots of different kinds of societal issues.
Black Mirror tends to focus on technology, has a kind of specific or consistent thread.
And well, I'm a big fan.
Yeah.
Same here.
Same here.
So what are the specific, you know, the top three things that you want folks to take
away from your presentation and not just take away, but like go do.
Right.
I mean, I think the first one, you walk out of my talk, and you go back to your, you
know, maybe tomorrow or this afternoon, you go back to your desk at the office.
And sit down and think about how confirmation bias, motivated reasoning, and opportunities
for denial exist in the current decision-making workflow in your company, your organization,
and how your own work may be contributing to that, or helping to save it.
And really, not to be overly negative, but, you know, I think that there is, everybody
can think of examples of this.
And I run a company, I can think of examples.
I mean, I'm heavily motivated to sell my products, and that will impact my decision-making.
And I have told my team, I have high expectation of them to step in and say, well, hold on, Drew,
like, why are you, why do you think that that's a good choice to make?
And this would be specifically in product building, even model selection for some of the,
you know, algorithms that we may choose, you know, this stuff is, this stuff is not,
you know, abstract, right?
There's a real examples.
Right.
And I think the second one, you know, which is, I think, maybe more fun is, you know,
go out and seek the first hand, you know, go try to, try to meet folks and see systems
where this kind of intersection of bias data and decision-making exists in your community,
you know, local government in, you know, in your local community organizing, you know,
this stuff is everywhere.
And one of the things that's nice about kind of professionally applying statistical methods
and computing to this, in this context is that you can bring your own expertise to potentially
a much smaller scale problem that impacts many, many more people's lives and allows people
that you are your neighbors.
And, you know, what, I've had a great opportunity in my life to work with, you know, the government
in New York City and work with local communities there.
And, you know, I was, I was, I was a co-founder of an organization called Data Kind, which
really kind of grew out of this, really this question of how do you, how do you bridge
the gap between talented data scientists and engineers and product managers and a social
sector that has great data, really interesting problems, but doesn't have access to those
talented data scientists and engineers.
And Data Kind exists to do that.
And, you know, you can go sign up on Data Kind's website, that's one way to do it.
And the easier way to do it is just, you know, jump to a community meeting, see what,
see what the Board of Ed is talking about, you know, they're making decisions about what
data to collect on students, you can help, you know, help them make better choices.
And the final one I'll say, which was the final, the sign I'll kind of call to action
of my talk, which is, you know, if you're in a position of hiring people, which I'm
sure many listeners are, you ought to really think hard about hiring veterans.
I had an opportunity, as I said, to work alongside active service folks in the military.
And there's some of the most brilliant, dedicated, you know, technically competent folks that
even now I've ever had a chance to work with.
And I think there are many underrepresented groups in technology and I think veterans
is a group that people don't talk a lot about.
And I think the transition from a career as a signal, you know, a signal analyst on a
submarine to working as a data scientist is actually a lot narrower than certainly folks
in that higher technologists think.
And part of the issue is that a lot of people who come out of the service and then try
to, you know, transition into a professional job, they just didn't even know that these
jobs exist.
You know, and I've worked with an organization in New York called the Iraq and Afghanistan
Veterans of America to try to build, you know, access, you know, just to kind of make
bring these two communities together.
And again, you know, I think this is something that I do in New York, here in San Francisco,
these, all these organizations exist and I would encourage folks in all their communities
to try to work and do that.
How do the biases that you talked about express themselves and your customers when they're
trying to apply industrial AI and data driven decision making, what, how do you see them
show up?
That's a great question and, you know, we see a lot of the bias coming from customers
at a point now where the buzz around AI and machine learning in the context of their
work.
So, you know, I know you're in, you're doing this series, I mean, the industry 4.0,
you know, digital transformation, all these kind of terms, terms of art, basically what
that means is, okay, we're not a software company, but we believe that we need to transform
part of our business to rely more heavily on data and software, because that's what
we think we need to do.
But those are just words and so when you get to actual practical implications, you start
talking to folks who, you know, their job is to make sure that the oil refinery has
no safety issues, ever again, how do we do that?
And because there's a potentially very large white space around what AI and machine learning
can actually do to solve that problem, it's very difficult for that, you know, site
coordinator or plant manager to think creatively about how to solve those problems.
And so, the bias is that we see is, you know, particularly in the beginning, the first
year of Ovaluvium, the bias that we saw is that when you, when you try to rely on customers
creativity, they come back and say, I don't understand this.
I don't, I don't see how it could possibly help my work, because there's no real tangible
connection to all of this fancy math and compute that you want to throw at it and how this
translates to my workforce getting home safely every day and my facility producing at
maximum yield.
And so I think for us, the journey that we've gone through as a company is actually trying
to figure out how to more narrowly go after a set of, really a set of metrics and kind
of value, values that can apply to answering that question because the denial bias, that's
like, I'm going to poke holes in this because it's easier for me to say no than it is to try
to do the work to understand this, you know, it's hard to, it's hard to fault the customer
for that.
Yeah, there, you know, our customers are folks who have worked 15, 20, 25, even 30 years,
you know, in an oil rig, on a, you know, in a manufacturing floor and a power station,
like software is not their job, software is my job.
So I should be able to help them with that.
And so how I can kind of minimize the gap in that is, is a way of minimizing that bias
to try to help them understand and connect the dots between the work they do every day,
the service that we can provide and then how that makes their work better.
Interesting comments around kind of the role, the role of software, I think we go around
ideas like software is eating the world and then, you know, you have big industrial companies
like GE and Ford saying that they kind of committed themselves to become software companies,
right?
So, you know, on the one hand, I think those, you know, I guess maybe I'm a cheerleader
for the industry and say, hey, you know, these companies, you know, every one of these
companies need to start thinking a little bit more, at least like a software person in
some way.
But, you know, you're right, one of the things that has come up repeatedly in talking
about the industrial AI in particular is how, you know, you are, you know, fundamentally
trying to pair, you know, something is like the cutting edge of, you know, software and
technology with, you know, someone who's, you know, been working in a particular, you
know, working with a machine, you know, a CNC machine, you know, or a, you know, a boiler
or something like that.
And they know it's like they're the CNC whisperer, right, that thing makes a certain noise,
they know that it's going to need some care and feeding and they know how to do it.
And I think it's an interesting responsibility for us as technologists to try to figure out
how to bridge these worlds.
Yep.
You know, at Aluvium, we, we, we really think about this as a first class part of what
we're trying to do.
No, we, we have, you know, set of company values and one that we, we really hold dear
as an idea.
We want to put people first.
And I think, you know, that's, that's sort of easy to say and in the context of what
our work, what that means is I have, I have no idea what a CNC whisperer knows and I'll
never know.
I will, I could, I could start today and work through a main in my career and never be
as good as that.
I mean, we've met people who are those, those, those folks, I mean, single, single individuals
working in massive multinational companies that themselves have institutional knowledge
that is probably invaluable to those organizations.
And so to build software to support that, I think the, the admission that you have to
make as a technologist is that you'll never know what they know.
And so how can you build tools that shift the cognitive responsibility of that individual
away from having to constantly be, you know, checking that CNC machine and pulling data
off of it so that it knows exactly that series of vibrations or heat or spin that may indicate
imminent failure to providing them with information in a timely manner that draws them to that,
you know, opportunity to make a choice, to make a decision.
And then they can get back to the work that they're really good at.
And I think in some sense, it's just a matter of increasing cognitive margin.
You know, there's, there's this, you know, this, this upward slope of data, right, constantly
everywhere.
And I think in the industrial space, which is, is largely not talked about outside of
those kind of professional conferences is just as steep or, or more, you know, but the
labor dynamics in those, in those industries are, you know, fixed or shrinking.
Yeah.
And so now you have this, this very problematic asymmetry between humans having to understand
and, and deal with data and make decisions and systems that are just drowning them.
Yeah.
And information.
And for us, he's the simplest way to bridge that gap as well, let's leverage that expertise
that they have so that that, that, like, shifting of cognitive responsibility for the routine,
boring, observational stuff can go to the computer and the, you know, imminent high value need
to know now decisions can go to those experts and leverage that and really stitch that stuff
together.
Right.
Right.
Awesome.
Sure, folks, to check out what you're doing, try to get down and engage with you.
Yeah.
So, you know, websites alluvium.io, that's A-L-L-U-V-I-U-M.io, alluvium.
The easiest way to engage with me, you know, I'm, I'm on Twitter at Drew Conway, C-O-N-W-A-Y.
I'm, I'm pretty easy, I'm a pretty easy Google.
So, you know, I, I, I, I, I answer, I answer my emails, I like to, I like to engage with
folks.
So, if you have any questions about the company, we are, we are hiring as I think everybody
is.
So, you know, for us, the, the nucleus of, of our company is really around data science
and engineering, but I think with a specific bent on streaming data and, you know, semi-supervised
and unsupervised learning.
So, if that's the kind of stuff with, you know, high volumes of streaming data that get
you excited, we'd love to hear from you.
Great.
All right.
Well, thanks so much, Drew.
Thank you, Sam.
It was a lot of fun.
All right.
All right, everyone, that's our show for today, thanks so much for listening and for your
continued support of this podcast.
For the notes for this episode, to ask any questions, or to let us know how you like
the show, leave a comment on the show notes page at twomolei.com slash talk slash 39.
Thanks again to our sponsor for the Rangle Conference Series Cloud Era.
To learn more about Cloud Era and the company's data science workbench family of products,
visit them at cloudera.com and be sure to tweet to them at at Cloud Era C-L-O-U-D-E-R-A
to thank them for their support of this podcast.
If you're interested in joining our meetup, you can register for that at twomolei.com slash
meetup and don't forget to sign up for the newsletter at twomolei.com slash newsletter.
Thanks again for listening and catch you next time.
