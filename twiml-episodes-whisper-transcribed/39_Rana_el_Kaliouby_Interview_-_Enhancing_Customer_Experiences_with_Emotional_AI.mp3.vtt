WEBVTT

00:00.000 --> 00:15.580
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.580 --> 00:20.280
people doing interesting things in machine learning and artificial intelligence.

00:20.280 --> 00:22.880
I'm your host Sam Charrington.

00:22.880 --> 00:30.720
To show you about to hear as part five of our five part O'Reilly AI New York series, sponsored by Intel Nirvana.

00:30.720 --> 00:39.160
Now if you've listened to the entire series, you've heard me say this again and again, but I am super grateful to them for helping make this series possible.

00:39.160 --> 00:49.680
And I'm also excited about the products and projects that they launch at the O'Reilly AI conference, including version 2.0 of their Nirvana framework and their new Nirvana Graph project.

00:49.680 --> 00:53.320
Be sure to check them out at intelnervana.com.

00:53.320 --> 01:08.560
And if you haven't already listened to the first show in this series, where I interview Naveen Rao, the head of Intel's AI products group and Hanlon Tang and algorithms engineer on that team, it's Twimble Talk number 31 and you definitely want to check it out.

01:08.560 --> 01:11.840
My guess for this show is Rana Alkayubi.

01:11.840 --> 01:23.640
Rana is co-founder and CEO of Affectiva. If you remember my conversation about Emotional AI with Pascal Fung from last year's O'Reilly AI conference, you're going to love this one.

01:23.640 --> 01:32.600
My conversation with Rana kind of picks up where that one left off with a focus on how her company is bringing Emotional AI services to market.

01:32.600 --> 01:36.160
It's a great conversation and I know you'll love the show.

01:36.160 --> 01:37.160
Let's get to it.

01:37.160 --> 01:52.840
All right, everyone. I am on the line with Rana Alkayubi, who is the co-founder and CEO of Affectiva.

01:52.840 --> 02:00.760
Rana was here at the O'Reilly AI conference and delivered a presentation on Emotional AI.

02:00.760 --> 02:13.160
This topic may sound familiar to you because last year at the O'Reilly AI conference, I interviewed one of her colleagues in the field, Pascal Fung.

02:13.160 --> 02:23.800
And so I'm looking forward to speaking with you, Rana, to learn a little bit about Affectiva and how you're applying Emotional AI and making it useful for enterprises.

02:23.800 --> 02:25.960
Welcome to the show, Rana.

02:25.960 --> 02:36.200
Thanks, Sam. Thanks for having me. So, you know, when we look at human intelligence, there's many aspects to that and one important aspect is Emotional Intelligence.

02:36.200 --> 02:47.560
And that references the ability that we each have to read and understand other people's emotional and social and mental states and then adapt our behavior accordingly.

02:47.560 --> 03:02.520
And so, based on years of research, we know that people who have higher emotional intelligence tend to do better in life. They're more likable, they're more persuasive, they are more successful in their professional lives, and they even live longer.

03:02.520 --> 03:15.480
So, our thesis is that advanced AI systems and autonomous vehicles and, you know, all the technologies that are surrounding us today, they all have high cognitive intelligence,

03:15.480 --> 03:27.480
but they're really lacking in this emotional intelligence area. And we are, you know, my team and I are on a mission to humanize technology by bringing in artificial emotional intelligence.

03:27.480 --> 03:43.720
We see that there are two sides to this coin. One is to build technologies and digital experiences and even devices that can sense emotional, you know, consumers emotional states in real time and adapt to that accordingly.

03:43.720 --> 03:59.960
So, that's kind of one stream of work that we do. The other is more around gathering data about how people feel towards things. It could be online video ads or product concepts or new experiences and we collect all that data and aggregate it.

03:59.960 --> 04:07.720
And as it turns out, this information is very, very important and meaningful for organizations as well as consumers.

04:07.720 --> 04:15.720
Interesting. How did you get started in this area? What led you to study emotional AI?

04:15.720 --> 04:28.720
So, my own background as an undergraduate, I studied computer science. I was fascinated by the role technology plays and actually changing how we connect with one another as human beings.

04:28.720 --> 04:41.720
And then I got my first, you know, living a broad opportunity. I got the opportunity to do my PhD at Cambridge University and at the time I used to live in Egypt, so I moved to Cambridge, UK.

04:41.720 --> 04:50.720
And I realized I was spending more time with my laptop than I was with any other human being. Kind of sad.

04:50.720 --> 04:58.720
And of course, it was frustrating because even though we were intimate, the laptop and I, it had absolutely no idea how I was feeling.

04:58.720 --> 05:07.720
So, there were oftentimes when I would be stressed because I had, you know, a project deadline or a paper deadline and it was just completely oblivious to all of that.

05:07.720 --> 05:15.720
But I also realized that it was my main portal of communication back to my family back home. And this was before we had smartphones.

05:15.720 --> 05:23.720
And I think it's still true with smartphones, right? Like it's, you think of these devices and they are becoming the main portal of communication.

05:23.720 --> 05:24.720
Absolutely.

05:24.720 --> 05:34.720
Exactly. But yet I felt like even though, you know, this kind of, you know, social platforms have allowed us and messaging apps and all that have allowed us to connect with more people.

05:34.720 --> 05:49.720
The quality of the connection is quite core because a lot of the nuances and the non-verbals are kind of lost in cyberspace. So I, you know, one example that really kind of stuck in my brain and has motivated a lot of my research.

05:49.720 --> 05:57.720
I remember feeling very lonely in Cambridge and, you know, I'd be chatting with my family back home and I'd be sometimes in tears.

05:57.720 --> 06:07.720
Like I'd be really upset and they would have absolutely no clue until I send them that little emoji with tears, you know, I would be like, oh, so you're kind of sad.

06:07.720 --> 06:20.720
And it just, it was very explicit and it took away from the gen, you know, what do you call it, the sincerity of the emotion and that made me realize that we could do better.

06:20.720 --> 06:41.720
And so I started imagining what would it take to build a computer that had some emotional intelligence skills the way my friend would or my mom does, you know, and just got very interested in the psychology aspect of human emotional intelligence and then how to replicate that in a machine.

06:41.720 --> 06:52.720
It is really interesting the degree to which, you know, something as simple as an emoji expands the bandwidth we have for communicating emotion.

06:52.720 --> 07:10.720
And so I'm curious how you, you know, with that as kind of a backdrop, tell us a little bit about affectiva and the specific things that you guys are offering to allow companies to better, you know, harness emotion.

07:10.720 --> 07:27.720
Yeah, so like all great things I came to affectiva, you know, my accident. So my plan, my career plan was to do my PhD and then apply for a faculty position and take an academic and research career.

07:27.720 --> 07:41.720
I ended up joining Professor Rosalind Picard's group at MIT Media Lab. So she wrote the bill, affective computing in the late 90s and that's kind of what inspired me to get into this research space.

07:41.720 --> 07:42.720
Okay.

07:42.720 --> 07:54.720
And I met with her and ended up joining her lab at MIT in 2006 and she and I collaborated on the application of emotion sensing to specifically mental health.

07:54.720 --> 08:03.720
And being at MIT Media Lab twice a year, we would invite all these sponsor companies and they would say, you know what, we're very interested in this work you're doing.

08:03.720 --> 08:13.720
Like we want to apply to advertising research or, you know, monitoring drivers in cars, there was just like a whole slew of applications.

08:13.720 --> 08:23.720
And after about like four years of doing this at MIT, we decided that there was a commercial opportunity and that the market was ready for, you know,

08:23.720 --> 08:27.720
was kind of technology. So we spun affective out of the media lab.

08:27.720 --> 08:28.720
Okay.

08:28.720 --> 08:33.720
With the vision of bringing emotion AI to the market in a commercializable way.

08:33.720 --> 08:44.720
And it's been a few years, we found that, you know, we still have a very broad vision. We still see that this technology is a core AI capability that's going to play in a lot of areas.

08:44.720 --> 08:54.720
The idea is to sequence the markets. And so the first use case, if you like, where we've applied this technology is an advertising research and market research.

08:54.720 --> 08:59.720
And the problem we're solving there is, you know, like think about Coca-Cola or Kellogg's or Pepsi.

08:59.720 --> 09:06.720
They want to understand how people are engaging emotionally with their content.

09:06.720 --> 09:07.720
Interesting.

09:07.720 --> 09:16.720
Yeah, because emotions drive behavior. So, right, if you feel very positive or inspired by an ad, you're going to share it with your network.

09:16.720 --> 09:21.720
You're going to go out and, you know, you're going to build a positive brand affinity or loyalty.

09:21.720 --> 09:22.720
You're going to buy the product.

09:22.720 --> 09:31.720
So a lot of, you know, a lot of the marketers and advertisers work with want to elicit a very strong positive emotion.

09:31.720 --> 09:41.720
Sometimes it's sentimental and it's not necessarily positive, but they want to drive an emotional journey, but they struggle to measure how effective they are doing it.

09:41.720 --> 09:46.720
And before our technology, what they did is they just asked people, okay, how are you feeling about this?

09:46.720 --> 09:52.720
And we know that this is very unreliable and susceptible to all sorts of bias.

09:52.720 --> 09:55.720
Right. It's expensive as well.

09:55.720 --> 10:06.720
So what we do instead is we use our technology, which is essentially a machine learning and a computer vision platform that uses a camera on any device.

10:06.720 --> 10:12.720
And it reads your facial expressions in real time and it maps it to an emotional state.

10:12.720 --> 10:13.720
Oh, interesting.

10:13.720 --> 10:29.720
So do you position it as kind of a virtual focus group type of opportunity or is it more, I guess, passive observation of people engaging in, you know, real media experiences?

10:29.720 --> 10:30.720
Does that question make sense?

10:30.720 --> 10:31.720
Yes, absolutely.

10:31.720 --> 10:32.720
It's both.

10:32.720 --> 10:46.720
So most of the work we've done to date is the latter. So we send people a URL and we say, okay, if you click on this, do you have your permission to turn the camera on while you're watching this movie trailer?

10:46.720 --> 10:53.720
And if they say yes, all they do is they, you know, they watch the movie trailer, the cameras on, it's streaming their responses.

10:53.720 --> 11:05.720
We do that for a thousand people and then we aggregate their responses and we end that dashboard, that moment by moment journey is what we present to, you know, to the client or the TV studio.

11:05.720 --> 11:06.720
Okay.

11:06.720 --> 11:07.720
Interesting.

11:07.720 --> 11:14.720
I can't help but think about a conversation that I had yesterday with Raza Zade.

11:14.720 --> 11:23.720
Raza Zade, you know, Raza from matrioid.

11:23.720 --> 11:41.720
What he and his company is doing is they are indexing TV streams and allowing companies to build detectors that can identify, you know, different brands and objects in the TV screen in the streams, not necessarily TV, but, you know, media streams.

11:41.720 --> 11:54.720
And I wonder if the degree to which some of the brands as they have, you know, the product placements are huge in some of these TV shows and things like that.

11:54.720 --> 12:13.720
You know, if they'd get some additional insight by having kind of real time, you know, detection of emotional response to product placement, kind of in situ, in context, in a, you know, media as media is being consumed random thought.

12:13.720 --> 12:33.720
We often get asked, you know, we will often get approached by an advertiser who, you know, the ad, they are featuring two celebrities and they want to know which version resonates the most or, yeah, they often play with different product placements or different music in the background.

12:33.720 --> 12:52.720
And the idea is to really detect, because it's often hard for people to articulate if they like something, it's very hard to articulate what is it that resonated and so we can detect your visceral reaction on a frame by frame basis so we can say, okay, when, you know, when the product revealed happened, this is what we saw.

12:52.720 --> 13:10.720
And I've got to imagine that for, you know, obviously emotion is very subtle and the expression of emotion, particularly if you're where I'm assuming you're using laptop cameras and we're looking primarily looking at face, you know, we're talking about fine motor movements, things like that.

13:10.720 --> 13:20.720
How do you convince the skeptic that, you know, this technology can really accurately pinpoint an emotional response.

13:20.720 --> 13:29.720
So when we first started doing this work, the very first question we had to answer was, you know, when you're in front of a laptop, do you even emote, right?

13:29.720 --> 13:32.720
Because there's no other human, right?

13:32.720 --> 13:40.720
But I'm smiling and laughing in front of my laptop now as I'm engaging with you, but yes, I can, I definitely get the question.

13:40.720 --> 13:58.720
If you're just watching an online video ad, do you even show any expressions? And so we collected, I mean, now, you know, today we've collected five and a half million face videos from 75 countries in the world of people interacting with their laptops, with their phones, playing games, driving and cars.

13:58.720 --> 14:10.720
So we have a very substantive data set of people's emotions. But when we first started, we did not have all this data. And so we initially collected data of people watching Super Bowl ads.

14:10.720 --> 14:17.720
And it was, you know, we knew that some of them were successful and went viral and garnered a lot of attention and some of them didn't.

14:17.720 --> 14:33.720
And we found that, yes, people emote expressions, even if there is no other human being present, and we saw the entire gamut of expressions we saw laughter, we saw swerks, when people were kind of, you know, really skeptical.

14:33.720 --> 14:41.720
We saw confusion, we saw boredom. And that data set became the data that we used to train our algorithms.

14:41.720 --> 14:56.720
So we used, you know, deep learning to train models to detect very, very nuanced expressions. You know, the machines now able to or the algorithm is now able to identify 20 different facial expressions.

14:56.720 --> 15:08.720
And it maps them to eight different emotional states. And just to give you an example, some of the expressions that we're able to identify now include a smile or a brow farrow, which are kind of obvious.

15:08.720 --> 15:21.720
But then it also is able to read like a lip sock or an ice quint, you know, like very, very fleeting nuanced expressions. And that, that's all because we have the data to drive the machine learning.

15:21.720 --> 15:33.720
And I recall from Pascal that there, there's a standardized chart for emotions like it's there are two axes and the emotions are mapped out on these axes.

15:33.720 --> 15:37.720
Do you know what I'm referring to? And can you remind me what that's called?

15:37.720 --> 15:51.720
Yes. So there's a certain complex model of emotions and basically that maps or it's the dimensional model of emotions. So it, it posits that you can think of emotions in terms of these main dimensions like valence.

15:51.720 --> 16:00.720
Valence is how positive or negative your experience is. And that actually is really, you can get a very accurate measure of valence from the face.

16:00.720 --> 16:16.720
And then the other axes is arousal, kind of the intensity of the emotion. And that is best quantified through your voice. And also your physiology like if you're measuring heart rate or heart rate variability or skin conductance.

16:16.720 --> 16:32.720
Okay. And so is the implication then that your technology is primarily concerned with valence or do you attempt to measure both and map everything on to this two dimensional model?

16:32.720 --> 16:43.720
So it's interesting because we have a valence measure and we have a proxy to arousal. We use like, you know, level of engagement, which is how much emotions are you showing?

16:43.720 --> 16:54.720
Yes, fixation and stuff like that. Yeah, exactly. Are you paying attention or not? Are you fidgeting too much? So all of these measures could be kind of indirect proxies to arousal.

16:54.720 --> 17:06.720
But we're also very interested in measuring discrete emotions. So that's an alternative model for thinking about emotional states. And that is triangulating exactly what the emotion is. So is it joy?

17:06.720 --> 17:22.720
Is it surprise? Is it confusion? Is it contempt? And the face again is quite good at the specificity of the emotion. Whereas if you're looking at heart rate variability, it's often very hard to depict what kind of emotion is it?

17:22.720 --> 17:37.720
Even though, you know, you know, it's a higher-ousal emotion, but it's hard to tell what exactly it is. We also provide our partners and our customers with, you know, a specific emotion measure. And we give you a probability scores because sometimes they co-occur.

17:37.720 --> 17:42.720
Sure, sure. Have you applied this at all in the political sphere?

17:42.720 --> 17:52.720
Not in the most recent political cycle. In fact, the most recent political cycle, we have a partner company called Higher View. They are in the recruitment space.

17:52.720 --> 18:05.720
So they work with big companies that hire many, many people, but they have to screen, you know, thousands of people. And instead of requiring that a candidate sends in a word resume, you send in a short video interview.

18:05.720 --> 18:23.720
Yep. And then they use our technology to rank order the video. So, you know, if you're a sales, if you're applying for a sales position or a flight attendant, having social skills is really a key component of that job. And so we're able to quantify that.

18:23.720 --> 18:42.720
And so anyway, so their position was that, you know, the presidential debates are the ultimate job interview. So they, you know, they use our technology to analyze both Trump and Clinton's expressions. And we published that. So it's available online.

18:42.720 --> 18:48.720
Oh, interesting. Interesting. I'll get the URL from you on that and we'll include it in the show notes.

18:48.720 --> 18:58.720
Tell me a little bit about the underlying technology said it's based on deep learning. How is the technology evolved? And how would you characterize the approach?

18:58.720 --> 19:09.720
Yeah, so it's so we use deep learning. The idea is that we collect, you know, hundreds of thousands of images of people and videos of people expressing emotion in the wild.

19:09.720 --> 19:20.720
So it's not your selfies. It's not, right? It's not like your Facebook profile pictures. It's actually people, you know, sitting in front of a TV and just like chilling, right?

19:20.720 --> 19:29.720
Or driving around Boston in their car in the typical Boston traffic. So it's very spontaneous data, which means that it's very hard data.

19:29.720 --> 19:39.720
Like the background is messy. Sometimes it's quite dark. There's multiple people in the video. The expressions are not exaggerated. In fact, they're very nuanced and subtle.

19:39.720 --> 19:48.720
So it's real data. It's what you'd see in the real world. And we take that data. We get a portion of it labeled for training, a portion of it labeled for validation.

19:48.720 --> 20:00.720
And that becomes the data we inject into the, say, the convolutional neural nets. We also have temporal models because expressions are very dynamic and they unfold over time.

20:00.720 --> 20:09.720
And over the years, we've continued to accumulate this data. So we now have about 2 billion facial frames. Not all of it is labeled.

20:09.720 --> 20:23.720
And we've recently decided as a company that it's time to include other modalities. So we've started to research your emotions as they are represented in your voice as well, not just your face.

20:23.720 --> 20:43.720
OK, OK, make sense. How did you collect all of this data? Do you have an app that provides some value to the consumer to incent them to just turn it on when they're driving or watching TV or is it all via clients?

20:43.720 --> 20:53.720
I mean, at some point you had to kickstart this or cold start it and accumulate data before you had clients. How did you go about doing that?

20:53.720 --> 21:06.720
We partnered with the very, very first project we did was with Forbes. And we put this thing on their website. And you can just go watch Super Bowl ads with the camera turned on. Then you would see your emotional profile.

21:06.720 --> 21:18.720
But since then, we've done, you know, we've done this in 75 countries around the world through partners. So we have partners that are collecting this data as, you know, as part of, you know, testing ads.

21:18.720 --> 21:30.720
OK, now it sounds a little bit like was it OK, keep it or the dating site that did the quizzes to get people to psychologically profile themselves.

21:30.720 --> 21:44.720
One of the dating services was known for doing these small personality quizzes that were kind of entertaining for the people that were participating in them. But as a result, they were helping the company collect lots of profile data.

21:44.720 --> 21:53.720
And you're doing something similar. It sounds like with your partners around ads and other media experiences.

21:53.720 --> 22:05.720
So you're using convolutional neural nets and you mentioned some time domain neural nets as well, like LSTMs I'm imagining.

22:05.720 --> 22:20.720
What is the, like, can you tell me a little bit about the data pipeline? How do you manage all of this video and get it ready to train the networks?

22:20.720 --> 22:36.720
Yeah, so we have a cloud-based infrastructure that we've invested a lot into building that over the years. So all of our data goes into S3, then we use Amazon Web Services to do all the, so all the training is done in the cloud.

22:36.720 --> 22:54.720
We also built our own cloud-based data labeling or video labeling infrastructure. So, and we have a team of certified, you know, face coders who, you know, basically their job is to watch these videos and identify when an emotional expression occurs.

22:54.720 --> 23:12.720
And then we've built, again, in the cloud, we have this infrastructure that takes all this data and says, okay, there's agreement on the set of videos. There's no agreement on the set of videos. If there is agreement on a set of videos, that becomes, you know, split, it gets split into training and validation data.

23:12.720 --> 23:24.720
And if there's no agreement, it goes back to the labeling tile. We also use active learning because we have a ton of videos. A lot of it has no emotions. Like if you're driving around Boston for the most part, you're just neutral.

23:24.720 --> 23:35.720
We don't necessarily care about all these frames. So we use what we call active learning, which is we use the machine learning models to bootstrap the labeling.

23:35.720 --> 23:53.720
So the machine takes a first stab at the labels and then based on what the machine says, we either fork it to actual human labelers for more labeling and more fine tuning or we kind of, you know, not throw the data away, but we park the data if it's not interesting.

23:53.720 --> 24:14.720
So it's a human in the loop labeling infrastructure. And then once we have that, we say, for instance, you know, so let me take a step back. We do have a cloud based set of APIs where you would send a set of frames or videos and we would send you back the emotion probabilities in that video.

24:14.720 --> 24:31.720
We have our partners use the technology that way. We call it emotion as a service, but we also have a number of partners that are developing real time experiences like think of a social robot that needs to respond to you in real time or Amazon Alexa kind of.

24:31.720 --> 24:49.720
And so we can't be streaming, you know, tons of videos to the cloud and waiting for a response. And in that case, we have developed on device software development kits so that you could integrate the core engine into your device or your app and have run have everything run on the device.

24:49.720 --> 25:08.720
You don't need to talk to the cloud at all. And so in that case, there's a trade off between accuracy and speed and computational weight, you know, how lightweight is it. And so we run experiments in the cloud, you know, where we say, okay, we're going to train with, I don't know, 200,000 samples.

25:08.720 --> 25:28.720
And we are going to evaluate this model on how accurate it is across all these different emotions, but also how fast it is and how heavy it is. And that all happens in the cloud and our scientists, you know, will evaluate, you know, we come in the morning and there's a spreadsheet of all the different experiments that were run overnight.

25:28.720 --> 25:40.720
And our team will kind of evaluate, okay, which ones, if we want one for the mobile SDK, then we're going to really care about how fast it is and how lightweight it is.

25:40.720 --> 25:41.720
Does that make sense?

25:41.720 --> 25:47.720
It does. It does. So there's mobile SDK. What are the platforms that that runs on?

25:47.720 --> 25:58.720
We have it running on iOS, Android, Unity 3D Raspberry Pi, which was quite challenging windows, Linux, Mac OS X.

25:58.720 --> 26:02.720
And was Raspberry Pi challenging because of RAM and CPU limitations?

26:02.720 --> 26:03.720
Yeah, exactly.

26:03.720 --> 26:23.720
Okay, okay. And is the idea with a Raspberry Pi type of implementation that it might be the back-and-compute module for a camera that's doing, you know, some kind of set top box or surveillance type of application?

26:23.720 --> 26:36.720
We stay away from surveillance applications, but set top box or, you know, a connected home device that is, yeah, that is powering a social robot or, you know, a home monitor.

26:36.720 --> 26:43.720
And why do you stay away from surveillance? Is that an ideological decision or technological decision?

26:43.720 --> 27:05.720
It's ideological. So when we spun out my co-founder Rosalind Picard and I basically made a decision that we would, you know, in respecting how important and how personal this data is, that we would only work in use cases or industries where you knew that the camera was on.

27:05.720 --> 27:13.720
And you knew that you were being analyzed in this way and we would veer away or steer away from applications where that was not the case.

27:13.720 --> 27:18.720
We have stayed away from surveillance and security applications, yeah.

27:18.720 --> 27:20.720
Well, I certainly respect and appreciate that.

27:20.720 --> 27:32.720
I've seen some very scary surveillance demos recently and to imagine layering on this emotional context, which I'm, you know, it's one of those things.

27:32.720 --> 27:42.720
It's always one of those things where like someone's going to do it at some point, but I can appreciate your decision to not be that someone that does it.

27:42.720 --> 27:49.720
Right. Our thesis is if we're going to spend a lot of our mindset and our passion and our energy solving a problem.

27:49.720 --> 27:52.720
You know, there's a lot of other problems we can solve.

27:52.720 --> 27:53.720
Yeah.

27:53.720 --> 27:57.720
Right. Right. Where do you see the space going?

27:57.720 --> 28:05.720
Very exciting. So the ultimate vision is that our devices on our technologies are going to have emotion AI embedded in them.

28:05.720 --> 28:11.720
We see a world where, you know, this is going to run passively in your phone. It's going to track your mood.

28:11.720 --> 28:24.720
And on the path to that, we're seeing a lot more interest in the automotive space, for instance, where our kind of technology can help improve safety, especially in a world where we have semi autonomous vehicles.

28:24.720 --> 28:33.720
So you can imagine, you know, your car is on autopilot, but it may need to relinquish control back to you as a driver.

28:33.720 --> 28:41.720
And so the question becomes, you know, in a matter of seconds, it needs to know, are you awake or not? Are you paying attention or not?

28:41.720 --> 28:44.720
And that's where our technology comes into play.

28:44.720 --> 28:54.720
Yeah, I can also imagine a world where you've got digital billboards and you have some driver opt-in, some driver opt-in to this system.

28:54.720 --> 29:03.720
Maybe they get, you know, some benefit or compensation, but you're reading their emotion as they're engaging, quote, unquote, with these billboards.

29:03.720 --> 29:22.720
Lots of interesting applications. And I can, you know, certainly as, well, I was going to talk about, you know, being a New York City born in bread driver and road rage, but I probably won't go there and producers will cut this segment out of the conversation.

29:22.720 --> 29:33.720
Road rage is definitely one of the emotions that we get asked about, but there's actually, oh yes, yes, road rage, drowsiness, distraction.

29:33.720 --> 29:36.720
And you know what, too, like just general infotainment.

29:36.720 --> 29:50.720
So a lot of these cars are now rethinking the brand experience, the in-cap brand experience, and they want to, you know, get a sense of the sentiment inside the vehicle, not just the driver or the cold pilot, but also the passengers,

29:50.720 --> 29:59.720
and adapt the experience about the lighting, adapt the music, adapt the general environment and personalize it. So we get a lot of interest around that as well.

29:59.720 --> 30:13.720
Okay, so that's where the business and use cases are going. Where do you see the technology going to, are there things the technology needs to go to enable these, you know, new and more ubiquitous use cases?

30:13.720 --> 30:29.720
Yeah, definitely multimodal. So combining both, you know, computer vision with speech to get a better understanding of how you feel or, you know, your social and emotional signals, and that's especially important in conversational context.

30:29.720 --> 30:34.720
So it's okay to just focus on your facial expression if you're just watching a movie, that's fine.

30:34.720 --> 30:45.720
But if you are in a conversation with a chatbot or a personal assistant or, you know, social robot, or your car, then combining all these different modalities is going to be key.

30:45.720 --> 30:54.720
That's definitely one class of product development and, you know, core science development that we're focused on.

30:54.720 --> 31:14.720
But just more generally, there's a lot of applications in, say, healthcare, where there has been academic research that showed that, you know, your facial expressions and your voice can, can be biomarkers for pain, can be by markers for depression, even suicide assessment.

31:14.720 --> 31:26.720
Oh, wow. So I would like to really see the technology move into that area and that would require, you know, substantial data collection of healthy population versus, you know, a depressed population.

31:26.720 --> 31:32.720
But it's, yeah, I think that's where the technology can be really transformative.

31:32.720 --> 31:39.720
Fantastic. Fantastic. Well, thank you so much for taking the time to speak with us.

31:39.720 --> 31:49.720
These applications, this whole emotional space is one that I find really fascinating and I'll definitely be keeping my eye on what you guys are doing.

31:49.720 --> 31:52.720
And, yeah, I wish you the best of luck. Thanks so much.

31:52.720 --> 31:54.720
Thank you. Thanks for having me.

31:54.720 --> 31:56.720
All right. Thanks, Ronald.

31:59.720 --> 32:07.720
All right, everyone. That is our show. Thanks so much for listening and for your continued support, comments and feedback.

32:07.720 --> 32:11.720
Special thanks goes out to our series sponsor, Intel Nirvana.

32:11.720 --> 32:27.720
If you didn't catch the first show in the series where I talked to Naveen Rao, the head of Intel's AI product group about how they plan to leverage their leading position and proven history in Silicon innovation to transform the world of AI, you're going to want to check that out next.

32:27.720 --> 32:33.720
For more information about Intel Nirvana's AI platform, visit intelnervana.com.

32:33.720 --> 32:41.720
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference to enter.

32:41.720 --> 32:52.720
Just let us know what you think about any of the podcasts in the series or post your favorite quote from any of them on the show notes page on Twitter or via any of our social media channels.

32:52.720 --> 33:03.720
Make sure to mention at Twomo AI, at Intel AI, and at the AI come so that we know you want to enter the contest.

33:03.720 --> 33:06.720
Full details can be found on the series page.

33:06.720 --> 33:11.720
And of course, all entrants get one of our slick Twomo laptop stickers.

33:11.720 --> 33:22.720
Speaking of the series page, you can find links to all of the individual show notes pages by visiting Twomo AI.com slash O'Reilly AINY.

33:22.720 --> 33:42.720
Thanks so much for listening and catch you next time.

