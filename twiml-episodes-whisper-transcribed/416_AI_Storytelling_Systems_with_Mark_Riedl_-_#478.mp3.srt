1
00:00:00,000 --> 00:00:15,840
All right, everyone, I am here with Mark Redell. Mark is a professor at Georgia Institute

2
00:00:15,840 --> 00:00:19,080
of Technology. Mark, welcome to the Twomla app podcast.

3
00:00:19,080 --> 00:00:21,320
It's delightful to be here. Thank you.

4
00:00:21,320 --> 00:00:29,320
Hey, I'm really looking forward to our chat. We've been Twitter friends, Twitter connecties

5
00:00:29,320 --> 00:00:36,600
for quite a while, but it's been I think a long time in the works trying to get this conversation

6
00:00:36,600 --> 00:00:42,000
going. So thanks once again, and welcome. You know, we usually start these conversations

7
00:00:42,000 --> 00:00:48,520
by having you introduce yourself to our audience. Tell us a little bit about your journey

8
00:00:48,520 --> 00:00:50,960
and how you came to work in AI.

9
00:00:50,960 --> 00:00:55,440
Yeah, well, when I was a kid, I watched this movie called Tron, and I've been trying

10
00:00:55,440 --> 00:01:06,320
to get it to the gaming grid ever since the one of the quality of Tron, but the maybe

11
00:01:06,320 --> 00:01:10,520
that went back too far, right? How do you feel about the reboot?

12
00:01:10,520 --> 00:01:16,320
You know, it's okay. They changed the lights, cycles can't be too happy about that, but

13
00:01:16,320 --> 00:01:23,920
you know, ever since then, it's kind of been a series of happy accidents, but you know,

14
00:01:23,920 --> 00:01:28,320
my voyage into artificial intelligence and machine learning came through human computer

15
00:01:28,320 --> 00:01:33,920
interaction. So I get really interested in this in school, took way too many psychology

16
00:01:33,920 --> 00:01:38,840
and human factors classes, and that was even before discovering this thing called artificial

17
00:01:38,840 --> 00:01:42,960
intelligence. And the day I took that class, I was hooked. And then somewhere along the

18
00:01:42,960 --> 00:01:51,200
way, I got invited to explore some research, got invited to look at AI for computer games,

19
00:01:51,200 --> 00:01:54,960
and from that point on, like everything just kind of came together.

20
00:01:54,960 --> 00:02:01,520
Nice, nice. When you started with this focus on AI and computer games, what were some

21
00:02:01,520 --> 00:02:04,000
of the things that you were looking at?

22
00:02:04,000 --> 00:02:08,120
Well, we were looking at, and this was, gosh, I don't know, I don't want to say how many

23
00:02:08,120 --> 00:02:14,320
years ago this was, but we were looking at adaptive games. So we wanted to know whether

24
00:02:14,320 --> 00:02:18,920
games could change based on what the user wanted to do or what the interests of the user

25
00:02:18,920 --> 00:02:23,000
were. And this is sometimes called interactive storytelling, the idea that you don't have

26
00:02:23,000 --> 00:02:27,000
to follow a linear course through a computer game with all these plot points, you could

27
00:02:27,000 --> 00:02:31,480
decide to do something different. Like maybe you could even decide to be the bad guy, right?

28
00:02:31,480 --> 00:02:36,760
And then the story would have to change, or twist, or add new elements into that. From

29
00:02:36,760 --> 00:02:42,480
there, it became, you know, quest generation, then it became story generation. Then we kind

30
00:02:42,480 --> 00:02:46,520
of eventually put the graphics aside and just said, well, you know, can we just write

31
00:02:46,520 --> 00:02:52,760
a novel? Can we write a story that people would actually want to read? And I've been working

32
00:02:52,760 --> 00:03:02,080
on that ever since. Nice. And so when you think about your kind of broad research, you

33
00:03:02,080 --> 00:03:09,240
know, focus areas under this banner of storytelling, like how do you articulate all the various

34
00:03:09,240 --> 00:03:11,760
things you're working on and how they tie together?

35
00:03:11,760 --> 00:03:16,400
Well, the simplest ties that I get to work on are official intelligence and have fun

36
00:03:16,400 --> 00:03:23,560
because it's stories and games. But but seriously, you know, I think broadly of my umbrella

37
00:03:23,560 --> 00:03:28,320
of artificial intelligence is something called human centered artificial intelligence.

38
00:03:28,320 --> 00:03:33,440
And this is turned out to be mean a lot of different things. But for me, it's about centering

39
00:03:33,440 --> 00:03:39,720
the human inside the algorithm or with algorithms around the human, really instead of pushing

40
00:03:39,720 --> 00:03:44,960
the human off to the side. And to for me, that means can we build AI systems that enrich

41
00:03:44,960 --> 00:03:50,320
the human experience? So entertainment is one way of enriching the human experience storytelling

42
00:03:50,320 --> 00:03:55,760
as another way. We can do it through design. So building explanation systems that help

43
00:03:55,760 --> 00:04:00,600
people do their jobs better. But also, you know, you can kind of flip the equation around

44
00:04:00,600 --> 00:04:04,760
and say, well, if you want to put humans, if you want to enrich the human condition, do

45
00:04:04,760 --> 00:04:09,080
we have to build AI systems that actually understand the human condition? Because maybe

46
00:04:09,080 --> 00:04:13,320
AI systems and humans are a little bit alien to each other. They think in very different

47
00:04:13,320 --> 00:04:18,080
ways, the algorithms and the computer are not the same as the mental processes that

48
00:04:18,080 --> 00:04:23,040
go on inside the human mind. So how do we reconcile that bridge or how do we build a bridge

49
00:04:23,040 --> 00:04:28,240
between these two sorts of alien intelligences? And then how do we use it to do interesting

50
00:04:28,240 --> 00:04:37,160
fun or even fundamental things? And so in the context of storytelling, where do those

51
00:04:37,160 --> 00:04:44,760
two threads connect? I look at storytelling as actually this thing that brings a lot

52
00:04:44,760 --> 00:04:50,720
of different AI problems together. So if you think about humans, let's start with humans,

53
00:04:50,720 --> 00:04:54,560
right? Storytelling is this fundamental part of the human experience, the human life,

54
00:04:54,560 --> 00:05:00,160
right? We do use it for entertainment, but we also use it for education. We use it for

55
00:05:00,160 --> 00:05:07,000
training. We use it to build rapport human to human. We kind of chat around the dining

56
00:05:07,000 --> 00:05:12,840
room table. What was your day like? These sorts of things. So this is this way of humans

57
00:05:12,840 --> 00:05:20,280
connecting to other humans in a very rich modality of communication. But this is something

58
00:05:20,280 --> 00:05:24,640
that computers don't engage in. They don't understand our stories. They don't understand

59
00:05:24,640 --> 00:05:28,040
why we tell stories and they can't communicate back through stories even when that's the

60
00:05:28,040 --> 00:05:33,320
most appropriate use of communication. And just to kind of like giving an example of

61
00:05:33,320 --> 00:05:37,600
why this is such a, or how this is such a powerful modality, like one of my favorite

62
00:05:37,600 --> 00:05:42,560
stories in the world is only six words long. It's attributed to Hemingway. It's, you

63
00:05:42,560 --> 00:05:47,200
might have heard of it, it's baby shoes for sale. Never worn. Yeah. Right. Those six

64
00:05:47,200 --> 00:05:52,520
words can convey, convey just a massive amount of emotion. There's a story behind it.

65
00:05:52,520 --> 00:05:57,200
Why are these shoes for sale? Why were they never worn? Should I be sad? Is there something

66
00:05:57,200 --> 00:06:04,080
else going on here? Right. But in just conveying that, I can convey these rich mental processes

67
00:06:04,080 --> 00:06:09,920
inside the listener. And, you know, computers are not communicating like that. And the flip

68
00:06:09,920 --> 00:06:13,440
side is, you know, if we said that back to the computer, they're not going to understand

69
00:06:13,440 --> 00:06:20,960
all the nuance that went into those six words. Right. Right. So how do you begin to get

70
00:06:20,960 --> 00:06:30,220
a computer to think about what the listener is thinking about? Yeah. Well, that is the

71
00:06:30,220 --> 00:06:35,440
central challenge. Right. So because this is human, human communication, you do want

72
00:06:35,440 --> 00:06:42,000
to think about what the listeners think about. You know, right now we do a lot with, um,

73
00:06:42,000 --> 00:06:47,000
in artificial intelligence and machine learning with big data sets. Um, this has been a,

74
00:06:47,000 --> 00:06:53,080
a godsend, right? I mean, we see amazing things. We see GPT 2 and GPT 3, these text generators

75
00:06:53,080 --> 00:06:59,440
that just kind of spit out such human-like, um, language. But they're also not very good

76
00:06:59,440 --> 00:07:03,500
at planning out what they're saying. They're kind of these, what are, there's sometimes

77
00:07:03,500 --> 00:07:08,280
referred to as intelligent babblers or some outstanding babblers. They kind of say, here

78
00:07:08,280 --> 00:07:12,280
statistically, if I have this at words, I should say this next word, then I should say

79
00:07:12,280 --> 00:07:17,680
this next word and they should, I should say this next word. That's very good for creating

80
00:07:17,680 --> 00:07:23,520
text that looks kind of plausible. Right. But stories are not just about babbling, right?

81
00:07:23,520 --> 00:07:30,440
Stories have a goal. Stories have a point. So when I, here I am talking to you Sam and,

82
00:07:30,440 --> 00:07:33,560
you know, you asked me a question, I have to figure out, you know, here's my goal. Here's

83
00:07:33,560 --> 00:07:38,320
the point I want to drive to. How hard do I lay out those words and thoughts, um, to

84
00:07:38,320 --> 00:07:43,760
get to my goal? So I've got to plan these things out. So the first part of it is, how do

85
00:07:43,760 --> 00:07:48,640
you do planning and language? Instead of looking backwards to what I've said in the past

86
00:07:48,640 --> 00:07:53,920
and whether my next word is statistically plausible, I have to be looking to the future and

87
00:07:53,920 --> 00:07:59,080
saying, based on where I want to go, does this next word or next sentence, um, support

88
00:07:59,080 --> 00:08:05,200
my goal? And then the next part, and that's hard enough, by the way. But the next part

89
00:08:05,200 --> 00:08:12,440
is, um, you know, kind of a theory of mind, which is, um, am I choosing words or statements

90
00:08:12,440 --> 00:08:18,200
or phrases that will have the cognitive effect on the hearer on the listener or on the reader

91
00:08:18,200 --> 00:08:24,760
that I wanted to? Um, so going back to the baby shoes for sale example, um, you know,

92
00:08:24,760 --> 00:08:30,600
if you're not a parent, you know, maybe that is not a particularly powerful story. If

93
00:08:30,600 --> 00:08:35,640
you are a parent, it is powerful. So what are the, um, cognitive thoughts that are

94
00:08:35,640 --> 00:08:40,760
vote by these particular words? And are they the sorts of thoughts that we want people

95
00:08:40,760 --> 00:08:51,600
to have? Mm-hmm. Um, you, when you talked about the planning part of that, there is,

96
00:08:51,600 --> 00:08:57,040
you know, lots of work around planning in a traditional sense and the application of

97
00:08:57,040 --> 00:09:05,640
machine learning to, uh, you know, logistical planning and, and, um, you know, planning

98
00:09:05,640 --> 00:09:13,920
in a shop floor for robotics and that kind of thing is your work able to draw any inspiration

99
00:09:13,920 --> 00:09:20,360
or even techniques from, you know, other domains and applications of planning or is planning

100
00:09:20,360 --> 00:09:28,040
in the storytelling sense, it's own unique, uh, animal that, that requires, um, ground

101
00:09:28,040 --> 00:09:33,920
up thinking. Oh, no, the reason I love working on storytelling, um, again, is I get to work

102
00:09:33,920 --> 00:09:39,680
on fundamental hard problems that are broader than just storytelling itself. So we, so when

103
00:09:39,680 --> 00:09:44,840
I talk about planning out of story, like we, we take a lot of, we draw a lot from, um,

104
00:09:44,840 --> 00:09:50,680
historical planning, literature, logistics, uh, reinforcement learning, all these sorts

105
00:09:50,680 --> 00:09:54,000
of things. So what we're really trying to do is we're pulling threads together. We're

106
00:09:54,000 --> 00:09:57,600
saying, all right, planning, planning plus language, how does language change the planning

107
00:09:57,600 --> 00:10:02,480
process? Mm-hmm. Um, and so on and so forth, what happens when we put commonsense reasoning

108
00:10:02,480 --> 00:10:05,960
inside the planning process? Maybe UPS doesn't need commonsense reasoning inside their

109
00:10:05,960 --> 00:10:10,360
planning process? Maybe they do. But, you know, we're drawing another thread together and

110
00:10:10,360 --> 00:10:13,960
we're saying we're going to connect a bunch of other things together that other people

111
00:10:13,960 --> 00:10:18,720
have not had to put together because maybe they weren't kind of looking at this holistic

112
00:10:18,720 --> 00:10:24,720
humanistic sort of problem. So when I started working on story generation, uh, we did a

113
00:10:24,720 --> 00:10:30,880
lot of symbolic planning systems. Okay. And actually, we, we were able to generate some

114
00:10:30,880 --> 00:10:37,240
pretty good stories because making a story is not that different from, you know, setting

115
00:10:37,240 --> 00:10:43,880
up the logistics for, you know, a package or a store, right? Um, just, you know, just

116
00:10:43,880 --> 00:10:47,880
we're talking about fairytale characters instead of, you know, packages and whatnot.

117
00:10:47,880 --> 00:10:52,320
Um, you know, and then at some point you run into a limit and say, all right, well, you

118
00:10:52,320 --> 00:10:57,440
know, we're using a lot of hard coded data, um, or models. Now we want to look at more

119
00:10:57,440 --> 00:11:02,840
learned models. Can we now start to learn from reading stories as opposed to typing

120
00:11:02,840 --> 00:11:09,000
in kind of logistical symbols and so on and so forth? So like anything, as we solve

121
00:11:09,000 --> 00:11:13,240
a problem in, in the, in the way we think it's the right way to do it first, then we say,

122
00:11:13,240 --> 00:11:17,880
are we there yet? And if we're not there yet, what do we layer on top? So we layer on top

123
00:11:17,880 --> 00:11:22,520
data, then we layer on top theory of mind, then we layer on top commonsense reasoning and

124
00:11:22,520 --> 00:11:30,040
so on and so forth. And we just build in that way. In the, the case of theory of mind,

125
00:11:30,040 --> 00:11:35,720
but can you give us an example of how you've layered that in?

126
00:11:35,720 --> 00:11:41,640
Yeah. So some of the work that we're doing now, and I should give a lot of props to one

127
00:11:41,640 --> 00:11:46,480
of my former PhD students, Laura Martin, who just graduated as well as some of my other

128
00:11:46,480 --> 00:11:52,080
team, um, we went in and we looked at these, um, neural language models that we see

129
00:11:52,080 --> 00:11:56,800
on in the news all the time. And we said, why aren't they able to, um, why do they make

130
00:11:56,800 --> 00:12:00,800
weird choices in terms of coherence? Why do they go off the rails and kind of go into

131
00:12:00,800 --> 00:12:04,840
weird places? And we, we kind of looked at and say, well, because they're not really

132
00:12:04,840 --> 00:12:12,280
thinking about whether what they say is building up a coherent, um, any connections to the

133
00:12:12,280 --> 00:12:18,120
past. Right. So we then said, well, let's get a neural system to generate. All right.

134
00:12:18,120 --> 00:12:21,880
So now we're dealing with neural networks. But then we said, well, you know what? Let's

135
00:12:21,880 --> 00:12:26,760
actually go back to the past. Like in the past, we had symbols and symbols had meaning.

136
00:12:26,760 --> 00:12:33,000
And we think symbols have meaning to humans. So let's build in parallel a symbolic layer

137
00:12:33,000 --> 00:12:38,560
as well as a neural layer and have the two reference each other. So here's a fact.

138
00:12:38,560 --> 00:12:44,080
I say something. My neural network says something. Let's convey that as a fact. Is this fact

139
00:12:44,080 --> 00:12:52,880
consistent with previous facts? Does this fact build on previous facts? And so now what

140
00:12:52,880 --> 00:12:56,520
we're kind of doing is we're kind of assuming if these are the sorts of facts that people

141
00:12:56,520 --> 00:13:04,360
will recognize in the language itself, then maybe this set of symbolic facts is actually

142
00:13:04,360 --> 00:13:09,520
a reader model. It is what we think the human is thinking about or inferring from the

143
00:13:09,520 --> 00:13:15,280
language, kind of a high level semantic kind of orientation towards the language. And

144
00:13:15,280 --> 00:13:22,640
if we can get that high level semantic model correct, then our stories that we generate

145
00:13:22,640 --> 00:13:27,280
should come out more coherent because we can pick and choose. We can say if I say this,

146
00:13:27,280 --> 00:13:31,400
this connects to my symbolic layer. If I say this other thing, it doesn't connect to

147
00:13:31,400 --> 00:13:36,880
my symbolic layer. So it's probably off topic. And we were able to show that we can in fact

148
00:13:36,880 --> 00:13:42,080
improve the coherence of stories. We can keep the stories on track longer. And people

149
00:13:42,080 --> 00:13:46,320
report that they see more coherent as a whole.

150
00:13:46,320 --> 00:13:53,080
Nice. And this is using off the shelf language models, like some of the ones you mentioned,

151
00:13:53,080 --> 00:14:00,120
GPT2, GPT3, that kind of thing. Yeah, we do a lot with the, we might do some fine tuning

152
00:14:00,120 --> 00:14:04,800
on them, whatever. But by and large, we're taking these very large models, much larger

153
00:14:04,800 --> 00:14:09,560
than the ones that we can train or interested in training. And we're basically putting a

154
00:14:09,560 --> 00:14:14,040
control layer on top. We're saying, you're not allowed to say something unless it connects

155
00:14:14,040 --> 00:14:21,160
to my theory of how humans understand the language. So if you babble, and it doesn't connect

156
00:14:21,160 --> 00:14:26,680
to my theory, my control theory, then I reject you and I ask you to come up with something

157
00:14:26,680 --> 00:14:38,040
else. And your control plane control theory that sounds like it's not a learned layer or

158
00:14:38,040 --> 00:14:46,560
mechanism, is it, or is it, or is it, you know, exclusively rules based, like how do you

159
00:14:46,560 --> 00:14:52,920
create that symbolic layer? Yeah, it's a combination of multiple things. We've looked at multiple

160
00:14:52,920 --> 00:15:00,600
ways of doing this. We find that there's a lot of heuristics or rules that we can apply

161
00:15:00,600 --> 00:15:06,360
there, which kind of dips back into the old classical planning, sorts of literatures,

162
00:15:06,360 --> 00:15:11,920
cognitive science, so on and so forth. Sometimes we also turn to learned models. You know,

163
00:15:11,920 --> 00:15:19,040
we might say, well, what can we teach an AI system, a second AI system, to predict

164
00:15:19,040 --> 00:15:26,360
what humans will think. And, you know, so teaching AI systems to understand common sense.

165
00:15:26,360 --> 00:15:29,600
So we'll kind of, and now what we're starting to do is we're starting to experiment with

166
00:15:29,600 --> 00:15:35,400
a combination of both kind of, kind of rule based cognitive models plus learned models

167
00:15:35,400 --> 00:15:42,080
together, because I don't think any one system is quite the right way of doing it.

168
00:15:42,080 --> 00:15:50,560
And so in describing the symbolic layer, at least the example that you gave early on

169
00:15:50,560 --> 00:16:01,640
was essentially focused on maintaining consistency and coherence within the story that is a

170
00:16:01,640 --> 00:16:09,320
part of making a story plausible or impactful for human, but there's also, you know, word

171
00:16:09,320 --> 00:16:15,680
choice and other aspects of, you know, what the reader listener is thinking of. And it

172
00:16:15,680 --> 00:16:21,280
brought to mind for me, you know, a task that's almost like, you know, invert sentiment

173
00:16:21,280 --> 00:16:27,600
analysis, like sentiment conveyance or something like that. Is there an element in your work

174
00:16:27,600 --> 00:16:35,080
that is focused on kind of the emotional impact of word choice by these systems?

175
00:16:35,080 --> 00:16:40,920
Not at the moment, but our long term goal, yes. And so I'll kind of spill my beans a little

176
00:16:40,920 --> 00:16:46,280
bit and talk about kind of where some of this work is going. What I really want to do,

177
00:16:46,280 --> 00:16:50,520
what I think would be a really kind of amazing demonstration of these technologies is to

178
00:16:50,520 --> 00:16:59,040
do build a system that can generate suspenseful stories. And suspense is emotion, but suspense

179
00:16:59,040 --> 00:17:03,920
is a particularly interesting emotion where you see an idea. So think of James Bond, right?

180
00:17:03,920 --> 00:17:08,280
So James Bond looks like he's in trouble. He's going to get captured and tortured and

181
00:17:08,280 --> 00:17:13,200
killed by the bad guys, right? But then he always escapes at the end, right? So the way

182
00:17:13,200 --> 00:17:19,160
to convey suspense is to see an idea in the mind of the watcher or the listener of a negative

183
00:17:19,160 --> 00:17:24,320
consequence. And then at the last minute, you pull the rug out and you reveal that there's

184
00:17:24,320 --> 00:17:32,080
a way out. So suspense, I think, is very powerful emotion, very hard to create accidentally.

185
00:17:32,080 --> 00:17:37,840
We can do surprise. Surprise is easy to do accidentally, but suspense requires buildup.

186
00:17:37,840 --> 00:17:43,640
So you have to see all these points in the story because you're planning ahead. So you

187
00:17:43,640 --> 00:17:51,200
have emotion, you have planning, you have expectation. So what do humans think are going

188
00:17:51,200 --> 00:17:57,640
to happen as opposed to what is actually happening? And so this, to me, this notion of suspense

189
00:17:57,640 --> 00:18:03,840
brings, again, in all these threads of all these really kind of fundamental problems of

190
00:18:03,840 --> 00:18:10,280
modeling, planning, so on and so forth in kind of a fun little demonstration.

191
00:18:10,280 --> 00:18:18,000
Yeah, maybe think about it from, you know, there are all these story archetypes like heroes

192
00:18:18,000 --> 00:18:25,480
journey and what have you that, in theory, you know, could serve as like a rule based

193
00:18:25,480 --> 00:18:33,080
or some set of heuristics for a generative algorithm. Is that kind of what you had to

194
00:18:33,080 --> 00:18:38,960
have in mind with this with suspense being one of those or are there other ways that

195
00:18:38,960 --> 00:18:43,760
those kind of story models come into play in your work?

196
00:18:43,760 --> 00:18:50,480
Yeah, so I think humans have a lot of expectations. So I talk about this idea of expectations,

197
00:18:50,480 --> 00:18:57,400
right? So in some ways storytelling is self-referential. We watch a lot of movies, we build

198
00:18:57,400 --> 00:19:03,760
this schema of what movies should be like. And then you play with that schema in interesting

199
00:19:03,760 --> 00:19:09,280
ways. This is getting like into narrative theory here and away from the AI, right? But,

200
00:19:09,280 --> 00:19:13,640
you know, but that's also important, right? So you don't want to tell stories that don't

201
00:19:13,640 --> 00:19:19,800
fit people's expectations. You also have to deviate from those expectations, just modeling

202
00:19:19,800 --> 00:19:25,720
what a, what is a human's expectation? Well, that's, you know, we're, we believe that

203
00:19:25,720 --> 00:19:31,640
we can get that from data. So read a lot of stories, read a lot of movie scripts, you

204
00:19:31,640 --> 00:19:38,440
know, those are the things you'll get to, you get a sense for what stories tend to do.

205
00:19:38,440 --> 00:19:43,640
But data again is backwards looking, right? I'm not trying to emulate every story I've

206
00:19:43,640 --> 00:19:47,120
seen before. I'm not trying to recreate the stories of the past. I need stories of the

207
00:19:47,120 --> 00:19:53,640
future. So we start from that, we try to learn a model of expectation of what should or

208
00:19:53,640 --> 00:19:59,360
should not be in stories from the data. But then the planning is the look ahead part

209
00:19:59,360 --> 00:20:03,400
to say, well, what is the author's goal? You know, do they want the hero to win? Do they

210
00:20:03,400 --> 00:20:09,160
want the bad guy to win? Do they want to create suspense? Do they want to create sadness?

211
00:20:09,160 --> 00:20:13,200
Based on those two things coming together and the push and pull of those two things that

212
00:20:13,200 --> 00:20:21,320
I think, at least that's my kind of idea of how, how stories can be told. It strikes

213
00:20:21,320 --> 00:20:28,080
me that, you know, to some degree, if you figure this out, it would be very valuable to

214
00:20:28,080 --> 00:20:34,440
these film studios who want to know if a story is going to resonate with an audience early.

215
00:20:34,440 --> 00:20:41,240
Is that something that, you know, you're focused more on the generative side, but is that

216
00:20:41,240 --> 00:20:47,000
something that is, you know, is that a solve problem? Because we can, you know, kind

217
00:20:47,000 --> 00:20:52,280
of do, you know, predictive things, you know, we're think a bit further along with predictive

218
00:20:52,280 --> 00:21:00,720
things with machine learning than, than generative. Or do you think they're interesting things

219
00:21:00,720 --> 00:21:04,640
happening there as well? Is it even something that you're paying attention to?

220
00:21:04,640 --> 00:21:13,280
Yeah, I pay attention to how AI is being used in various industries. I'm not 100% convinced

221
00:21:13,280 --> 00:21:20,400
that predictive modeling in, let's say Hollywood studios to find the best, the next best,

222
00:21:20,400 --> 00:21:25,040
you know, blockbuster hit is, is that big of a, is going to have that big of an impact?

223
00:21:25,040 --> 00:21:29,400
I mean, I think someone will find a way of making a billion dollars off of this, regardless.

224
00:21:29,400 --> 00:21:35,200
Right. But again, you know, it's backwards looking. It won't necessarily lead to better

225
00:21:35,200 --> 00:21:42,120
films. Is that the core content in there? Yeah, I mean, if you're looking for things

226
00:21:42,120 --> 00:21:47,160
that look like the past, then predictive models are very good. Because in some sense, all

227
00:21:47,160 --> 00:21:53,200
machine learning is pattern matching. So I find patterns I've seen before. I, I activate

228
00:21:53,200 --> 00:21:57,760
on that. Yeah. We're going to need something else to say, well, here's something that's

229
00:21:57,760 --> 00:22:04,200
completely new. That's never been seen before, but could be a big hit. But I think that's

230
00:22:04,200 --> 00:22:09,640
a big, that's a big ask for an algorithm. I mean, algorithms right now, we fully honest,

231
00:22:09,640 --> 00:22:13,480
we're talking about baby shoes for sale and, and very complicated sorts of stories,

232
00:22:13,480 --> 00:22:19,120
instruments, you know, we're lucky to put a couple paragraphs together, really, and have

233
00:22:19,120 --> 00:22:23,720
it make sense all the way through without someone saying, wow, that was a weird direction

234
00:22:23,720 --> 00:22:32,920
that this thing just went. No, that's, that's good context to, to ground on. You talked

235
00:22:32,920 --> 00:22:41,560
about how story is self-referential and, you know, the role of human expectations and

236
00:22:41,560 --> 00:22:50,320
it, it seems like there's a, there's a kind of a fine line there where, for the story

237
00:22:50,320 --> 00:22:55,200
to be interesting, you wanted to have, you know, just enough surprise, but not so much

238
00:22:55,200 --> 00:23:00,600
that it's kind of like you said off the rails or, you know, maybe avant-garde, if it was

239
00:23:00,600 --> 00:23:09,400
produced by, you know, by an artist, how do you kind of control for that in the systems

240
00:23:09,400 --> 00:23:15,920
you're building? I don't want to say I have all the answers yet, but, you know, what,

241
00:23:15,920 --> 00:23:19,960
what we're, you know, I also, you know, while you look at storytelling, I also look a lot

242
00:23:19,960 --> 00:23:25,680
at human creativity and AI creativity. And I think, you know, creativity is kind of this

243
00:23:25,680 --> 00:23:31,000
weird thing. I think we all have an intuitive sense that some things are creative or not,

244
00:23:31,000 --> 00:23:38,440
but I like to think about intentional creativity. So one of the big issues that machine learning

245
00:23:38,440 --> 00:23:43,520
algorithms have right now, when they're used to create, whether it's stories or text

246
00:23:43,520 --> 00:23:50,080
or dialogue or poems or pictures, even, is, you know, unintentional creativity versus

247
00:23:50,080 --> 00:23:55,920
intentional creativity. So unintentional creativity is randomness, right? So sometimes AI systems,

248
00:23:55,920 --> 00:24:02,200
they just make a statistically improbable decision because they're statistical machines.

249
00:24:02,200 --> 00:24:06,400
And it ends up being a happy accident and there's something kind of beautiful and unexpected

250
00:24:06,400 --> 00:24:12,200
that comes out of the computer, but it's completely non-replicable, right? You do it again

251
00:24:12,200 --> 00:24:17,360
and you get something boring because a different random choice was made. So humans seem to

252
00:24:17,360 --> 00:24:21,680
have this ability to say, and I think it's because we're goal driven, whereas AI systems

253
00:24:21,680 --> 00:24:27,120
aren't always the machine learning systems aren't always goal driven. To say, well, you

254
00:24:27,120 --> 00:24:31,880
know, I need this thing to be different than the things I've seen in the past. Now I have

255
00:24:31,880 --> 00:24:36,080
to, in some sense, search through all the different ways to make things different. How

256
00:24:36,080 --> 00:24:40,800
do I know which differences are good differences? Which differences are bad differences? Which

257
00:24:40,800 --> 00:24:46,400
differences are just random unintentional noise that sometimes comes out good and sometimes

258
00:24:46,400 --> 00:24:52,840
comes out bad? So I don't think we have a good theory of intentional creativity yet.

259
00:24:52,840 --> 00:25:06,160
Specifically to AI or broadly in the sense of the human element of creativity.

260
00:25:06,160 --> 00:25:14,600
Well, I think for both, I'm not a study psychology, of course, but I think definitely for AI

261
00:25:14,600 --> 00:25:24,480
systems, we don't know how to build systems that can make kind of distinguish between

262
00:25:24,480 --> 00:25:33,640
good and bad as well as they should. I think we're very good at building practical systems.

263
00:25:33,640 --> 00:25:37,760
You know, if I have a practical objective function, I want to maximize X or I want to

264
00:25:37,760 --> 00:25:44,480
minimize Y. I think we've gotten very good at that. But the problem is maximize enjoyment

265
00:25:44,480 --> 00:25:51,080
like that's probably the goal for any creative system or maximize pleasure or aesthetics.

266
00:25:51,080 --> 00:25:55,560
How do I define that mathematically? What is, what is, how do I define aesthetic or good

267
00:25:55,560 --> 00:26:02,240
or pleasurable in art story, you know, visual arts, anything like that? I don't think you

268
00:26:02,240 --> 00:26:06,960
know. And what we do is we turn to our kind of our toolbox and we say, well, what I can

269
00:26:06,960 --> 00:26:11,120
do is I can at least compare it to the past. So train on a whole bunch of images. I'll

270
00:26:11,120 --> 00:26:16,920
get more images like that. If I train on pretty images, I'll get pretty images. But you

271
00:26:16,920 --> 00:26:21,000
know, who's creative there? The person who put together the data set, not necessarily

272
00:26:21,000 --> 00:26:24,560
the algorithm. How do we get an algorithm to say, all right, you give me a whole bunch

273
00:26:24,560 --> 00:26:28,320
of pretty pictures, but I think I could change this in a particular way and go off on this

274
00:26:28,320 --> 00:26:33,360
weird tangent over here and get something better. That's what we don't know how to do.

275
00:26:33,360 --> 00:26:38,480
And humans seem to be able to do it. And I don't know if it's just because humans have

276
00:26:38,480 --> 00:26:43,880
a lived experience that is much broader and greater than any AI system or whether there's

277
00:26:43,880 --> 00:26:47,840
some mechanisms in the human mind that we don't know how to model it. I think that's an

278
00:26:47,840 --> 00:26:57,960
outstanding issue. Are there adjacent areas in machine learning that you're most excited

279
00:26:57,960 --> 00:27:05,360
about for their, you know, potential contribution to the kind of research you're doing? Like, you

280
00:27:05,360 --> 00:27:09,160
know, maybe a few years ago, you might have said, Transformers, if you foresaw all that

281
00:27:09,160 --> 00:27:14,640
that was going to the brand, you know, what's the thing that's next? Is it some crazy reinforcement

282
00:27:14,640 --> 00:27:21,280
learning thing or do you not know yet? Well, I'm very interested in, like, I think

283
00:27:21,280 --> 00:27:26,200
story generation is going to result in new sorts of models and those sorts of algorithms.

284
00:27:26,200 --> 00:27:33,400
But aside from like the big, grand AI challenges of, can we build systems that generate stories

285
00:27:33,400 --> 00:27:40,400
from scratch? I'm also very interested in the applied side of storytelling. And to that

286
00:27:40,400 --> 00:27:45,520
end, one of the things I've been working on is explainable AI systems. So explainable

287
00:27:45,520 --> 00:27:50,760
AI systems is this idea that we have these machine learning systems. They're black boxes.

288
00:27:50,760 --> 00:27:54,440
We don't understand what's going on inside of them. They make decisions, which might be

289
00:27:54,440 --> 00:28:01,200
wrong or correct, but confusing. And so there's been a lot of interest in kind of quote-unquote

290
00:28:01,200 --> 00:28:08,520
opening the black box. I've been thinking a lot about end users. So the people who are

291
00:28:08,520 --> 00:28:13,080
going to use the Roombas of the Future, who are going to have the Roombas go off and do

292
00:28:13,080 --> 00:28:17,440
really weird things. And they're going to ask, why? Why did you do that? Why did you run

293
00:28:17,440 --> 00:28:23,200
over the cat or why did you wake up the baby or things like that? And the sorts of things

294
00:28:23,200 --> 00:28:29,120
that we're doing now in an interpretable AI of, you know, I'm going to do data analysis

295
00:28:29,120 --> 00:28:33,200
on this neural net and figure out which neurons are misfiring. That's not the sort of thing

296
00:28:33,200 --> 00:28:40,000
that end users of Roombas are going to want to know. And my theory is that the explanations

297
00:28:40,000 --> 00:28:44,360
that are going to be most useful to non-technical people are going to be the ones that are going

298
00:28:44,360 --> 00:28:49,520
to feel like stories. So if the Roombas saying, well, I went over here because, et cetera,

299
00:28:49,520 --> 00:28:54,880
et cetera. In some sense, they might be telling a little story. That story might be tapping

300
00:28:54,880 --> 00:29:00,440
into a theory of mind that says, all right, well, how can I help my human user reconcile

301
00:29:00,440 --> 00:29:04,880
their understanding of what the right answer action was? And my understanding of what

302
00:29:04,880 --> 00:29:08,560
the right action was, I'm not sure who's right or who's wrong here. We just have a different

303
00:29:08,560 --> 00:29:16,280
understanding of what was right. And can I explain through sharing experiential stories

304
00:29:16,280 --> 00:29:25,680
why I did the things I did, I being the robot here. All right. And how far along are you

305
00:29:25,680 --> 00:29:33,720
in that direction, that research area? We'd have some a few basic systems. Nothing

306
00:29:33,720 --> 00:29:37,600
particularly complicated. Nothing, I would even say, is really telling a story. And then

307
00:29:37,600 --> 00:29:44,640
at this particular point, we got into this work in end user explanation and then we realized

308
00:29:44,640 --> 00:29:50,160
we actually didn't know what a good explanation was for end users. So my students and I have

309
00:29:50,160 --> 00:29:55,200
been running human factor studies to say, well, what is it that the humans actually want

310
00:29:55,200 --> 00:30:01,080
from an explanation? Okay. I think I understand the debuggers explanation in general are specific

311
00:30:01,080 --> 00:30:07,520
to explanations of neural systems, for example, of general, right? So maybe like, whatever's

312
00:30:07,520 --> 00:30:13,200
in that black box, you know, what is it? What is the explanation doing? Is it, is it helping

313
00:30:13,200 --> 00:30:18,560
them understand? Is it helping them debug? Is it helping them just feel good about owning

314
00:30:18,560 --> 00:30:26,080
a strange mysterious thinking device in their house? What kinds of explanations make them

315
00:30:26,080 --> 00:30:32,200
trust the device and want to use it more versus less? What makes it a more pleasurable ownership

316
00:30:32,200 --> 00:30:37,440
experience? Right. So it turned out there's a whole set of human, and this goes back to

317
00:30:37,440 --> 00:30:41,440
my interest in human computer interaction, actually, because this whole set of questions

318
00:30:41,440 --> 00:30:47,360
about what humans think about when they use complicated intelligent black box devices and

319
00:30:47,360 --> 00:30:51,520
we didn't have the answers to any of these things. So we actually kind of set the algorithms

320
00:30:51,520 --> 00:30:56,000
aside and say, well, before we build algorithms, we actually need to know what the algorithms

321
00:30:56,000 --> 00:31:03,880
are going to target. I would imagine that there's not a single answer to that that, you know,

322
00:31:03,880 --> 00:31:08,920
some people want to know a lot more about what's happening in order to help them, you know,

323
00:31:08,920 --> 00:31:13,520
or an explanation to be satisfying to them, whereas others, you know, just get me to the

324
00:31:13,520 --> 00:31:20,520
point. Yeah, absolutely. We're finding it's, it's vastly multi-dimensional. We just conducted

325
00:31:20,520 --> 00:31:26,440
a study looking at people with computer science backgrounds versus people without computer

326
00:31:26,440 --> 00:31:32,600
science backgrounds and doing explanations. And as you might predict, you know, people

327
00:31:32,600 --> 00:31:37,600
with technical backgrounds have a very different orientation towards their devices, towards

328
00:31:37,600 --> 00:31:43,120
their, you know, automated systems in their homes. Like to them, they want to figure out

329
00:31:43,120 --> 00:31:48,320
what went wrong and how to make that never happen again. People who have, you know, aren't

330
00:31:48,320 --> 00:31:54,320
engineers, you know, they had a very different orientation. They just, they just wanted to

331
00:31:54,320 --> 00:31:59,800
feel that something was thinking inside, right? And just talking through the process of

332
00:31:59,800 --> 00:32:06,200
what the AI system was doing was just enough. And it made them happier, right? Yeah, yeah.

333
00:32:06,200 --> 00:32:11,880
And that was, you know, maybe in retrospect, obvious, but also kind of like the really

334
00:32:11,880 --> 00:32:20,480
interesting result. And so, you know, granted that you kind of, you, you started with this

335
00:32:20,480 --> 00:32:25,880
goal of trying to build explanations for these black box systems, you took a step back

336
00:32:25,880 --> 00:32:35,240
to try to understand what a good explanation is. When are you done there? That sounds like

337
00:32:35,240 --> 00:32:42,240
a huge challenge, but how far do you need to get there before you are able to return back

338
00:32:42,240 --> 00:32:46,000
to this core challenge? Or are you kind of pushing on the different directions in parallel?

339
00:32:46,000 --> 00:32:51,360
And how do you keep them synced up? Yeah, I think you can do some of these in parallel.

340
00:32:51,360 --> 00:32:56,920
I think we know enough for now to start revisiting the design of algorithms. And so, like to

341
00:32:56,920 --> 00:33:02,320
me, human centered artificial intelligence is you have to understand what, what it is

342
00:33:02,320 --> 00:33:08,360
about the human experience working with an AI system before you know what to build.

343
00:33:08,360 --> 00:33:14,600
So asking those sorts of questions about the human experience is the first step to any

344
00:33:14,600 --> 00:33:19,160
good research, right? So I felt like it was important to step back. Now, I think we're

345
00:33:19,160 --> 00:33:23,000
we know enough to say, well, we can start targeting different applications and different

346
00:33:23,000 --> 00:33:27,200
use cases with algorithm again. Because now I know what I think the algorithms need to

347
00:33:27,200 --> 00:33:32,000
do. And it wasn't necessarily what I thought they needed to do, you know, two years ago

348
00:33:32,000 --> 00:33:37,320
when I first started thinking about this. Can you drill in on that point like what do

349
00:33:37,320 --> 00:33:46,760
you think now versus what you thought then? Well, you know, we thought that explanations

350
00:33:46,760 --> 00:33:54,360
were going to need to be a lot more technical than they were. And what we've really discovered

351
00:33:54,360 --> 00:34:01,800
is that a lot of explanations for certain types of users and certain types of circumstances

352
00:34:01,800 --> 00:34:07,800
and I don't know how, how universal this could be, but at least right now it seems that

353
00:34:07,800 --> 00:34:12,880
you know, a lot of the explanation systems are going to be about confidence building.

354
00:34:12,880 --> 00:34:16,920
Okay, so there's this black box. I don't know how it thinks. I don't know what is thinking.

355
00:34:16,920 --> 00:34:20,760
Does it think like me? Does it think like something else? Does it even understand what

356
00:34:20,760 --> 00:34:27,000
is doing or right? And so communicating in a way that sounds somewhat humanistic or human

357
00:34:27,000 --> 00:34:33,000
system that says, you know, here's what I know about the situation that I'm in. Here's

358
00:34:33,000 --> 00:34:38,720
what I'm seeing. Here's what I think should be done with that sort of situation. Maybe

359
00:34:38,720 --> 00:34:43,960
the sort of explanations, which is very different from this is why my algorithm made choice

360
00:34:43,960 --> 00:34:52,640
A versus B. Right, right. It also makes me think a little bit about the role of story

361
00:34:52,640 --> 00:35:00,840
in the relationship between two things over time, like the system and the human. Maybe

362
00:35:00,840 --> 00:35:09,160
the goal of the system is to gradually increase the human's confidence or understanding

363
00:35:09,160 --> 00:35:17,320
or some other metric over time and is that something that we, how do we get to that?

364
00:35:17,320 --> 00:35:23,680
Well, I think, you know, I mentioned one of the reasons why storytelling is as useful

365
00:35:23,680 --> 00:35:31,080
from human to human is is rapport building. Before the pandemic, we used to go to conferences

366
00:35:31,080 --> 00:35:36,080
and you know, there's the where are you from? How did you get here? You know, what are

367
00:35:36,080 --> 00:35:40,200
you working on? You know, these these chitchat sorts of things. Often lightweight stories

368
00:35:40,200 --> 00:35:44,240
who are not telling Steven Spielberg style epic thrillers, right? We're just kind of

369
00:35:44,240 --> 00:35:49,920
saying here's my daily life, you know, here's who I am kind of conveyed in this interesting

370
00:35:49,920 --> 00:35:58,880
kind of story like way. Can AI systems put people at ease? Make people want to use them

371
00:35:58,880 --> 00:36:06,920
more? Have confidence in using them by doing this chitchat and folding in stories, but

372
00:36:06,920 --> 00:36:10,400
also listening to stories. Oh, yeah, okay, you're this is the story you're telling about

373
00:36:10,400 --> 00:36:15,840
you. Now, I have an understanding of how I need to respond to you and act with you.

374
00:36:15,840 --> 00:36:22,760
That I think is would be kind of an amazing sort of step forward. Also one that has a little

375
00:36:22,760 --> 00:36:28,880
bit of risk, right? We don't want to artificially create trust when we shouldn't artificially

376
00:36:28,880 --> 00:36:35,680
create trust. So there's a little bit of a kind of a double sided kind of sword aspect

377
00:36:35,680 --> 00:36:42,240
to this as well. So we want to convey confidence or and still confidence in users and convey

378
00:36:42,240 --> 00:36:46,800
trust and build rapport when that's an appropriate thing to do, but we need to be very careful

379
00:36:46,800 --> 00:36:50,040
that we don't step into manipulation and persuasion.

380
00:36:50,040 --> 00:37:06,840
And so the what are your what are kind of the future directions in the explainability dimension

381
00:37:06,840 --> 00:37:12,600
in that line of research? Yeah, well, I think there's still a lot of papers to be written

382
00:37:12,600 --> 00:37:20,400
about the human centered aspect of non technical end user explanations. And then there's

383
00:37:20,400 --> 00:37:24,720
I think going to be a lot of work to actually build the algorithms because the algorithms

384
00:37:24,720 --> 00:37:30,200
that we might need to build might not be exactly the ones that that we know how to build

385
00:37:30,200 --> 00:37:38,680
right now. Do you envision them starting from kind of existing explainability algorithms

386
00:37:38,680 --> 00:37:47,960
like adding a story generation layer to lime or something like that, or are they kind

387
00:37:47,960 --> 00:37:57,480
of ground up more likely? It's a little bit unclear, I guess. You know,

388
00:37:57,480 --> 00:38:06,560
it's hard to predict the future, but what I do think is I think our explanation systems

389
00:38:06,560 --> 00:38:12,720
are going to have to borrow a lot from the sorts of questions I've already been looking

390
00:38:12,720 --> 00:38:18,560
at in the storytelling domain. So a theory of mind. So how do I answer this question

391
00:38:18,560 --> 00:38:25,080
about what I just did? Well, what is it that the user actually wants to understand here?

392
00:38:25,080 --> 00:38:30,080
What do they already understand about my behavior? I'm going to have the tune to that tune

393
00:38:30,080 --> 00:38:36,840
to my reader, tune to my listener, tune to my audience sort of thing. Planning out the

394
00:38:36,840 --> 00:38:43,160
explanation, right? So if it's going to be more than a single utterance, how do I now

395
00:38:43,160 --> 00:38:47,360
I have a goal of helping you understand what I just did? Well, maybe I'm going to have

396
00:38:47,360 --> 00:38:52,280
to say three or four or five different things to get you to understand that. So planning

397
00:38:52,280 --> 00:38:58,880
ahead as opposed to just kind of using statistical language models is another thing that I think

398
00:38:58,880 --> 00:39:05,640
is very likely to play an important role in here. So whether that whether we build from

399
00:39:05,640 --> 00:39:10,280
there into something new or we take what's already out there and existing explanation

400
00:39:10,280 --> 00:39:15,400
systems, we layer on top, I'm not sure. And I think it could go either way.

401
00:39:15,400 --> 00:39:28,160
Nice. Yeah. One of you can maybe take us through some additional examples of papers

402
00:39:28,160 --> 00:39:32,920
that you and your team have written around storytelling in general, not necessarily

403
00:39:32,920 --> 00:39:39,080
the explainability, but that too. Just to give us some more concrete ideas of how you

404
00:39:39,080 --> 00:39:45,360
start to kind of lay the bricks or put the pieces in place to push this broad research

405
00:39:45,360 --> 00:39:53,600
agenda forward. Yeah. So I like to build big systems. So a lot of my papers are like,

406
00:39:53,600 --> 00:39:57,160
here's a piece of the system, here's a piece of the system, here's a piece of the system.

407
00:39:57,160 --> 00:40:07,320
So in 2017, 2018, I can't remember we had a paper at AAAI, which was on using neural

408
00:40:07,320 --> 00:40:12,480
language models to build stories. So that was really kind of an early example of how to

409
00:40:12,480 --> 00:40:17,920
use neural generators to build stories. But it was doing a lot of the things I just

410
00:40:17,920 --> 00:40:23,280
talked about as kind of a naive way of doing it. We just statistically sample, we kind

411
00:40:23,280 --> 00:40:31,040
of run the sampler forward until we get bored of the outputs. Doing that work was when

412
00:40:31,040 --> 00:40:36,680
we realized we had to be much more forward looking, goal forward looking as opposed to statistical

413
00:40:36,680 --> 00:40:44,080
backwards looking. So in Ijkai, I'm forgetting the dates on these, might have been 20,

414
00:40:44,080 --> 00:40:49,840
1890. So about a year later, a year and a half later, we had an Ijkai paper that looked

415
00:40:49,840 --> 00:40:55,600
at goal-driven storytelling. So given a goal, I want, you know, I want happily ever after

416
00:40:55,600 --> 00:41:00,920
I want the hero and the hero and to dig it married. How do we force a neural network

417
00:41:00,920 --> 00:41:11,840
to drive in a particular direction? And that was very successful. And then the next piece

418
00:41:11,840 --> 00:41:16,040
of work after that, which is very recent, was Laura Martin's thesis, which should kind

419
00:41:16,040 --> 00:41:24,440
of come out any day now, literally, was really now this neuro symbolic got to, got to talk

420
00:41:24,440 --> 00:41:29,640
about her work is great. Was the, was the neuro symbolic thing to say, well, statistically

421
00:41:29,640 --> 00:41:33,480
even if you drive towards a goal, you can still make these weird statistical leaps that

422
00:41:33,480 --> 00:41:39,280
people just don't understand. So now we wanted to tie the neural generator to this symbolic

423
00:41:39,280 --> 00:41:45,000
model of what we thought the humans would, how humans would link the different parts of

424
00:41:45,000 --> 00:41:51,960
the story together to drive the story generator forward. So that was then the, the reader model

425
00:41:51,960 --> 00:41:56,680
or the listener model being incorporated on top of that. And that kind of brings us

426
00:41:56,680 --> 00:42:02,120
to where we are now. We've done a little bit with commonsense reasoning. We had a AAI paper

427
00:42:02,120 --> 00:42:07,920
just this year about trying to incorporate commonsense. So each paper kind of had another

428
00:42:07,920 --> 00:42:13,440
little dimension to the, to the story, the story of storytelling, I guess, nice, nice.

429
00:42:13,440 --> 00:42:20,480
How did you approach the commonsense problem? Well, commonsense was just one of these things

430
00:42:20,480 --> 00:42:28,720
that I felt was just missing from the beginning of storytelling. All right, it's really,

431
00:42:28,720 --> 00:42:39,440
what is, you know, if you make commonsense mistakes, people observe them immediately. Okay,

432
00:42:39,440 --> 00:42:43,480
so you make a mistake about telling you something as simple as going to a restaurant, you can

433
00:42:43,480 --> 00:42:49,440
things out of order. And people like, whoa, like, what the heck is your career? Storytelling

434
00:42:49,440 --> 00:42:54,280
is great like that. You can't, you can't get anything past the readers. The readers were

435
00:42:54,280 --> 00:43:00,040
cold bullshit on you. Very fast, right? So we wanted work to, to be looking at how we

436
00:43:00,040 --> 00:43:06,440
could use models of commonsense reasoning to, to factor into our AI systems.

437
00:43:06,440 --> 00:43:13,440
And so did you use kind of existing off the shelf, quote unquote, existing research in

438
00:43:13,440 --> 00:43:20,000
commonsense reasoning, or did you tailor something to the specific problem?

439
00:43:20,000 --> 00:43:25,520
We did. There's this great piece of work out of University of Washington, not our team.

440
00:43:25,520 --> 00:43:32,000
It's Yej Enchoy and Juan Boselet and the great crew over there. They put together some

441
00:43:32,000 --> 00:43:38,120
commonsense modeling neural networks that can predict, you know, what people will think

442
00:43:38,120 --> 00:43:43,440
about a sentence. So we were able to bootstrap off of that to save ourselves the trouble

443
00:43:43,440 --> 00:43:48,600
of building big data sets and training off of those things. What we brought in though

444
00:43:48,600 --> 00:43:54,760
was the notion of how it folds into the story planning process.

445
00:43:54,760 --> 00:44:01,960
So not kind of naive filtering of things that don't pass the commonsense test, but incorporating

446
00:44:01,960 --> 00:44:04,960
that signal into the planning process itself.

447
00:44:04,960 --> 00:44:12,360
Yeah, we actually, we've done it both ways now. We can filter. So this sentence doesn't

448
00:44:12,360 --> 00:44:16,600
pass the commonsense sniff test. So we're going to filter and we're going to regenerate

449
00:44:16,600 --> 00:44:23,200
until we pass the sniff test. We can also use commonsense to actually plan forward

450
00:44:23,200 --> 00:44:28,040
a little bit. We can say, well, if I see this, like, again, this is this idea of expectation

451
00:44:28,040 --> 00:44:33,120
comes back again, right? If I see this, then my commonsense model says I would expect that.

452
00:44:33,120 --> 00:44:44,480
So I better put that into the story. Are there kind of top line takeaways or lessons from

453
00:44:44,480 --> 00:44:50,120
your work on story that you think should be more broadly applied by researchers or

454
00:44:50,120 --> 00:44:58,800
practitioners in other areas of AI? Well, I think, I mean, again, I'll kind of harp

455
00:44:58,800 --> 00:45:03,400
on this, right? But the idea that working on storytelling allows us to address these

456
00:45:03,400 --> 00:45:08,480
fundamental problems. So, you know, I do think that some of the algorithms we're putting

457
00:45:08,480 --> 00:45:14,000
together for storytelling can be pulled out of the story telling framework and be used

458
00:45:14,000 --> 00:45:21,840
for, let's say, human AI or AI interaction, trying to use language to plan. So think

459
00:45:21,840 --> 00:45:24,920
about getting together with your friends and trying to put the plan together for the

460
00:45:24,920 --> 00:45:28,000
afternoon. Right? What are you doing? You're planning, but you're communicating back and

461
00:45:28,000 --> 00:45:33,680
forth with language. So we think that this will apply to dialogue. We think it will

462
00:45:33,680 --> 00:45:39,400
apply to, you know, series style personal assistance or Cortana or Alexa style personal

463
00:45:39,400 --> 00:45:47,400
assistance. We haven't tested these application areas. I'll probably leave that to other

464
00:45:47,400 --> 00:45:54,720
people. At the end of the day, you know, to me, this storytelling is what makes these

465
00:45:54,720 --> 00:46:00,640
hard problems fun. So I kind of like to leave it in the sphere of having fun and let

466
00:46:00,640 --> 00:46:07,240
other people kind of look for the applications. But you know, there's another valuable aspect

467
00:46:07,240 --> 00:46:12,600
of working on storytelling is which is building big systems that are not one off papers that

468
00:46:12,600 --> 00:46:16,400
say, well, I have to bring together a lot of things and build complex theories of how

469
00:46:16,400 --> 00:46:22,400
all these different models fit together. You know, common sense models and planning models

470
00:46:22,400 --> 00:46:27,960
they don't naturally fit together. So there's a lot to learn in terms of integrating multiple

471
00:46:27,960 --> 00:46:34,960
types of AI styles problems together into one thing that and see it work and when it

472
00:46:34,960 --> 00:46:40,720
works, it's great. Awesome. Awesome. Well, Mark, thanks so much for sharing a bit about

473
00:46:40,720 --> 00:46:44,880
what you're doing with storytelling. Very cool stuff. It's been an absolute pleasure.

474
00:46:44,880 --> 00:46:51,720
I'm finally glad to have had the chance to talk to you. Same here. Thanks, Mark. All right,

475
00:46:51,720 --> 00:47:06,160
take care.

