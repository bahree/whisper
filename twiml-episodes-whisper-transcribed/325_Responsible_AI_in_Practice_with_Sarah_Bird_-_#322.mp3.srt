1
00:00:00,000 --> 00:00:13,400
Welcome to the Twilmal AI Podcast.

2
00:00:13,400 --> 00:00:15,600
I'm your host Sam Charrington.

3
00:00:15,600 --> 00:00:23,200
Hey, what's up everyone?

4
00:00:23,200 --> 00:00:24,200
This is Sam.

5
00:00:24,200 --> 00:00:29,040
A quick reminder that we've got a bunch of newly formed or forming study groups, including

6
00:00:29,040 --> 00:00:34,800
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders

7
00:00:34,800 --> 00:00:36,760
part one courses.

8
00:00:36,760 --> 00:00:42,960
It's not too late to join us, which you can do by visiting twilmalai.com slash community.

9
00:00:42,960 --> 00:00:47,960
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.

10
00:00:47,960 --> 00:00:50,360
If you're at either event, please reach out.

11
00:00:50,360 --> 00:00:52,360
I'd love to connect.

12
00:00:52,360 --> 00:00:57,860
Alright, this week on the podcast I'm excited to share a series of shows recorded in

13
00:00:57,860 --> 00:01:01,580
Orlando during the Microsoft Ignite Conference.

14
00:01:01,580 --> 00:01:05,820
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship

15
00:01:05,820 --> 00:01:07,140
of this series.

16
00:01:07,140 --> 00:01:11,900
Thanks to decades of breakthrough research and technology, Microsoft is making AI real

17
00:01:11,900 --> 00:01:18,660
for businesses with Azure AI, a set of services that span vision, speech, language processing,

18
00:01:18,660 --> 00:01:21,740
custom machine learning, and more.

19
00:01:21,740 --> 00:01:26,180
Millions of developers and data scientists around the world are using Azure AI to build

20
00:01:26,180 --> 00:01:31,260
innovative applications and machine learning models for their organizations, including

21
00:01:31,260 --> 00:01:35,300
85% of the Fortune 100.

22
00:01:35,300 --> 00:01:41,260
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven

23
00:01:41,260 --> 00:01:47,420
enterprise grade capabilities and innovations, wide range of developer tools and services

24
00:01:47,420 --> 00:01:49,460
and trusted approach.

25
00:01:49,460 --> 00:01:54,500
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS

26
00:01:54,500 --> 00:02:00,260
and DevOps professionals across all skill levels to increase productivity, operationalize

27
00:02:00,260 --> 00:02:06,660
models at scale, and innovate faster and more responsibly with Azure machine learning.

28
00:02:06,660 --> 00:02:11,540
Learn more at aka.ms slash Azure ML.

29
00:02:11,540 --> 00:02:13,980
Alright, onto the show.

30
00:02:13,980 --> 00:02:17,780
Alright, everyone.

31
00:02:17,780 --> 00:02:20,220
I am here in sunny Orlando.

32
00:02:20,220 --> 00:02:26,580
Actually, it's not all that sunny today, it's kind of gray and gray and rainy, but it is

33
00:02:26,580 --> 00:02:28,140
still sunny Orlando, right?

34
00:02:28,140 --> 00:02:31,100
How could it not be at Microsoft Ignite?

35
00:02:31,100 --> 00:02:33,820
And I've got the wonderful pleasure of being seated with Sarah Bird.

36
00:02:33,820 --> 00:02:38,460
Sarah is a Principal Program Manager for Azure Machine Learning Platform.

37
00:02:38,460 --> 00:02:41,060
Sarah, welcome to the Twomo AI Podcast.

38
00:02:41,060 --> 00:02:42,060
Thank you.

39
00:02:42,060 --> 00:02:43,060
I'm excited to be here.

40
00:02:43,060 --> 00:02:44,060
Absolutely.

41
00:02:44,060 --> 00:02:49,540
I am really excited about this conversation we're about to have on Responsible AI, but before

42
00:02:49,540 --> 00:02:52,540
we do that, I'd love to hear a little bit more about your background.

43
00:02:52,540 --> 00:03:00,460
You've got a very enviable position at the nexus of research and product and tech strategy.

44
00:03:00,460 --> 00:03:02,460
How did you create that?

45
00:03:02,460 --> 00:03:06,380
Well, I started my career in research.

46
00:03:06,380 --> 00:03:13,780
I did my PhD in machine learning systems at Berkeley, and I loved creating the basic

47
00:03:13,780 --> 00:03:18,380
technology, but then I wanted to take it to the next step, and I wanted to have people

48
00:03:18,380 --> 00:03:25,220
who really used it, and I found that when you take research into production, there's

49
00:03:25,220 --> 00:03:30,780
a lot more innovation that happens, and so I really sense graduating and have styled

50
00:03:30,780 --> 00:03:35,300
my career around living at that intersection of research and product and taking some

51
00:03:35,300 --> 00:03:38,980
of the great cutting-edge ideas and figuring out how we can get them in the hands of people

52
00:03:38,980 --> 00:03:45,820
as soon as possible, and so my role now is specifically focused on trying to do this for

53
00:03:45,820 --> 00:03:51,060
Azure Machine Learning, and Responsible AI is one of the great new areas that there's

54
00:03:51,060 --> 00:03:56,180
a ton of innovation in research, and people need it right now, and so we're working to

55
00:03:56,180 --> 00:03:58,540
try to make that possible.

56
00:03:58,540 --> 00:04:05,780
That's fantastic, and so between your grad work at Berkeley and Microsoft, what was the

57
00:04:05,780 --> 00:04:07,300
path?

58
00:04:07,300 --> 00:04:13,940
So I was in John Lankford's group in Microsoft Research, and was working on a system

59
00:04:13,940 --> 00:04:18,900
for contextual bandits and trying to make it easier for people to use those in practice,

60
00:04:18,900 --> 00:04:24,340
because a lot of the times when people were trying to deploy that type of algorithm, the

61
00:04:24,340 --> 00:04:26,420
system infrastructure would actually get in the way.

62
00:04:26,420 --> 00:04:30,580
You wouldn't be able to get the features to the point of decision or the logging would

63
00:04:30,580 --> 00:04:35,420
not work, and it would break the algorithm, and so we designed a system that made it

64
00:04:35,420 --> 00:04:39,020
correct by construction, so it was easy for people to go and plug it in, and this is

65
00:04:39,020 --> 00:04:43,780
actually turned into the personalizer cognitive service now.

66
00:04:43,780 --> 00:04:47,940
But through that experience, I learned a lot about actually working with customers and

67
00:04:47,940 --> 00:04:53,340
doing this in production, and so I decided that I wanted to have more of that in my career,

68
00:04:53,340 --> 00:04:59,780
and so I spent a year as a technical advisor, which is a great role in Microsoft, where

69
00:04:59,780 --> 00:05:06,180
you work for an executive and advise them and help work on special projects, and it enables

70
00:05:06,180 --> 00:05:13,540
you to see both kind of the business and the strategy side of things, as well as all

71
00:05:13,540 --> 00:05:17,860
the operational things how you run orgs, and then of course the technical things, and

72
00:05:17,860 --> 00:05:22,900
I realized that I think that makes this very interesting, and so after that I joined

73
00:05:22,900 --> 00:05:29,660
Facebook, and my role was at the intersection of Fair, Facebook AI Research, and AML,

74
00:05:29,660 --> 00:05:33,700
which was the applied machine learning group, with this role of specifically trying to

75
00:05:33,700 --> 00:05:39,420
take research into production and accelerate the rate of innovation.

76
00:05:39,420 --> 00:05:46,420
So I started the Onyx project as a part of that, enabling us to solve a tooling gap where

77
00:05:46,420 --> 00:05:51,900
it was difficult to get models from one framework to another, and then also worked on PyTorch

78
00:05:51,900 --> 00:05:57,420
and enabling us to make that more production ready, and since then I've been working in

79
00:05:57,420 --> 00:05:58,420
AI ethics.

80
00:05:58,420 --> 00:06:04,180
Yeah, if we weren't going to be focused on AI ethics and responsible AI today, we would

81
00:06:04,180 --> 00:06:10,380
be going deep into personalizer, what was Microsoft decision service, and this whole contextual

82
00:06:10,380 --> 00:06:16,940
bandits thing, a really interesting topic, not the least of which because we talk a lot

83
00:06:16,940 --> 00:06:22,540
about reinforcement learning, and if it's useful, and while it's not this deep reinforcement

84
00:06:22,540 --> 00:06:27,540
learning, game playing thing, it's reinforcement learning, and people are getting a lot of

85
00:06:27,540 --> 00:06:29,940
use out of it, in a lot of different contexts.

86
00:06:29,940 --> 00:06:35,420
Yeah, it's actually, when it works, it doesn't work in all cases, but in which it works,

87
00:06:35,420 --> 00:06:36,420
it works really well.

88
00:06:36,420 --> 00:06:40,820
It's the kind of thing where you get the numbers back, and you're like, can this be true?

89
00:06:40,820 --> 00:06:45,900
And so I think it's a really exciting technology going forward, and there's a lot of cases

90
00:06:45,900 --> 00:06:49,540
where people are using it successfully now, but I think there'll be a lot more in the

91
00:06:49,540 --> 00:06:50,540
future.

92
00:06:50,540 --> 00:06:51,540
Awesome.

93
00:06:51,540 --> 00:06:55,500
We'll have to take a rain check on that aspect of the conversation, and kind of segue

94
00:06:55,500 --> 00:07:03,180
over to the responsible AI piece, and I've been thinking a lot about a tweet that I saw

95
00:07:03,180 --> 00:07:10,220
by Rachel Thomas, who is a former guest of the podcast, a longtime friend of the show,

96
00:07:10,220 --> 00:07:16,980
and currently the UCSF Center for Applied Data Ethics head, and she was kind of lamenting

97
00:07:16,980 --> 00:07:22,060
that there are a lot of people out there talking about AI ethics like it's a solve problem.

98
00:07:22,060 --> 00:07:23,740
Do you think it's a solve problem?

99
00:07:23,740 --> 00:07:25,340
No, absolutely not.

100
00:07:25,340 --> 00:07:27,860
I didn't think so.

101
00:07:27,860 --> 00:07:33,860
So I think there are fundamentally hard and difficult problems when we have a new technology,

102
00:07:33,860 --> 00:07:37,340
and so I think we're always going to be having the AI ethics conversation.

103
00:07:37,340 --> 00:07:40,820
This is not something that we're going to solve and go away.

104
00:07:40,820 --> 00:07:46,740
But what I do think we have now is a lot more tools and techniques and best practices

105
00:07:46,740 --> 00:07:52,380
to help people start the journey of doing things responsibly, and so I think the reality

106
00:07:52,380 --> 00:07:57,020
is there's many things people could be doing right now that they're not, and so I feel

107
00:07:57,020 --> 00:08:02,140
like there's an urgency to get some of these tools into people's hands so that we can

108
00:08:02,140 --> 00:08:03,140
do that.

109
00:08:03,140 --> 00:08:06,300
So I think we can quickly go a lot farther than we have right now.

110
00:08:06,300 --> 00:08:14,500
In my conversations with folks that are working on this and thinking about the role that

111
00:08:14,500 --> 00:08:23,020
are responsible, AI plays in the way they do machine learning.

112
00:08:23,020 --> 00:08:26,820
A lot of people get stopped at the very beginning like, who should own this?

113
00:08:26,820 --> 00:08:27,820
Where does it live?

114
00:08:27,820 --> 00:08:35,820
Is it like a research kind of function or is it a product function or is it kind of more

115
00:08:35,820 --> 00:08:42,220
of a compliance kind of thing like a chief data officer or a chief security officer kind

116
00:08:42,220 --> 00:08:48,940
of function like one of those executive functions and oversight or compliance is the better

117
00:08:48,940 --> 00:08:51,540
word?

118
00:08:51,540 --> 00:08:57,020
What do you see folks doing and do you have any thoughts on where successful patterns

119
00:08:57,020 --> 00:08:58,540
of where it should live?

120
00:08:58,540 --> 00:09:05,660
Yeah, I think the models that we've been using are thinking a lot about the transition

121
00:09:05,660 --> 00:09:16,180
to security, for example, and I think the reality is it's not one person's job or one function.

122
00:09:16,180 --> 00:09:19,580
Everybody now has to think about security, even your basic software developers have to

123
00:09:19,580 --> 00:09:22,580
know and think about it when they're designing.

124
00:09:22,580 --> 00:09:27,060
However, there are people who are experts in it and handle the really challenging problems.

125
00:09:27,060 --> 00:09:31,700
There was, of course, legal and compliance pieces in there as well, and so I think we're

126
00:09:31,700 --> 00:09:38,260
seeing the same thing where we really need every role to come together and do this.

127
00:09:38,260 --> 00:09:44,860
So one of the patterns we are seeing is part of the challenge with responsible AI and

128
00:09:44,860 --> 00:09:51,660
technology is that we've designed technology to abstract away things and enable you to

129
00:09:51,660 --> 00:09:56,060
just focus on your little problem and this has led to a ton of innovation.

130
00:09:56,060 --> 00:10:00,820
However, the whole idea of responsible AI is actually you need to pick your head up,

131
00:10:00,820 --> 00:10:04,300
you need to have this larger context, you need to think about the application in the

132
00:10:04,300 --> 00:10:07,940
real world, you need to think about the implications and so we have to break a little bit of our

133
00:10:07,940 --> 00:10:14,900
patterns of my problem as just this little box and so we're finding that like user research

134
00:10:14,900 --> 00:10:21,660
and design, for example, is already trained and equipped to think about the people element

135
00:10:21,660 --> 00:10:27,260
in that and so it's really great to bring them into more conversations as we're developing

136
00:10:27,260 --> 00:10:33,500
the technology, so that's one pattern that we're finding adds a lot of value.

137
00:10:33,500 --> 00:10:40,460
In my conversation with Jordan Edwards, your colleague, many of his answers were all

138
00:10:40,460 --> 00:10:41,460
of the above.

139
00:10:41,460 --> 00:10:44,900
It sounds like this one is in all of the above response as well.

140
00:10:44,900 --> 00:10:49,740
Yeah, I think doing machine learning and practice takes a lot of different roles as Jordan

141
00:10:49,740 --> 00:10:54,980
was talking about in operationalizing things and then responsible AI just adds an extra layer

142
00:10:54,980 --> 00:10:57,780
of more roles on top of that.

143
00:10:57,780 --> 00:11:02,660
It's one of the challenges that kind of naturally evolves when everyone has to be thinking

144
00:11:02,660 --> 00:11:07,380
about something is that it's a lot harder, right?

145
00:11:07,380 --> 00:11:11,860
The developer is trained as a developer and now they have to start thinking about this

146
00:11:11,860 --> 00:11:19,180
security thing and it's changing so quickly and the best practices are evolving all the

147
00:11:19,180 --> 00:11:21,660
time and it's hard to stay on top of that.

148
00:11:21,660 --> 00:11:27,180
Now, if we're to replicate that same kind of model in responsible AI, which sounds like

149
00:11:27,180 --> 00:11:31,620
the right thing to do, how do we support the people that are kind of on the ground trying

150
00:11:31,620 --> 00:11:32,620
to do this?

151
00:11:32,620 --> 00:11:39,580
Yeah, and I think it's definitely a challenge because the end result can't be that every

152
00:11:39,580 --> 00:11:46,060
individual person has to know the state of the art in every area in responsible AI and

153
00:11:46,060 --> 00:11:51,780
so one of the ways that we're trying to do this is as much as possible, build it into our

154
00:11:51,780 --> 00:11:54,460
processes and our tooling, right?

155
00:11:54,460 --> 00:12:01,100
So that you can say, okay, well, you should have a fairness metric for your model and you

156
00:12:01,100 --> 00:12:04,660
can talk to experts about what that fairness metric should be, but you should know the

157
00:12:04,660 --> 00:12:08,540
requirement that you should have a fairness metric, for example.

158
00:12:08,540 --> 00:12:14,380
And so we first are starting with that process layer and then in Azure Machine Learning,

159
00:12:14,380 --> 00:12:17,860
we've built tools that enable you to easily enact that process.

160
00:12:17,860 --> 00:12:23,820
And so the foundational piece is the MLOPS story that Jordan was talking about where we

161
00:12:23,820 --> 00:12:29,660
actually enable you to have a process that's reproducible, that's repeatable.

162
00:12:29,660 --> 00:12:33,700
So you can say, before this model goes into production, I know that it's passed these

163
00:12:33,700 --> 00:12:39,460
validation tests and I know that a human looked at it and said, it looks good.

164
00:12:39,460 --> 00:12:44,500
And if it's out in production and there's an error or there's some sort of issue that rises,

165
00:12:44,500 --> 00:12:48,940
you can go back, you can recreate that model, you can debug the error.

166
00:12:48,940 --> 00:12:53,660
And so that's the real foundational piece for all of it.

167
00:12:53,660 --> 00:12:59,780
And then on top of that, we're trying to give data scientists more tools to analyze the

168
00:12:59,780 --> 00:13:01,700
models themselves.

169
00:13:01,700 --> 00:13:04,060
And there's no magic button in here.

170
00:13:04,060 --> 00:13:08,940
It's not just, oh, we can run a test and we can tell you everything you want to know.

171
00:13:08,940 --> 00:13:13,420
But there's lots of great algorithms out there in research that help you better understand

172
00:13:13,420 --> 00:13:14,940
your model.

173
00:13:14,940 --> 00:13:18,900
Like Shap or Lime are common interpretability ones.

174
00:13:18,900 --> 00:13:24,300
And so we've created a toolkit called Interpret ML where this is an open source toolkit,

175
00:13:24,300 --> 00:13:26,380
you can use it anywhere.

176
00:13:26,380 --> 00:13:32,940
But the idea is, enables you to easily use a variety of these algorithms to explain your

177
00:13:32,940 --> 00:13:37,660
model behavior and explore it and see if there are any issues.

178
00:13:37,660 --> 00:13:44,180
And so we've also built that into our machine learning process so that if I build a model,

179
00:13:44,180 --> 00:13:47,220
I can easily generate explanations for that model.

180
00:13:47,220 --> 00:13:51,260
And when I've deployed it in production, I can also deploy an explainer with it.

181
00:13:51,260 --> 00:13:55,220
So individual predictions can be explained while it's running.

182
00:13:55,220 --> 00:13:59,500
So I can understand if I think it's doing the right thing and if I want to trust it,

183
00:13:59,500 --> 00:14:05,340
for example, it strikes me that there's a bit of a catch 22 here in the sense that the

184
00:14:05,340 --> 00:14:11,100
only way we could possibly do this is by putting tools in the hands of the folks that are

185
00:14:11,100 --> 00:14:15,500
working data scientists and machine learning engineers that are working on these problems.

186
00:14:15,500 --> 00:14:22,460
But the tools in their very nature kind of abstract them away from the problem and allow

187
00:14:22,460 --> 00:14:28,100
them, if not encourage them to think less deeply about what's going on underneath.

188
00:14:28,100 --> 00:14:30,460
How do we address that?

189
00:14:30,460 --> 00:14:37,420
Yeah, I think. Or do you agree with that first of all? No, I completely agree with that.

190
00:14:37,420 --> 00:14:44,860
And it's a challenge that we have in all of these cases where we want to give the tool

191
00:14:44,860 --> 00:14:46,860
to help them and to have more insight.

192
00:14:46,860 --> 00:14:51,380
But it's easily for people then to just use it as a shortcut.

193
00:14:51,380 --> 00:14:57,340
And so in a lot of cases, we're being very thoughtful about the design of the tool and

194
00:14:57,340 --> 00:15:03,420
making sure that it is helping you surface insights, but it's not saying this is the answer

195
00:15:03,420 --> 00:15:09,820
because I think when you start doing that, like if you have some net flags and says this

196
00:15:09,820 --> 00:15:13,460
is a problem, then people really start relying on that.

197
00:15:13,460 --> 00:15:17,940
And maybe someday we will have the techniques where we have that level of confidence and

198
00:15:17,940 --> 00:15:18,940
we could do it.

199
00:15:18,940 --> 00:15:20,780
But right now we really don't.

200
00:15:20,780 --> 00:15:25,740
And so I think a lot of it is making sure that we design the tools that encourages this

201
00:15:25,740 --> 00:15:31,020
mindset of exploration and deeper understanding of your models and what's going on.

202
00:15:31,020 --> 00:15:33,780
And not just, oh, this is just another compliance test.

203
00:15:33,780 --> 00:15:34,780
I have to pass.

204
00:15:34,780 --> 00:15:37,460
I just run this test and it says green and I go.

205
00:15:37,460 --> 00:15:43,540
You alluded to this earlier in the conversation, but it seems appropriate here as well.

206
00:15:43,540 --> 00:15:45,260
And it's maybe a bit of a tangent.

207
00:15:45,260 --> 00:15:54,420
But so much of pulling all these pieces together is kind of a user experience and design.

208
00:15:54,420 --> 00:15:55,420
Any thoughts on that?

209
00:15:55,420 --> 00:16:00,220
Is that something that you've kind of dug into and studied a lot or do other folks worry

210
00:16:00,220 --> 00:16:04,540
about that here?

211
00:16:04,540 --> 00:16:10,020
It's not in my background, but to me, it's an essential part of the function of actually

212
00:16:10,020 --> 00:16:12,580
making these technologies usable.

213
00:16:12,580 --> 00:16:18,300
And particularly when you take something that as complex as an algorithm and then you're

214
00:16:18,300 --> 00:16:23,100
trying to make that abstracted and usable for people, the design is a huge part of this

215
00:16:23,100 --> 00:16:24,100
story.

216
00:16:24,100 --> 00:16:29,020
What we're finding in responsible AI is that we need to think about this even more.

217
00:16:29,020 --> 00:16:36,540
And a lot of our, a lot of the guidelines are saying, you know, be more thoughtful and

218
00:16:36,540 --> 00:16:39,660
include sort of more careful design.

219
00:16:39,660 --> 00:16:43,500
For example, people are very tempted to say, well, this is the data I have.

220
00:16:43,500 --> 00:16:48,300
So this is the model I can build and so I'm going to put it in my application that way.

221
00:16:48,300 --> 00:16:53,980
And then if it has too much inaccuracy, then you spend a lot of resources to try to make

222
00:16:53,980 --> 00:16:59,460
the model more accurate where you could have just had a more elegant UI design, for example,

223
00:16:59,460 --> 00:17:05,300
where you actually get better feedback based on the UI design or the design can tolerate

224
00:17:05,300 --> 00:17:07,900
more errors and you don't need that higher model accuracy.

225
00:17:07,900 --> 00:17:12,500
And so we're really encouraging people to co-design the application in the model and

226
00:17:12,500 --> 00:17:17,100
not just take it for granted that this is what the model does and that's the thing we're

227
00:17:17,100 --> 00:17:18,780
going to focus on.

228
00:17:18,780 --> 00:17:26,860
With the interpret ML tool, what's the kind of user experience like?

229
00:17:26,860 --> 00:17:28,340
It depends on what you're trying to do.

230
00:17:28,340 --> 00:17:33,620
There's two types of interpretability that people think about.

231
00:17:33,620 --> 00:17:36,740
One is where we call glass box models.

232
00:17:36,740 --> 00:17:41,300
And the idea there is I want my model to be inherently interpretable.

233
00:17:41,300 --> 00:17:46,660
So I'm going to pick something like, you know, a linear model or decision trees where

234
00:17:46,660 --> 00:17:52,060
I can actually inspect the model and enable you to build a model of that that you can

235
00:17:52,060 --> 00:17:53,460
actually understand.

236
00:17:53,460 --> 00:18:00,900
And so we support a bunch of different glass box explainer models and then so you can

237
00:18:00,900 --> 00:18:03,460
actually use it then to train your own model.

238
00:18:03,460 --> 00:18:11,580
And the other part is black box explainers where I have a model that I is a black box and

239
00:18:11,580 --> 00:18:16,140
I can't actually inspect it, but I can use these different algorithms to explain the

240
00:18:16,140 --> 00:18:17,700
behavior of the model.

241
00:18:17,700 --> 00:18:22,980
And so in that case, what we've done is made it easy for you to just call an explainer

242
00:18:22,980 --> 00:18:29,180
and ask for global explanations and ask for local explanations and ask for feature importance.

243
00:18:29,180 --> 00:18:34,660
And then all of those are brought together in an interactive dashboard where you can

244
00:18:34,660 --> 00:18:41,780
actually explore the explanations and try to kind of understand the model behavior.

245
00:18:41,780 --> 00:18:50,220
So a lot of the experience hits an SDK and so it's all easy calls to ask for explanations.

246
00:18:50,220 --> 00:18:55,540
But then we expect a lot of people to spend their time kind of in that dashboard exploring

247
00:18:55,540 --> 00:18:57,060
and understanding.

248
00:18:57,060 --> 00:19:03,860
I did a really interesting interview with Cynthia Rudin, who you may know, may know.

249
00:19:03,860 --> 00:19:10,340
She's a Duke professor and the interview was focused on her research essentially says

250
00:19:10,340 --> 00:19:16,660
that we should not be using black box models in I forget the terminology that she used

251
00:19:16,660 --> 00:19:20,860
but something like critical kind of mission critical scenarios or something along those

252
00:19:20,860 --> 00:19:25,700
lines where we're talking about someone's, you know, life or liberty.

253
00:19:25,700 --> 00:19:31,180
That kind of thing does providing kind of interpretability tools that work with black

254
00:19:31,180 --> 00:19:36,620
box models like encourage their use in scenarios that they shouldn't really be used in and are

255
00:19:36,620 --> 00:19:43,300
their ways that you kind of advise folks when and when not, they should be using those

256
00:19:43,300 --> 00:19:44,980
types of models.

257
00:19:44,980 --> 00:19:54,460
So we have people who do publish sort of best practices for interpretability and it's

258
00:19:54,460 --> 00:19:58,820
a very active area of work for the company and we work with the partnership on AI to try

259
00:19:58,820 --> 00:20:02,660
to make sort of industry-wide recommendations for that.

260
00:20:02,660 --> 00:20:07,740
I don't think it's completely decided on this idea that models should be interpretable

261
00:20:07,740 --> 00:20:12,700
in these settings versus, well, we want other mechanisms to make sure that they're doing

262
00:20:12,700 --> 00:20:13,700
the right thing.

263
00:20:13,700 --> 00:20:16,980
Like interpretability is one way that we could be sure that they're doing the right thing

264
00:20:16,980 --> 00:20:20,620
but we also could have more robust testing regimes, right?

265
00:20:20,620 --> 00:20:24,940
There's a lot of technologies where we don't understand every detail of the technology

266
00:20:24,940 --> 00:20:29,540
but we've been able to build safety critical systems on top of it, for example.

267
00:20:29,540 --> 00:20:36,620
So yeah, as a company, we do try to provide guidance but I don't think the industry has

268
00:20:36,620 --> 00:20:42,980
really decided the final word on this and so the mindset of the toolkit is enabling

269
00:20:42,980 --> 00:20:48,540
you to use these techniques if it's right for you but that doesn't specifically say that

270
00:20:48,540 --> 00:20:52,540
you should go use a neural net in a particular setting.

271
00:20:52,540 --> 00:21:00,500
So in addition to the interpret ML toolkit, you also announced this week here from Ignite

272
00:21:00,500 --> 00:21:02,540
a fair-learn toolkit.

273
00:21:02,540 --> 00:21:04,180
What's that all about?

274
00:21:04,180 --> 00:21:10,340
So it's the same spirit as interpret ML where we want to bring together a collection

275
00:21:10,340 --> 00:21:16,060
of fairness techniques that have been published in research and make it easy for people to

276
00:21:16,060 --> 00:21:19,420
use them all in one toolkit.

277
00:21:19,420 --> 00:21:24,900
That's the same spirit that you want to be able to analyze your model and understand

278
00:21:24,900 --> 00:21:29,300
how it's working so that you could make decisions around fairness.

279
00:21:29,300 --> 00:21:36,140
And so the toolkit, there's famously many, many different fairness metrics published.

280
00:21:36,140 --> 00:21:39,860
I think there was a paper, you know, cataloging 21 different fairness metrics.

281
00:21:39,860 --> 00:21:45,420
And so we built many of these common ones into the toolkit and then it makes it easy

282
00:21:45,420 --> 00:21:50,620
for you to compare how well your model works for different groups of people in your data

283
00:21:50,620 --> 00:21:51,620
set.

284
00:21:51,620 --> 00:21:58,100
So for example, I could say does this model have the same accuracy for men and women?

285
00:21:58,100 --> 00:22:02,620
Does this model have the same outcomes for men and women?

286
00:22:02,620 --> 00:22:08,780
And so we have an interactive dashboard that allows you to explore these differences between

287
00:22:08,780 --> 00:22:13,700
groups and model performance through a variety of these metrics that have been published

288
00:22:13,700 --> 00:22:15,220
in research.

289
00:22:15,220 --> 00:22:21,700
Then we've also built in several mitigation techniques so that if you want to do mitigation

290
00:22:21,700 --> 00:22:26,220
via post-processing in your model, then you can do that, for example, setting thresholds

291
00:22:26,220 --> 00:22:27,620
per group.

292
00:22:27,620 --> 00:22:31,900
And in a lot of cases, it might be that you actually want to go and fix the underlying

293
00:22:31,900 --> 00:22:34,180
data or you want to make some different decisions.

294
00:22:34,180 --> 00:22:39,260
So the mitigation techniques aren't always what you would want to do, but they're available

295
00:22:39,260 --> 00:22:41,220
if you want to do that.

296
00:22:41,220 --> 00:22:45,860
And so the name of the toolkit actually comes from one of these mitigation techniques

297
00:22:45,860 --> 00:22:52,540
from Microsoft Research, where the algorithm was originally called Fairlearn.

298
00:22:52,540 --> 00:23:00,300
And the idea is that you say, I want to reduce the difference between groups on a particular

299
00:23:00,300 --> 00:23:01,300
dimension.

300
00:23:01,300 --> 00:23:06,620
So you pick the metric and you pick the groups and the algorithm actually retrain your

301
00:23:06,620 --> 00:23:12,380
model by re-weighting data and iteratively retraining to try to reduce that disparity.

302
00:23:12,380 --> 00:23:14,180
So we've built that into the toolkit.

303
00:23:14,180 --> 00:23:18,900
So now you can actually look at a variety of your versions of your model and see if one

304
00:23:18,900 --> 00:23:24,380
of them has properties that works better for what you're looking for to deploy.

305
00:23:24,380 --> 00:23:27,460
Again, I'm curious about the user experience in doing this.

306
00:23:27,460 --> 00:23:35,300
How much kind of knob turning and tuning does the user need to do when applying that technique

307
00:23:35,300 --> 00:23:40,580
you were describing or is it more, I'm envisioning something like contextual band, it's reinforcement

308
00:23:40,580 --> 00:23:43,140
learning where it's kind of tooling the knobs for you.

309
00:23:43,140 --> 00:23:47,620
Yeah, no, it's like, it is doing the knobs and the retraining.

310
00:23:47,620 --> 00:23:53,060
But what you have to pick is which metric you're trying to minimize.

311
00:23:53,060 --> 00:24:00,540
Like, do I want to reduce the disparity between the outcomes or do I want to reduce the disparity

312
00:24:00,540 --> 00:24:05,500
in accuracy or some other, you know, there's many different metrics you could pick, but you

313
00:24:05,500 --> 00:24:09,500
have to know what the metric is that's right for your problem.

314
00:24:09,500 --> 00:24:13,860
And then you also need to select the groups that you want to do.

315
00:24:13,860 --> 00:24:20,980
So it can work in a single dimension, like as we're saying, making men and women more

316
00:24:20,980 --> 00:24:26,740
equal, but then it would be a totally separate thing to do it for age, for example.

317
00:24:26,740 --> 00:24:31,260
So you have to pick both kind of the sensitive attribute that you are trying to reduce

318
00:24:31,260 --> 00:24:35,300
disparity and you have to pick the metric for disparity.

319
00:24:35,300 --> 00:24:40,740
Were you saying that you're able to do multiple metrics in parallel or you're doing them

320
00:24:40,740 --> 00:24:42,380
seriously?

321
00:24:42,380 --> 00:24:46,940
Right now the techniques work for just one metric.

322
00:24:46,940 --> 00:24:49,860
So it will produce a series of models.

323
00:24:49,860 --> 00:24:52,780
And if you look at the graph, you can actually plot disparity.

324
00:24:52,780 --> 00:24:56,700
My accuracy and you'll have a models that are kind of on that Pareto optimal curve to look

325
00:24:56,700 --> 00:24:57,700
at.

326
00:24:57,700 --> 00:25:01,740
But then if you said, okay, well, now I want to look at that same chart for age.

327
00:25:01,740 --> 00:25:06,740
The models might be all over the place in the space of disparity and accuracy.

328
00:25:06,740 --> 00:25:13,220
So it's not a perfect technique, but there are some settings where it's quite useful.

329
00:25:13,220 --> 00:25:22,380
So kind of going back to this idea of abstraction and tools versus deeply understanding the problem

330
00:25:22,380 --> 00:25:28,340
domain and how to think about it in the context of your problem domain.

331
00:25:28,340 --> 00:25:32,780
I guess the challenge domain and your problem domain I don't know what the right terms are.

332
00:25:32,780 --> 00:25:35,300
But you mentioned the paper.

333
00:25:35,300 --> 00:25:40,580
There's that paper with all of the different disparity metrics and the like, is that

334
00:25:40,580 --> 00:25:45,940
the best way for folks to get up to speed on this or are there other resources that you've

335
00:25:45,940 --> 00:25:47,580
come across that are useful?

336
00:25:47,580 --> 00:25:54,580
Yeah, I think for fairness in particular, it's better to start, I think, with your application

337
00:25:54,580 --> 00:26:01,060
domain and understand, for example, if you're working in an employment setting, how do we

338
00:26:01,060 --> 00:26:04,380
think about fairness and what are the cases?

339
00:26:04,380 --> 00:26:11,180
And so in that case, we actually recommend that you talk to domain experts and even your

340
00:26:11,180 --> 00:26:15,700
legal department to understand what fairness means in that setting.

341
00:26:15,700 --> 00:26:20,180
And then you can go to the academic literature and start saying, okay, well, which metrics

342
00:26:20,180 --> 00:26:25,020
kind of line up with that higher level concept of fairness for my setting.

343
00:26:25,020 --> 00:26:30,980
But if you start with the metrics, it's very, I think it can be very overwhelming.

344
00:26:30,980 --> 00:26:36,660
And there's just many different metrics and a lot of them are quite different in other

345
00:26:36,660 --> 00:26:38,100
ways, they're very similar with each other.

346
00:26:38,100 --> 00:26:43,020
And so I find it much easier to first think, start with the domain expertise and know

347
00:26:43,020 --> 00:26:46,860
what you're trying to achieve in fairness and then start finding the metrics that line

348
00:26:46,860 --> 00:26:47,860
up with that.

349
00:26:47,860 --> 00:26:51,540
You're also starting to do some work in the differential privacy domain, tell me a little bit about

350
00:26:51,540 --> 00:26:52,540
that.

351
00:26:52,540 --> 00:26:59,620
Yeah, we announced a couple weeks ago that we are building an open source privacy platform

352
00:26:59,620 --> 00:27:02,740
with Harvard.

353
00:27:02,740 --> 00:27:07,980
And differential privacy is a really fascinating technology.

354
00:27:07,980 --> 00:27:13,460
It was first published in Microsoft Research in 2006.

355
00:27:13,460 --> 00:27:19,900
And it's a very interesting idea, but it has taken a while for it as an idea to mature

356
00:27:19,900 --> 00:27:23,060
and develop and actually be able to be used in practice.

357
00:27:23,060 --> 00:27:29,380
Harvard, now we're seeing several different companies who are using it in production.

358
00:27:29,380 --> 00:27:35,540
But in every case, the deployment was a very bespoke deployment with experts involved.

359
00:27:35,540 --> 00:27:39,780
And so we're trying to make a platform that makes it much easier for people to use these

360
00:27:39,780 --> 00:27:43,900
techniques without having to understand them as much.

361
00:27:43,900 --> 00:27:49,900
And so the idea is the open source platform can go on top of a data store and able you

362
00:27:49,900 --> 00:27:57,020
to do queries in a differentially private way, which means that actually it adds noise to

363
00:27:57,020 --> 00:28:03,060
the results so that you can't reconstruct the underlying data.

364
00:28:03,060 --> 00:28:07,900
And also then potentially use the same techniques to build simple machine learning models.

365
00:28:07,900 --> 00:28:14,660
And so we think this is particularly important for some of our really societally valuable

366
00:28:14,660 --> 00:28:15,660
data sets.

367
00:28:15,660 --> 00:28:20,340
For example, if I, there are data sets where people would like to do medical research,

368
00:28:20,340 --> 00:28:27,100
but because we're worried about the privacy of individuals, there's limits to what they

369
00:28:27,100 --> 00:28:28,100
can actually do.

370
00:28:28,100 --> 00:28:33,260
And if we use a differential private interface on that, with a lot more privacy guarantees

371
00:28:33,260 --> 00:28:39,860
and so we can unlock a new type of innovation and research and understanding our data.

372
00:28:39,860 --> 00:28:47,060
So I think we're really excited and think this could be the future of privacy in certain

373
00:28:47,060 --> 00:28:48,380
applications.

374
00:28:48,380 --> 00:28:49,980
But the tooling just isn't there.

375
00:28:49,980 --> 00:28:54,460
And so we're working on trying to make it easier for people to do that.

376
00:28:54,460 --> 00:28:59,260
We're building it in the open source because it's important that people can actually,

377
00:28:59,260 --> 00:29:02,900
it's very easy to get the implementation of these algorithms wrong.

378
00:29:02,900 --> 00:29:09,180
And so we want the community and the privacy experts to be able to inspect and test the

379
00:29:09,180 --> 00:29:12,580
implementations and have the confidence that it's there.

380
00:29:12,580 --> 00:29:15,940
And also we think this is such an important problem for the community.

381
00:29:15,940 --> 00:29:20,300
We would like anybody who wants to be rejoining in and working on this.

382
00:29:20,300 --> 00:29:22,980
This is not something that we can solve on our own.

383
00:29:22,980 --> 00:29:31,460
Yeah, differential privacy in general and differentially private machine learning are fascinating

384
00:29:31,460 --> 00:29:35,180
topics and ones that we've covered fairly extensively in the podcast.

385
00:29:35,180 --> 00:29:41,620
We did a series on differential privacy a couple of years ago maybe and it's continuing

386
00:29:41,620 --> 00:29:46,820
to be an interesting topic like the Census Bureau I think is using differential privacy

387
00:29:46,820 --> 00:29:53,460
for the first time next year and it's is both providing the kind of anticipated benefits

388
00:29:53,460 --> 00:29:59,980
but also raising some interesting concerns about an increased opacity I guess by on the

389
00:29:59,980 --> 00:30:03,420
part of researchers to the data that they want to get access to.

390
00:30:03,420 --> 00:30:06,260
Are you familiar with that challenge it?

391
00:30:06,260 --> 00:30:07,740
Yeah, absolutely.

392
00:30:07,740 --> 00:30:12,100
So the reality is, you know, people always want the most accurate data, right?

393
00:30:12,100 --> 00:30:16,300
It doesn't sound great to say, well, we're adding noise in the data is less accurate.

394
00:30:16,300 --> 00:30:17,300
Yeah.

395
00:30:17,300 --> 00:30:22,740
In a lot of cases, it is accurate enough for the tasks that you want to accomplish and

396
00:30:22,740 --> 00:30:29,620
I think we have to recognize that privacy is one of the sort of fundamental values that

397
00:30:29,620 --> 00:30:34,900
we want to uphold and so in some cases it's worth the cost.

398
00:30:34,900 --> 00:30:40,380
For the Census in particular, right, they to motivate the decision to start using this

399
00:30:40,380 --> 00:30:49,860
for the 2020 Census, they did a study where they took the reports from the 1940 Census and

400
00:30:49,860 --> 00:30:55,780
they were able to recreate something like 40% of Americans, you know, data with a result

401
00:30:55,780 --> 00:30:57,420
of just the outputs from the Census.

402
00:30:57,420 --> 00:30:58,420
Wow.

403
00:30:58,420 --> 00:31:01,900
They talked about me personally identify 40% of Americans.

404
00:31:01,900 --> 00:31:07,660
Yeah, that's, they talk, he talks about this in his ICML keynote for last year.

405
00:31:07,660 --> 00:31:10,340
So if you want to learn more, you can watch the keynote.

406
00:31:10,340 --> 00:31:14,500
But yeah, basically they took all the reports and they use some of these privacy attacks

407
00:31:14,500 --> 00:31:20,140
and they could basically recreate a bunch of the underlying data and you know, this is

408
00:31:20,140 --> 00:31:26,940
a real risk and so we have to recognize that yes, the Census results are incredibly important

409
00:31:26,940 --> 00:31:32,500
and they help us make many different decisions but also protecting people's data is important.

410
00:31:32,500 --> 00:31:36,780
And so some of it is education and changing our thinking and some of it is making sure

411
00:31:36,780 --> 00:31:43,220
that we use the techniques in the right way in that domain where you're not losing what

412
00:31:43,220 --> 00:31:46,900
you were trying to achieve in the first place but you are adding these privacy benefits.

413
00:31:46,900 --> 00:31:52,740
There are a couple of different ways that people have been applying differential privacy.

414
00:31:52,740 --> 00:31:58,380
One is a more centralized way where you're applying it to a data store.

415
00:31:58,380 --> 00:32:00,740
It sounds a little bit like that's where your focus is.

416
00:32:00,740 --> 00:32:06,100
Other like apples kind of a noted use case where they're applying differential privacy

417
00:32:06,100 --> 00:32:14,980
in a distributed manner kind of at the handset to keep user data on the iPhone but still

418
00:32:14,980 --> 00:32:21,020
provide information to kind of centrally for analysis.

419
00:32:21,020 --> 00:32:25,540
Am I correct that your focus is on the central wise use case or does the toolkit also support

420
00:32:25,540 --> 00:32:27,660
kind of the distributed use case?

421
00:32:27,660 --> 00:32:30,420
We are focusing on the global model.

422
00:32:30,420 --> 00:32:36,740
The local model works really well and particularly some of these user telemetry settings but it

423
00:32:36,740 --> 00:32:38,940
limits what you can do.

424
00:32:38,940 --> 00:32:45,340
You need like much larger volume to actually get the accuracy for a lot of the queries

425
00:32:45,340 --> 00:32:48,700
that you need and there aren't as many queries that you can do.

426
00:32:48,700 --> 00:32:53,980
And so the global model on the other hand, there's a lot more that you can do and still

427
00:32:53,980 --> 00:33:00,180
have reasonable privacy guarantees and so we felt like as I was saying we were motivated

428
00:33:00,180 --> 00:33:04,500
by these cases where we have the data sets like somebody is trusted to have the data

429
00:33:04,500 --> 00:33:10,020
sets but we can't really use them and so that looks like a global setting and so to start

430
00:33:10,020 --> 00:33:16,220
we're focused on the global piece but there are many cases where the local is promising

431
00:33:16,220 --> 00:33:21,820
and there are cases where we are doing that in our products and so it's certainly a direction

432
00:33:21,820 --> 00:33:23,940
that things could go.

433
00:33:23,940 --> 00:33:29,220
And differential privacy from a data perspective doesn't necessarily get you to a differentially

434
00:33:29,220 --> 00:33:30,580
private machine learning.

435
00:33:30,580 --> 00:33:36,660
Are you doing anything in particular on the differential private ML side of things?

436
00:33:36,660 --> 00:33:44,540
The plan is to do that but the project is pretty new so we haven't built it yet.

437
00:33:44,540 --> 00:33:49,940
And I guess before we wrap up you're involved in a bunch of kind of industry and research

438
00:33:49,940 --> 00:33:57,220
initiatives in the space that you've mentioned, SISML, MLSIS, a bunch of other things.

439
00:33:57,220 --> 00:34:03,300
Can you talk a little bit about some of the broader things that you're doing?

440
00:34:03,300 --> 00:34:12,460
Yeah so I helped found the, now I think named MLSIS systems in machine learning research

441
00:34:12,460 --> 00:34:18,780
conference and that was specifically because I've been working at this intersection for

442
00:34:18,780 --> 00:34:24,260
a while and there was some dark days where it was very hard to publish work because the

443
00:34:24,260 --> 00:34:29,060
machine learning community was like this is a systems result and the systems community

444
00:34:29,060 --> 00:34:35,420
was like this doesn't seem like a systems result and so we started the conference about

445
00:34:35,420 --> 00:34:40,980
two years ago and apparently many other people were feeling the same pain because even

446
00:34:40,980 --> 00:34:46,060
from the first conference we got excellent work, people's kind of top work which is always

447
00:34:46,060 --> 00:34:49,060
a challenge for the research conferences because people don't want to submit their best

448
00:34:49,060 --> 00:34:53,900
work to an unnamed conference, but there was such a gap for the community.

449
00:34:53,900 --> 00:34:59,140
So it's been really exciting to sort of see that community form more and now have a home

450
00:34:59,140 --> 00:35:01,620
where they can put their work in and connect.

451
00:35:01,620 --> 00:35:08,760
So I've also been running the machine learning systems, workshops at NURBS for several

452
00:35:08,760 --> 00:35:14,180
years now and that's been a really fun place because it really has helped us form the community

453
00:35:14,180 --> 00:35:19,020
particularly before we started the conference, but it's also a place where you can kind of

454
00:35:19,020 --> 00:35:23,740
explore new ideas like this last year, we're starting to see a lot more innovation at

455
00:35:23,740 --> 00:35:28,020
the intersection of programming languages and machine learning and so in the workshop

456
00:35:28,020 --> 00:35:32,660
format we can you know have several of those talks highlighted and kind of have a dialogue

457
00:35:32,660 --> 00:35:37,580
and show some of the emerging trends so that's been a really fun thing to be involved

458
00:35:37,580 --> 00:35:38,580
in.

459
00:35:38,580 --> 00:35:39,580
Awesome.

460
00:35:39,580 --> 00:35:40,580
Yeah.

461
00:35:40,580 --> 00:35:48,100
Was it last year that there was both the CISML workshop and ML for systems workshop

462
00:35:48,100 --> 00:35:49,580
and got really confusing?

463
00:35:49,580 --> 00:35:50,580
Yeah.

464
00:35:50,580 --> 00:35:51,580
This year too.

465
00:35:51,580 --> 00:35:52,580
This year too.

466
00:35:52,580 --> 00:35:54,500
Yeah.

467
00:35:54,500 --> 00:35:58,660
And I think you know that's a sign that the field is growing that it used to be that

468
00:35:58,660 --> 00:36:02,820
it felt like we didn't even have enough people for one room at the intersection of machine

469
00:36:02,820 --> 00:36:07,780
learning and systems and I think this last year there was maybe four or five hundred people

470
00:36:07,780 --> 00:36:13,980
in our workshop alone and so that's great now there's definitely room to have workshops

471
00:36:13,980 --> 00:36:20,940
on more focused topics right and so I think machine learning for systems is an area that

472
00:36:20,940 --> 00:36:26,420
people are really excited about now that we've kind of have more depth in understanding

473
00:36:26,420 --> 00:36:27,700
the intersection.

474
00:36:27,700 --> 00:36:33,940
For me it's very funny because that is really kind of the flavor of my thesis and which

475
00:36:33,940 --> 00:36:39,580
was a while ago and so it's fun to see it now starting to become kind of an area that

476
00:36:39,580 --> 00:36:41,260
people are excited about.

477
00:36:41,260 --> 00:36:47,500
The other conference that we didn't talk about ML for systems is all about using machine

478
00:36:47,500 --> 00:36:53,540
learning within computational systems networking systems as a way to optimize them.

479
00:36:53,540 --> 00:37:00,700
So for example ML to do database query optimization also a super interesting topic.

480
00:37:00,700 --> 00:37:01,700
Yeah.

481
00:37:01,700 --> 00:37:02,700
No.

482
00:37:02,700 --> 00:37:08,100
It absolutely is and I actually I really believe in that and I think for several years

483
00:37:08,100 --> 00:37:12,820
people were just trying to replace kind of all of the systems intelligent with one machine

484
00:37:12,820 --> 00:37:16,580
learning algorithm and it was not working very well and I think what we're seeing now

485
00:37:16,580 --> 00:37:22,580
is recognizing that a lot of the algorithms that we use to control systems were designed

486
00:37:22,580 --> 00:37:28,100
for that way and they work actually pretty well but on the other hand there's something

487
00:37:28,100 --> 00:37:34,460
that's dynamic about the world or the workload and so you do want this prediction capability

488
00:37:34,460 --> 00:37:40,460
built in and so a lot of the work now has a more sort of intelligent way of plugging

489
00:37:40,460 --> 00:37:46,060
the algorithms into into the system and so now we're actually starting to see promising

490
00:37:46,060 --> 00:37:48,740
results at this intersection.

491
00:37:48,740 --> 00:37:55,180
So my thesis work actually was a resource allocation that built models in real time in

492
00:37:55,180 --> 00:38:00,260
the operating system and allocated resources and it was exactly this piece where there was

493
00:38:00,260 --> 00:38:06,860
a modeling and a prediction piece but the final resource allocation algorithm was not

494
00:38:06,860 --> 00:38:08,340
purely machine learning.

495
00:38:08,340 --> 00:38:09,340
Awesome.

496
00:38:09,340 --> 00:38:10,340
Awesome.

497
00:38:10,340 --> 00:38:11,340
Wonderful conversation.

498
00:38:11,340 --> 00:38:14,780
Looking forward to catching up with you at NERP's hopefully.

499
00:38:14,780 --> 00:38:17,580
Thanks so much for taking the time to chat with us.

500
00:38:17,580 --> 00:38:21,060
Yes, thanks for having me and I look forward to seeing you at NERP's.

501
00:38:21,060 --> 00:38:22,060
Thank you.

502
00:38:22,060 --> 00:38:31,180
Alright everyone that's our show for today to learn more about this episode visit Twomolei.com.

503
00:38:31,180 --> 00:38:47,380
As always, thanks so much for listening and catch you next time.

