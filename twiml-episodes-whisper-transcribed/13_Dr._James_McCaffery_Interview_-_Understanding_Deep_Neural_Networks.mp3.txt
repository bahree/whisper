Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
If you listened to last week's show you know that today Friday May 3rd is the last day
to register for our O'Reilly Strata Hadoop World Conference giveaway.
You've got until midnight Pacific time to register, and you can do that via our new Facebook
page.
Whether or not you're interested in attending Strata Hadoop, we'd really appreciate you
taking a moment to like our Facebook page, as well as subscribe to our new YouTube channel.
We'll link to both of these in the show notes.
Last week I mentioned that I'm working on an event called the Future of Data Summit,
and I'm excited to share some of the details of that event with you now.
The event is part of a larger IT industry conference called Interop ITX, and I've worked
with the team at UBM that organizes the event for several years now.
At last year's conference, I presented a workshop called the IT Leaders Guide to Machine
Learning, and based on the strong response to that session, they asked me to work with
them to do something bigger this time around.
The result is a two-day future of data summit that will bring together noted experts and
practitioners to discuss the future of enterprise data from a variety of technology perspectives.
We'll be exploring the innovation and opportunity being offered in areas such as, of course,
machine learning in AI and cognitive services, but also IoT and edge computing, augmented
and virtual reality, blockchain, algorithmic IT operations, data security and privacy,
and more.
I've handpicked the speakers to both inspire summit attendees with a view into what's
possible, as well as to provide practical insights into how to get there.
To give you a taste of what I've got planned, here are just three of the 16 great speakers
on our agenda for the summit.
Well, first off, you remember Josh Bloom, a former guest on the podcast who start up
wise.io, was recently acquired by GE.
Well, Josh will be joining us to speak about building AI products from idea to production.
Intel's Asaf Araki will give us a view into the next five plus years of compute, storage
and network innovation in his talk titled, How the Future of Hardware Enables the Future
of Data.
And Diana Kelly, global executive security advisor at IBM, will be discussing the future
of threat landscape and how to protect cloud, IoT and big data systems.
I've got more information about the event, as well as a preliminary agenda posted at
twimmolai.com slash future of data.
On that page, you'll also find details for registering for the conference and a code
offering a special discount for twimmolisteners.
To give you a bit of a sample of the type of content you'll get at the event, our guest
on the show today is James McCaffrey, who's a research engineer at Microsoft Research.
James will be speaking at the summit on understanding deep neural networks, and that's the focus
of our conversation on the podcast as well.
We had a good time with this conversation, and even if you know your way around a DNN,
I think you'll pick up some interesting tidbits.
Enjoy the show and check out the event page at twimmolai.com slash future of data, or
the show notes page at twimmolai.com slash talk slash 13, for more information on James
or the summit.
And now on to the show.
Hey, everyone, I am here with James McCaffrey.
James is with Microsoft Research, and we've got an exciting show for you this time, and
we're going to be spending some time digging into deep neural nets.
James, why don't you introduce yourself?
Hi, Sam.
Thanks for having me today.
My name is James McCaffrey.
I work at Microsoft Research.
Before working at research in the research division of Microsoft, I worked in the product
groups, so I have some experience sort of on the pragmatic side of things.
And before joining Microsoft, I was a university professor in mathematics and computer science.
So I made that transition from academia to industry.
At Microsoft, my area of expertise is machine learning, and in particular, neural networks.
One of the things I do here at Microsoft Researcher, my role, is somewhat of a hybrid.
At Microsoft Research, we have, I'm going to guess maybe in the neighborhood of 300 serious
researchers, world class guys, that have very specific domain knowledge.
And my role is because I have enough mathematical knowledge to understand these guys.
And also my software engineering background, I act as a interface between the engineering
groups here.
Microsoft and the research groups.
And I do, of course, some research on my own.
Interesting.
So the way we got connected was, in fact, you're going to be speaking at an event that I'm
organizing as part of the NROP ITX conference in May, and that event is called the Future
of Data.
And you're going to be speaking there about understanding deep neural nets.
That's a topic that you've been spending quite a bit of time on of late, isn't it?
But sort of interesting, yes and no.
I'd say I've been looking at neural networks for many, many, many years.
But we're sort of in this area of the third wave of artificial intelligence, and in particular
deep neural networks.
And there's no clear consensus on exactly why deep neural networks, which is a sort of
I'd call it a subset of artificial intelligence or a tool that enables artificial intelligence.
Why they're making this giant come back again?
Maybe some of your listeners can remember back in the 80s, the first wave of neural networks
that held great promise, at least theoretically, but they tended to over-promise and under-deliver.
And then for a long time, artificial intelligence, the phrase, wasn't really used because it
had sort of gotten a stigma attached to it.
But then here's the analogy I always use for people.
I think many of your listeners might remember some of the speech recognition software that
was popular in the 90s, Dragon was very well known.
In fact, there still really are well known too.
But then all of a sudden, about two years ago, two and a half years ago, seemingly
out of nowhere, we had Siri and Cortana and Alexa from Apple, Microsoft and Amazon of course.
And the speech recognition just seemed to take this gigantic quantum jump in improvement.
And I'm fortunate to be working directly with those guys that created that quantum jump.
In fact, it really was.
And it was all due to deep neural networks.
And now, you mentioned speech recognition that's clearly one of the big application areas.
There was some news recently, I think, within the past three months or so about a group
that I guess just hit a new kind of past, a new bar in terms of speech recognition accuracy.
I think it was 95 or high 90s percent.
Was that at Microsoft?
It's interesting because there's several different benchmarks.
And right now, there's tremendous amounts of excitement.
I'm sort of beating around the bushes.
My bottom line answer in a second will be, I'm not sure it could well have been.
But there's literally breakthroughs.
I was sitting in a talk just a few weeks ago where there was sort of like an arms race
on some of these benchmark problems.
And therefore speech recognition, text recognition, basically any kind of input type things.
And literally, one research group after another is improving and jumping over the others
on a week-by-week basis on it.
Or reminds me of the early days of jet aircraft in the 1950s when there seemed to be a new
speed record set every few months or a few weeks or months.
Well, we're sort of in that same area of frantic activity.
That doesn't sound quite right.
It's not so much frantic activity, but significant advances are happening weekly now in several
areas of AI, and most of them are directly related to deep neural networks.
So maybe let's take a step back.
I'm curious, how do you define and describe deep neural nets to people?
This is very interesting.
I have a way, I get asked this question so much, so excuse me, even among my colleagues
who have PhDs in all different kinds of fields.
And my peer engineers, who are some of the best engineers in the world, it's very, very
difficult to explain what deep neural networks are without a picture of some sort.
I found that the only way I can describe completely to the satisfaction of anyone is to use
a diagram, and that's what I do in many of my talks, including the one I'll be doing
for you.
So using vocabulary, it's not, but the main differences are, I'll try to express as best
I can.
Whenever I try to explain what a deep neural network is, I start and say, and it kind of
makes sense, you have to have an absolute solid understanding of what a so-called regular
neural network is, and because the distinction until recently, when you said neural network,
you meant what is now called a single hidden layer neural network.
They're the simplest forms of neural network.
And deep neural networks can actually have several different meanings.
At the basic level, a deep neural network is simply, I mean, it's really simple.
It's just a more complicated basic neural network with multiple hidden layers.
If I can interrupt you and go back to this single hidden layer neural network, we're talking
about a neural network that will have an input layer and then this hidden layer and an output
layer.
Basically, each of those layers has a set of weights assigned to them and using some math
and algorithms, backpropagation, for example, you're able to, based on throwing a bunch
of training data at these neural networks, come up with a, quote unquote, optimal set of
weights, which really is what defines the neural network.
Is that like, is that a good way to describe what this single...
That's absolutely correct.
Put another way.
In the end, a neural network is just a very complex mathematical equation that can be used
to make predictions.
The number of inputs is determined by your data.
Suppose you're trying to predict the political party affiliation of a person and that could
be democratic, republican, or other.
So that's what you're trying to predict, and your features, which means the variables
you use to make the prediction, suppose that could be four things.
The person's age, their annual income, their level of education, and some other metrics.
So four.
Therefore, your neural network would have four input nodes and it would have three output
nodes.
But the hidden layer processing nodes, this hidden layer is where the processing, most
of the processing is done.
The number of those nodes has to be determined by trial and error.
But in a regular neural network, there is one such layer.
So it might be maybe ten hidden nodes.
But with a deep neural network, you just add multiple layers.
You might have three hidden layers of ten processing nodes, twenty processing nodes,
and then ten processing nodes.
And in fact, neural networks have been getting much, much deeper than three layers of
late.
Is that right?
Quite right.
Until, relatively recently, there's been two things have been occurring that have led
to these dramatic increases across multiple areas of artificial intelligence.
One of them is that we're just getting more raw horsepower to process these things.
It turns out that they get exponentially more complex and so it turns out that we're
just getting more and more processing power.
But the second thing is combined at the same time is that we're getting very clever with
architecture.
And that is combining these different hidden layers in very clever ways instead of doing
it naively.
The analogy in this case reminds me of the advances in computer chess programs where computer
chess programs all of a sudden got very, very good, better than any human being, someone
unexpectedly.
And it was it was not due merely to more processing power and it wasn't due simply to better
algorithms.
It was a combination of the two.
So we're getting quickly to an area that I find really interesting.
And that is the architecture of deep neural nets.
I have a ton of questions about this so I'm excited that we get a chance to chat about
it.
I know.
I guess as a as a as a preface to this, I know that Microsoft research has been one of
many research organizations that's been kind of pushing the front to here.
And in fact, in 2015, they authored a paper on what's called deep residual learning that
won the image net competition that year.
And so, you know, I guess what I want to talk about is like what is deep neural net architecture
and, you know, what is deep residual learning and what are convolutional layers like, you
know, so take us from this description of deep neural net and layers through how those
the architecture of those networks has evolved and, you know, what are what are how do we
think about all that right now?
OK, I'll do my best to describe these again.
When I do describe these, I almost always have to use a diagram because I'm going to
talk about architecture, it's sort of going to be a bunch of nodes and how they're connected.
So let me sort of talk about all of these things that you've mentioned are closely related.
They're somewhat cousins to each other.
Let's let's take the residual neural network that you just described.
Now this is more of an exotic variety and in my mind, at least, the residual neural network
is very, very close to a close cousin to a type of neural network called a recurrent
neural network.
They're usually abbreviated RNNs.
Right.
Now, what makes it recurrent neural network special and by the way, there's a ton of research
activity on all of these things that we're talking about now.
But a standard neural network does not maintain state.
You feed it some inputs and it produces some outputs.
Then, the next set of inputs come along.
The neural network is essentially wiped clean.
It doesn't maintain state from that previous set of inputs and outputs.
A recurrent neural network has memory and internal memory, so to speak, and that manifests itself
with just some extra nodes.
If you can imagine a regular neural network with a hidden layer of nodes, say, 10 nodes
in there, there's going to be a recurrent neural network has a second group of 10 nodes
that maintain the memory of the previous input.
This allows this, just intuitively, you can tell that this makes the neural network much
more powerful and smart because it has, in an English word, it has context.
This means, for instance, suppose you're trying to predict, you're coming along and your
inputs are words in a sentence and you're trying to predict what the next word might be.
This is something that you might see on like a smartphone when you're typing a message
and it tries to predict what your next word might be, although there's a pretty rudimentary
right now.
If you just used a regular neural network to do that, input is separate and you wouldn't
have any context, but a recurrent neural network would have a shadow of the memory of the
previous inputs and it would be able to make a better guess at what the next word is because
the next word in a sentence is clearly going to depend on what the first words were.
These recurrent neural networks sometimes are categorized into short termy recurrent
neural networks where they only have a limited ability to remember quite recent inputs or
they can be one of the most exciting areas of research right now is these long term recurrent
neural networks and they just maintain more memory and these things have the potential
to be super powerful and to get to sort of close the circle here.
In my mind, I view the Microsoft residual neural networks as one of these long term recurrent
neural networks with some special architecture features thrown in sort of customer.
So you mentioned long-term memory and short-term memory and in fact on this show, I've
talked quite a bit about applications using LSTM RNN which is long short-term memory.
How does that relate to long-term and short-term?
Well, to tell you the truth, the vocabulary is not very standardized and they all from
a...
So maybe these long short-term memories are what you're referring to as long-term and
that's also...
Yeah, okay.
Thank you.
Okay.
You precisely said what I was trying to say.
Got it.
Got it.
Okay.
So, yeah.
We have been hearing tons about different applications of these LSTM networks, you know, often
relating to the example that you use which is you're predicting...
Trying to predict words or things like that from a sentence.
Which kind of brings us to maybe the difference between predictive networks and generative
networks?
Oh.
Okay.
Very good.
This is...
If I had to pick one area where there's more excitement, intellectual excitement in the
research community than any other, it's exactly said these generative neural networks.
They're called GAN.
One of the most popular forms of these is called GAN, a generative adversarial network.
Sure, it's really conceptually a little bit difficult to grasp, and here's how I think
about it.
A generative neural network does just what you might expect from its description is it
doesn't try to make a prediction based on input.
It more or less tries to create new inputs in some sense, which is a little bit hard
to grasp.
Now, I'll be the first to say that I don't fully understand these things.
Like everybody else, they've only been around...
Really, the biggest name is a guy named Ian Goodfellow, who is the best-known name in
this area.
And these things have only really been around for a matter of months now.
By that, I mean, maybe a year and a half to two years or so.
So a lot of us are still trying to figure it out.
The classic example, at least, that I used on my blog post, is that you can feed a neural
network a bunch of Van Gogh paintings, and then that generative neural network will be
able to generate and create paintings based on the style of Van Gogh.
In short, what it's doing is it's sort of separating out.
It's learning to separate style from content.
Well, this is all very difficult for me to get my head around.
And I'll say that people who are much, much smarter than me figure that this is something
that could lead to tremendous breakthroughs in the future.
And for folks that want to dig into that last use case, I believe the paper is called
Neural Artistic Style Transfer.
Or at the very least, if you Google that or Bing that, you'll be able to find lots of
information about that application.
Right.
Exactly right.
But yeah, so there's generative networks and GANs.
In fact, I just had an opportunity to hear in Goodfellow talk about this last week.
I was at an event, a deep learning summit, rework deep learning summit in San Francisco.
And the basic idea there, as I understand it, is you've got, as you mentioned, a network
that is kind of trained to produce or approximate inputs.
And then you feed the stuff that it spits out to another network that is, I think, called
a discriminator network that's trained to basically measure how close those inputs are
to the real life thing, the thing that you're trying to approximate.
And then you basically have a feedback loop between these two.
That's correct.
They're called adversarial because really under the covers, there's two neural networks
going on.
Number one is trying to generate information and fake out the other neural networks.
So they're adversarial.
They're working against each other.
And this relates more to the architecture, the engineering architecture.
But as you said, the real goal is to generate information.
And the idea being there, that if a neural network is smart enough to generate information,
then it's also smart enough to understand and discriminate information.
So we talked about RNNs.
What about convolutional neural nets?
How are those different from RNNs and other types of deep neural nets?
It's funny that talking about convolutional networks, they're usually abbreviated CNNs,
that now they seem like they're just sort of old news.
But in fact, they're quite new still.
The main problem with deep neural networks, as I described, a basic deep neural network,
which is a simple architecture, but with just lots of nodes in multiple layers.
The problem there is the training.
The number of weights and biases that you have to compute, or using your optimization
algorithm, just becomes intractable.
The number of weights you have to do is 5 times 6, plus 6 times 3, plus 6, plus 3.
As you expand the number of nodes, in English, it increases exponentially, that's not mathematically
correct.
Let's just say it gets really big, really fast, it gets intractable.
It doesn't drive you crazy when people do say that, and it's not actually exponential.
It depends on how much I've been drinking.
You know, I try to see, because here at Microsoft, I speak to different audiences, I'll speak
to business leaders, I'll speak to engineers, and I'll speak to mathematicians.
When you're speaking to anybody but the mathematicians, if you try to phrase yourself too carefully
and be correct, you mess up your argument, but when I say, for instance, that the output
of a neural network, a classifier, are probabilities, oh, my math colleagues will go nuts and go,
no, they're not.
No, they're not.
I go, okay, yeah, I know they're not.
But we're back to convolutional neural networks, because a straightforward approach just
isn't intractable computationally.
The idea, and let's see, this was, I always have trouble, remember, is Yan Lee-Kun?
Is the big name here?
He created an architecture where the main idea of this architecture was to make these
things tractable.
CNNs are used almost exclusively for image processing.
This is an area that I'm not too familiar with, I mean, I'm from Earth, math of it all.
But imagine you have an image, or a set of images, and you want to classify them.
The classic example is called the M-ness database, where there's a data set of umpteen thousand
handwritten digit characters that were called from IRS tax returns and digitized.
And so suppose you want to classify, you know, what is this?
Is it a digit one?
Is it a digit two or so forth?
Well, even a very small image is going to have thousands of pixels, and each pixel is
going to be one input.
Now if you get, go up to like a seriously large picture, or even something that a smartphone
can take, you've got millions of inputs.
And millions of inputs, you just can't deal with that in a basic way.
So the, the, the brilliancy of convolutional neural networks is to simplify.
It still uses the same basic ideas of neural networks, but it uses them in very clever
ways by slicing and dicing the image up and sharing weights instead of having to calculate
a million times a million, which is whatever that is, weights.
You can break it up, and there's a part of the secret sauce is shared weights where weights
in a particular area of inputs, meaning a particular area of the image are shared.
And there's a lot more to it than that.
Convolutional neural networks are really a remarkable achievement of architectural design,
and they're now considered more or less standard.
Many of the tools that you can find in particular Google's tool, whose name I can never remember
because I don't use it, it runs strictly on Linux, do you know which one I'm talking
about?
Say I'm with it.
Google's TensorFlow.
TensorFlow.
Oh, thank you, thank you.
Yes, sir.
Anyway, so Google's TensorFlow can do CNNs, and Microsoft has a recently released, basically
a same idea called CNTK, not a real, it doesn't slide off the tongue really easily there.
But these things are now well known, but I always like to point out that it took a lot
of researchers a lot of years.
In fact, the CNN version that's in common use now is called CNN version five or something
like that, which means there were many major iterations and tons of work that went
on.
So, in short, to summarize, you know, these CNNs to the best of my knowledge are used almost
exclusively for image processing, but they are the state of their art.
However, they have some really interesting problems that a lot of, there's a lot of thought
about some of the limitations of CNNs.
Can you speak a bit to those?
Yeah, I sure there was a very interesting, fascinating paper that came out of Google
Research, what was it called?
It was called the intriguing properties of neural networks, something like this.
And the key takeaway is, and I like to use this example of which I don't think was in
the paper, but other people followed up on it.
You can, suppose you train a CNN to recognize images, you can feed it a picture of a school
bus, and it's clearly a school bus, and the CNN will recognize it.
But by cleverly messing up just a few of the pixels, the image is completely unchanged
to the human eye.
However, this exact same classifier now sees the school bus as an ostrich, so it's the
bus to ostrich effect.
Well, this is very troubling in a lot of ways.
It raises, by the way, you can't just throw, you can't just randomly mess up the picture,
you have to do it in a very clever way, but it raises some important issues.
One of them is, at least the whole question of comprehension.
Does a CNN really understand things if you can hoax it this way?
It also raises questions of, if people are going to, and they are, using these CNNs for
things, which have security implications, or imagine medical imaging, where it has implications
for health and safety.
Are law enforcement exactly?
If these things have this inherent weakness, maybe there's something wrong with CNNs.
This is all just the speculation that's going to, and no one really knows, but at least
just some very interesting questions, and the research goes on at just giving more
interest in research, in particular, some of my colleagues are working on trying to go
back to the very, very early days, where instead of just using raw math and raw processing,
we're going to try to do some symbolic and some sort of a deeper level of understanding.
Can you elaborate on that?
What does that mean in this context, and how will we apply symbolic, symbolic here?
Because we just hired the person who's considered the leading guy in this area, and he only started
here, here is Paul, and I'll spell his last name, S-M-O-L-E-N-S-K-Y, Paul Smolensky.
We just hired him out of Johns Hopkins University, and he's been, what many people,
claiming he considered the leading researcher in this area of symbolic reasoning and machine
learning.
I got his book, and I'm a fairly bright guy, I have a PhD, but this was a complicated
book.
He's thinking at a different level, and he's trying to, I had an interesting chat with
him in the hallway the other day.
He sits right behind me, and the analogy goes like this, when I was an undergraduate, my
very first degree was in cognitive psychology, which, through various things, that led to
math and that led to computer.
Anyway, when I was in my cognitive psychology days, I worked with a brilliant researcher
Art Duncan-Luce, and his goal was to create a complete mathematical framework and description
of certain areas of psychology in the human mind.
In other words, try to map cognition.
How do people think?
Because still, we still don't know how people think.
To map that call is attempting, in some ways, to create a meta framework for symbolic
reasoning and logic.
This is, right now, deep neural networks have been remarkably effective in doing what
I call the sort of sensory aspect of artificial intelligence.
Imagine the five senses that we have.
Even vision and pattern and image recognition, they're really good at speech recognition.
They're really good at, even the robotics manipulation, they're really good at, but the
one thing that they just were not even close right now is the reasoning aspects of it.
That's what the symbolic type of process is designed to do, or one area.
It's one attack on this.
I know that was a little bit vague and fishy, but maybe you can get Paul in a future one
of your podcasts to talk about.
I'd love to hear what he has to say.
Okay.
Awesome.
Yeah.
That would be great.
We've got CNNs, RNNs, and I still want to probe around the idea of network architecture
and residual learning.
There was a blog post by a guy named Steven Merity who's at Salesforce now.
He came in via one of their recent acquisitions.
If I remember correctly, he wrote this blog post and the title was something along the lines
of network architecture is the new feature engineering, meaning in traditional machine
learning, a big part of the job was trying to figure out how to massage your data and
how to create whether natural or man-made features that express the underlying properties
of your data in a way that your machine learning algorithms can easily train on those
and produce accurate results.
This new world, defining the network architecture of your deep neural nets is the moral equivalent
if you will.
It's the new thing that we need to do to massage our data and our solutions to produce accurate
results.
I'm trying to, I'm wondering if you can help us wrap our heads around what that process
looks like and what are the things that researchers or engineers are thinking about as they start
with a problem and say, I've got this data set and I think deep neural net is the way
to solve this problem.
How do they then get to, oh, well, the optimal answer is something that I'm going to call
the deep residual network that has 150 layers and these convolutional layers and every fifth
layer is a residual layer and that whole process.
Is that something you can speak to?
Well, yeah, I'll talk about this because sadly the bottom line is there's no good answer
to this.
If sort of the phrase that everybody has heard a million times is that machine learning
and AI and deep learning and all this is still as much art as it is science and that has
been true and it still is true, there are some incredibly bright people who work in this
field.
I'm fortunate enough to work with some of the greatest minds, I mean, they're world
famous and leaders.
But when we sit around drinking coffee and chatting about this, there's so much unknown.
And the brightest guys in the world are learning daily and new stuff and for instance, another
related thing here is that another hot area is reinforcement learning, which is how
does that fit in?
Even among my colleagues, we're talking about, you know, we're knowledge junkies.
We're just constantly trying to soak this information up, but things are happening so fast
and there's so much unknown.
The area you're talking about, network architecture, that's one way, I mean, that would be a good
surrogate term for exactly what's going on in all of research.
Now, it's almost all related directly or indirectly to the architecture.
Now, I'm a pretty, you know, I believe in simplicity and for me, network architecture,
deep architecture is really simple in the one hand where it's just how you combine your
processing nodes and not so much input output in different ways.
And it boils down to, think about the human brain, it's been some interesting work done
by all things DARPA, the defense agency in conjunction with IBM, where one of the
projects they have and Microsoft has a similar project that I don't think I can talk about
now.
It's name, it's still under wraps, but I can give you a rough idea of what we're doing
by talking about the IBM and the department defense thing, where the idea here is that
instead of, it's almost too simple, instead of using the approach we're using right now,
which is to get very clever with very specific types of architecture, very, you know, just
think of a blueprint.
Instead, take the approach that the human brain may have and that is just make your architecture
a bunch of, a bunch of nodes totally connected, in other words, like the human brain.
And then instead of using supervised learning, where you have to have labeled data, you
have to have known correct outputs with your, you know, inputs, use unsupervised learning.
And I'm sort of tossing out a schmorker's board of terms here, but unsupervised learning
is another incredibly hot area of research right now, where we realize that methods that
require labeled training data, which is just the way to say data, where you tag what the
correct output is, that can only take you so far.
It's just not going to scale to the, you know, the kinds of things that we want to do.
Anyway, back to the DARPA IBM thing, they're creating this thing, where their goal is
to create a processor in hard work, because, you know, IBM is known for that, that kind
of work, that is, you know, skills to biological levels.
And as far as I can recall from last time I read that, an article on it, they believe
that they have successfully created in-hardware a neural network, and they're not calling
it that, that roughly simulates the complexity of the brain of a honeybee.
And then, okay, the question here is not, okay, so how does it learn?
And you know, that wraps around back to symbolic thing.
So, I'm sorry, that was kind of a rambling answer here, but I agree that you're right,
that it's right now, if you wanted to summarize all of the, or most of the areas, including
these generative adversarial networks, the long short-term memory networks, the residual
networks, it's all about the architecture.
So to, to maybe further summarize, I guess the way, the way I kind of take away, what
I would take away from what you are saying is, you know, maybe on the one hand, you know,
to ask, you know, how do we create new network architectures for a given problem?
We're just too early to, to, right now, we're in the stage where the fact that we come
up with a new architecture for a problem that is, that works and is useful, like that's
a big deal, and, and we're going to have to do a lot of that before we can say, oh, this
is the process for creating new architectures for our given problems.
Let me interrupt by saying, sorry for interrupting you, but you just recalled to my mind a very
well-known paper, it's actually not even a paper, it's basically a blog post, but it's
extremely well-known in the, you know, in our field, it's called the unreasonable effectiveness
of recurrent neural networks. With the idea being that, you know, there's no obvious
connection, you know, for the person who, I tried to look up, I could not find too much
history on recurrent neural networks. There was some indication, but it's not exactly
clear who thought of them first, but it's not at all obvious, you know, you have these recurrent
architecture that has worked unexpectedly well, so unreasonably well. So the point is,
I know the stuff is obvious, and we're still in the very, very early stages of figuring
all this out exactly what you said.
And then, I guess along those lines, if I am a listener and I'm building, I want to,
I want to create a solution, you know, does it stand a reason that, you know, what, where
would you start if you were building something? Like, would you even, would you even try to
build your own deep neural net, or would you use some off-the-shelf implementation? Would
you, you know, use a service? Would you, if you thought that you, like, how would you
know if you needed to build your own thing?
That's an interesting question, and there's a, there's, I want to say controversy, but
there are differences of opinion here. My personal opinion is that whenever I'm going to tackle
a problem, for instance, one of the problems that I'm fascinated by, and I've worked on
for many years, is predicting America and National NFL football scores. And I like that as
an interesting problem, because it's concrete, it's practical, and you can determine your,
you know, how good you are right away. And I originally started using sort of standard
canned approaches. I started with sort of regression techniques, and then I started
using regular sort of neural networks from a tool, like WCA, like TensorFlow, and things
like that. And I got up to a certain level of accuracy or goodness, and I just couldn't
get better. No matter what I did, I couldn't get better. Until I threw it all away and created
my own neural architecture from scratch, where it was custom designed for this problem.
In much the same way that convolutional networks are absolutely custom designed for image
recognition with, you know, they're optimized because an image has pixel values and the RGB
or the red, green, blue values. Anyway, to cut to the chaser to reiterate, I totally believe
that at least now with the tools that we have, you get the best results by far by creating
your own custom version. And I do. Now, the problem here is that I write code every day.
And these things are not easy, right, even for extremely advanced developers. And it's
very time consuming and very difficult. So there aren't, you know, I'm not trying to
sound boastful, but there aren't many people like me who can spin up a custom design neural
network in, you know, two days or a week. So I think it's going to boil down to eventually
boil down to problems. One of the things that we talk about a lot at Microsoft is the
democratization of AI or machine learning. And the analogy here is maybe you and some
of your listeners can remember the days, the very early days when spreadsheets, Lotus
1, 2, 3, we're just becoming popular. And a lot of people were saying, what, why, why
are companies, including Microsoft, making these spreadsheets? Why would people, why would
normal people ever want to use a spreadsheet? This is just something for accounts. So,
but then Lotus 1, 2, 3, and later Excel and the others democratized numeric processing
with spreadsheets. And then all of a sudden, all kinds of interesting good things happened
from that. In much the same way, the goal to democratize machine learning is the idea
that if you give some basic machine learning tools and knowledge to millions of people,
they're going to find interesting ways to use it and solve problems that we haven't
even thought of. That said, though, I still believe that just like you can only do so
much with Excel and numeric processing, you'll only ever be able to do so much with a
canned program, or no matter how powerful the tool is, and that there's always going
to be the need for machine learning artisans. I don't know if I said that word, right?
Go in and create custom models and custom prediction models for particular problems.
Your description made me prompting me to ask myself, what is the VBA for deep neural
nets? Exactly.
And let's skip that right over a second. Before we leave, I want to talk to you about applications,
including the stuff that you do around NFL scores. But before we leave that, there are
a couple of areas that I wanted to dig into around. And these are all things that I noted
that you wrote blog posts around. One of them is around, I made this comment about network
architecture being the new feature engineering. But in fact, it sounds like there is some of
the old feature engineering that's still important and that needs to be done around data encoding
and normalization when dealing with neural nets and deep neural nets. And I was wondering
if you could speak to that. And then I wanted to ask you about drop out and cross entropy
error as well.
Okay, so I didn't quite follow the first part of what you're asking, exactly.
Oh, you wrote this blog post about data encoding and normalization. And I didn't dig into
that post in a lot of detail, but I was wondering if there are specific techniques in those areas
related to neural nets and deep neural nets beyond the kind of things that you do in traditional
machine learning.
Very, very interesting. This is something that I'll answer in direct as usual. It seems
like I always do. The bottom line is, okay, neural networks, no matter how you slice and
dice it currently, they only understand numbers. They're number crunchers, now very, very
interesting complex number crunchers. So all of your input data eventually has to be,
or not eventually has to be right away turned into some kind of numeric form, so it can
be understood by the neural network. And people who are new to the field, this is often
one of the most discouraging parts of learning machine learning is that it seems that there's
an endless number of data transformation techniques and just all this data massaging before
you can ever get to the really interesting part. And it can get quite depressing for
new people, but I always tell my audiences when I'm doing training and things like that.
That fortunately, there's only a discrete number of these things. You have to learn,
for instance, there are four real ways, I mean, four major ways to normalize your data,
so it's all scaled to roughly in the same range or so. Now, once you know those four,
and once you understand when they're used and when they're not used to have a few examples,
then you got it. But at first, you know, when you're first trying to learn it, it seems
like, hopeless, oh, man, I've got to worry about data normalization. I've got to worry
about data encoding in the same way that there is only a few ways for data encoding. Now,
so the answer is that, and then, and none of those things have changed with deep neural
networks. Got it. However, I'm always cautious to say because that's sort of the accepted,
generally accepted truth. But I love to, you know, take, whenever I hear something like
that, in fact, I hadn't really thought about it until you asked this question. I always
like to go back and look and go, you know what? Is this really true? Just because everyone
says it's true, it just sort of like creates like this viral thing. Here's an example
where I argue all the time with my colleagues. It's something very basic. Suppose you're
trying to use a neural network, this is going to be a little bit technical, but you're supposed
you're trying to use a neural network to predict something that can only take one of two
values. For instance, you're trying to predict whether a person is male or female based
on a voting behavior, based on age, based on, based on all these other things. So another,
it's a binary classification problem. Now, these standard and totally accepted by everybody,
except me, technique is to create a neural network that has only a single output node.
And that single output node is going to be a number between zero and one where values
less than 0.5 are going to indicate one of the two outcomes, male, say, and values greater
than 0.5 are going to indicate the other female. So, and that is mathematically efficient
as opposed to the alternative of having a neural network that has two output nodes explicitly,
where they sum to one. So, you still get the same result. In other words, if your listeners
know about a multi-class classifier, you just use the exact same architecture, but with
two output nodes. In other words, when you're doing classification, you will never ever
see a two output node, neural network classifier, because the idea being that if you're trying
to predict one of two things, just make it a single node. Well, I tell everybody, I go,
you know, okay, yeah, it makes sense, but I haven't, you know, explain that to me, you
know, prove that to me that one node is exactly equivalent to two nodes. And so, but anyway,
my point, I'm getting fired up because I'm, like I'm, I'm passionate about questioning
common knowledge. So, back to your thing. So, it's common knowledge now that the data
encoding and normalization techniques, that were commonly used and are being used for
standard neural networks. We don't need anything new for deep neural networks. I'm not so
sure.
Well, before we leave the specific example, are you, are you excited about questioning the
fact that a two node network in this example is inferior to a one node network, or have
you demonstrated that there are some cases that a two node network is superior to a single
node network, or for some external reasons, by external reasons, I mean, like, you know,
maybe they're the same in terms of accuracy, but implementation wise, one is better than
any other. What's the source of your excitement around this question?
There's two, I say a couple, or at least two reasons. Okay, one, and primarily, I think
it's hard to sometimes to be, you know, self-evaluate. I think it's probably psychological
in my part, where in my world, knowledge is power. Knowing more than someone else is
considered, you know, our mark of success. Yeah, it take, I work with a lot of guys who
work in some form of sales. And for them, you know, I mean, everybody's competitive,
but for them, a measure of success for them is how much money they make, because that's
the external kind of manifestation of their goodness in some way. So in, you know, research
and stuff, your measure is no one something, or coming, understanding something, publishing
something first, they're the people that, so I think that there's a psychology there,
where, if most people like me, you know, were competitive in some sense of the definition,
where if everybody is saying this, and I'm somehow able to prove, everybody else was
wrong, I'd get great satisfaction out of that. So that does, I mean, it sounds kind
of terrible, but I think that's part of it. Now, the other part is, from an implementation
point of view, I know that working on the code end of things, every implementation that
I've seen has a completely has sort of two different code bases for neural network classifiers,
one for binary classification, and one for all other cases. But if, when you're classifying,
doing a binary classification, and you have two output nodes, then you only have one
code base. In other words, the neural network is the neural network, where the number of
output nodes is the number of classes that you're trying to predict. So from that engineering
point of view, it's very appealing.
There's an elegance to have the same solutions at the same code base for independent of
the specifics of the problem, or to not have the exception of the single class or the
two class prediction.
And then regarding training deep neural nets, what's the state of the art there? And I
think my sense is that that training techniques, or tell me if this is true or not, that training
techniques are tied very closely to architecture. At this point, meaning the research papers
that talk about new architectures are also talking about specific training techniques for
those architectures, or is that not the case, and then talk about dropout, which is, I
think that was Jeff Hinton's group in 2014.
Go ahead.
If that's enough to get going with.
Well, first of all, I agree with you for the first part of your question, and that in
general, there are a few exceptions, but in general, if you create a custom network architecture,
then you'll have to use a custom training algorithm optimization. Now, there's, I'll make
a parenthetical remark that an area that I believe has great promise. And again, I'm
in a very much of a minority view here, is that there are certain optimization algorithms
and techniques that can be applied to any network architecture. And in general, they're called
swarm intelligence optimization algorithms, particle swarm optimization, and so forth.
There's some others. And basically, they just use absolute brute force, whereas most
opt, they're not based, the swarm techniques are not based on calculus and gradients and
things. So that's, you know, most optimization algorithms are based on calculus, and you
have to calculate derivatives, and the derivatives depend on the architecture. So that's why you
got to basically, in most cases, create a custom training algorithm, if you have a custom
training custom neural architecture. And then, as I mentioned, I'm intrigued by the
idea of applying the swarm optimization to these things. I've made a few stabs at it,
but like anything else, there's just not enough time. Going back, I remember, in the
previous discussion we were talking about, you know, my two node versus one node binary
cluster, I just haven't had time to look at it. It would take, you know, I'd have to
dedicate a week or two to that. And it's like all of us, you know, I mean, I've got more
things that I have to do, than things, than time to do. So in short, I agree with you
that custom training algorithms are needed with the possible exception of swarm optimization,
which in my few stabs, I haven't been entirely successful, but I'm not ready to give up
on them. Now, with regards to drop out as interest, there's a whole bunch of, not a whole
bunch of techniques, but quite a few techniques and drop out is one. Now, I remember drop out
training, which is closely related to jittering input jittering and so on, are all designed,
are mostly designed to prevent overfitting during training. That's sort of their motivation,
in most cases. And drop out training was everywhere, I'd say two to three years ago. It was a very
hot area of research, a lot of excitement around it, and that sort of faded out for reasons
which aren't clear to me. I have this in nagging suspicion a lot of times that trends in research,
and you know, very high end mathematical research, are subject to trends and fashions, just
like a lot of things are, and sometimes things fall out of favor for no apparent reason.
An example of this that I like to point out is that there's a neural network training
algorithm called resilient back propagation. It's a form of obviously a variation of back
propagation. I did some experimentation on it where I generated artificial data sets, very
large artificial data sets, and the resilient back propagation algorithm, I mean clearly outperformed
normal back propagation. Now, I have to say that with an asterisk. The problem with, it's almost
impossible to compare training algorithms because they all have so many hyperparameters
typically the learning rate, momentum rate, regularization, you know, L1 regularity. There's
just too many parameters. You're not completely comparing apples to oranges, but you're comparing
two different kinds of apples, perhaps. So it's very difficult to tell. So anyway, drop out
training is something that just seems to not be in fashion, but is there and I'm a believer
in dropout training, but you know, it's kind of funny and now that you asked this question,
I'm thinking back to recent neural networks that I've done. And I haven't been using dropout to
tell you the truth because it is, it's in my world, you know, I spin up custom on neural networks
myself and they're quite difficult to implement. It creates a lot of extra work. And so I take the
often take the easy way out. And what is that easy way out? Is it, I mean besides I'm not using dropout,
are there other things that you're doing with your data or there are other algorithms that have
the same effect of avoiding overfitting? Or is it, you know, your standard, you know, data
segmenting validation sets, that kind of thing? Yeah, you know, I got to be honest with you,
I don't really have a good answer to that question, you know, I'm not sure to be perfectly honest.
For one, you know, here, I was talking to the chief architect of Microsoft's CNTK tool,
which is our, or it's released to the public, you can find it on GitHub. It's our version of
TensorFlow. Deep neural networks, including convolutional neural networks and recurrent neural networks
and stuff. And I was talking to him one time because he's not only a, he's a great, the main architect,
very bright guy and named Frank. And we were talking and I asked him a question. I saw some really
weird behavior that I didn't understand. I don't remember what the weird behavior was. And so I saw him
in the hallway and I said, hey, Frank, you know, and then I described the phenomenon. And you go,
I said, can you think of anything that would cause that to happen? By the way, later turned out,
it was just a weird, it was just weird random randomness. But Frank thought about it before I
goes, you know, the only thing I can think of is that you've got a bug in your code.
And that was, I mean, he was, and I tell you, you know, when I saw that, when I saw the behavior
that I've described, that's what, that was my first question. Oh, man, I must have a serious bug
in my code somewhere. Well, it turns out that it wasn't a bug at all. I was just sort of bizarre
behavior. And, but the conversation led us to talk about. And then, you know, we're a conversation
sort of meandered. And I said, yeah, I remember. I told him the story how I spun up a neural network
as a few years ago. And I was using it for, you know, work. It was actually using it. It was
performing very well until one day I was looking at the code, dusted off the code, and realized,
I'd missed a, I'd completely missed updating one of the bias values. In other words,
I completely was ignoring one of the constants in the equation. And yet, the neural network was
performing well. So the moral of the story. And so I mentioned that to Frank, he goes, yeah, I've
done this many times myself. So what's happening here is when you create a neural network, it's
really, really hard to tell it if it's good or not, because you can get, you can get good results
and have a seriously flawed implementation. So in the same way by coming back to this dropout
thing where adding dropout or moving dropout, you'd think it'd be a relatively easy to tell
is this helping me or hurting me? It's not at all easy to determine. And in total, there are
a few things that have been really interesting for me about this conversation. But one of the most
is the, like the hard definitive stand you took on the need to craft your own networks. And
I think how that relates to here is, I don't know, I guess the idea that deep neural nets are,
you know, they're kind of magic black boxes, right? And they're particularly magic,
they're magic black boxes, even if you built them from scratch. And they're particularly,
they're going to be even worse if you are using something out of the box that you don't fully
understand. Absolutely. And I think this also relates to, you know, there's always this question
around, you know, using these out of the box tools. And, you know, for many types of problems,
you're trying to get from zero to 80 percent. And, you know, the researchers are trying to get
from 95.2 percent to 95.7 percent. And so that's kind of an argument for, well, you know, just use the tool.
You know, but if you, yeah, even if you're just trying to get to 80 percent, if you really need to
understand what's happening, or you need to be able to understand what's happening in the case where
it generally works great, but for whatever some variant in your input data produces
outlandishly wrong results, like you have to know what's going on under the covers.
Quite, I mean, I think you, I think you phrased that really, really well.
I guess, how are you doing on time? Are you still, you have, I have a meeting that started
right now. Okay. So we'll have to wrap this up. I'm afraid. Okay. So we'll wrap this up.
Maybe I can just ask you to quickly tell us about you, you mentioned you do your own research, you've
done some projects like NFL scores, like what's your, what's the project that you're most excited
about? And maybe give us a quick overview of that. And if it's something that's public where we can
learn more. Well, the project I'm working on right now, interestingly enough, is that Microsoft
recently launched what's called the AI school. Okay. This is a big deal. Microsoft is a large
organization and we create products and services. Microsoft has made a massive investment, both
money wise and sort of culture wise, where our senior leadership believes that putting intelligence,
real intelligence into every product and service that we do is critically important.
So some of our senior leaders I've seen say something where they believe that this wave of
adding artificial intelligence and machine learning intelligence into our products is every
bit as important as, you know, the internet came to pass. So towards that, Microsoft created what's
called the AI school. And I was hired from my previous to help run the AI school because I had
a background in education. And I'd say that I'm pretty relative to most of my peers. I have a
pretty broad knowledge of many areas of machine learning AI. I'm not nearly as deep as they are,
of course. So that's what I'm working on right now. I'm trying to spin up, trying to determine how
to transfer knowledge of all these things that we just talked about and place that knowledge into
the hands of the software developers that we have, the project managers that we have, the business
decision makers that we have, the salespeople who sell our products and generate the revenue that
you know, keeps me employed because everybody I was surprised we we sent out an announcement to
this like over creating the AI school. And we thought we'd get a, you know, maybe a couple hundred
messages of interest exclusively from engineers and developers. But we got thousands and thousands
literally we're I mean, we were overwhelmed by the response and not only just from engineers.
I think engineers see pretty clearly that machine learning and artificial intelligent skills are
quickly becoming must have skills for them. In other words, they're going to have to know how to
put logistic regression in or they're going to have to know the difference between this kind of
classifier and that kind of classifier. But so that made sense, but we were surprised by the number
of people, designers, UI people, literally across the organization people, exactly. So that's
what I'm working on now. And I'm very, you know, passionate about this and very interested in it.
And trying to deliver this knowledge while at the same time, in fact, I remember the the
researcher hired me to run this and he's actually in charge of this one of the most famous names
in speech recognition. He basically created the technology behind Cortana, which is the same as
technology behind here. I mean, extremely famous guy, but and he told me when I was interviewing
for this position from my my old position just upstairs, by the way, he told me, you know, how
you going to manage or how are you going to balance doing, you know, your job of creating training
classes and delivering classes and doing videos and stuff with the need to stay up to date because
things are rolling out on a, you know, weekly, monthly basis. Right. And I said, well, you know,
that's that's the challenge. So, you know, I'll conclude by saying, you know, I'm really excited
about working on the Microsoft AI school, but also really excited about all the all the things that
are going on, generative adaptive neural networks and and all these other things. And now you've got,
you certainly got me excited about this AI school and probably a lot of listeners as well as
this primarily an internal resource or it will it be a public resource that my Microsoft is
promoting. Yeah, it's a good good good. Something that we've talked about and we're really not quite
sure. You know, our our mandate, of course, initially at least is to provide this internally.
No, it's not a secret or anything, but we don't have any externally facing kind of information
very much. But on the other hand, a lot of people are saying, Hey, you know, I mean, we want to the
the content that we develop could be useful to everybody. Right. The deal here is that there's
a lot of content out there already. What we're trying to do is find our sweet spot where
how can we use our particular areas of expertise? We don't want to just rehash and redo,
say, for instance, most of your listeners probably know about Andrew Eng out of Stanford, his
excellent online course at Switch, which I think are probably pretty much state of the art.
We don't want to just try to replicate that for a couple of reasons. We'd be wasting our time
and we probably wouldn't do as good a job. So we're trying to find areas where we have our internal
expertise and certainly not only the knowledge, but the method of delivery. And once we figure that
out, then I fully believe that we'll be able to share that with everybody. Great. Great. With that
James, you've been very gracious with your time. Thank you so much. And look forward to
keeping in touch and to our, you know, when we meet in person at the the future of data summit.
Thanks, Sam. It was a pleasure chatting with you and thanks for your time.
All right, everyone. That's our show for today. Once again, thank you so much for listening and
for your continued support. Please remember that we want to hear from you. You can comment on
the show via the show notes page via the at Twomo AI Twitter handle or my own at Sam Charrington
handle via our new Facebook and YouTube pages or just via good old fashioned email to Sam at Twomo AI.com.
Please do show some love to our new Facebook and YouTube pages though. Your likes and
subscribers there will really help support the show. And remember, if you're catching this
podcast on Friday, you've still got time to register for our Strata Hadoop giveaway. The winner
will be announced on next week's show. The notes for this show and all the links I've mentioned
will be posted at Twomo AI.com slash talk slash 13. Thanks again for listening and catch you next time.
