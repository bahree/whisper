WEBVTT

00:00.000 --> 00:04.880
The conversation you're about to hear was recorded live at Twomokon AI

00:04.880 --> 00:11.680
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us

00:11.680 --> 00:18.400
on Twitter at Twomokon AI. But first, a word from our sponsor.

00:18.400 --> 00:23.840
Thanks to our friends at Dot Science for being a founding sponsor of Twomokon AI

00:23.840 --> 00:28.960
platforms. Dot Science is excited to have met so many amazing attendees at this

00:28.960 --> 00:33.360
year's conference and looks forward to continuing those conversations. If you

00:33.360 --> 00:35.680
weren't able to connect with their team at the conference

00:35.680 --> 00:39.040
or aren't interested in what you heard on stage or at their exhibit in the

00:39.040 --> 00:43.760
community hall, head over to dot science dot com slash deploy

00:43.760 --> 00:48.800
for a free demo and see just how easy it is to get your models into production

00:48.800 --> 00:57.440
and keep them performing. So please take the opportunity to

00:57.440 --> 01:03.520
join me in welcoming our first guest Hussein Mahana is the head of artificial

01:03.520 --> 01:09.360
intelligence and machine learning at cruise and Hussein has a very unique

01:09.360 --> 01:14.800
set of experiences that I'm excited to dig into

01:14.800 --> 01:20.160
in this interview. He was founding engineering leads on the Facebook's FB

01:20.160 --> 01:26.560
learner platform went on to Google to work on Google Cloud and now is

01:26.560 --> 01:31.440
running ML and AI at cruise. Hussein, welcome to Twomokon.

01:31.440 --> 01:36.320
As I mentioned in the interview, you have had a very unique set of experiences

01:36.320 --> 01:42.160
in this space, starting with joining Facebook at a really pivotal

01:42.160 --> 01:45.600
time I think for the company clearly before there was a machine learning

01:45.600 --> 01:51.280
platform in helping get that effort launched. Tell us a little bit about those

01:51.280 --> 01:56.720
you know that time period that you were there. I imagine it was a crazy

01:56.720 --> 02:03.520
period and you've got some more stories. Yeah, it was 2012 and I think it was

02:03.520 --> 02:08.400
the really best time to work on machine learning at Facebook at the time because

02:08.400 --> 02:13.440
the product was mature enough to support machine learning.

02:13.440 --> 02:16.960
Before you can do machine learning you also need good data infrastructure

02:16.960 --> 02:21.600
but the machine learning platform and capabilities themselves were very little.

02:21.600 --> 02:26.000
So at the time there was probably maybe two algorithms at most,

02:26.000 --> 02:32.800
probably maybe two people using machine learning at Facebook and maybe

02:32.800 --> 02:39.120
over the you know an entire year maybe 20 models were trained. By the time I left

02:39.120 --> 02:42.800
we had 25% of engineering using machine learning so we were in the

02:42.800 --> 02:48.080
thousands and we were training maybe two to three million models a month and

02:48.080 --> 02:51.920
a big portion of those were automatically trained and triggered just like

02:51.920 --> 02:57.520
whenever new data comes they just got launched. And most important metric of

02:57.520 --> 03:03.920
all is the time it takes to ship a model from idea to production.

03:03.920 --> 03:07.440
A relatively simple model would take three months, four months in those days.

03:07.440 --> 03:10.800
When I started. When I started and then we got to a point where

03:10.800 --> 03:15.520
multiple models a day could be launched. Wow, that's incredible. So the early

03:15.520 --> 03:20.960
models were that I'm imagining ads and feed? I mean yes, I mean ads and news

03:20.960 --> 03:27.520
feed and search at Facebook are the biggest consumers of machine learning

03:27.520 --> 03:31.760
and they continued but the earliest models were very very simple,

03:31.760 --> 03:35.760
very rudimentary but at the time I left we've supported sort of all sorts of

03:35.760 --> 03:40.720
algorithms and models you can think of. I can imagine. So when you got started was

03:40.720 --> 03:47.200
how big was that effort? Was there a minimum viable product for the platform or

03:47.200 --> 03:52.240
there was no Facebook wide platform. There were like sort of pockets of people

03:52.240 --> 03:56.560
who decided that they had to work on a platform for their team because they

03:56.560 --> 04:00.400
just couldn't ship anything and I became one of those. There was there was a

04:00.400 --> 04:05.840
small tool that we had and I went to my manager after several months working in

04:05.840 --> 04:09.440
the ads team and I said you know what I'm just going to focus on the platform.

04:09.440 --> 04:14.000
It is our biggest bottleneck and so I started working and I remember the

04:14.000 --> 04:18.000
first thing I shipped I locked myself for three weeks. We were training a simple

04:18.000 --> 04:23.600
boosted decision tree that took three days and when I got it down to 21 hours I

04:23.600 --> 04:29.120
could just see the tears of joy because my fears were like you're seeing I could

04:29.120 --> 04:32.560
I could train a model in a day and get the results in a day you know in the

04:32.560 --> 04:36.480
morning and then and then just be done like they were extremely happy and we

04:36.480 --> 04:42.480
eventually got that down to two hours and so and you can see a correlation

04:42.480 --> 04:47.600
between sort of the innovation we had and the productivity that machine learning

04:47.600 --> 04:51.920
engineers had so I decided to focus on that and then as the journey of the

04:51.920 --> 04:57.120
platform evolved we realized that there's an opportunity to support applied

04:57.120 --> 05:01.200
research because now when you have a platform that unifies the company if you

05:01.200 --> 05:07.440
drop in a advanced algorithm everyone else can use it and so the

05:07.440 --> 05:12.320
platform supported applied research in a in a great way and so the work you did

05:12.320 --> 05:16.240
with the boosted decision trees how did you achieve that speed?

05:16.240 --> 05:21.360
Was it just working on the algorithm itself or was it scaling it some way

05:21.360 --> 05:27.360
distributing a combination of everything so the first early wins were mostly

05:27.360 --> 05:34.080
about just avoiding some inefficiencies often machine learning data

05:34.080 --> 05:38.400
scientists or or machine learning scientists are not the best infrastructure

05:38.400 --> 05:45.200
builders and so there was just very common mistakes and those were the early

05:45.200 --> 05:49.040
or low hanging fruit but eventually we got to a point where it was all about

05:49.040 --> 05:54.160
making the algorithm itself faster there's a paper that Facebook one

05:54.160 --> 05:58.240
published about image net in an hour that's an example of how such

05:58.240 --> 06:01.680
scaling efforts eventually evolved where it wasn't just about the

06:01.680 --> 06:05.600
infrastructure but it's also about exploring different algorithms learning

06:05.600 --> 06:08.240
algorithms that can speed up the training

06:08.240 --> 06:14.160
training overall. Did the speed for training a model versus

06:14.160 --> 06:17.520
cycle time what like what ended up being your driving metric what was the thing

06:17.520 --> 06:22.240
you were focused on? That's a fantastic question it's the end-to-end

06:22.240 --> 06:27.200
machine learning cycle all the way from the moment you prepare your data

06:27.200 --> 06:32.160
to generate your features to training your models to

06:32.160 --> 06:37.440
deploying and then back again that cycle was essentially our entire focus

06:37.440 --> 06:42.160
and moving through that cycle faster and faster meant that we're essentially

06:42.160 --> 06:47.200
building models that are learning faster because that cycle itself is one

06:47.200 --> 06:52.160
cycle of learning and reality so we focused on the entire end-to-end.

06:52.160 --> 06:56.960
We talked about the MVP can you talk about the evolution of the platform there?

06:56.960 --> 07:02.720
Yeah we went through three phases in 2012 I focused on what was at the time

07:02.720 --> 07:07.520
called ab learner that was just for ads and it became really successful that

07:07.520 --> 07:11.360
other teams started coming to us and saying hey you know we want to give up our

07:11.360 --> 07:14.400
platform start using your things so that's when we moved it to FB

07:14.400 --> 07:18.560
learner and we had like two other groups at Facebook

07:18.560 --> 07:23.440
but then we realized that so we were scaling on the dimension of teams or

07:23.440 --> 07:27.680
scenarios but we're not scaling on the dimension of algorithms and techniques so

07:27.680 --> 07:32.800
it was a platform basically basically based around the couple of algorithms so

07:32.800 --> 07:37.120
we then moved to the third generation which is FB learner flow and that's when

07:37.120 --> 07:41.520
we scaled the number of scenarios significantly and the number of algorithms

07:41.520 --> 07:47.040
that we support and we built that platform with extensibility

07:47.040 --> 07:51.120
from the get go and not necessarily extensibility by the core FB learner team

07:51.120 --> 07:56.640
but the entire company and that's when the adoption rate in Facebook picked up

07:56.640 --> 08:01.040
one of the things I'm really proud of is that we balance flexibility with

08:01.040 --> 08:04.560
ease of use that we had about maybe a hundred to two hundred users from

08:04.560 --> 08:08.400
Cheryl Sandberg's org and their finance and business and so we're really proud

08:08.400 --> 08:11.840
about democratizing machine learning within Facebook to that extent

08:11.840 --> 08:16.560
at the same time we had the latest convolutional nets we had deep learning for NLP

08:16.560 --> 08:22.480
so we successfully I think we successfully built a a good multi-layered

08:22.480 --> 08:28.000
platform that scaled for different use cases. So you just mentioned use cases you

08:28.000 --> 08:33.440
started with a teams dimension and an algorithms dimension do those two get you

08:33.440 --> 08:38.480
use cases or is there another abstraction that you thought of where you're

08:38.480 --> 08:43.040
trying to generalize a use case beyond the specific team and an algorithm?

08:43.040 --> 08:50.000
Yes so if we talk about abstraction layer FB learners flows

08:50.000 --> 08:54.480
contribution I would say and we talked about this on on Facebook log is

08:54.480 --> 08:58.480
I think we nailed the right upstraction it's a workflow and a workflow consists

08:58.480 --> 09:02.080
of an operator that takes generic data whatever data types you want

09:02.080 --> 09:07.040
and gives generic outputs. The nice thing is that once you have an operator you

09:07.040 --> 09:10.720
can create any workflow you want but what's even more powerful is the entire

09:10.720 --> 09:15.520
workflow can become an operator and yet another workflow and this composition

09:15.520 --> 09:20.480
allowed us to build multi-layered platform. Let me give you a concrete example so

09:20.480 --> 09:24.800
let's say you build a convolutional net that takes in an image and produces a

09:24.800 --> 09:29.920
model that classifies based on the data and the labels we offer that in itself is

09:29.920 --> 09:34.320
extremely powerful requires someone who's deep in in convolutional nets and deep

09:34.320 --> 09:38.880
learning but at the same time it's still hard for a lot of other machine learning

09:38.880 --> 09:43.760
users even data scientists to use so what you can do is you can wrap that in

09:43.760 --> 09:48.480
another workflow that simplifies it by saying just give us the data and we'll

09:48.480 --> 09:52.160
do hyper parameter tuning on your behalf and then you can wrap that with another

09:52.160 --> 09:56.480
workflow that makes it more useful from a business case so

09:56.480 --> 10:00.800
filtering images and in different scenarios like if you're filtering them to

10:00.800 --> 10:06.320
remove explicit content is different than if you're filtering them to

10:06.320 --> 10:11.120
propose an image for you to share later so these are all different use cases so

10:11.120 --> 10:16.960
essentially that multi-layering allowed us to scale and the most important part is

10:16.960 --> 10:22.000
making everything shareable and reusable so whenever some team builds

10:22.000 --> 10:26.320
something every other team at Facebook has the opportunity of reusing it

10:26.320 --> 10:31.040
and to make it reusable we had to make it discoverable so we created search

10:31.040 --> 10:36.320
we would rank these workflows by how many times they've been used how many times

10:36.320 --> 10:40.320
they've been successful so that the rest of Facebook which started growing

10:40.320 --> 10:44.160
into the thousands would know like oh this is the most popular convolutional

10:44.160 --> 10:49.200
network workflow or this is the most popular text classification workflow

10:49.200 --> 10:53.760
and so on so that was very useful at Facebook and it helped spread adoption

10:53.760 --> 10:59.040
Facebook in a lot of ways has demonstrated a huge commitment to

10:59.040 --> 11:02.160
open source and open is generally if you look at

11:02.160 --> 11:05.680
the kind of the bottom of the stack they do open compute at the top of the

11:05.680 --> 11:09.920
stack they're doing things like PyTorch and Onyx but they haven't

11:09.920 --> 11:13.840
you know open source the platform and I've had conversations on the

11:13.840 --> 11:18.880
podcast with folks on the T man while I didn't ask this my sense was that

11:18.880 --> 11:23.440
a lot of the way things are done or work are very Facebook specific like

11:23.440 --> 11:26.560
yep there was no Kubernetes when we started this so we're doing our own

11:26.560 --> 11:31.600
distributed compute the connections to data is that kind of yeah no that's

11:31.600 --> 11:34.800
absolutely right once you start going into end-to-end machine learning

11:34.800 --> 11:40.400
unlike PyTorch which is heavily contained the integration into whatever

11:40.400 --> 11:44.960
infrastructure system you have is a much bigger component and so when we were

11:44.960 --> 11:48.560
looking at open sourcing of the learner flow we had to open source the rest of

11:48.560 --> 11:53.280
Facebook with it now the beauty of Kubernetes is that it gives you this

11:53.280 --> 11:58.560
abstraction layer and through my time at Google we invested in

11:58.560 --> 12:05.920
Qflow pipelines which is you can think of it as a moving workflow engine that

12:05.920 --> 12:10.480
you can take with you wherever Kubernetes exists yeah yeah speaking of

12:10.480 --> 12:14.720
Google after Facebook you went to Google can you talk a little bit about yeah

12:14.720 --> 12:19.760
absolutely so I was very passionate about you know building machine learning

12:19.760 --> 12:24.080
and helping tools or platforms and helping others adopt machine learning and

12:24.080 --> 12:27.920
frankly there's no better place to do this than being at Google because of

12:27.920 --> 12:31.440
its amazing machine learning brand amazing machine learning research

12:31.440 --> 12:36.400
capabilities and Google Cloud and so that trifecta was extremely

12:36.400 --> 12:42.640
exciting for me and I joined Google and you know I just like my time at

12:42.640 --> 12:47.040
Facebook was extremely amazing but then seeing the platter of machine learning

12:47.040 --> 12:52.480
use cases from healthcare to autonomous vehicles to

12:52.480 --> 12:58.160
finance and so on just was fabulous and amazing like there's a lot of

12:58.160 --> 13:02.480
diversity in machine learning so you joined Facebook at

13:02.480 --> 13:07.760
in 2012 and Google in 2017 they had this amazing machine learning brand

13:07.760 --> 13:10.880
the work was all done they were much further along

13:10.880 --> 13:17.920
Google you mean Google so not necessarily actually Google and Facebook their

13:17.920 --> 13:21.680
investments in machine learning follow their strategies so let me explain

13:21.680 --> 13:27.280
what I mean by that Google has a very broad set of products like documents

13:27.280 --> 13:33.200
doorbells cameras all you name it Facebook is far more focused okay right and

13:33.200 --> 13:36.240
so when you look at machine learning investments Google has a broad

13:36.240 --> 13:41.680
portfolio Facebook is far more focused there are places where Facebook is more

13:41.680 --> 13:45.120
is better or advanced and vice versa the nice thing is that the two are

13:45.120 --> 13:49.040
complimentary so I'll give you an example I think PyTorch demonstrates

13:49.040 --> 13:55.600
Facebook's focus on usability and ease of use but then you look at Facebook

13:55.600 --> 13:59.600
Google's broad portfolio of research it demonstrates

13:59.600 --> 14:04.560
its breadth so I won't say one is better than the other they just have two

14:04.560 --> 14:09.440
different styles that they've inherited from how they invest in technology in

14:09.440 --> 14:12.880
general one of the themes that's

14:12.880 --> 14:21.760
recurred in my interviews here and beyond this conference is the idea that

14:21.760 --> 14:28.480
right now productivity and machine learning is primarily a usability

14:28.480 --> 14:33.360
challenge as opposed to a you know a modeling challenge for example would you

14:33.360 --> 14:37.520
agree with that or I agree to a very large extent to that that that's

14:37.520 --> 14:41.520
absolutely right and that's one of my you know pieces of advice that I give

14:41.520 --> 14:45.360
to people who are trying to build a new platform it's not necessarily about

14:45.360 --> 14:51.360
sophistication because your end customer may not need it yet right your end

14:51.360 --> 14:55.440
customer might have a usability problem and and through Google Cloud we found

14:55.440 --> 14:59.840
that different customer types had different usability issues even the very

14:59.840 --> 15:05.200
sophisticated customers who would come to us and say just give me GPUs even the

15:05.200 --> 15:10.960
usability of giving them a GPU easily and clearly meant a big difference to them

15:10.960 --> 15:16.160
so I think my advice is do not ignore usability

15:16.160 --> 15:21.600
machine learning is obviously a very deep technical area but approach it with a

15:21.600 --> 15:28.480
product sense and that's why at Google and at times that Facebook designers and

15:28.480 --> 15:34.480
user-experient experts were my friends because I would ask them to help me

15:34.480 --> 15:38.560
figure out where the usability bottlenecks that users are suffering from and

15:38.560 --> 15:42.240
then if we solve them you see massive increase in productivity are there

15:42.240 --> 15:46.560
examples of where designers have kind of come in oh yeah

15:46.560 --> 15:51.520
what you thought about a problem absolutely so one example is at Google we had a

15:51.520 --> 15:56.960
fantastic user experience team and we were building a lot of great machine

15:56.960 --> 16:01.840
learning tools for text and image and and so on and then what we noticed is

16:01.840 --> 16:05.280
that our users wanted to combine all these together

16:05.280 --> 16:11.600
right and so our our we deployed our UX team and they would sit

16:11.600 --> 16:15.440
with our users to go through an end-to-end machine learning workflow

16:15.440 --> 16:19.360
and those users are other companies and then when we showed them these end-to-end

16:19.360 --> 16:23.840
workflows you could see how it just thinks click in the eyes of our users are

16:23.840 --> 16:28.240
like oh I understand why it takes so long because I would spend so much time in

16:28.240 --> 16:32.240
this part like preparing data when I thought that the hardest part was using

16:32.240 --> 16:37.840
TensorFlow but I actually need to invest in improving this so that was one of

16:37.840 --> 16:40.960
the examples and it just happened again and again and became our most

16:40.960 --> 16:45.200
favorite topic with customers that customers would come to us and say

16:45.200 --> 16:49.040
where's your UX team so it was really interesting

16:49.040 --> 16:54.480
awesome so you've been at cruise for about six months now yes

16:54.480 --> 16:59.360
what's your why cruise and what's your role yeah absolutely I

16:59.360 --> 17:04.320
so I started in machine learning in 2012 when things were just ramping up and

17:04.320 --> 17:06.960
not a lot of people thought that machine learning is the

17:06.960 --> 17:13.600
sexiest thing I actually see another trend that is about to happen and that's

17:13.600 --> 17:18.640
the domain of autonomous vehicles I think that's going to push AI to a whole

17:18.640 --> 17:22.320
new level in terms of the depth of the problem

17:22.320 --> 17:28.160
and in terms of scale so let me give you concrete examples

17:28.160 --> 17:31.600
most of the machine learning applications that are dominant in the industry

17:31.600 --> 17:36.480
today are either advertisement or recommendations for movies or

17:36.480 --> 17:41.200
products and these are very decent and extremely impactful

17:41.200 --> 17:45.440
applications but if you compare them to what a car needs to do

17:45.440 --> 17:51.600
navigate the roads of San Francisco safely the decision making is

17:51.600 --> 17:55.520
maybe an order of magnitude less so the car has to

17:55.520 --> 17:59.440
track so many objects including pedestrians

17:59.440 --> 18:03.440
funny pedestrians who might be wearing a palm tree costume the car needs to have

18:03.440 --> 18:07.600
common sense that happens here in San Francisco

18:07.600 --> 18:13.600
I've seen a gentleman who had a bike with a four trolley system behind them

18:13.600 --> 18:17.200
so you see a lot of these interesting things and then the car needs to

18:17.200 --> 18:20.880
predict what these different agents are going to do

18:20.880 --> 18:24.960
and so the decision-making is very complex and you need to do that in a hundred

18:24.960 --> 18:30.640
milliseconds so the problem is far deeper and that just pushes

18:30.640 --> 18:34.080
AI to new limits the other thing that is really important is

18:34.080 --> 18:39.200
the accuracy required is very high in order to drive a car safely

18:39.200 --> 18:42.720
no one gets hurt if you show them a bad ad or a bad recommendation

18:42.720 --> 18:46.080
right and what's interesting is that you can learn from that but

18:46.080 --> 18:51.680
in autonomous vehicles we have to push our accuracy limits and then finally

18:51.680 --> 18:56.080
every car moving in the street has multiple sensors

18:56.080 --> 19:00.240
so they have multiple cameras multiple lidars that are generating gigabytes of

19:00.240 --> 19:04.800
data a second and when you have multiple cars of these roaming around

19:04.800 --> 19:08.800
the data scale is actually gigonomous

19:08.800 --> 19:13.520
so scale is an interesting point we had a really interesting exchange

19:13.520 --> 19:17.680
early on when we were talking about the you participating in the conference

19:17.680 --> 19:22.240
and I kind of approach you with hey you are at these huge scale places

19:22.240 --> 19:25.440
Google and Facebook that you know nothing's bigger than that right and you're

19:25.440 --> 19:29.840
like hold on autonomous vehicles is way bigger scale

19:29.840 --> 19:34.160
elaborate on that you know how do you think about the two relative to one

19:34.160 --> 19:40.000
oh absolutely here's a way to analyze scale if you look at your training data

19:40.000 --> 19:44.400
right don't just look at the number of rows but look at the size of a record

19:44.400 --> 19:51.200
one record most of the time when you leverage when you leverage recommendations

19:51.200 --> 19:54.640
it's tabular data the record size is not huge

19:54.640 --> 19:57.920
but if you look at the record size for an autonomous vehicle

19:57.920 --> 20:00.960
it could be thousands of lidar points

20:00.960 --> 20:05.040
multiple you know tens of thousands of pixels

20:05.040 --> 20:09.120
even audio matters because the car needs to listen if an

20:09.120 --> 20:13.280
emergency vehicle is coming around we actually need to figure out if

20:13.280 --> 20:18.000
if a pedestrian is waving to us or looking at the car or looking at the phone

20:18.000 --> 20:22.880
so the record is just far bigger so that's one example of scale

20:22.880 --> 20:27.680
there's another thing that is interesting machine learning is just part of

20:27.680 --> 20:33.200
the application it's it's it's among so in web development or web-based

20:33.200 --> 20:37.360
applications like recommendations the tool chain is well developed

20:37.360 --> 20:41.040
in autonomous vehicles we're still innovating that we're still building it

20:41.040 --> 20:45.600
um and we're still trying to scale to the amount of data

20:45.600 --> 20:50.640
and um that's one other example of how scale matters in autonomous vehicles

20:50.640 --> 20:55.600
because the tool chain doesn't yet exist um we're in we're inventing that

20:55.600 --> 20:58.720
as we're inventing the cars themselves

20:58.720 --> 21:03.760
are scale and the need to build out the tools or those that the only challenges

21:03.760 --> 21:07.600
that you face or are the problems themselves in early

21:07.600 --> 21:11.920
challenging all of them but they are in what ways yeah i mean the

21:11.920 --> 21:16.720
you get into a cycle of like i can solve this problem if i could just scale

21:16.720 --> 21:21.120
then after you scale okay i solve this problem now i i discover a new problem

21:21.120 --> 21:25.520
and i need to scale further so one of the things that attracted me to

21:25.520 --> 21:29.680
the company is this understanding the interplay between the applied research

21:29.680 --> 21:35.360
problem and the underlying tooling and that's why um our co-founder Kyle says

21:35.360 --> 21:39.280
he's building two things he's building the car and he's building the tools

21:39.280 --> 21:43.040
that are going to build the car because no one has no one has built that before

21:43.040 --> 21:47.680
if you compare this to the web world the tools the tool chain is far more mature

21:47.680 --> 21:50.960
we've been building web applications and mobile applications for

21:50.960 --> 21:54.720
for a couple of decades now and so the tool chain is far more mature but

21:54.720 --> 21:59.120
you look at autonomous vehicles that's just not there it cruises your approach is

21:59.120 --> 22:03.840
multimodal in terms of the types of information that you're taking in off the

22:03.840 --> 22:06.640
vehicle you've mentioned LiDAR you've mentioned imagery you've mentioned

22:06.640 --> 22:13.840
sound even uh within all of those are there specific categories of problems

22:13.840 --> 22:21.600
that are uh key or oh yeah yeah i can list so many i mean the basic is just

22:21.600 --> 22:27.040
detecting um objects in the scene understanding that this is a car

22:27.040 --> 22:31.280
and that the scar is heading this way um understanding whether the scar is moving

22:31.280 --> 22:36.880
or not um you have the same problem for pedestrians bicycles and now scooters

22:36.880 --> 22:41.520
and all of these agents have different behaviors and it's not just

22:41.520 --> 22:45.920
enough to say this is where they at now you also need to predict what are they

22:45.920 --> 22:51.040
going to do in the future and so behavioral prediction is also another problem

22:51.040 --> 22:55.040
one of the other interesting areas to apply machine learning which i don't think

22:55.040 --> 23:01.280
people know uh a lot about uh autonomous vehicles the style of driving how fast you

23:01.280 --> 23:05.600
break and how fast you accelerate uh may not

23:05.600 --> 23:09.840
necessarily affect safety but affects your experience as a writer

23:09.840 --> 23:14.400
not so because if you break hard um the writers may get nauseous

23:14.400 --> 23:20.320
right and so the driving style so my wife tells me

23:20.320 --> 23:23.920
exactly and so the driving style itself is a machine learning problem i mean i

23:23.920 --> 23:28.480
think it's it's a Pandora box like frankly um i went there because the the

23:28.480 --> 23:32.240
number of machine learning problems is just endless and i think it will

23:32.240 --> 23:35.600
it will it will be an exciting place for anyone excited about

23:35.600 --> 23:39.040
machine learning uh problems or machine learning tools

23:39.040 --> 23:44.720
and one of the ways you were introduced to cruises that they were doing some

23:44.720 --> 23:48.800
of their work on the google platform can you talk a little bit about

23:48.800 --> 23:54.560
yeah how you're platforming uh the you know machine learning i cruise today

23:54.560 --> 23:59.840
so google cloud we offer multiple sort of suites of products to our AI

23:59.840 --> 24:04.160
customers cruises obviously an AI company so they were mostly

24:04.160 --> 24:09.200
focused on the lower level infrastructure like GPUs and some tooling around it

24:09.200 --> 24:13.280
um i remember my first meeting with cruise and i was like okay well you know i'm

24:13.280 --> 24:16.320
getting introduced to these people they seem really enthusiastic

24:16.320 --> 24:19.680
and then i look at our numbers and i start seeing the GPU usage

24:19.680 --> 24:23.920
you know uh rocketing up to the extent that we have to go and

24:23.920 --> 24:28.640
you know um scrambled to get them more GPUs and i was like something is happening there

24:28.640 --> 24:35.440
um and for me you know you know a company's uh focus on machine learning

24:35.440 --> 24:40.080
by how much they spend on the infrastructure and how much they use it and so

24:40.080 --> 24:42.720
it's very clear to me that something big is happening there

24:42.720 --> 24:46.480
and then i visited them a few months later and the progress rate

24:46.480 --> 24:53.840
was phenomenal um so i joined uh there um a few months ago and it was

24:53.840 --> 24:58.080
eight months into building their own machine learning uh dedicated platform

24:58.080 --> 25:02.480
and it just advanced uh really quickly and just in the five months that i've

25:02.480 --> 25:06.320
been there it even advanced further um so i'm really glad

25:06.320 --> 25:09.360
them there because the problem is deep the

25:09.360 --> 25:13.760
pace is really fast uh and that's something i enjoy and i think it's very

25:13.760 --> 25:19.680
important for machine learning um and i get to again redefine what

25:19.680 --> 25:22.560
the future of machine learning i believe is going to look like because i

25:22.560 --> 25:26.640
believe in uh just like ads and and and recommendations

25:26.640 --> 25:29.520
propelled the application of machine learning in the industry the last three

25:29.520 --> 25:33.840
to five years i think in the next five years it's going to be robotics

25:33.840 --> 25:37.120
and autonomous vehicles and it's going to push machine learning to levels we

25:37.120 --> 25:40.880
haven't seen and i'm really excited about that period i think machine learning

25:40.880 --> 25:46.800
and AI will fulfill their promise uh through the vehicle of robotics

25:46.800 --> 25:51.520
uh so you you you're bringing a whole an incredible wealth of

25:51.520 --> 25:56.160
platform building experiences to a new problem one that has unique

25:56.160 --> 26:00.800
challenges how do you customize you know what you've

26:00.800 --> 26:04.240
done before what you've learned or what you believe about you know

26:04.240 --> 26:08.800
delivering platforms to that specific problem you know both

26:08.800 --> 26:13.920
generally philosophically but also you know concretely what does the cruise

26:13.920 --> 26:18.640
platform look like sure so i think for cruise particularly and this is

26:18.640 --> 26:22.400
something i did super well at facebook it's the interplay between applied

26:22.400 --> 26:27.360
research and platforms how can you scale applied research

26:27.360 --> 26:30.720
by shipping it through a platform because applied research and machine

26:30.720 --> 26:35.040
learning is costly and you don't want every team every sub team to do their

26:35.040 --> 26:38.560
own applied research what you'd rather do is if one team innovates you want

26:38.560 --> 26:42.720
everyone else to use it and so that interplay is important is that is is applied

26:42.720 --> 26:47.360
research the print your primary user at cruise as opposed to data

26:47.360 --> 26:50.800
scientists machine learning engineers that are both other function all of

26:50.800 --> 26:54.800
these are all in in by org so the applied uh research

26:54.800 --> 26:59.280
part of the autonomous vehicle the data scientists and the machine learning

26:59.280 --> 27:02.240
platform are all one community that's essentially cruise the eye and that's

27:02.240 --> 27:06.080
one of the things that i think is unique about cruise often if you see

27:06.080 --> 27:09.200
companies who split the two in the early days at

27:09.200 --> 27:14.800
facebook these were combined google also the machine

27:14.800 --> 27:19.760
intelligence group is also a combined group and i think that approach is

27:19.760 --> 27:24.640
really important because the platform evolves through the demands of the

27:24.640 --> 27:28.160
research scientists and and the the data scientists

27:28.160 --> 27:32.080
but also the platform has an ability to influence them and

27:32.080 --> 27:36.160
improve their research so that sort of yang and yang is very important

27:36.160 --> 27:40.880
in terms of um general advice about a platform i would say

27:40.880 --> 27:44.000
first thing is what i mentioned bring your scientists and your platform

27:44.000 --> 27:48.400
builders together uh the second thing is uh be extremely

27:48.400 --> 27:54.000
customer driven uh sometimes it's enticing to chase a very sophisticated

27:54.000 --> 27:57.040
scenario but that may not be what you needed the moment

27:57.040 --> 28:01.360
so be customer driven the the third thing is be careful bringing

28:01.360 --> 28:07.440
biases of other experiences so i often seen that big data techniques

28:07.440 --> 28:11.920
failed for machine learning um so as an example if you use map

28:11.920 --> 28:16.240
produce um it's just it doesn't scale as well as one

28:16.240 --> 28:20.320
gpu machine and i had this example at facebook where there's a sophisticated

28:20.320 --> 28:24.320
graph processing system it ran on 500 machines

28:24.320 --> 28:29.760
and it ran slower than one single machine um and ran even

28:29.760 --> 28:36.000
much much slower compared to one gpu so um bringing some of these biases

28:36.000 --> 28:40.560
is dangerous and then finally i believe a proper platform is an algorithm

28:40.560 --> 28:44.800
agnostic platform um that's the basic minimum level of

28:44.800 --> 28:48.560
extensibility uh that a platform has if you build it around an algorithm

28:48.560 --> 28:52.640
machine learning changes so much what is hot today is

28:52.640 --> 28:57.280
tomorrow's um is is tomorrow's deprecated algorithm

28:57.280 --> 29:00.560
and so if your entire platform is basically based on a bunch of them

29:00.560 --> 29:04.080
then you're going to be wasting a lot of work and effort

29:04.080 --> 29:09.600
a lot of teams struggle with uh this generalizability

29:09.600 --> 29:13.840
aspect that you're mentioning you know do we support a specific

29:13.840 --> 29:17.600
framework do we support a bunch of frameworks we support specific algorithms

29:17.600 --> 29:22.080
versus a bunch of algorithms do you think there's a certain scale that

29:22.080 --> 29:26.720
you have to be at before you can afford to do to be general

29:26.720 --> 29:29.840
or is that something that you need to commit to

29:29.840 --> 29:34.720
independent of the size that you're at well it depends on the company i

29:34.720 --> 29:40.000
think we made an early decision in 2014 at facebook

29:40.000 --> 29:45.360
that we are going to generalize because it was very clear that the company

29:45.360 --> 29:48.800
is going to require different algorithms different use cases and we don't

29:48.800 --> 29:52.480
want to reinvent the wheel um but maybe a different

29:52.480 --> 29:56.320
different companies have different uh circumstances so just work it

29:56.320 --> 30:00.000
backwards from where you think the customer will be not today

30:00.000 --> 30:03.680
but in three years um i believe companies like facebook,

30:03.680 --> 30:07.600
cruise, google and so on will benefit from a general platform

30:07.600 --> 30:13.040
um smaller companies may benefit from more specialized platforms like if you're

30:13.040 --> 30:16.000
a fintech company and you know you're not going to be dealing with

30:16.000 --> 30:20.320
images at all and there's no value for you then focus on tabular data

30:20.320 --> 30:24.240
and build your platform that way um but if you're a company that has mixed

30:24.240 --> 30:27.840
sources of data very likely you need to generalize

30:27.840 --> 30:32.160
um curious about how you how you approach the organizational side of things

30:32.160 --> 30:36.320
you mentioned that you've got uh all of the kind of key

30:36.320 --> 30:41.360
customer groups uh in the same org as the platform and that's key are there

30:41.360 --> 30:47.040
other things that you've uh learned or believe organizationally that

30:47.040 --> 30:50.720
require then are required to make this successful. Yeah you know over my time at

30:50.720 --> 30:55.200
facebook and google and in cruise i've ran organizations for AI

30:55.200 --> 30:59.120
that consisted of different uh functions

30:59.120 --> 31:04.480
not just machine learning experts um and my advice is treat all of them

31:04.480 --> 31:08.960
equally you need them all to deliver a good machine learning

31:08.960 --> 31:13.040
product and so as an example at facebook one of the very early decisions

31:13.040 --> 31:17.280
we realized that machine learning has a usability problem so we need to hire

31:17.280 --> 31:22.560
user uh interface ui people and we elevated them to the same level as our

31:22.560 --> 31:25.760
researchers because they didn't want to join a team where they felt they're

31:25.760 --> 31:30.800
going to be second class so um a big believer in this cross functional uh

31:30.800 --> 31:34.640
mixture of talent and actually what's interesting is they

31:34.640 --> 31:39.760
each function brings its diverse uh perspective and that aggregation of

31:39.760 --> 31:42.480
perspectives creates really fantastic machine learning

31:42.480 --> 31:46.560
product. So where do you see platforms going

31:46.560 --> 31:51.600
overall or machine learning in kind of the enterprise context?

31:51.600 --> 31:56.240
Well i'm going to be biased because i've made my bet through google cloud i

31:56.240 --> 32:01.440
think um there's going to be a lot in kubernetes um and that's the reason why

32:01.440 --> 32:08.320
we invested in q-flow i think um uh notebooks um interacting well with machine

32:08.320 --> 32:12.480
learning workflows that's why q-flow has a notebook solution and has q-flow

32:12.480 --> 32:17.280
pipelines and then i'm very excited about the ability to share

32:17.280 --> 32:20.880
so if someone builds a interesting machine learning component or an interesting

32:20.880 --> 32:24.320
machine learning workflow how can they share this with the rest of their

32:24.320 --> 32:29.280
company or the rest of the community? So i think these um pillars are

32:29.280 --> 32:31.840
necessary for successful machine learning platforms

32:31.840 --> 32:37.040
and i believe cloud providers are going to play a very big role

32:37.040 --> 32:40.080
in that aspect but i think it's going to be on kubernetes.

32:40.080 --> 32:43.760
Nice i should mention that i have not seen them yet but uh

32:43.760 --> 32:48.480
david aronchick one of the founding team members of q-flow is here somewhere

32:48.480 --> 32:52.400
and uh there's quite a bit of interest in a

32:52.400 --> 32:56.320
q-flow session in the unconference if that's of interest uh definitely check

32:56.320 --> 33:00.960
that out uh who's saying thanks so much for joining us here at twomelcon AI

33:00.960 --> 33:09.520
platforms. Thank you very much. Appreciate it.

33:09.520 --> 33:13.360
All right everyone i hope you enjoyed our show straight from the main

33:13.360 --> 33:18.800
stage at twomelcon AI platforms. For more information about today's show

33:18.800 --> 33:23.440
visit twomelai.com and for more twomelcon coverage

33:23.440 --> 33:28.640
visit twomelcon.com slash news. Thanks so much for listening

33:28.640 --> 33:55.360
and catch you next time.

