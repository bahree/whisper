WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:16.240
I'm your host Sam Charrington.

00:16.240 --> 00:24.440
Hey, what's up everyone?

00:24.440 --> 00:28.920
Before we get to today's show, I want to send a huge shout out to our friends at Waitin

00:28.920 --> 00:30.880
Biasis.

00:30.880 --> 00:35.920
Last week, we premiered my conversation with WNB Founder Lucas B.Wald on our YouTube

00:35.920 --> 00:36.920
channel.

00:36.920 --> 00:41.680
It was a great conversation about managing artifacts in the machine learning life cycle,

00:41.680 --> 00:46.680
and you can find it at www.tumelai.com slash artifacts.

00:46.680 --> 00:51.080
Waitin Biasis is a lightweight toolkit for machine learning practitioners.

00:51.080 --> 00:55.680
Reviews their original experiment tracking tool for quite a while in the Tumel community

00:55.680 --> 01:00.600
as part of various study groups, and folks have found it to be super useful.

01:00.600 --> 01:04.440
Now that they've added data set versioning and model management capabilities, you've

01:04.440 --> 01:10.240
got a one-stop shop for managing and visualizing your complete machine learning pipeline.

01:10.240 --> 01:15.400
If you'd like to learn more, Waitin Biasis is extending a special offer to Tumel listeners

01:15.400 --> 01:20.920
and viewers, including unlimited private projects and priority support.

01:20.920 --> 01:28.440
For more details or to start tracking your models today, visit WNB.com slash Tumel.

01:28.440 --> 01:31.120
And now on to the show.

01:31.120 --> 01:32.120
Enjoy.

01:32.120 --> 01:34.280
All right, everyone.

01:34.280 --> 01:36.240
I am here with Sherry Rose.

01:36.240 --> 01:40.080
Sherry is an associate professor at Harvard Medical School.

01:40.080 --> 01:42.440
Sherry, welcome to the Tumel AI podcast.

01:42.440 --> 01:43.720
Thank you for having me.

01:43.720 --> 01:46.160
It is great to have a chance to chat with you.

01:46.160 --> 01:53.320
I'm looking forward to digging into your background and your research and the things you're

01:53.320 --> 01:57.280
doing related to COVID to help out there.

01:57.280 --> 01:58.880
Let's start at the beginning.

01:58.880 --> 02:05.800
How did you become interested in machine learning and the intersection of that and healthcare?

02:05.800 --> 02:11.240
I always was very interested in science and mathematics and physics, and I didn't really

02:11.240 --> 02:18.360
have a good sense of how you could use that to solve problems when I was going to college.

02:18.360 --> 02:24.880
And it was during college that I was exposed to this summer program called the Summer Institute

02:24.880 --> 02:26.720
for Training and Biostatistics.

02:26.720 --> 02:31.920
And it really sounded like what I was interested in, which was bringing quantitative reasoning

02:31.920 --> 02:35.840
and thinking to problems in health and public health.

02:35.840 --> 02:42.880
And I realized very quickly that I needed more than my bachelor's degree in statistics

02:42.880 --> 02:46.280
in order to really solve a lot of those problems.

02:46.280 --> 02:50.360
And I didn't actually get any training in machine learning in my bachelor's degree.

02:50.360 --> 02:55.160
I graduated in 2005, and the curriculum definitely did not include it at that point.

02:55.160 --> 03:00.560
And so when I went to graduate school at UC Berkeley in biostatistics, that's where I saw

03:00.560 --> 03:07.000
the benefit of having really general frameworks in which to solve problems.

03:07.000 --> 03:11.840
And that's when I started working on non-parametric machine learning and having these kind of

03:11.840 --> 03:17.320
big picture ways to attack big problems in population health.

03:17.320 --> 03:23.040
And that was for me, that's been both machine learning in non-parametric models for prediction,

03:23.040 --> 03:25.600
but also causal inference.

03:25.600 --> 03:31.960
And the driver for me was really the ability to use these flexible tools to solve hard

03:31.960 --> 03:35.520
problems in health care and medicine.

03:35.520 --> 03:39.520
It must have been helpful having that undergrad in stats.

03:39.520 --> 03:40.520
It's been very helpful.

03:40.520 --> 03:45.240
I actually started as a mechanical and aerospace engineering major.

03:45.240 --> 03:51.600
And I did not feel very invigorated by the coursework there.

03:51.600 --> 04:00.480
And I also was a little frustrated that I was often the only woman in the classes.

04:00.480 --> 04:04.160
And there was a lot of reasons why I didn't feel like the right fit for me.

04:04.160 --> 04:09.880
I ended up taking my second semester in college statistics course, and I immediately saw

04:09.880 --> 04:14.080
how statistics could be used for solving lots of different problems.

04:14.080 --> 04:15.880
And engineering can as well.

04:15.880 --> 04:20.920
But for me, the statistics was really how I saw bringing all of my interests together.

04:20.920 --> 04:24.040
You mentioned non-parametric machine learning.

04:24.040 --> 04:29.000
What is that and how does that relate to both the broader field as well as the health

04:29.000 --> 04:30.000
care field?

04:30.000 --> 04:31.000
Yeah.

04:31.000 --> 04:35.640
So when I talk about non-parametrics, I mean it in the very broad statistical sense.

04:35.640 --> 04:40.880
A non-parametric model is a larger model space where we're making many fewer assumptions.

04:40.880 --> 04:46.600
And whereas with parametric models, more standard parametric models, we might be making really

04:46.600 --> 04:50.840
strict assumptions about the functional form, the underlying unknown functional form of the

04:50.840 --> 04:51.840
data.

04:51.840 --> 04:57.560
With non-parametrics, I want to really have a large model space, so I have a much better

04:57.560 --> 05:02.840
opportunity to uncover the truth with my machine learning estimator.

05:02.840 --> 05:07.080
So meaning like you're not assuming a normal distribution which has a couple of parameters

05:07.080 --> 05:10.080
and meaning and a standard deviation, it can be anything.

05:10.080 --> 05:11.080
Definitely not.

05:11.080 --> 05:12.080
Definitely not.

05:12.080 --> 05:14.760
That would be a limiting assumption in your work?

05:14.760 --> 05:16.760
Absolutely.

05:16.760 --> 05:22.080
And most of the data that I work with does not conform to those types of strict assumptions.

05:22.080 --> 05:28.200
Talk a little bit more about the scope of your research interests and where you apply

05:28.200 --> 05:29.200
machine learning.

05:29.200 --> 05:35.280
It sounds like you are interested both in the kind of the systematic issues, the health

05:35.280 --> 05:42.360
care system with the relationships between the providers and the payers, as well as clinical

05:42.360 --> 05:43.360
issues.

05:43.360 --> 05:44.360
Absolutely.

05:44.360 --> 05:49.320
Our services research were really interested in the whole broad scope of the health care

05:49.320 --> 05:54.360
system that includes costs, quality, access to providers and services, and also a health

05:54.360 --> 05:56.000
outcomes following care.

05:56.000 --> 06:00.960
So that clinical piece often comes into the health outcomes following care.

06:00.960 --> 06:06.480
And some of the major areas that I've worked in intersect with the health spending aspects,

06:06.480 --> 06:13.280
the financing aspects like mental health and telemedicine and cardiovascular treatments,

06:13.280 --> 06:20.640
all of these things intersect within this system that relies on the cost, the quality and

06:20.640 --> 06:21.840
the access to providers.

06:21.840 --> 06:27.920
So it's a really having a research program that encompasses both pieces of that can allow

06:27.920 --> 06:32.040
you to ask and answer questions in more integrated ways.

06:32.040 --> 06:38.840
It's difficult, but I find that if you understand those underlying systems and try and bring

06:38.840 --> 06:43.240
them into your work when you're looking at clinical work, it can really help you.

06:43.240 --> 06:45.920
You inform better answers.

06:45.920 --> 06:55.520
And when you are looking at those kinds of questions, are you primarily trying to understand

06:55.520 --> 06:57.000
or influence?

06:57.000 --> 06:58.000
Great question.

06:58.000 --> 07:02.360
So a lot of the work that I do, we are trying to understand some kind of phenomena in the

07:02.360 --> 07:07.760
system, but influence, yes, in a sense, that we're trying to inform policy.

07:07.760 --> 07:13.760
So understanding the comparative effectiveness of multiple different types of treatments,

07:13.760 --> 07:18.760
I would like to understand which treatments have better health outcomes.

07:18.760 --> 07:25.480
But if we find a particular treatment has very bad outcomes, we want to inform policy

07:25.480 --> 07:31.800
to the FDA or to the relevant stakeholder in order to potentially have that treatment removed

07:31.800 --> 07:33.080
from market.

07:33.080 --> 07:40.080
And we're talking towards the end of April, many of us have been in some form of another

07:40.080 --> 07:43.160
of lockdown due to COVID.

07:43.160 --> 07:47.640
You mentioned that your dog may start barking in time.

07:47.640 --> 07:49.640
He may, he may.

07:49.640 --> 07:54.600
My neighbor just, I think my neighbor is finished cutting the grass now.

07:54.600 --> 08:01.000
This is just the times, but it sounds like your work intersects with COVID as well.

08:01.000 --> 08:03.360
Can you talk about that intersection a little bit?

08:03.360 --> 08:04.360
Absolutely.

08:04.360 --> 08:11.080
A large focus of my work, because I'm so integrated in starting with the substantive problem

08:11.080 --> 08:15.320
and bringing either existing machine learning tools or developing new machine learning tools

08:15.320 --> 08:20.720
to answer those questions, it really, there has to be the strong grounding in data.

08:20.720 --> 08:27.040
And the coronavirus pandemic has really illuminated for a lot of people how much we need to care

08:27.040 --> 08:28.440
about data.

08:28.440 --> 08:36.000
And I, I mean, we have misclassification, we have missingness in the types of data that

08:36.000 --> 08:41.040
we're collecting for coronavirus, both for cases and mortality counts.

08:41.040 --> 08:46.160
And these are things that are very, very common in most of the electronic health data that

08:46.160 --> 08:50.560
we use in the healthcare system, where a lot of my work has focused on dealing with some

08:50.560 --> 08:51.760
of these types of issues.

08:51.760 --> 08:58.760
I mean, we use billing claims, we use clinical records, registry data, and on and on.

08:58.760 --> 09:01.160
And these data types were not designed for research.

09:01.160 --> 09:07.000
And so we need to be really aware of the issues in these types of data.

09:07.000 --> 09:11.840
And some of the newer forms of data, like wearable and implantable technology that people have

09:11.840 --> 09:14.160
been very excited about, measuring physical activity.

09:14.160 --> 09:19.280
We're now using in the coronavirus pandemic, you know, smartphone location data to try

09:19.280 --> 09:26.080
and understand how people are social distancing or with potentially with contact tracing.

09:26.080 --> 09:31.720
And then digital types of data like Google search trends and Twitter data, which has been

09:31.720 --> 09:34.280
used for different types of research questions in the past.

09:34.280 --> 09:39.520
Now Google is developing and has released this location history website where they're

09:39.520 --> 09:42.800
showing about, you know, how we can understand social distancing.

09:42.800 --> 09:49.160
And so a lot of the data related work that I've been focused on is very relevant to the

09:49.160 --> 09:53.760
pandemic and understanding our data sources and trying to bring rigorous flexible methods

09:53.760 --> 09:54.760
to them.

09:54.760 --> 10:00.320
Specifically, I had been working the last two years with my now former postdoctoral fellow

10:00.320 --> 10:02.600
who's an infectious disease expert.

10:02.600 --> 10:07.880
My am a jummer who's now faculty at Boston Children's Hospital and Harvard Medical School.

10:07.880 --> 10:13.640
We had been looking at news media data, CDC data and electronic health data to understand

10:13.640 --> 10:19.080
the generalizability of these data sources for both infectious disease and chronic disease.

10:19.080 --> 10:22.560
And now this has become very, very relevant to the coronavirus pandemic.

10:22.560 --> 10:27.320
We had been one of the conditions we've been studying was, was flu-like illnesses and

10:27.320 --> 10:33.520
understanding, you know, what electronic health data sources like billing claims and electronic

10:33.520 --> 10:37.560
health records, what we can really understand from these data sources.

10:37.560 --> 10:44.560
And we've seen people, many people now start modeling and making projections about cases

10:44.560 --> 10:47.320
and death counts.

10:47.320 --> 10:52.480
What we're going to start seeing next, once people start having access to different types

10:52.480 --> 10:59.080
of electronic health resources is trying to use this data to understand, you know, to

10:59.080 --> 11:04.320
predict outcomes, maybe to predict clinical courses or to try and do causal imprage,

11:04.320 --> 11:06.840
which is even more difficult.

11:06.840 --> 11:11.760
And it's very, very important that people understand the limitations of these data sources.

11:11.760 --> 11:17.840
And so that's one of the things that we're working on and hopefully the first paper from

11:17.840 --> 11:21.360
that work will be able to release in the next coming weeks.

11:21.360 --> 11:27.200
But this is something that's relevant for the coronavirus pandemic, but has been, you

11:27.200 --> 11:32.360
know, a problem going back, you know, decades is using data that people don't understand.

11:32.360 --> 11:38.040
And that's been at the forefront of my work is really making sure, especially, you know,

11:38.040 --> 11:42.040
with the theme of one of the themes of this podcast machine learning, a lot of people

11:42.040 --> 11:45.880
get very excited about machine learning and they throw a tool at data without understanding

11:45.880 --> 11:46.880
the data.

11:46.880 --> 11:50.720
And we're now in the midst of something where it's really crucial that people do not do

11:50.720 --> 11:51.720
that.

11:51.720 --> 11:52.720
Yeah.

11:52.720 --> 12:00.640
We just had a panel discussion, a tum live discussion earlier this week on responsible

12:00.640 --> 12:08.240
data science in the fight against COVID-19 and talked quite extensively about this issue.

12:08.240 --> 12:14.080
And, you know, a lot of the panel initially grew out of the reactions I was seeing to,

12:14.080 --> 12:19.120
you know, folks jumping in, wanting to, you know, help out, produce dashboards and models.

12:19.120 --> 12:22.600
And then you'd have this kind of counter reaction of folks saying, hey, you know, you're not

12:22.600 --> 12:28.000
an epidemiologist, stay in your lane kind of thing, which I kind of object to, to a large

12:28.000 --> 12:32.680
degree because, you know, a, people want to help and be, you know, people, you know, want

12:32.680 --> 12:37.120
to learn and, and, you know, everyone's bringing something.

12:37.120 --> 12:40.560
But at the same time, you know, the stakes are high.

12:40.560 --> 12:47.280
And even if you're an expert, it's easy to get things wrong because the data is, you

12:47.280 --> 12:51.360
know, as you mentioned, everyone's reporting different things.

12:51.360 --> 12:52.360
The data is messy.

12:52.360 --> 12:58.320
You know, talk a little bit more about the kinds of things you're, you're seeing in the data.

12:58.320 --> 13:03.800
And you mentioned you have a paper coming out is the, the objective of this paper to try

13:03.800 --> 13:11.560
to kind of quantitatively, qualitatively provide measures for data quality as applied to

13:11.560 --> 13:15.760
some of these use cases or what exactly are you trying to do with this paper?

13:15.760 --> 13:20.320
Yeah, this, this first paper is one of the main things that, that people will see.

13:20.320 --> 13:25.240
And this is true of most health conditions, but particularly among infectious diseases

13:25.240 --> 13:32.200
if that is that in billing claims in electronic health records, you will see under counts

13:32.200 --> 13:34.320
of infectious disease conditions.

13:34.320 --> 13:39.200
And so using this data and not understanding all of the different reasons why we might

13:39.200 --> 13:44.760
under count a particular health condition is, it would be very problematic.

13:44.760 --> 13:51.320
So one of the things that we will do in this work is try to quantify and we've got multiple

13:51.320 --> 13:56.880
years of data and so we can show trends over time, quantifying this under counting in electronic

13:56.880 --> 14:01.560
health data for influenza-like illnesses.

14:01.560 --> 14:04.560
And but this is even true of chronic diseases.

14:04.560 --> 14:10.880
We see with chronic diseases that one of the, in order to be counted in an electronic

14:10.880 --> 14:15.360
health database, you have to have an encounter with the health care system.

14:15.360 --> 14:18.720
And we know that there's many reasons why people may not have an encounter with the health

14:18.720 --> 14:19.720
care system.

14:19.720 --> 14:24.240
We know that people in rural communities whose hospitals have closed may not have an encounter.

14:24.240 --> 14:29.760
We know that people who do not have insurance or are low income may have additional barriers

14:29.760 --> 14:32.520
to getting to, to having care.

14:32.520 --> 14:39.800
Is the issue that the records that we do have under count because folks, there are folks

14:39.800 --> 14:44.360
out there that, you know, contract COVID and don't interface with the health care system

14:44.360 --> 14:49.840
or is it that even of those folks that are interfacing with the health care system,

14:49.840 --> 14:53.400
they're systematic under counting for some reasons.

14:53.400 --> 14:54.400
It's both.

14:54.400 --> 14:55.400
It's both.

14:55.400 --> 14:58.240
So not ever, so not, we won't see people who don't have an encounter with the health

14:58.240 --> 14:59.240
care system.

14:59.240 --> 15:02.800
And even people who do have an encounter with the health care system, they may not be coded.

15:02.800 --> 15:08.160
So there's now an ICD-10 code, which is a billing code for health conditions.

15:08.160 --> 15:12.440
There will be people who have coronavirus who will not be coded as having coronavirus,

15:12.440 --> 15:14.200
even though they have an encounter with the health care system.

15:14.200 --> 15:18.440
And there will be many reasons for this, including the fact that we don't have enough testing.

15:18.440 --> 15:21.880
But there's lots of reasons why somebody might not have a code.

15:21.880 --> 15:23.520
They might get coded for something else.

15:23.520 --> 15:26.120
They might get coded for flu instead of coronavirus.

15:26.120 --> 15:28.760
They might get coded for just a higher level.

15:28.760 --> 15:32.120
And then we're going to have people who are coded for coronavirus who don't actually have

15:32.120 --> 15:33.120
it.

15:33.120 --> 15:36.360
So there's going to be in this classification in both directions.

15:36.360 --> 15:39.160
So you're supposed to wait for a positive test.

15:39.160 --> 15:44.560
But again, because they're so little testing, a physician might be inclined to code somebody

15:44.560 --> 15:48.680
for coronavirus if they're a suspected case without a confirmed test, or because they're

15:48.680 --> 15:52.560
such a delay in getting the test results back.

15:52.560 --> 15:58.280
And so when you're trying to characterize this type of undercounting, does machine learning

15:58.280 --> 15:59.680
come into play there?

15:59.680 --> 16:02.840
And if so, what are the tools that you're using?

16:02.840 --> 16:09.640
So right now, this is not using, this is not a prediction question, it calls a inference

16:09.640 --> 16:10.640
question yet.

16:10.640 --> 16:12.840
This is a data quality question.

16:12.840 --> 16:14.640
This is a data quality question.

16:14.640 --> 16:18.200
And so there's data science techniques that go into this, for example, when we need to

16:18.200 --> 16:26.440
use different types of data aggregation methods in order to extract data from PDFs when

16:26.440 --> 16:29.080
we're comparing to maybe CDC reviews.

16:29.080 --> 16:34.440
One of the things that has been part of this project for the last two years has been,

16:34.440 --> 16:41.640
we wanted to understand the impact of news deserts on infectious disease outbreaks.

16:41.640 --> 16:45.920
And we have a lot of different types of data in order to understand communities where their

16:45.920 --> 16:48.880
local newspapers have closed.

16:48.880 --> 16:53.880
And it will be interesting over the long term to see how that, whether that will even

16:53.880 --> 16:57.080
matter for the coronavirus given that it's a global pandemic.

16:57.080 --> 17:03.240
And that's not necessarily how people are becoming aware of the pandemic.

17:03.240 --> 17:07.400
And that's not necessarily the way that we're counting cases anymore, with much smaller

17:07.400 --> 17:14.080
outbreaks, local news media is really vital in informing the community and also for researchers

17:14.080 --> 17:20.160
to use those local news reports to get another source of case counts.

17:20.160 --> 17:26.040
So not just informing the community of citizens, but informing the medical community, there's

17:26.040 --> 17:31.560
not a back channel of, hey, be on the lookout for this disease.

17:31.560 --> 17:34.880
Or if there is, it's inefficient, is that what you're saying?

17:34.880 --> 17:40.080
Well, I guess I'm highlighting that the local news media data is another resource.

17:40.080 --> 17:44.720
It's another way that we find out about outbreaks when they're smaller outbreaks.

17:44.720 --> 17:50.160
And it's another way for researchers to, again, there's no gold standard.

17:50.160 --> 17:55.480
There's limitations in infectious disease outbreaks in the data from the CDC, from an

17:55.480 --> 18:00.760
electronic, you know, billing claims resource from local news media data.

18:00.760 --> 18:04.880
And so one of the overarching goals of this, this project that had been ongoing was really

18:04.880 --> 18:09.400
to understand the generalizability of all these data sources and try to quantify how we

18:09.400 --> 18:13.960
could leverage multiple data sources to get at accurate case counts.

18:13.960 --> 18:19.600
And then more broadly, when you're applying machine learning to these types of problems,

18:19.600 --> 18:26.240
I'm curious about the tools that you end up using. You mentioned causality, causal

18:26.240 --> 18:27.240
inference.

18:27.240 --> 18:32.960
I saw in maybe the publications page on your site, which of course we'll link to in the

18:32.960 --> 18:43.120
show notes picture where you have your, it looks like you are trying to relate the, you

18:43.120 --> 18:45.240
know, costs of different interventions.

18:45.240 --> 18:49.240
So you've got some baseline formula and then, you know, mental health as a broad category

18:49.240 --> 18:51.720
of intervention, maybe, and then substance use.

18:51.720 --> 18:55.600
You can tell me what you're actually saying in this diagram.

18:55.600 --> 19:01.560
And then you've got increases and decreases of compensation, actually, so that may not

19:01.560 --> 19:02.560
be cost.

19:02.560 --> 19:06.960
But it kind of strikes me that you're looking at, it's your applying causal inference

19:06.960 --> 19:08.920
and you're looking at interventions.

19:08.920 --> 19:16.120
Yeah, so I'd have to confirm which diagram that was, but I believe it's probably from

19:16.120 --> 19:20.480
one of the studies where we're examining the impact of different types of payment models

19:20.480 --> 19:21.720
in the healthcare system.

19:21.720 --> 19:22.720
Okay.

19:22.720 --> 19:28.160
And to understand the impact of, you know, changing how spending occurs and the different

19:28.160 --> 19:33.360
types of what we call spending models, you know, we have payment systems in the U.S.,

19:33.360 --> 19:39.120
where sometimes we have sort of what we call a bundle payment, a certain specific amount

19:39.120 --> 19:44.560
for a particular type of procedure and other types of spending models where we have, you

19:44.560 --> 19:52.400
know, fee for service, every single thing that you do, whether it's a lab or a procedure,

19:52.400 --> 19:55.840
it has a particular dollar amount attached to it.

19:55.840 --> 19:59.840
And so then the incentive, of course, is if you're getting a fee for service is to have

19:59.840 --> 20:01.600
many, many services.

20:01.600 --> 20:08.360
And so a number of the studies that I've worked on is trying to understand the impact

20:08.360 --> 20:13.120
of these types of spending models, but one thing that I will highlight that I think

20:13.120 --> 20:20.560
a lot of people who don't work in health economics may not realize is that the impact of changing

20:20.560 --> 20:26.360
either, you know, improving how we allocate funding in the healthcare system or the

20:26.360 --> 20:31.200
impact of different funding models has the potential to improve human health.

20:31.200 --> 20:36.000
So it's not an exercise in trying to save money at the cost of human health.

20:36.000 --> 20:41.600
It's really about more efficiently serving people such that we can improve human health

20:41.600 --> 20:47.200
in one of the areas where we've seen this is in mental health care, where over, you

20:47.200 --> 20:56.360
know, the second half of the decade, sorry, the second half of, you know, 1950 to 2000,

20:56.360 --> 21:03.520
we saw that the vast majority of improvements in mental health treatment really came from

21:03.520 --> 21:07.920
changes in the financing of health care and improvements, things that led to improvements

21:07.920 --> 21:09.240
in access.

21:09.240 --> 21:11.560
And so it wasn't necessarily new treatments.

21:11.560 --> 21:16.440
But as far as the types of methods that I work on, so a lot of the work that I do in causal

21:16.440 --> 21:19.680
inference is focused on, on sampling methods.

21:19.680 --> 21:24.560
So bringing together multiple algorithms so we don't have to rely on a single algorithm.

21:24.560 --> 21:29.880
So especially when different types of algorithms become the new flashy tool.

21:29.880 --> 21:33.480
So when I was in grad school, it was random forest and now it's, you know, deep learning

21:33.480 --> 21:36.680
and neural networks and everyone's like, should I do a regression or should I do a random

21:36.680 --> 21:42.800
forest and I say you can do both and more by incorporating, you know, rigorous ensemble

21:42.800 --> 21:47.000
techniques by using multiple algorithms in a priori specified metrics.

21:47.000 --> 21:52.520
And then within causal inference, you know, we bring these ensembles into so-called double

21:52.520 --> 21:54.000
robust estimation.

21:54.000 --> 21:56.720
So we don't just use information from an outcome regression.

21:56.720 --> 22:02.560
We also use information for estimating what the, you know, the probability that you

22:02.560 --> 22:06.680
would have been treated and so a functional form for that, a flexible functional form

22:06.680 --> 22:07.680
for that.

22:07.680 --> 22:13.680
And so I've co-authored two books on this topic and really a lot of my research has focused

22:13.680 --> 22:19.160
on bringing double robust methods and machine learning together and again integrating that

22:19.160 --> 22:24.920
into health services research and really trying to develop tools to specifically answer

22:24.920 --> 22:27.840
questions in health services research.

22:27.840 --> 22:31.800
You mentioned rigorous ensemble methods.

22:31.800 --> 22:38.120
What does that mean for an ensemble method to be rigorous and how does one achieve the

22:38.120 --> 22:39.640
requisite level of rigor?

22:39.640 --> 22:40.640
Yes.

22:40.640 --> 22:45.120
So I threw in rigorous because anytime I mention ensemble techniques, I'm always a little

22:45.120 --> 22:50.040
bit concerned that for people who might be unfamiliar and I mentioned in a priori metrics

22:50.040 --> 22:53.800
that's being rigorous, but we really have to decide upfront.

22:53.800 --> 22:57.280
If we're going to run multiple algorithms, we really need to specify upfront what they

22:57.280 --> 22:59.400
will be, how they will be evaluated.

22:59.400 --> 23:01.880
We need to incorporate cross validation.

23:01.880 --> 23:06.640
Meaning as opposed to I'm working on a Kaggle competition, I'm going to throw every model

23:06.640 --> 23:12.280
I can think of against this dataset and see which produces the best accuracy on my test

23:12.280 --> 23:13.280
set.

23:13.280 --> 23:17.080
Well, you could do that if you make sure to choose beforehand that accuracy is going to

23:17.080 --> 23:19.120
be your metric and you incorporate cross validation.

23:19.120 --> 23:24.040
We can come back to why leaderboard accuracy is bad.

23:24.040 --> 23:28.200
Single metric, single metrics is another one of the thing, you know, I get data quality

23:28.200 --> 23:34.360
is a soapbox for mine, single metrics is another, but I threw in the word rigorous because

23:34.360 --> 23:38.800
I, a problem that you see a lot is people run an algorithm and then they tweak it and

23:38.800 --> 23:42.960
they run it again or they run multiple algorithms, but they run them in sequence and then they

23:42.960 --> 23:43.960
try another thing.

23:43.960 --> 23:49.160
And so, yeah, it's the machine learning version of P hacking.

23:49.160 --> 23:52.960
And once you start touching the data that's your estimator and I really feel like you

23:52.960 --> 23:59.400
need to, you need to be really upfront about what your estimator is going to be, what

23:59.400 --> 24:04.120
your metrics are going to be and, you know, a single metric is often not going to be sufficient.

24:04.120 --> 24:09.920
You can get a really high accuracy and have incredibly poor true positive rate.

24:09.920 --> 24:14.920
And I mean, and a lot of our, a lot of the problems that we're dealing with, you know,

24:14.920 --> 24:19.880
and a lot of the, unfortunately, a lot of the papers that we see in, you know, clinical

24:19.880 --> 24:24.280
medicine and health services research and health outcomes, now that people are starting

24:24.280 --> 24:29.160
to bring machine learning to this space, a lot of the papers are very much that.

24:29.160 --> 24:38.280
Oh, we got a, we got an accuracy of 0.97 versus the, the parametric regression was 0.95.

24:38.280 --> 24:42.840
And that, that type of a difference, you haven't explained whether it's meaningful, you haven't

24:42.840 --> 24:47.840
looked at any other metrics, you've usually only used a single data set and a single medical

24:47.840 --> 24:52.400
center, and suddenly they make these big broad claims that it can be useful in clinical

24:52.400 --> 24:55.680
practice, which is, is dangerous, is, is frankly dangerous.

24:55.680 --> 25:00.880
And when you put it in a clinical journal, the standards need to be really, really high

25:00.880 --> 25:05.680
for clinical research and, and making claims with machine learning.

25:05.680 --> 25:09.400
And that's, a lot of the work is not what I would call rigorous.

25:09.400 --> 25:14.080
And this would be also be very, very true of the coronavirus when we're putting these

25:14.080 --> 25:20.000
papers out there, they, you know, this, the speed cannot, cannot be an excuse for lack

25:20.000 --> 25:21.500
of rigor.

25:21.500 --> 25:29.920
One of the things you said was that when you apply a model to your data, that's your estimator.

25:29.920 --> 25:32.320
Oh, something along, like, along those lines.

25:32.320 --> 25:34.560
I mean, on that, what does that mean?

25:34.560 --> 25:35.560
Yes.

25:35.560 --> 25:38.840
So start touching your data, that's, that's, that's an estimator.

25:38.840 --> 25:44.160
So you need to incorporate all of that uncertainty into.

25:44.160 --> 25:47.680
What does it mean for that to be an estimator?

25:47.680 --> 25:54.400
So for example, in the, the P hacking scenario, if you run a regression and then you, you

25:54.400 --> 25:57.080
know, change it, run it again, change it, run it again.

25:57.080 --> 25:58.760
So now you've run it three times.

25:58.760 --> 26:02.800
Well, that, those three time, that whole thing is your estimator.

26:02.800 --> 26:06.640
It's not the last regression that you ran, but normally what people would do is they

26:06.640 --> 26:11.320
would publish that last regression they ran with the standard errors based on having only

26:11.320 --> 26:16.080
run that single regression, which means the standard errors aren't correct.

26:16.080 --> 26:20.720
And so the second you start touching your data, the second you run any kind of algorithm

26:20.720 --> 26:23.280
that you need to build that into your estimator.

26:23.280 --> 26:28.120
So if you do a multiple stages of estimation, you need to account for that.

26:28.120 --> 26:31.400
And that's why, you know, for a prediction problem, that's why you should just state

26:31.400 --> 26:37.560
all of your estimators up front, run them all together and, you know, we have both finite

26:37.560 --> 26:41.560
sample and asymptotic properties that allow us to, you know, know that this will have good

26:41.560 --> 26:43.400
statistical properties.

26:43.400 --> 26:51.080
But this, this iterative cherry picking of running algorithms can really disperius results.

26:51.080 --> 26:57.800
In the machine learning community, we often describe the fundamental process as one that

26:57.800 --> 27:04.000
is inherently iterative, where engineering features and trying out models and, you know,

27:04.000 --> 27:06.720
that's, that's just the job.

27:06.720 --> 27:11.040
And how do you reconcile that with what you're describing here?

27:11.040 --> 27:15.720
I think you need to be really clear when your analysis is hypothesis generating.

27:15.720 --> 27:20.000
So if you really are doing exploratory data analysis, be very clear about that.

27:20.000 --> 27:24.840
If you're doing feature selection, if you're trying to discover relationships, if you're

27:24.840 --> 27:28.600
doing something that's unsupervised learning and you're trying to discover groups, we'll

27:28.600 --> 27:29.600
be very clear about that.

27:29.600 --> 27:35.400
If you're then going to use those groups to define some causal intervention, again, you

27:35.400 --> 27:39.080
need to incorporate the uncertainty in how those groups were defined.

27:39.080 --> 27:45.080
So I think the transparency of what you're doing and what the goal is is, is absolutely

27:45.080 --> 27:46.400
paramount.

27:46.400 --> 27:51.240
And if you get into a place where you're then trying to do causal inference, then again,

27:51.240 --> 27:58.560
you can't have this sort of cherry picking iterative style because then our, the reliability

27:58.560 --> 28:02.080
of the results are going to be very flawed.

28:02.080 --> 28:09.280
Causal inferences sometimes presented as a, a tool that solves the kind of problems

28:09.280 --> 28:12.000
you're describing kind of by its very nature.

28:12.000 --> 28:18.360
It's more rigorous in, in some way than kind of the general, the other stuff that people

28:18.360 --> 28:24.800
do, but it sounds like there are lots of pitfalls and opportunities to do it wrong.

28:24.800 --> 28:30.480
Yeah, I think that the same way that there are some people who think that, you know, electronic

28:30.480 --> 28:36.480
health record data is the gold standard data and has no issues, which is false.

28:36.480 --> 28:41.400
There may be some people who think causal inferences, this, this, this, this area that is,

28:41.400 --> 28:48.040
does not have magic and pixie dust and it really, it really isn't.

28:48.040 --> 28:55.040
So again, you need the causal inferences difficult, the underlying research question should

28:55.040 --> 28:57.280
be driving whether it's causal inferences or not.

28:57.280 --> 29:01.200
If you're interested in a prediction question or an effect question, that should really

29:01.200 --> 29:08.200
be guiding what types of techniques you need to use, but that you can have really terrible

29:08.200 --> 29:13.360
causal analyses, the same way you can have really terrible prediction analyses or clustering

29:13.360 --> 29:14.960
analyses.

29:14.960 --> 29:20.040
And the transparency about what your assumptions are, what the limitation of the, of your

29:20.040 --> 29:24.960
data are, all of that is just something that needs to be up front.

29:24.960 --> 29:32.160
And it's something that I, I mean, that we recently had a, a workshop in the fall back

29:32.160 --> 29:38.160
when we, we gathered in groups of people at the, at the national academies.

29:38.160 --> 29:45.520
And I was really advocating for us, we need to have as a research community, you know,

29:45.520 --> 29:52.280
a baseline of Stanford machine learning research, especially in, in, in the clinical medicine.

29:52.280 --> 29:56.200
And when we, when we have prediction research, when you causal inference research in these

29:56.200 --> 30:01.000
clinical journals, the standards need to be incredibly high and at a minimum, we need

30:01.000 --> 30:02.840
to be very, very transparent.

30:02.840 --> 30:05.240
We need to share code.

30:05.240 --> 30:10.760
We need to be very explicit about what the causal assumptions are and when they might not hold.

30:10.760 --> 30:15.920
And creating a culture around this that's much less about having magic flashy results

30:15.920 --> 30:25.120
and much more about genuine discovery and having incredibly clear, probably appendices about

30:25.120 --> 30:26.520
these issues.

30:26.520 --> 30:29.720
But having that be the standard.

30:29.720 --> 30:39.160
Code sharing is, I think just starting to catch on and become a, I don't know if it's even

30:39.160 --> 30:45.200
one would say an accepted practice at the NURPS conference for the past couple of years.

30:45.200 --> 30:51.640
They've, there's been a repeatability effort that has encouraged researchers to submit

30:51.640 --> 30:53.160
papers with code.

30:53.160 --> 31:01.120
I've got to imagine, perhaps less so in the, you know, more traditional sciences, statistics,

31:01.120 --> 31:05.000
medicine, or I don't speak out of turn.

31:05.000 --> 31:06.520
So I'll just say this is my impression.

31:06.520 --> 31:12.000
I am a journal co-editor for one of the journals and statistics that the journal biostatistics.

31:12.000 --> 31:16.160
We are standard is that you have to share code when you publish.

31:16.160 --> 31:19.360
And a number of statistics journals have this now.

31:19.360 --> 31:23.280
So I think we might be a little bit ahead of the machine learning community.

31:23.280 --> 31:28.440
There might be a little bit more buy-in, but that's just my perception.

31:28.440 --> 31:36.800
I would say in the clinical journals, it's very early stages to have that be something

31:36.800 --> 31:40.240
that people think is reasonable and expected to do.

31:40.240 --> 31:46.760
And some of this has been helpfully driven by funders where they require that every project

31:46.760 --> 31:51.320
you might do in the clinical space that you have to share code.

31:51.320 --> 31:55.280
And I think that this is something else that I brought up at that National Academy's

31:55.280 --> 32:01.160
workshop was we need funders, we need journals, the researchers will do it if they're forced

32:01.160 --> 32:02.160
to do it.

32:02.160 --> 32:10.640
And we need to get allies on our side to make that happen because it's not necessarily

32:10.640 --> 32:16.720
going to keep erroneous results from being published, but again, it's that transparency.

32:16.720 --> 32:22.960
And with high impact work, with all of the work we were doing before the coronavirus and

32:22.960 --> 32:27.560
now that we have the coronavirus, when data can be reasonably shared and not all data

32:27.560 --> 32:31.960
can be shared, so a lot of electronic health data cannot be shared because of privacy

32:31.960 --> 32:32.960
considerations.

32:32.960 --> 32:38.480
But many of the data sources with coronavirus can be shared and if it can be shared, it

32:38.480 --> 32:39.480
should be shared.

32:39.480 --> 32:44.760
The same way your code should be shared and that should be the standard.

32:44.760 --> 32:50.840
We started talking about single metrics and leaderboard efforts.

32:50.840 --> 32:55.600
I felt like you wanted to jump in there, we didn't like jump in there.

32:55.600 --> 33:03.840
We did touch on it a bit in my frustration of what seems to be published in not just

33:03.840 --> 33:05.480
clinical journals, but other journals.

33:05.480 --> 33:08.840
And I say this in a critical sense, I mean, if you go back five years and look at some

33:08.840 --> 33:12.920
of my papers, I was publishing a single metric as well.

33:12.920 --> 33:17.280
I know you recently had a podcast about algorithmic fairness.

33:17.280 --> 33:21.120
And so one of the other areas that I work in is algorithmic fairness.

33:21.120 --> 33:26.960
And so not only do we need multiple metrics of global fit, like your accuracy or your

33:26.960 --> 33:32.720
AUCs and your R-squareds, they tell us different things, but we also really need to center

33:32.720 --> 33:38.320
group fit and understanding how particular algorithm might further marginalize already

33:38.320 --> 33:39.800
marginalized groups.

33:39.800 --> 33:42.520
And in the healthcare system, we have many different groups that we already know are

33:42.520 --> 33:46.920
marginalized, individuals with mental health and substance use disorders, individuals in

33:46.920 --> 33:51.040
rural communities, black and African-American individuals, there's a lot of groups that

33:51.040 --> 33:54.880
we need to make sure that if we're saying in a clinical paper, this algorithm should

33:54.880 --> 33:55.880
be deployed.

33:55.880 --> 33:59.760
And again, I already mentioned the fact that there's lots of other issues in that paper,

33:59.760 --> 34:04.960
a single medical center, not using cross validation, et cetera, et cetera.

34:04.960 --> 34:09.040
But they also haven't looked, they're saying use this when they haven't studied the harms

34:09.040 --> 34:10.040
of that algorithm.

34:10.040 --> 34:11.240
What are the potential harms?

34:11.240 --> 34:15.000
And one of the ways that we can assess that is with metrics of group fit.

34:15.000 --> 34:16.000
So-

34:16.000 --> 34:18.680
The current state of that in clinical practice?

34:18.680 --> 34:19.680
Almost nonexistent.

34:19.680 --> 34:28.320
I mean, the concept of studying algorithms for issues of fairness is starting to make

34:28.320 --> 34:33.360
a dent, but when you look at published papers, the vast, vast, vast majority of published

34:33.360 --> 34:36.880
papers in clinical journals do not even consider it.

34:36.880 --> 34:41.520
What's your sense for what it will take to resolve that?

34:41.520 --> 34:46.760
Is it, you know, just time or what are the things that you're doing in the worlds that

34:46.760 --> 34:53.320
you influence and the journals that you mentioned to try to drive the community towards considering

34:53.320 --> 34:55.360
those kinds of metrics?

34:55.360 --> 35:02.800
I'm excited to see more people working on algorithmic fairness in the health space.

35:02.800 --> 35:07.680
This is something that, historically, a number of the conferences in fairness have not had

35:07.680 --> 35:10.200
any papers on health.

35:10.200 --> 35:12.200
And so I had previously submitted some-

35:12.200 --> 35:13.200
Yes.

35:13.200 --> 35:15.720
And so it was only maybe two years ago that I saw the first paper.

35:15.720 --> 35:18.440
It's like, oh, they took a paper in health.

35:18.440 --> 35:24.120
And one of my papers that was recently published in a statistics journal in biometrics on

35:24.120 --> 35:30.000
fair regression for health care spending, that paper was rejected from one of the top

35:30.000 --> 35:32.360
fairness conferences.

35:32.360 --> 35:36.960
And so I had been trying to kind of bring these issues to some of the existing fairness conferences.

35:36.960 --> 35:42.080
I'm glad to see that the last two years, there's more health work at these conferences.

35:42.080 --> 35:48.480
There's also some newer conferences incorporating both people working in health and people working

35:48.480 --> 35:49.680
in fairness.

35:49.680 --> 35:56.520
So I do think that there are a number of people now in the fair, there's a growing consortium

35:56.520 --> 36:01.480
of people who care about fairness specifically in the health space.

36:01.480 --> 36:07.080
And I'm excited to see everyone in that community who's really trying and actively working to

36:07.080 --> 36:15.400
make sure that it gets more attention, especially given the fact that the health care system

36:15.400 --> 36:21.320
is just one of the biggest levers in the country that can have an impact on social policy.

36:21.320 --> 36:28.200
And so if we bring algorithms into this huge system without really vetting them for

36:28.200 --> 36:33.920
these issues, we can make people who are already incredibly marginalized, much less healthy.

36:33.920 --> 36:38.560
And so I'm speaking at a conference this summer, which I'm assuming will be virtual.

36:38.560 --> 36:41.160
One of these conferences, a chill conference.

36:41.160 --> 36:47.760
And I'm really thankful to the organizers who are really pushing forward this endeavor.

36:47.760 --> 36:53.760
And so I'm hopeful, but I'm also concerned.

36:53.760 --> 36:58.840
The paper that you mentioned, fair regression for health care spending, let's talk a little

36:58.840 --> 36:59.840
bit about that.

36:59.840 --> 37:04.840
What are the goals and what is fair regression?

37:04.840 --> 37:07.320
How does one achieve fair regression?

37:07.320 --> 37:09.880
So there were two main goals of this paper.

37:09.880 --> 37:16.640
So the methodological goal was that a lot of the work in algorithmic fairness and both

37:16.640 --> 37:20.520
from methods and definitions really focused on binary outcomes.

37:20.520 --> 37:24.800
And there was very little work in continuous outcomes.

37:24.800 --> 37:29.160
And so a large portion of my work is in health care spending, which is a bounded continuous

37:29.160 --> 37:30.320
outcome.

37:30.320 --> 37:36.400
And so we developed some new fair regression methods for continuous outcomes and also

37:36.400 --> 37:40.120
compared them to the few methods that are existed.

37:40.120 --> 37:44.320
But what are the two groups, fairness or some other type of?

37:44.320 --> 37:45.320
Yes.

37:45.320 --> 37:50.920
Relative to various measures of group fairness and also global fit.

37:50.920 --> 37:58.000
And so what fairness means for health care spending is that the formulas that we use

37:58.000 --> 38:03.080
to pay health plans are called risk adjustment formulas.

38:03.080 --> 38:06.920
And these aim to distribute health care funds based on health.

38:06.920 --> 38:12.840
But unfortunately, health plans can discriminate against groups, including those that are defined

38:12.840 --> 38:15.960
by certain health conditions, I mentioned one group earlier that I've worked on a

38:15.960 --> 38:20.160
lot, individuals with mental health and substance use disorder.

38:20.160 --> 38:26.040
So if these groups are currently costly to the insurer.

38:26.040 --> 38:33.440
So any group that is costly incentivizes the health insurer to discriminate against

38:33.440 --> 38:38.080
them through various mechanisms, like changing which providers are available to enrollees

38:38.080 --> 38:41.280
or the cost of the copay of certain prescription drugs.

38:41.280 --> 38:50.200
So our goal was to try and create regressions formulas that were fairer in the sense that

38:50.200 --> 38:54.160
under compensated groups would be less likely to be discriminated against.

38:54.160 --> 38:59.320
So if we could redistribute the funds within the formulas such that individuals with mental

38:59.320 --> 39:04.960
health and substance use disorders were not massively under compensated, then the insurers

39:04.960 --> 39:11.480
would have less of an incentive to try and change their plans to harm those enrollees.

39:11.480 --> 39:16.480
And we found these regressions performed incredibly well.

39:16.480 --> 39:24.160
We could improve fairness for groups by over something like 98% while global fit was reduced

39:24.160 --> 39:25.480
by maybe 4%.

39:25.480 --> 39:33.760
Just a very small loss of global fit for incredible improvements in group fairness.

39:33.760 --> 39:39.440
As some of my continued work in that space, that paper was written with a PhD student in

39:39.440 --> 39:40.680
a zinc.

39:40.680 --> 39:46.520
We then after that paper collaborated with our economist colleague, Tom McGuire, where

39:46.520 --> 39:51.160
we tried to look at not just a single group of multiple groups and bringing together fair

39:51.160 --> 39:55.760
regression with other types of interventions in the healthcare system.

39:55.760 --> 40:01.520
And because we were concerned, okay, if we help, if we improve under compensation for

40:01.520 --> 40:04.160
one group, what happens to other groups?

40:04.160 --> 40:09.800
And we found that by improving fairness for multiple groups, we picked four groups that

40:09.800 --> 40:15.840
we knew were under compensated and we knew that insurers might have an incentive to discriminate

40:15.840 --> 40:16.840
against them.

40:16.840 --> 40:22.200
By improving fairness for those four groups, we improved fairness for 88% of additional

40:22.200 --> 40:26.160
groups that we didn't even bring into the loss function.

40:26.160 --> 40:30.600
And we were also able to reduce the number of variables that were needed in the formula

40:30.600 --> 40:32.840
by something like 60%.

40:32.840 --> 40:35.560
What's your intuition for why that works?

40:35.560 --> 40:36.960
Why does that happen?

40:36.960 --> 40:41.360
Well, so what it ends up doing is it takes funds away from, so there are people in the

40:41.360 --> 40:45.680
healthcare system that are over compensated because the formula expects them to have more

40:45.680 --> 40:48.280
expenses that they don't have.

40:48.280 --> 40:51.840
So it actually redistributes funds in a very smart way.

40:51.840 --> 40:55.840
So people who are healthy and are currently over compensated, it moves that money to

40:55.840 --> 40:58.320
people who are being under compensated.

40:58.320 --> 41:00.400
And so we're really excited about this.

41:00.400 --> 41:09.000
You mentioned at the beginning of our discussion about am I trying to, is it about understanding

41:09.000 --> 41:10.560
for the influence?

41:10.560 --> 41:18.280
And I've been working on fairness methods and risk adjustment methods for a long time

41:18.280 --> 41:19.280
now.

41:19.280 --> 41:22.800
And we're really getting into a place where we're able to bring all of these advances together

41:22.800 --> 41:29.960
and make recommendations at a certain point to not just in the US, there's many different

41:29.960 --> 41:35.040
health systems in the world that use risk adjustment formulas.

41:35.040 --> 41:42.120
And I'm excited about the potential, again, to have such a potential big impact on a risk

41:42.120 --> 41:48.640
adjustment system that can have a tremendous influence on improving health and social

41:48.640 --> 41:49.640
policy.

41:49.640 --> 41:51.840
That's great.

41:51.840 --> 41:58.160
Before we wrap up any other, any parting thoughts or things that other things that you're excited

41:58.160 --> 42:01.560
about in the space that you're working?

42:01.560 --> 42:04.040
Oh, great question.

42:04.040 --> 42:10.320
I think my parting thoughts would be about reading research in general, but especially

42:10.320 --> 42:16.440
during a pandemic, read it with a critical eye, read things before you share them.

42:16.440 --> 42:20.920
I don't know if I'm naive or it's because I'm an elder millennial, but I think read things

42:20.920 --> 42:25.640
before you share them should really be the baseline.

42:25.640 --> 42:32.600
But please read things before you share them and be careful about, ask questions when

42:32.600 --> 42:37.800
you're reading whether it's a coronavirus paper or it's a flashy machine learning paper

42:37.800 --> 42:39.960
in a clinical journal.

42:39.960 --> 42:41.640
What data source did they use?

42:41.640 --> 42:44.760
Did they talk about any limitations of the data source?

42:44.760 --> 42:46.640
What kinds of metrics did they use?

42:46.640 --> 42:51.840
If they only used accuracy or AUC or R-squared, that's a red flag.

42:51.840 --> 42:53.160
What's the true positive rate?

42:53.160 --> 42:55.160
What's the false positive rate?

42:55.160 --> 42:56.160
What are their conclusions?

42:56.160 --> 42:57.160
Are they overstated?

42:57.160 --> 43:00.160
How many, what populations did they study?

43:00.160 --> 43:01.920
What does it mean for generalizability?

43:01.920 --> 43:05.680
Ask all of these questions, whether it's machine learning research or not.

43:05.680 --> 43:12.440
Just be interested, be excited, but be skeptical and thoughtful.

43:12.440 --> 43:18.960
What I love about your comment is that it doesn't require some kind of secret decoder

43:18.960 --> 43:25.480
ring, years of statistical stats courses or what have you.

43:25.480 --> 43:33.760
It's just asking, being a critical reader and consumer of all the news and articles

43:33.760 --> 43:40.360
and journal papers that you're reading and just thinking broadly about the stuff that

43:40.360 --> 43:42.000
the claims that they're making.

43:42.000 --> 43:43.000
Absolutely.

43:43.000 --> 43:44.000
Awesome.

43:44.000 --> 43:48.720
Well, Sherry, thanks so much for taking the time to share a bit about what you're up

43:48.720 --> 43:49.720
to.

43:49.720 --> 43:51.200
Well, thank you so much for chatting.

43:51.200 --> 43:52.200
Great.

43:52.200 --> 43:53.200
Thank you.

43:53.200 --> 43:59.280
All right, everyone, that's our show for today.

43:59.280 --> 44:05.080
For more information on today's show, visit twomolai.com slash shows.

44:05.080 --> 44:15.080
As always, thanks so much for listening and catch you next time.

