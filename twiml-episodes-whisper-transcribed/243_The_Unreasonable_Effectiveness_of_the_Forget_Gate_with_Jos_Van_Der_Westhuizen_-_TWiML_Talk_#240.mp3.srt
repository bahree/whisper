1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,800
I'm your host Sam Charrington.

4
00:00:31,800 --> 00:00:38,080
Today we're joined by Yas fund of S. Tyson, PhD student in engineering at Cambridge University.

5
00:00:38,080 --> 00:00:44,400
Yas' research focuses on applying LSTMs or long short-term memory neural networks to biological

6
00:00:44,400 --> 00:00:46,720
data for various tasks.

7
00:00:46,720 --> 00:00:52,080
In our conversation we discuss his paper, The Unreasonable Effectiveness of the Forgetgate,

8
00:00:52,080 --> 00:00:57,320
in which he explores the various gates that make up an LSTM module and the general impact

9
00:00:57,320 --> 00:01:01,960
of getting rid of gates on the computational intensity of training the networks.

10
00:01:01,960 --> 00:01:06,960
Yas eventually determines that leaving only the Forgetgate results in a quote unquote

11
00:01:06,960 --> 00:01:11,640
unreasonably effective network and we discuss why.

12
00:01:11,640 --> 00:01:17,800
He also gives us some great LSTM-related resources, including references to you against Schmidhuber,

13
00:01:17,800 --> 00:01:27,040
whose research group invented the LSTM and who I spoke to back in Twimble Talk No. 44.

14
00:01:27,040 --> 00:01:31,280
Before we get to the episode, I'd like to join Pegasystems, this episode's sponsor,

15
00:01:31,280 --> 00:01:38,600
and inviting you to meet me at the MGM Grand in Las Vegas, June 2nd through 5th at Pegaworld,

16
00:01:38,600 --> 00:01:42,560
the company's annual digital transformation conference.

17
00:01:42,560 --> 00:01:48,160
Pegasystems puts AI in the center of its customer engagement software so that it optimizes

18
00:01:48,160 --> 00:01:52,080
every customer test point on every channel in real time.

19
00:01:52,080 --> 00:01:57,000
That way each interaction is relevant and timely to each individual customer, no matter if

20
00:01:57,000 --> 00:02:02,760
it's a sales call, a digital marketing campaign, or a customer service chat either online

21
00:02:02,760 --> 00:02:04,160
or in store.

22
00:02:04,160 --> 00:02:09,800
And the system is always learning in real time to make the next interaction better.

23
00:02:09,800 --> 00:02:15,000
Pegas Customers are the real stars at Pegaworld, there you'll hear great stories of AI applied

24
00:02:15,000 --> 00:02:18,600
to the customer experience at real Pegas Customers.

25
00:02:18,600 --> 00:02:24,080
The event is a great way to learn from a who's who of the Fortune 500, and of course I'll

26
00:02:24,080 --> 00:02:26,600
be there in speaking as well.

27
00:02:26,600 --> 00:02:33,080
To register, visit Pegaworld.com and use the promo code Twimble19 when you sign up for

28
00:02:33,080 --> 00:02:34,080
$200 off.

29
00:02:34,080 --> 00:02:38,320
Again, that's Twimble19, it's as easy as that.

30
00:02:38,320 --> 00:02:39,960
Hope to see you there.

31
00:02:39,960 --> 00:02:43,200
And now on to the show.

32
00:02:43,200 --> 00:02:49,000
Alright everyone, I am on the line with Yas Fundovesthizen.

33
00:02:49,000 --> 00:02:54,160
Yas is completing his PhD in engineering at Cambridge University.

34
00:02:54,160 --> 00:02:57,240
Yas, welcome to this week in machine learning and AI.

35
00:02:57,240 --> 00:02:58,480
Thank you Sam.

36
00:02:58,480 --> 00:03:05,280
So we were joking earlier, you're kind of in this weird intermediary state, intermediate

37
00:03:05,280 --> 00:03:10,160
state between having finished all of the requirements for your PhD and kind of waiting to get the

38
00:03:10,160 --> 00:03:13,160
actual degree conferred.

39
00:03:13,160 --> 00:03:17,920
Tell us a little bit about your background and what you studied at Cambridge.

40
00:03:17,920 --> 00:03:25,120
Yeah, so in all honesty, I fell into the machine learning field a bit by accident.

41
00:03:25,120 --> 00:03:31,680
And I think this story is kind of true for a lot of people with the big current hype.

42
00:03:31,680 --> 00:03:36,720
So where it all started is I started with biomedical engineering undergrad, I'm South African,

43
00:03:36,720 --> 00:03:40,440
and I studied at Statenbach University over there.

44
00:03:40,440 --> 00:03:45,200
And straight out of the undergrad, I realized that I still want to learn a bit more.

45
00:03:45,200 --> 00:03:50,160
So I started the masters in like effectively computational neuroscience.

46
00:03:50,160 --> 00:03:54,280
And this is where I really had like my first little experience with machine learning.

47
00:03:54,280 --> 00:04:00,200
We played with like basic models, like linear discriminant analysis and like self organizing

48
00:04:00,200 --> 00:04:01,680
maps.

49
00:04:01,680 --> 00:04:06,160
And actually like halfway through the masters, I was accepted at Cambridge and actually

50
00:04:06,160 --> 00:04:08,880
Oxford and like received scholarships for both.

51
00:04:08,880 --> 00:04:11,000
So I was quite happy.

52
00:04:11,000 --> 00:04:15,960
And I decided to stop the masters to go start a PhD at Cambridge.

53
00:04:15,960 --> 00:04:22,080
And my initial like plan was to do I was I think a bit ambitious, but my initial plan

54
00:04:22,080 --> 00:04:23,080
was to.

55
00:04:23,080 --> 00:04:27,960
I wanted to effectively create like a wristwatch that could like measure anything about your

56
00:04:27,960 --> 00:04:31,640
body, like kind of look at detail at your blood.

57
00:04:31,640 --> 00:04:35,400
And like tell you, for example, to like today you have to eat a banana, otherwise tomorrow

58
00:04:35,400 --> 00:04:39,440
you're going to get a heart attack that kind of that level of accuracy.

59
00:04:39,440 --> 00:04:42,880
So kind of like a tricorder prize type of device?

60
00:04:42,880 --> 00:04:44,880
Yeah, yeah, pretty much.

61
00:04:44,880 --> 00:04:48,440
And then my supervisor at the time told me like, oh well, there's a range of problems

62
00:04:48,440 --> 00:04:51,000
that you have to solve to get to this end product.

63
00:04:51,000 --> 00:04:55,280
And she said one of the like most fundamental ones is probably like analyzing these signals

64
00:04:55,280 --> 00:04:57,200
and making predictions.

65
00:04:57,200 --> 00:05:02,440
So I kind of started, yeah, I guess looking at ways of like analyzing these signals and

66
00:05:02,440 --> 00:05:08,520
making predictions and just speaking around in a really awesome lab, I heard about like

67
00:05:08,520 --> 00:05:12,200
these temporal machine learning techniques that could easily solve this problem.

68
00:05:12,200 --> 00:05:14,440
And that's kind of where I started playing with them.

69
00:05:14,440 --> 00:05:18,760
And like I played with theater market models, which is like a more conventional technique

70
00:05:18,760 --> 00:05:23,000
and also with some of the new deep learning techniques like recurrent neural networks.

71
00:05:23,000 --> 00:05:31,360
And I guess after seeing my first few lines of like of gradient descent happening, I completely

72
00:05:31,360 --> 00:05:35,200
fell in love with deep learning and that's that's where it all kicked off, yeah.

73
00:05:35,200 --> 00:05:41,800
Have you been seeking to apply what you've been learning about machine learning and deep

74
00:05:41,800 --> 00:05:49,560
learning back to these biomedical applications or have you totally dived into the more theoretical

75
00:05:49,560 --> 00:05:52,360
side of ML and deep learning?

76
00:05:52,360 --> 00:05:55,200
Yeah, so that's a really good question actually.

77
00:05:55,200 --> 00:06:01,120
So this started my PhD was very lean heavily towards biological applications.

78
00:06:01,120 --> 00:06:05,240
And I guess my whole thesis has this whole theme as well.

79
00:06:05,240 --> 00:06:09,360
But pretty early on we realized that well my supervisor and I realized that it's a very

80
00:06:09,360 --> 00:06:13,600
like tough field with a lot of red tape and it's hard to get data, it's hard to get anything

81
00:06:13,600 --> 00:06:14,600
implemented.

82
00:06:14,600 --> 00:06:18,200
There's a lot of like bureaucracy in some areas.

83
00:06:18,200 --> 00:06:21,640
And I didn't like to like struggle with this during my PhD.

84
00:06:21,640 --> 00:06:27,080
So I think halfway through we decided that it might be good to kind of slightly pivot

85
00:06:27,080 --> 00:06:30,600
towards a more theoretical kind of thesis.

86
00:06:30,600 --> 00:06:32,880
So it has like both aspects.

87
00:06:32,880 --> 00:06:38,680
But I feel like I have a strong opinion that like it's the biological field because it has

88
00:06:38,680 --> 00:06:39,680
all these barriers.

89
00:06:39,680 --> 00:06:42,760
We need to like put a lot more effort into it.

90
00:06:42,760 --> 00:06:47,200
So I'm quite happy that I could kind of put a lot more effort into that kind of side.

91
00:06:47,200 --> 00:06:48,200
Yeah.

92
00:06:48,200 --> 00:06:51,200
And you mentioned that you're planning to start a company.

93
00:06:51,200 --> 00:06:56,160
Is it based on the work that you've done at Cambridge?

94
00:06:56,160 --> 00:06:57,160
So not really.

95
00:06:57,160 --> 00:07:04,520
It's it's based on Cambridge in a way because what I had to because I'm from South Africa,

96
00:07:04,520 --> 00:07:09,400
what I had to do quite often in Cambridge is do like a Skype call back home or a WhatsApp

97
00:07:09,400 --> 00:07:12,000
call or all of these internet-based calls.

98
00:07:12,000 --> 00:07:17,160
And like after the PhD, well, I guess like even before the PhD, I knew I wanted to try

99
00:07:17,160 --> 00:07:19,000
and start a company.

100
00:07:19,000 --> 00:07:24,920
And having like I'm sure you and a lot of other people have experienced the like the

101
00:07:24,920 --> 00:07:29,960
poor quality in any internet-based call like Skype or a WhatsApp voice call.

102
00:07:29,960 --> 00:07:33,680
And essentially I thought well why don't we just try and fix this like machine learning

103
00:07:33,680 --> 00:07:39,040
currently has has achieved remarkable things in terms of generating super-resolution images

104
00:07:39,040 --> 00:07:41,880
and like doing video interpolation.

105
00:07:41,880 --> 00:07:47,480
So essentially that's kind of how how my colleague and I started out with the new idea that

106
00:07:47,480 --> 00:07:48,800
we're trying to pursue.

107
00:07:48,800 --> 00:07:52,760
You're not calling a pie-piper are you?

108
00:07:52,760 --> 00:07:57,000
I'm glad you made that connection.

109
00:07:57,000 --> 00:08:00,760
That's probably a good thing.

110
00:08:00,760 --> 00:08:08,240
One of the papers that came out of your PhD is called the unreasonable effectiveness of

111
00:08:08,240 --> 00:08:09,400
the forget gate.

112
00:08:09,400 --> 00:08:11,000
How did that work come about?

113
00:08:11,000 --> 00:08:12,000
Yeah.

114
00:08:12,000 --> 00:08:15,120
So well, maybe I'll just quickly say where the name came from.

115
00:08:15,120 --> 00:08:20,400
I have to give homage to AndrÃ© Carpathy, who he had a blog post called the unreasonable

116
00:08:20,400 --> 00:08:24,320
effectiveness of RNNs, Ricardo Networks.

117
00:08:24,320 --> 00:08:27,400
And I think a lot of people really love this blog post and I also love it.

118
00:08:27,400 --> 00:08:28,400
I read it.

119
00:08:28,400 --> 00:08:31,520
So that's kind of where the name inspiration came from.

120
00:08:31,520 --> 00:08:38,360
But to to jump back to the question, essentially in the the final parts of my PhD, I started

121
00:08:38,360 --> 00:08:42,120
working a bit like I looked at two things at the same time.

122
00:08:42,120 --> 00:08:47,960
One was this biological application that we had, which was to essentially infer from peripheral

123
00:08:47,960 --> 00:08:55,120
neural signals what an agent or what a human or some animal is doing based on those signals.

124
00:08:55,120 --> 00:09:00,760
And essentially if you want to create a prosthetic device that can like in real time react to these

125
00:09:00,760 --> 00:09:04,480
neural signals, it has to be very low powered and very resource efficient.

126
00:09:04,480 --> 00:09:09,640
So we'd already had a solution that could infer what the human is doing based on those

127
00:09:09,640 --> 00:09:10,640
signals.

128
00:09:10,640 --> 00:09:15,080
But now we needed to make it much more computationally efficient.

129
00:09:15,080 --> 00:09:17,600
So that was like the one driver.

130
00:09:17,600 --> 00:09:22,200
The second or the thing I was working on in parallel was I looked at initialization techniques

131
00:09:22,200 --> 00:09:28,200
for in general kind of, I guess neural networks or deep learning models, but also specific

132
00:09:28,200 --> 00:09:34,240
more specifically for LSTMs or the long short term memory recurrent neural network.

133
00:09:34,240 --> 00:09:44,200
And I guess I played around with various approaches to initialization for LSTMs and with ways

134
00:09:44,200 --> 00:09:49,360
of like I guess quantizing them or pruning them or making them more efficient.

135
00:09:49,360 --> 00:09:55,680
And then I happened to stumble upon this architecture that I've explained in the paper, which

136
00:09:55,680 --> 00:10:01,320
essentially did like, first of all, we were surprised, okay, it kind of saves computation.

137
00:10:01,320 --> 00:10:06,040
But then yeah, we were really surprised when it did a lot better than the original LSTM.

138
00:10:06,040 --> 00:10:07,040
So yeah.

139
00:10:07,040 --> 00:10:08,040
Interesting.

140
00:10:08,040 --> 00:10:12,520
And before we get too far, I should probably note because there's somebody that's jumping

141
00:10:12,520 --> 00:10:18,480
up out of their chair on the on the bar or something like that, that the whole unreasonable

142
00:10:18,480 --> 00:10:24,920
effectiveness thing dates back before Carpathy to a guy Eugene Wigner who wrote a paper,

143
00:10:24,920 --> 00:10:30,160
the unreasonable effectiveness of mathematics and the natural sciences back in 1959.

144
00:10:30,160 --> 00:10:34,960
But we were joking a little bit before we started the interview that I noted that I've

145
00:10:34,960 --> 00:10:42,560
gone through the math of RNNs several times and it's not quite intuitive to me.

146
00:10:42,560 --> 00:10:48,200
You mentioned that you have the same experience having studied it for your degree as well.

147
00:10:48,200 --> 00:10:55,200
Maybe a good place to start is to talk about the, or LSTMs in particular, the role of

148
00:10:55,200 --> 00:10:57,200
gates in LSTMs?

149
00:10:57,200 --> 00:10:58,200
Yeah, yeah.

150
00:10:58,200 --> 00:11:02,320
So I guess I could give a little history of how it all developed.

151
00:11:02,320 --> 00:11:07,920
Essentially recurrent neural networks were designed because they share parameters across

152
00:11:07,920 --> 00:11:09,440
this time dimension.

153
00:11:09,440 --> 00:11:11,160
So it makes them a lot more efficient.

154
00:11:11,160 --> 00:11:16,120
Kind of like the same way that convolutional neural networks share parameters across the

155
00:11:16,120 --> 00:11:19,280
spatial dimension, the 2D spatial dimension.

156
00:11:19,280 --> 00:11:23,600
But one problem you run into with recurrent neural networks is that because you're kind

157
00:11:23,600 --> 00:11:29,600
of over time you're multiplying the same weights or the same information with each other the

158
00:11:29,600 --> 00:11:34,680
whole time, you run into, you usually run into gradient problems.

159
00:11:34,680 --> 00:11:41,920
And back in, I think it's 97, Hawthorite Renshmit Hoover realized that, okay, I guess they

160
00:11:41,920 --> 00:11:48,680
approach it from a slightly different direction, but they thought that, or argue that you have,

161
00:11:48,680 --> 00:11:52,960
when you do back propagation, you have through a recurrent neural network.

162
00:11:52,960 --> 00:11:56,400
Some of the, like if you, if you want to remember something and you want to forget something

163
00:11:56,400 --> 00:12:00,800
of the input sequence, there's kind of conflicting updates through the same edges.

164
00:12:00,800 --> 00:12:05,480
So they thought or proposed that if you use an input and output gate, this could kind

165
00:12:05,480 --> 00:12:10,480
of solve that conflict and kind of protect, so in there are an answer, it could kind of

166
00:12:10,480 --> 00:12:16,760
protect the cell or the memory from these conflicting updates.

167
00:12:16,760 --> 00:12:18,000
And that's worked pretty well.

168
00:12:18,000 --> 00:12:27,320
And then I think a few years later, it was Gares in 2000 who, he realized that this works

169
00:12:27,320 --> 00:12:35,720
well, but you can have, if the memory cell of the RNN or the, like, I guess the state doesn't

170
00:12:35,720 --> 00:12:40,240
have a mechanism of forgetting some of the information, there's a possibility that it could

171
00:12:40,240 --> 00:12:44,720
grow indefinitely and kind of break down the network.

172
00:12:44,720 --> 00:12:50,560
So he proposed a forget gate, and this would then allow the cell to kind of forget some

173
00:12:50,560 --> 00:12:53,280
of the information over time.

174
00:12:53,280 --> 00:12:56,320
And that's kind of the LSTM that we know today.

175
00:12:56,320 --> 00:13:02,840
That's kind of how the gates help the LSTM to prevent these gradient problems during training.

176
00:13:02,840 --> 00:13:03,840
Got it.

177
00:13:03,840 --> 00:13:11,080
So forget gate is part of the typical LSTM that is commonly in use nowadays.

178
00:13:11,080 --> 00:13:12,080
Yes.

179
00:13:12,080 --> 00:13:17,920
So the typical LSTM has three gates, the input gate, which controls how much information

180
00:13:17,920 --> 00:13:19,920
is input at each time step.

181
00:13:19,920 --> 00:13:24,120
The output gate, which controls how much information is output to the next cell or to your output

182
00:13:24,120 --> 00:13:26,000
layer at each time step.

183
00:13:26,000 --> 00:13:29,880
And then the forget gate, which essentially just says, how much should I forget at each

184
00:13:29,880 --> 00:13:31,160
time, time step?

185
00:13:31,160 --> 00:13:36,720
I'm trying to get that kind of your observations about the forget gate and what kind of caused

186
00:13:36,720 --> 00:13:43,040
you to start looking at that as an interesting part of this approach.

187
00:13:43,040 --> 00:13:48,200
In 2015, there were two papers that kind of at the same time said, or concluded that the

188
00:13:48,200 --> 00:13:52,080
forget gate is the most important part of the LSTM.

189
00:13:52,080 --> 00:13:56,200
They did like these ablation studies that removed the few gates at a time and they kind

190
00:13:56,200 --> 00:14:02,280
of found that every time you remove the forget gate, like the performance just drops drastically.

191
00:14:02,280 --> 00:14:07,200
And this was, I think, just aphowitz and a graph.

192
00:14:07,200 --> 00:14:09,760
And essentially, that was kind of my starting point.

193
00:14:09,760 --> 00:14:14,400
I realized, like, okay, let's, if they say it's this important, let's remove everything

194
00:14:14,400 --> 00:14:18,920
we can and just keep the forget gate and see where we go from there.

195
00:14:18,920 --> 00:14:23,960
And that was kind of, that was, yeah, it had decent performance.

196
00:14:23,960 --> 00:14:29,880
But like on a typical data set called MNIST, that I think everyone knows this pretty well

197
00:14:29,880 --> 00:14:31,360
now.

198
00:14:31,360 --> 00:14:37,640
If you kind of, if you process that in scanline order, you get many subsections that are

199
00:14:37,640 --> 00:14:40,880
like have 10 or 20 consecutive zeros.

200
00:14:40,880 --> 00:14:44,720
And essentially, this makes it super hard for the network if it's only forget gate to

201
00:14:44,720 --> 00:14:48,960
remember anything by the end of those zero, like 10 or 20 zeros.

202
00:14:48,960 --> 00:14:53,160
So that's where I had to like, I realized, okay, we need to kind of initialize this better

203
00:14:53,160 --> 00:14:58,760
to be able to kind of retain memory over those periods of zero.

204
00:14:58,760 --> 00:15:01,480
You know, I guess what's interesting about this is that it's counterintuitive.

205
00:15:01,480 --> 00:15:07,520
So you use the forget gate and the memory, but not the input and output gates.

206
00:15:07,520 --> 00:15:12,680
And found that you had this performance improvement.

207
00:15:12,680 --> 00:15:13,680
Yeah, yeah.

208
00:15:13,680 --> 00:15:16,680
So essentially, I saw this example recently.

209
00:15:16,680 --> 00:15:20,600
So if you like take an image and you want to classify what's in it, essentially what

210
00:15:20,600 --> 00:15:24,960
like a network or what you can visualize it as is like, you remove every single element

211
00:15:24,960 --> 00:15:27,880
of that image except the part that you care about.

212
00:15:27,880 --> 00:15:31,280
For instance, if it's a panda, you remove like all the buildings, branches and like trees

213
00:15:31,280 --> 00:15:32,280
behind it.

214
00:15:32,280 --> 00:15:34,680
And you just have to the panda face or something.

215
00:15:34,680 --> 00:15:38,480
So essentially like that could be summarized as to learn something, you just need to know

216
00:15:38,480 --> 00:15:41,320
what you need to forget.

217
00:15:41,320 --> 00:15:45,920
And I guess like that's kind of kind of in a way what happens here, but it's not completely

218
00:15:45,920 --> 00:15:51,480
true because we have only one gate, which is the forget gate, but this gate at the moment

219
00:15:51,480 --> 00:15:56,760
or in the new network controls both how much information is forgotten and also how much

220
00:15:56,760 --> 00:15:58,040
new information comes in.

221
00:15:58,040 --> 00:16:04,320
So it's kind of it's coupled with this gate just like performing both of those roles.

222
00:16:04,320 --> 00:16:11,240
Is that observation that learning is primarily about forgetting at odds with the emphasis

223
00:16:11,240 --> 00:16:17,400
on attention-based methods that are, you seem less about forgetting and more about trying

224
00:16:17,400 --> 00:16:19,200
to figure out what to remember?

225
00:16:19,200 --> 00:16:24,000
I think it's all just semantics in a way, it's just how you phrase or perceive it, yeah.

226
00:16:24,000 --> 00:16:28,680
So it could be either way and I think attention mechanisms could also be said as like, oh

227
00:16:28,680 --> 00:16:32,360
well attention mechanisms, just make sure that you kind of forget about the rest and focus

228
00:16:32,360 --> 00:16:35,360
on something that's like relevant.

229
00:16:35,360 --> 00:16:37,360
So it's just how you perceive it, I guess.

230
00:16:37,360 --> 00:16:42,280
I'm curious, have you tried applying them in concert with one another, you know, I'm

231
00:16:42,280 --> 00:16:45,320
thinking of forgetting all the way down?

232
00:16:45,320 --> 00:16:46,320
All right.

233
00:16:46,320 --> 00:16:47,320
No, I haven't.

234
00:16:47,320 --> 00:16:53,080
So you mean like applying this new network with attention mechanisms?

235
00:16:53,080 --> 00:16:54,080
Right.

236
00:16:54,080 --> 00:16:58,040
No, I haven't tried that, but that's a pretty cool idea.

237
00:16:58,040 --> 00:17:06,440
Based on this, you were inspired to create another network architecture called Janet.

238
00:17:06,440 --> 00:17:07,440
Yeah.

239
00:17:07,440 --> 00:17:09,720
What is Janet about?

240
00:17:09,720 --> 00:17:13,800
So Janet is essentially the network that we've been discussing now.

241
00:17:13,800 --> 00:17:14,800
Okay.

242
00:17:14,800 --> 00:17:15,800
Yeah.

243
00:17:15,800 --> 00:17:21,280
So the Janet is just the name I gave for this new, I guess, way of processing information

244
00:17:21,280 --> 00:17:24,920
or this, I guess, simplified version of the Alistair.

245
00:17:24,920 --> 00:17:28,120
We chose the name kind of in a lab meeting.

246
00:17:28,120 --> 00:17:32,320
I don't know if I'm allowed to say this, but I'll try.

247
00:17:32,320 --> 00:17:38,920
Essentially, I don't think that this is like a network that is the best.

248
00:17:38,920 --> 00:17:43,560
And because people make so many new networks every day, it's just like, this is just another

249
00:17:43,560 --> 00:17:44,560
one of those.

250
00:17:44,560 --> 00:17:46,400
So it's just another network in that sense.

251
00:17:46,400 --> 00:17:52,560
But it happened to also be a nice little acronym for Yosses Awesome Network.

252
00:17:52,560 --> 00:17:55,400
So we kind of have this little lab joke about it, but yeah.

253
00:17:55,400 --> 00:17:56,400
Okay.

254
00:17:56,400 --> 00:18:03,160
I think what threw me off in thinking that it was a different network was the pictures

255
00:18:03,160 --> 00:18:08,120
that I've seen of it still have these other gates in them, or at least what I look like

256
00:18:08,120 --> 00:18:13,520
to me, these other gates, am I reading those pictures incorrectly?

257
00:18:13,520 --> 00:18:16,880
What pictures have you seen?

258
00:18:16,880 --> 00:18:18,880
So, yeah.

259
00:18:18,880 --> 00:18:25,040
So the gates in the Janet are essentially, it's just one gate, which is the forget gate

260
00:18:25,040 --> 00:18:26,040
of the Alistair.

261
00:18:26,040 --> 00:18:30,640
So it's like, the Janet is just the LSTM with every thing I move except for the forget

262
00:18:30,640 --> 00:18:31,640
gate.

263
00:18:31,640 --> 00:18:39,640
So your paper was kind of presenting an analysis of this network and its performance on MNIST

264
00:18:39,640 --> 00:18:43,040
and potentially other data sets?

265
00:18:43,040 --> 00:18:44,040
Yeah.

266
00:18:44,040 --> 00:18:50,680
So I guess we, there are a few papers who like that bring up new types of recurrent neural

267
00:18:50,680 --> 00:18:51,680
networks.

268
00:18:51,680 --> 00:18:57,240
This memory problem, or I guess both the memory problem over very long sequences and the

269
00:18:57,240 --> 00:19:00,880
problem of exploding and vanishing gradients is quite a, quite a big problem.

270
00:19:00,880 --> 00:19:07,680
So you have quite a few of these papers that just kind of have these benchmark memory tests.

271
00:19:07,680 --> 00:19:09,520
And that's kind of what we try to follow over here.

272
00:19:09,520 --> 00:19:14,760
So one of them is MNIST, which has this hard problem, as I mentioned before, of like consecutive

273
00:19:14,760 --> 00:19:15,760
zeros.

274
00:19:15,760 --> 00:19:20,840
Then there's perturbed MNIST, which we also tested on, which essentially just does a random

275
00:19:20,840 --> 00:19:26,840
permutation of all the pixels in the MNIST image.

276
00:19:26,840 --> 00:19:31,760
And this kind of creates longer dependencies over time if you process this in scanline order.

277
00:19:31,760 --> 00:19:33,440
And then there are a few others.

278
00:19:33,440 --> 00:19:36,840
The two we test is the ad and the copy task.

279
00:19:36,840 --> 00:19:42,880
And essentially it's just for the ad task, it's two sequences where one is zeros and ones.

280
00:19:42,880 --> 00:19:45,440
And the other one is continuous values.

281
00:19:45,440 --> 00:19:51,680
And wherever you have ones in the binary sequence, you need to add those corresponding numbers

282
00:19:51,680 --> 00:19:54,960
in the continuous valued sequence.

283
00:19:54,960 --> 00:19:58,760
So it's like it kind of tests the memory of this network.

284
00:19:58,760 --> 00:20:02,760
And then the same for the copy task just needs to remember everything at the start of the

285
00:20:02,760 --> 00:20:07,520
sequence all the way to the end and be able to regenerate that at the end.

286
00:20:07,520 --> 00:20:12,600
So yeah, and then obviously we've tested it on a few other datasets since then.

287
00:20:12,600 --> 00:20:18,080
For the paper, we also tested it on one of my biological datasets called MIT, B-I-H,

288
00:20:18,080 --> 00:20:24,320
arrhythmia dataset, which is like a dataset of ECG heartbeats that are classified into

289
00:20:24,320 --> 00:20:27,320
five different types of heartbeats arrhythmias.

290
00:20:27,320 --> 00:20:30,440
And it worked better on that dataset as well.

291
00:20:30,440 --> 00:20:34,680
And when you say better relative to whatever the state of the art is on these datasets or

292
00:20:34,680 --> 00:20:38,480
relative to vanilla LSTMs.

293
00:20:38,480 --> 00:20:46,960
So for the dataset, I mean the MIT, the ECG dataset, there aren't that many, I guess, state

294
00:20:46,960 --> 00:20:47,960
of the art results.

295
00:20:47,960 --> 00:20:51,600
So we just compared it to the LSTM and it does better than the LSTM.

296
00:20:51,600 --> 00:20:54,360
On the other datasets, it's kind of, it's harder to say.

297
00:20:54,360 --> 00:20:58,840
So in all cases, the Janet does better than the LSTM on those datasets.

298
00:20:58,840 --> 00:21:03,040
But then there are like different, I guess, models that do even better than the Janet.

299
00:21:03,040 --> 00:21:07,120
So like the wave net does, I'm sure you're familiar with there.

300
00:21:07,120 --> 00:21:11,160
Yeah, the wave net is a new, very good model by DeepMind.

301
00:21:11,160 --> 00:21:15,600
And that model does quite well on perturbed MNIST, but then does slightly worse than Janet

302
00:21:15,600 --> 00:21:17,400
on the normal MNIST.

303
00:21:17,400 --> 00:21:22,040
And yeah, you have quite a few of the similar nuances for other models that have also been

304
00:21:22,040 --> 00:21:23,040
proposed.

305
00:21:23,040 --> 00:21:29,600
What was the role of the way you initialized the network in the results you saw?

306
00:21:29,600 --> 00:21:33,680
For the Janet to work on datasets like MNIST, you have to initialize it using this

307
00:21:33,680 --> 00:21:37,400
Corona initialization scheme that we proposed in the paper.

308
00:21:37,400 --> 00:21:42,120
But the initialization scheme was initially proposed by Talek and Oliver.

309
00:21:42,120 --> 00:21:48,160
It's also a really, really cool paper in 2018, published that I clear.

310
00:21:48,160 --> 00:21:52,360
But so if you don't use that initialization scheme for MNIST, it would, like, the Janet

311
00:21:52,360 --> 00:21:53,360
wouldn't learn anything.

312
00:21:53,360 --> 00:21:59,240
You would kind of have this accuracy around 5% throughout training for, and then for

313
00:21:59,240 --> 00:22:03,600
tasks that are slightly easier to learn that don't have, like, long consecutive periods

314
00:22:03,600 --> 00:22:09,640
of zero, the Janet, like, can still learn, but it definitely does a lot better if you

315
00:22:09,640 --> 00:22:12,120
use this initialization scheme.

316
00:22:12,120 --> 00:22:17,880
For the LSTM, you also get some benefits, I guess, like, more stability during training

317
00:22:17,880 --> 00:22:24,440
if you use Corona initialization, but the benefits aren't as clear as it is for the Janet.

318
00:22:24,440 --> 00:22:26,920
And how does the initialization scheme work?

319
00:22:26,920 --> 00:22:31,800
Yeah, so essentially, what Talek and Oliver found was that.

320
00:22:31,800 --> 00:22:40,040
If you, or the, I guess, the characteristic for getting time of the RNN is, sorry, of

321
00:22:40,040 --> 00:22:44,080
the LSTM, is one over the Forgetgate value.

322
00:22:44,080 --> 00:22:49,240
So at the start, if you're, if we initialize it according to the way we always initialize

323
00:22:49,240 --> 00:22:55,520
for Getgate, you essentially have a really small value for the Forgetgate or relatively

324
00:22:55,520 --> 00:22:57,080
small value.

325
00:22:57,080 --> 00:23:01,880
This means that you're forgetting the amount of time steps you can go through before you

326
00:23:01,880 --> 00:23:07,920
forget everything that you, that you saw before it is, like, is limited to probably four

327
00:23:07,920 --> 00:23:10,280
or five time steps.

328
00:23:10,280 --> 00:23:16,440
And this is problematic if you have, if you need to kind of retain memory over longer periods.

329
00:23:16,440 --> 00:23:20,440
And they kind of said, well, okay, we can, we can be smarter about how we initialize the

330
00:23:20,440 --> 00:23:26,640
Forgetgate, well, all the Gates, in fact, and that is essentially to, like, at the start

331
00:23:26,640 --> 00:23:32,560
of training, the values that matter a lot for, for the LSTM is the, the biases.

332
00:23:32,560 --> 00:23:36,760
And initially, or traditionally, we've always just initialize them to be zero.

333
00:23:36,760 --> 00:23:43,360
But Talek and Oliver kind of said, we should initialize them as the log of a uniform distribution

334
00:23:43,360 --> 00:23:47,040
between one and the maximum number of time steps we have.

335
00:23:47,040 --> 00:23:53,200
And that kind of gives you that, make sure that some of the Forgetgate kind of allow

336
00:23:53,200 --> 00:23:58,400
the network to remember very long periods up to the end of the sequence.

337
00:23:58,400 --> 00:24:02,600
And some of them are still, like, short term kind of cells, which kind of can't forget

338
00:24:02,600 --> 00:24:05,160
at every second or third step.

339
00:24:05,160 --> 00:24:12,600
What's the intuition for why this specific kind, well, first of all, did you try other initialization

340
00:24:12,600 --> 00:24:15,760
schemes with Janet?

341
00:24:15,760 --> 00:24:22,640
And if so, and there was kind of a binary, you know, work didn't work kind of situation.

342
00:24:22,640 --> 00:24:27,960
What's your intuition for why this one, you know, performed, whereas others didn't?

343
00:24:27,960 --> 00:24:28,960
Yeah.

344
00:24:28,960 --> 00:24:33,000
So I did, I did try a few other initialization schemes.

345
00:24:33,000 --> 00:24:37,000
I'm sure I've forgotten a few of them as well, because this was kind of, you're done quite

346
00:24:37,000 --> 00:24:38,000
a while ago.

347
00:24:38,000 --> 00:24:43,320
But the, like, one of the things I tried quite a lot is, or I saw from, I saw from

348
00:24:43,320 --> 00:24:49,920
Alchreiter and Schmidt Hooper that they tried this thing in 97 as well, which they just,

349
00:24:49,920 --> 00:24:53,120
they randomly guess some of the initial parameters between, like, certain values.

350
00:24:53,120 --> 00:24:57,640
And if you do that enough, you can actually get a network that does as well as a trained

351
00:24:57,640 --> 00:25:02,840
one without actually ever training it, you just initialize it to this perfect, like, network.

352
00:25:02,840 --> 00:25:07,480
I tried something similar where I would, I know kind of what our current initialization

353
00:25:07,480 --> 00:25:15,080
schemes are, and I would kind of guess random parameters in those ranges and see, kind

354
00:25:15,080 --> 00:25:21,320
of, if this kind of yields any better networks that you can then train from or, like, improve

355
00:25:21,320 --> 00:25:22,320
from.

356
00:25:22,320 --> 00:25:27,760
But this unfortunately didn't work or it gave the same results or, like, the same performance

357
00:25:27,760 --> 00:25:31,080
as you would get with the normal other steam initialization.

358
00:25:31,080 --> 00:25:36,240
And then beyond that, I, yeah, I don't think I can't remember anything else I tried.

359
00:25:36,240 --> 00:25:44,520
And how did you contextualize this within the sphere of biological applications for your

360
00:25:44,520 --> 00:25:45,520
thesis?

361
00:25:45,520 --> 00:25:50,920
Yeah, so I think I mentioned before that we were like, we were searching for a variant

362
00:25:50,920 --> 00:25:56,320
of the LSTM that is computationally more efficient than just the normal one.

363
00:25:56,320 --> 00:25:58,480
So there are various ways you can do this.

364
00:25:58,480 --> 00:26:04,840
Some of them are, like, it's called pruning, which I, it comes from way back.

365
00:26:04,840 --> 00:26:11,160
And another one is quantization, where essentially you just, you kind of quantize the weights to

366
00:26:11,160 --> 00:26:17,160
be kind of, yeah, certain values that don't take up too much memory and computation.

367
00:26:17,160 --> 00:26:22,800
And then I guess, like, I was kind of impressed by the gated recurrent unit, which is like

368
00:26:22,800 --> 00:26:28,200
another version of recurrent neural networks, which only used two gates instead of the three

369
00:26:28,200 --> 00:26:29,360
that the LSTM uses.

370
00:26:29,360 --> 00:26:33,840
So I was like, oh, can we do, can we do one better than the GRU?

371
00:26:33,840 --> 00:26:37,760
And we just have one gate and still do as well.

372
00:26:37,760 --> 00:26:38,760
So yeah.

373
00:26:38,760 --> 00:26:42,120
And then sorry, I just remembered something from the previous question, which is in terms

374
00:26:42,120 --> 00:26:48,520
of an initialization, one thing that I kind of found at the end was when we started using

375
00:26:48,520 --> 00:26:54,680
more units in the layers of the Janet and the LSTM, we found that the Janet kind of, instead

376
00:26:54,680 --> 00:26:59,160
of just doing better at the end, it kind of also learned a lot quicker.

377
00:26:59,160 --> 00:27:05,520
So it would get to the highest point of accuracy a lot quicker than a word with less units,

378
00:27:05,520 --> 00:27:07,600
which is kind of, yeah, that makes sense.

379
00:27:07,600 --> 00:27:11,920
But then I thought like a really cool thing we can try is to play around with this as

380
00:27:11,920 --> 00:27:16,240
in like, it's kind of, if you have a really big Janet, you can kind of just guess the right

381
00:27:16,240 --> 00:27:20,760
solution or very close to the right solution from the start and then remove a lot of units

382
00:27:20,760 --> 00:27:25,800
to say compensation and then just train the rest to kind of give you that perfect model.

383
00:27:25,800 --> 00:27:30,800
So that's something in terms of an initialization that I think is also worth pursuing.

384
00:27:30,800 --> 00:27:35,560
Is that a potential future direction or did you get specific results with that?

385
00:27:35,560 --> 00:27:37,120
That's a potential future direction.

386
00:27:37,120 --> 00:27:41,560
I have, yeah, I've been slightly busy with other things since then, but that's something

387
00:27:41,560 --> 00:27:42,560
I want to play with.

388
00:27:42,560 --> 00:27:43,560
Got it.

389
00:27:43,560 --> 00:27:52,440
What's the, in general, the impact of getting rid of the other gates on the computational

390
00:27:52,440 --> 00:27:56,000
intensity of training one of these networks?

391
00:27:56,000 --> 00:27:59,960
In terms of like computational time, it doesn't improve things that much.

392
00:27:59,960 --> 00:28:05,840
And also I guess in just the feed forward processing time during testing, because a lot of

393
00:28:05,840 --> 00:28:11,040
your like computational time is limited by the sequential nature of the LSTM, which in

394
00:28:11,040 --> 00:28:15,240
the case of the LSTM and the Janet are essentially the same, because you need to like process

395
00:28:15,240 --> 00:28:17,440
things at every time step.

396
00:28:17,440 --> 00:28:23,080
But the part where it does save, which also saves on I guess battery usage if this is a device

397
00:28:23,080 --> 00:28:29,560
that's to be used in a portable device, sorry, a model to be used in a portable device.

398
00:28:29,560 --> 00:28:34,240
Then having using less memory is quite helpful.

399
00:28:34,240 --> 00:28:40,760
So the Janet uses half the memory that the LSTM uses and kind of gets better accuracies

400
00:28:40,760 --> 00:28:43,920
for the datasets that we tested on, which is quite good.

401
00:28:43,920 --> 00:28:49,960
And that also meant that we could train much bigger Janet's on the same GPU, whereas

402
00:28:49,960 --> 00:28:54,760
for an LSTM, you would quickly run out of memory on the GPU.

403
00:28:54,760 --> 00:29:02,960
Did you look at the same memory and compute and power implications from an inference perspective

404
00:29:02,960 --> 00:29:04,920
with the train Janet model?

405
00:29:04,920 --> 00:29:06,560
Is there any difference there?

406
00:29:06,560 --> 00:29:09,720
No, so yeah, so I'm like, it's the same on that side.

407
00:29:09,720 --> 00:29:17,840
So for both like training a model and for doing inference, I guess it saves half the parameters

408
00:29:17,840 --> 00:29:21,040
and computational time is roughly the same.

409
00:29:21,040 --> 00:29:24,200
It's slightly shorter for the Janet, but nothing significant.

410
00:29:24,200 --> 00:29:32,280
You noted that you, you know, that compared to WaveNet this, you know, this underperforms.

411
00:29:32,280 --> 00:29:38,480
But is there a place for it within the domain of general purpose mobile network?

412
00:29:38,480 --> 00:29:44,480
So is it, you know, do you, are there other reasons why it's kind of a research experiment

413
00:29:44,480 --> 00:29:46,720
and not necessarily practical?

414
00:29:46,720 --> 00:29:50,000
It's hard to say off the bat.

415
00:29:50,000 --> 00:29:55,440
I think, yeah, I don't know, this, I guess at the time very much trapped in this very

416
00:29:55,440 --> 00:29:58,600
research focused, I guess, domain.

417
00:29:58,600 --> 00:30:02,280
I think like it's not the ultimate best solution that there is yet.

418
00:30:02,280 --> 00:30:06,960
I think it's just, it was kind of interesting to show that kind of, this is how much importance

419
00:30:06,960 --> 00:30:09,840
the forget gate has in this network.

420
00:30:09,840 --> 00:30:12,080
It's kind of interesting to see.

421
00:30:12,080 --> 00:30:18,040
I think there, yeah, for each specific problem, you would have to have a specific like network

422
00:30:18,040 --> 00:30:22,480
and a specific solution that's best suited for that problem.

423
00:30:22,480 --> 00:30:25,240
There are some where the Janet could be, could be that.

424
00:30:25,240 --> 00:30:30,880
I think specifically in like anything where you have very long term memory requirements

425
00:30:30,880 --> 00:30:33,480
and only a single output at the end.

426
00:30:33,480 --> 00:30:39,960
We kind of have that requirement plus a, I guess, a resource efficiency requirement.

427
00:30:39,960 --> 00:30:45,520
So yeah, I think like the wave net is quite like, it's quite a large network in terms

428
00:30:45,520 --> 00:30:48,960
of number of layers, but it uses very little parameters.

429
00:30:48,960 --> 00:30:52,680
So yeah, there are pros and cons in both.

430
00:30:52,680 --> 00:30:59,040
Are there specific examples that come to mind of scenarios where you'd have this, you

431
00:30:59,040 --> 00:31:02,040
know, long sequence in time with a single output?

432
00:31:02,040 --> 00:31:07,240
Yeah, so I guess like a lot of biological signals have that feature.

433
00:31:07,240 --> 00:31:13,080
So like, if you have, if you're measuring a heartbeat for a few seconds, that's quite

434
00:31:13,080 --> 00:31:18,800
a lot of time steps that you have to analyze and then you kind of have to make a single

435
00:31:18,800 --> 00:31:21,280
prediction at the end of all those time steps.

436
00:31:21,280 --> 00:31:27,240
So that's like a typical scenario where you would have a really long signal with a single

437
00:31:27,240 --> 00:31:28,560
output at the end.

438
00:31:28,560 --> 00:31:33,360
An example where it's not that like that is, for instance, when you're generating, when

439
00:31:33,360 --> 00:31:39,880
you're doing like translation, you have to generate the, or yeah, maybe just, I guess,

440
00:31:39,880 --> 00:31:41,640
sentence generation.

441
00:31:41,640 --> 00:31:47,000
You have to generate a word at each time step or you have to classify it's part of speech

442
00:31:47,000 --> 00:31:48,280
at each time step.

443
00:31:48,280 --> 00:31:53,240
That's kind of not a very long term dependency, or yeah, a long term dependency with a single

444
00:31:53,240 --> 00:31:56,040
output because you're outputting something at each time step.

445
00:31:56,040 --> 00:32:02,200
Okay, cool, you mentioned in our conversation a couple of things that you'd like to play

446
00:32:02,200 --> 00:32:03,200
with in the future.

447
00:32:03,200 --> 00:32:09,560
Are there areas that you would like to see someone continue working on beyond the ones

448
00:32:09,560 --> 00:32:10,560
we've discussed?

449
00:32:10,560 --> 00:32:12,320
No, I think I mentioned all of them.

450
00:32:12,320 --> 00:32:17,960
I think just applying this to more datasets, specifically datasets with long term dependencies

451
00:32:17,960 --> 00:32:21,040
and a single output at the end, that's cool.

452
00:32:21,040 --> 00:32:26,000
So if anyone can get more types of data than are like that, I think that would be really

453
00:32:26,000 --> 00:32:27,680
interesting to see.

454
00:32:27,680 --> 00:32:32,520
And then obviously also like playing around with this new initialization feature where

455
00:32:32,520 --> 00:32:39,440
you can kind of almost, in the first instance, guess the correct, well, very close to the

456
00:32:39,440 --> 00:32:44,720
correct network and then just do a few more steps of training to get there, like a K-shop

457
00:32:44,720 --> 00:32:46,280
learning approach.

458
00:32:46,280 --> 00:32:48,480
I think those would be interesting to see.

459
00:32:48,480 --> 00:32:49,480
Cool, interesting.

460
00:32:49,480 --> 00:32:52,240
Well, thanks for taking the time to chat with me about this.

461
00:32:52,240 --> 00:32:53,240
Cool stuff.

462
00:32:53,240 --> 00:32:54,240
Anytime.

463
00:32:54,240 --> 00:32:55,240
Thank you.

464
00:32:55,240 --> 00:33:01,000
All right, everyone, that's our show for today.

465
00:33:01,000 --> 00:33:07,240
For more information on Yoss or any of the topics covered in this episode, visit twomelai.com

466
00:33:07,240 --> 00:33:10,640
slash talk slash 240.

467
00:33:10,640 --> 00:33:27,800
As always, thanks so much for listening and catch you next time.

