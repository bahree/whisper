1
00:00:00,000 --> 00:00:11,720
All right, everyone. Welcome to another episode of the Twomel AI podcast. I am, of course,

2
00:00:11,720 --> 00:00:18,640
your host, Sam Charington. And today I'm joined by Shayan Mahanti, CEO of Watchful. Before

3
00:00:18,640 --> 00:00:22,160
we get going, be sure to take a moment to hit that subscribe button wherever you're

4
00:00:22,160 --> 00:00:27,960
listening to today's show. Shayan, we are going to spend some time talking about a topic

5
00:00:27,960 --> 00:00:34,920
that we both enjoy chatting about. So this should be a fun one. Data-centric AI in particular.

6
00:00:34,920 --> 00:00:38,400
But before we do, I'd love to have you share a little bit about your background.

7
00:00:38,400 --> 00:00:43,640
Awesome. Thanks so much for having me, Sam. My name is Shayan. I'm the CEO and co-founder

8
00:00:43,640 --> 00:00:48,320
of a company called Watchful. And we're building the platform for machine teaching, which

9
00:00:48,320 --> 00:00:53,520
is very much like a data-centric AI approach. And I'll talk about that a little bit later,

10
00:00:53,520 --> 00:01:01,920
I assume. But prior to this, I built systems at Facebook. So I was a tech lead for a stream

11
00:01:01,920 --> 00:01:07,400
processing team that processed all the ads-metrics data for all Facebook products. And I led

12
00:01:07,400 --> 00:01:12,480
some machine learning teams there as well. I'm also a guest scientist at Los Alamos National

13
00:01:12,480 --> 00:01:18,560
Labs, where I've given talks ranging from Atomada Theory to machine learning. So really, really

14
00:01:18,560 --> 00:01:24,120
pleased to be here. Yeah, before we get too far into the conversation, give us a quick

15
00:01:24,120 --> 00:01:29,280
overview of Watchful. Yeah. So as I mentioned before, we're building what we call a machine

16
00:01:29,280 --> 00:01:36,320
teaching platform. Really, our goal is to build tools to help people largely automate the

17
00:01:36,320 --> 00:01:42,120
process of labeling data for machine learning. We kind of see that as the biggest bottleneck

18
00:01:42,120 --> 00:01:48,320
to getting machine learning into most organizations, most companies. And we're really trying

19
00:01:48,320 --> 00:01:53,880
to sort of close that gap and make it so that we can increase the number of machine teachers

20
00:01:53,880 --> 00:01:58,920
in the world, so to speak, or allow companies and organizations to solve their hardest problems

21
00:01:58,920 --> 00:02:03,960
using machine learning. When you mentioned that you've seen labeling

22
00:02:03,960 --> 00:02:09,360
be the biggest gap, but I want to ask you to elaborate on that. But at the same time,

23
00:02:09,360 --> 00:02:12,920
it's like we've been talking about that for a long time. It shouldn't be that much of

24
00:02:12,920 --> 00:02:19,200
a surprise. And yet, you know, here we are with Andrew and others having to name this

25
00:02:19,200 --> 00:02:25,040
challenge and talk about data centric AI. You know, share a little bit about your experience

26
00:02:25,040 --> 00:02:29,840
with, you know, how labeling is actually done in organizations and the kind of challenge

27
00:02:29,840 --> 00:02:35,160
that it represents. So I would say that they're kind of like two main ways that labeling

28
00:02:35,160 --> 00:02:41,600
is done today. The first is by crowd. So that's where you get mostly managed vendors. That's

29
00:02:41,600 --> 00:02:46,400
where you used to have mechanical Turk. And now it's, you know, largely upended by the

30
00:02:46,400 --> 00:02:53,600
scales and appens of the world and so on. You can think of that shape of process as you

31
00:02:53,600 --> 00:02:57,800
have some data, you hand your data to an army of humans, and that army of humans labels

32
00:02:57,800 --> 00:03:04,640
it for you and hands it back. That works really well for tasks that require almost no context.

33
00:03:04,640 --> 00:03:10,240
So you want to do things like box stop signs or box pedestrians or you want to say, you

34
00:03:10,240 --> 00:03:16,920
know, get some sentiment on some tweets or something like that. That works really well.

35
00:03:16,920 --> 00:03:22,040
The second primary way that labeling is done today is by bringing it in house. So that's

36
00:03:22,040 --> 00:03:28,960
where you have vendors that provide interfaces for managing your own army of humans or that's

37
00:03:28,960 --> 00:03:34,160
kind of where we play, which is kind of like in the auto labeling space where the goal

38
00:03:34,160 --> 00:03:40,320
is instead of having an army of humans to begin with, you could have the one or two experts

39
00:03:40,320 --> 00:03:47,120
who are really like the de facto domain sort of like knowledge owners of a particular

40
00:03:47,120 --> 00:03:52,440
task or particular set of problems and have them label the data. But instead of requiring

41
00:03:52,440 --> 00:03:57,560
30 of them to spend a month in a room just labeling data, instead ideally they should

42
00:03:57,560 --> 00:04:01,960
only have to spend maybe four hours across two people and output the same quantity and

43
00:04:01,960 --> 00:04:07,200
the same quality of data, but several orders of magnitude faster. So that's kind of the

44
00:04:07,200 --> 00:04:14,440
space that we play. Now we're seeing more of a shift to kind of like this ladder way

45
00:04:14,440 --> 00:04:20,280
of labeling. What we found is that a lot of the time the things that you'd otherwise want

46
00:04:20,280 --> 00:04:25,760
to use crowd labeling for actually for the most part has been solved by way of these

47
00:04:25,760 --> 00:04:30,840
like foundation models. So now you've got open AI building things like GPT-3 and so

48
00:04:30,840 --> 00:04:36,080
on where you can largely just reuse those models for a lot of like the common tasks that

49
00:04:36,080 --> 00:04:40,600
you'd otherwise be using an army of humans for. Now obviously like in computer vision

50
00:04:40,600 --> 00:04:45,000
there's just like a longer tail on a lot of those tasks so that might not be the case.

51
00:04:45,000 --> 00:04:49,720
But especially in language, you can for the most part these days use a foundation model

52
00:04:49,720 --> 00:04:57,280
for a lot of these common tasks and not have to worry too much about army of human labeling.

53
00:04:57,280 --> 00:05:03,040
What gets really interesting is when you want to fine tune that foundation model on your

54
00:05:03,040 --> 00:05:10,040
task and that then requires bringing that labeling task in house and really trying to formulate

55
00:05:10,040 --> 00:05:14,280
what exactly the data set needs to look like in order to get good performance out of your

56
00:05:14,280 --> 00:05:18,440
system. And that's where data centric AI really can help.

57
00:05:18,440 --> 00:05:22,120
It strikes me that you're making a bit of a leap there that I'd love for you to elaborate

58
00:05:22,120 --> 00:05:27,840
on. Why is it that fine-tuning one of these foundation models requires you bringing

59
00:05:27,840 --> 00:05:31,760
the labeling in house. I suspect it has something to do with this idea of context that you

60
00:05:31,760 --> 00:05:32,760
refer to.

61
00:05:32,760 --> 00:05:38,760
Yeah, exactly. And again like what I'm not going to say is that every use case requires

62
00:05:38,760 --> 00:05:42,440
in house labeling. Like there are a fair number of them that you know you could you could

63
00:05:42,440 --> 00:05:45,920
just farm out to as many people as you'd like because it requires very little context.

64
00:05:45,920 --> 00:05:50,680
But the moment it requires context that is specific to your organization that's when

65
00:05:50,680 --> 00:05:55,200
you must bring it in house or you have to find some way to produce that context externally

66
00:05:55,200 --> 00:06:00,680
for your organization which frankly is like a philosophically hard problem. So that's

67
00:06:00,680 --> 00:06:06,000
where sort of like having as I mentioned you know your one or two experts who really

68
00:06:06,000 --> 00:06:11,080
are like the the de facto standard for how a particular problem is solved inside of

69
00:06:11,080 --> 00:06:12,080
your organization.

70
00:06:12,080 --> 00:06:16,240
If you could sit them down and have them label a very small amount of data and use that

71
00:06:16,240 --> 00:06:22,080
in the fine-tuning process that should yield a better system than if you were to try and

72
00:06:22,080 --> 00:06:26,240
capture that context or knowledge by way of like a set of instructions that you then

73
00:06:26,240 --> 00:06:29,480
hand to a bunch of people who have never done this task before.

74
00:06:29,480 --> 00:06:34,240
That's sort of the idea but then you get into all sorts of sort of interesting pitfalls

75
00:06:34,240 --> 00:06:41,240
where how much data is the right amount of data to get a good system you know like what

76
00:06:41,240 --> 00:06:46,120
parts of my data should I be focused on how much time should I be spending and in what's

77
00:06:46,120 --> 00:06:51,800
more is that these experts generally speaking have other things to do then sit there and

78
00:06:51,800 --> 00:06:56,520
label data all day right you know doctors have doctor things to do lawyers have lawyers

79
00:06:56,520 --> 00:07:02,320
things to do so like what you want to do is make sure that you're using their time as

80
00:07:02,320 --> 00:07:07,040
efficiently as possible and data centric AI is really just like a framework for how to

81
00:07:07,040 --> 00:07:08,720
think about all these things.

82
00:07:08,720 --> 00:07:13,440
And when you think about data centric AI in terms of a framework like how do you parse

83
00:07:13,440 --> 00:07:15,720
out the pieces of it?

84
00:07:15,720 --> 00:07:21,080
So it's kind of like a hard question I can tell you the way I think about it right which

85
00:07:21,080 --> 00:07:27,280
is they're kind of like two primary like there's one big decision you have to make which

86
00:07:27,280 --> 00:07:35,680
is do I want big data or do I want small data that's kind of like again a philosophically

87
00:07:35,680 --> 00:07:39,720
hard question answer but let me let me try and like unpack this.

88
00:07:39,720 --> 00:07:43,880
Not all data is made equal when we're talking about machine learning training a lot of

89
00:07:43,880 --> 00:07:47,640
the time when you're training on several millions of rows or several several million

90
00:07:47,640 --> 00:07:52,600
data points the vast majority of them are actually not moving the needle that much for

91
00:07:52,600 --> 00:07:53,960
a model.

92
00:07:53,960 --> 00:07:58,480
So that begs the question okay if I could just filter out the vast majority of my data

93
00:07:58,480 --> 00:08:02,320
especially so if I can interrupt especially so if you're starting with a pre-trained

94
00:08:02,320 --> 00:08:07,200
model totally that's already been trained on the general exactly right exactly so you

95
00:08:07,200 --> 00:08:12,480
want to like narrow in on on on the specific right but now the question is okay if I have

96
00:08:12,480 --> 00:08:17,520
a small data set like the reason why you might want to train on a larger data set is because

97
00:08:17,520 --> 00:08:21,760
it includes more diversity in that data set or there's more of a chance that it includes

98
00:08:21,760 --> 00:08:25,320
more diversity there's obviously not that guarantee it really depends on the distribution

99
00:08:25,320 --> 00:08:31,240
of your data but there's a higher likelihood that a much larger sample will yield the right

100
00:08:31,240 --> 00:08:35,840
diverse sort of like properties that you'd want in your data set for your model to generalize

101
00:08:35,840 --> 00:08:39,960
well on the other hand if you artificially constrain the size of the data set you have

102
00:08:39,960 --> 00:08:47,920
to be very careful about not introducing unintended bias or lack of diversity in the data because

103
00:08:47,920 --> 00:08:52,040
again you're artificially limiting it so that's sort of like the first question you have

104
00:08:52,040 --> 00:08:56,800
to ask do you have the right make up of data the right make up of techniques to be able

105
00:08:56,800 --> 00:09:02,600
to select just a small portion of the data set such that you still get the same properties

106
00:09:02,600 --> 00:09:06,840
of diverse who you're looking for for generalization of your model and you're not risking

107
00:09:06,840 --> 00:09:13,160
very much on the other hand if your data set or your task requires large amounts of diversity

108
00:09:13,160 --> 00:09:18,280
and you're not confident that you can sub select you know a tiny portion that is both

109
00:09:18,280 --> 00:09:23,120
needle moving and highly diverse you might actually be better off training on a larger data

110
00:09:23,120 --> 00:09:29,600
set but then you need ways to label that larger data set so depending on which direction

111
00:09:29,600 --> 00:09:35,840
you go with that fork you might end up with different techniques you know I'll throw

112
00:09:35,840 --> 00:09:42,120
two examples out there like active learning is a very good technique for the sub selection

113
00:09:42,120 --> 00:09:47,920
and sort of like iteration on very small data sets while something like weak supervision

114
00:09:47,920 --> 00:09:57,320
may be better at larger scale data labeling sort of tasks so depending on which side of

115
00:09:57,320 --> 00:10:02,760
that equation you err on you start going down a different path that then takes you through

116
00:10:02,760 --> 00:10:06,880
several different techniques that you can kind of like pull together like Lego blocks the

117
00:10:06,880 --> 00:10:11,800
yield a very good labeled data set but the properties of that will actually depend on both

118
00:10:11,800 --> 00:10:16,800
your underlying data set type of model you're trying to train and you know kind of the task

119
00:10:16,800 --> 00:10:23,240
at hand now for completeness I'd love to have you explain active learning and weak supervision

120
00:10:23,240 --> 00:10:29,520
but I think before we do that it's interesting and it strikes me as interesting and important

121
00:10:29,520 --> 00:10:36,960
to really emphasize that data centric AI isn't a technique it's like a theme or an idea

122
00:10:36,960 --> 00:10:44,720
and and that is an umbrella for a number of techniques that could help an organization

123
00:10:44,720 --> 00:10:50,720
be more data centric which I guess we haven't defined but yeah yeah and your end calls it

124
00:10:50,720 --> 00:10:57,920
a movement yeah which is pretty much what it is I mean like frankly a lot of these techniques

125
00:10:57,920 --> 00:11:05,080
a lot of these like um processes are not new you know like they've been done for decades

126
00:11:05,080 --> 00:11:10,920
already at several companies the reason why it's an interesting movement is because while

127
00:11:10,920 --> 00:11:17,040
a select few have been able to sort of intuit a lot of this motion just by nature of working

128
00:11:17,040 --> 00:11:23,160
with their data and working with their models many haven't yet and it's a valuable process

129
00:11:23,160 --> 00:11:29,720
to sort of like capture that knowledge and bring it to bear on the rest of the world by

130
00:11:29,720 --> 00:11:35,000
wave like a movement or a framework or set of frameworks so that's I think what makes

131
00:11:35,000 --> 00:11:40,280
data centric AI so interesting it's the fact that it's not a technique it's not a selection

132
00:11:40,280 --> 00:11:45,400
of techniques in fact it's it's really just like a shift in mentality which then means

133
00:11:45,400 --> 00:11:51,320
that like data scientists don't necessarily have to learn brand new types of machine learning

134
00:11:51,320 --> 00:11:54,840
you know you don't have to learn a brand new skill set in order to apply this you really

135
00:11:54,840 --> 00:12:00,440
just have like shift your mentality a little bit and that's what makes it so interesting

136
00:12:00,440 --> 00:12:05,320
so explain active learning for us so active learning takes a lot of different shapes and forms

137
00:12:05,320 --> 00:12:12,200
I think one of the most common shapes is sort of this idea of uncertainty sampling so mechanically

138
00:12:12,200 --> 00:12:19,960
what you do is you have let's say a large data set and maybe some of it is really well

139
00:12:19,960 --> 00:12:25,000
labeled other parts or not what you do is you train a model on the parts that are already labeled

140
00:12:25,640 --> 00:12:31,480
and then you inference against the parts that are not and you measure how uncertain the model is

141
00:12:31,480 --> 00:12:36,600
on certain data points there and the ideas that the parts that are most uncertain are the parts

142
00:12:36,600 --> 00:12:40,840
where a human should probably look at it and evaluate and adjust the label as necessary so

143
00:12:41,400 --> 00:12:45,720
those are theoretically the data points that will move the needle the most for the model when

144
00:12:45,720 --> 00:12:50,120
it's retrained on them and you kind of keep doing this until eventually the model is actually doing

145
00:12:50,120 --> 00:12:55,560
a pretty good job of predicting across the entire data set so that's sort of active learning and

146
00:12:55,560 --> 00:13:01,880
again it's very very good for these like you know small data problems so to speak and again I'm

147
00:13:01,880 --> 00:13:05,720
painting with a very broad brush here because active learning can also be applied to very large data

148
00:13:05,720 --> 00:13:11,640
problems but it's specifically very very good at these small data problems where you have a very

149
00:13:11,640 --> 00:13:17,320
very large set of data and you want to know what part of it is likely the most interesting for you

150
00:13:17,320 --> 00:13:22,520
to sort of spend your time looking at as a human that's where active learning can be like uniquely

151
00:13:22,520 --> 00:13:28,680
valuable on the other hand weak supervision is a technique that's been gaining popularity in the

152
00:13:28,680 --> 00:13:36,120
last couple of years where the idea is that perhaps you don't have any like highly trustworthy labels

153
00:13:36,120 --> 00:13:44,200
so as an example your data might be so huge or so varied or just require an immense amount of

154
00:13:44,200 --> 00:13:49,480
subject matter expertise that it's very difficult to get any amount of like direct supervision

155
00:13:50,280 --> 00:13:56,680
in any meaningful way so what you might do is create labeling functions or these like weak

156
00:13:56,680 --> 00:14:02,760
learners or or you know sources of weak supervision and it's weak supervision because you can kind of

157
00:14:02,760 --> 00:14:08,040
think of these as like functions you know a very simple function could be like a regax like if I

158
00:14:08,040 --> 00:14:14,280
see the word credit card somewhere in this text that I know that this customer support ticket is

159
00:14:14,280 --> 00:14:19,720
very likely payment related right and then you rinse and repeat with several other patterns like

160
00:14:19,720 --> 00:14:25,560
if I see bank or invoice or if I see interest rate or whatever then then you know it has something

161
00:14:25,560 --> 00:14:33,480
to do with money so in a way these are kind of like heuristics you know they're rules of thumb

162
00:14:33,480 --> 00:14:37,160
they're they're no better than just saying like you know this thing is going to be noisy some

163
00:14:37,160 --> 00:14:42,600
percentage the time but note that you're not going row by row anymore you're not saying okay I

164
00:14:42,600 --> 00:14:47,400
have to sit down and say yes or no whether this is you know money related or not or payment related

165
00:14:47,400 --> 00:14:52,280
or not or whatever you're saying generally speaking if I see these words and maybe

166
00:14:52,280 --> 00:14:57,800
approximately this order then it is very likely payment related so by doing this you create a

167
00:14:57,800 --> 00:15:03,720
collection of these functions that you can then pass into a model that learns a label based on

168
00:15:03,720 --> 00:15:09,880
the presence or lack thereof certain labeling functions and kind of like the combinations and

169
00:15:09,880 --> 00:15:14,840
the contradictions between them and then it doesn't really matter how much data you're passing

170
00:15:14,840 --> 00:15:20,120
into the system you know you could be passing 2000 rows which would be easy or 20,000 rows or

171
00:15:20,120 --> 00:15:25,640
200,000 rows or two million rows it doesn't really matter because the idea is that as long as

172
00:15:25,640 --> 00:15:31,720
you've sampled the data in a way where it's representative of the larger whole then the functions

173
00:15:31,720 --> 00:15:37,000
you've created are still representative of the larger whole you know you can you can apply them

174
00:15:37,000 --> 00:15:42,120
to a larger sample as long as it's sampled in the same way and get the same or predictable results

175
00:15:42,120 --> 00:15:51,560
um but you can imagine that like weak supervision as a side effect is very very good at labeling

176
00:15:51,560 --> 00:15:57,000
for like the head of a distribution where there might be a lot of commonality you know you can create

177
00:15:57,000 --> 00:16:02,280
labeling functions that cover large quantities of data but as you get further and further into

178
00:16:02,280 --> 00:16:07,880
long tail it becomes harder and harder you know all of a sudden you're creating labeling functions

179
00:16:07,880 --> 00:16:13,880
that cover maybe one row at a time and that's like strictly no better than having hand labeled them

180
00:16:13,880 --> 00:16:17,800
you know it might even be worse because you have to like sit there and articulate a rule for it

181
00:16:19,160 --> 00:16:23,880
active learning on the other hand can can help direct you to where that long tail might be or at

182
00:16:23,880 --> 00:16:28,840
least the parts of the long tail that are most needle moving for your model yeah well it strikes me

183
00:16:28,840 --> 00:16:35,240
is super interesting about the about weak supervision uh and you know tell me if this this resonates

184
00:16:35,240 --> 00:16:42,600
in a lot of ways is kind of what you might do absent machine learning like you create a bunch of

185
00:16:42,600 --> 00:16:50,760
heuristics and you you know we probably wouldn't have called it label something but like you assign a

186
00:16:50,760 --> 00:16:58,120
tag or you categorize a transaction and you know so we know how to do that you know but the problem

187
00:16:58,120 --> 00:17:04,360
is always that you know using a rejects isn't going to be exact or if you need to make it exact or

188
00:17:04,360 --> 00:17:10,520
account for kind of noise then your system gets a whole lot more complex if you're trying to do it

189
00:17:11,400 --> 00:17:17,800
declaratively or you know imperatively but machine learning helps with that because now all of

190
00:17:17,800 --> 00:17:25,240
these you know imputed labels are kind of probabilistic and we're assuming noise and so the two

191
00:17:26,040 --> 00:17:31,240
the heuristic and the machine learning kind of marry at well in this idea of of weak supervision

192
00:17:31,240 --> 00:17:36,760
yeah that that's exactly right I think like the theme of this conversation I think is things that

193
00:17:36,760 --> 00:17:43,000
have been done for decades but haven't really had like labels put on them uh like we we're talking

194
00:17:43,000 --> 00:17:49,960
about expert systems and rule systems and and so on like you know there's a very classic motion

195
00:17:49,960 --> 00:17:53,960
in just like software engineering in general where it's like there's a very real business problem

196
00:17:54,600 --> 00:17:58,840
the way you solve it first is you throw humans at it right you like have a bunch of people

197
00:17:58,840 --> 00:18:04,200
just like go and manually do the thing eventually you build simple software to solve that the very

198
00:18:04,200 --> 00:18:09,400
first iteration of simple software will be things like rules you know but then very quickly you discover

199
00:18:09,400 --> 00:18:16,600
that your rules are brittle and like as as business logic changes and as your problem space changes

200
00:18:16,600 --> 00:18:22,760
and so on it becomes very difficult to manage just large quantities of rules that they overlap or

201
00:18:22,760 --> 00:18:30,040
conflict in various ways um so there's this like non-linear jump they end up having to make

202
00:18:30,040 --> 00:18:35,880
from an expert system or rule system to machine learning and that to me is one of the most

203
00:18:35,880 --> 00:18:40,360
interesting jumps that an organization can can make and that's sort of why weak supervision is so

204
00:18:40,360 --> 00:18:45,800
interesting it's because as you rightfully pointed out you can repurpose a lot of those rules

205
00:18:46,760 --> 00:18:52,440
as heuristics you know it's it's a simple mentality shift which then yields an implementation change

206
00:18:52,440 --> 00:18:59,000
but still it's like instead of talking about them as rules treat them as rules of thumb right just

207
00:18:59,640 --> 00:19:03,720
assume that they're heuristic in nature assume that they're going to be noisy in some unknowable

208
00:19:03,720 --> 00:19:09,640
capacity and build robustness around your system that way and then output some label data that

209
00:19:09,640 --> 00:19:15,000
you can then train a model on that will hopefully then learn an even more robust and generalized

210
00:19:15,000 --> 00:19:20,440
form of the relationship between the input and the output and that's where you get some like really

211
00:19:20,440 --> 00:19:26,440
meaningful gains and and I think that gap is one of the most interesting to traverse because

212
00:19:27,640 --> 00:19:35,160
prior to the formalization like weak supervision there was no linearization of the process going

213
00:19:35,160 --> 00:19:39,400
from like a rule system to machine learning like you would have to stop what you're doing and then

214
00:19:39,400 --> 00:19:43,480
like go hand your data to a crowd and then like you know you have to kind of go all in on machine

215
00:19:43,480 --> 00:19:48,760
learning at the moment you do that here there's like all of a sudden a nice incremental process

216
00:19:48,760 --> 00:19:53,400
that you can you can adhere to which makes weak supervision a very interesting solution

217
00:19:53,400 --> 00:19:58,760
for lots of organizations that find themselves in that sort of like chasm between I know I need to

218
00:19:58,760 --> 00:20:05,160
get to AI but I still have a bunch of like legacy rule systems or expert systems that I don't

219
00:20:05,160 --> 00:20:08,680
want to like lose because you're providing value how do I sort of bridge that gap yeah I think the

220
00:20:08,680 --> 00:20:13,640
other part of my intuition and tell me if this is your experience is that we think of machine learning

221
00:20:13,640 --> 00:20:19,800
as you know more complex than kind of the traditional way of doing things in some ways right in

222
00:20:19,800 --> 00:20:25,560
other ways know but I guess I'm trying to get it like building a really robust rule based system

223
00:20:25,560 --> 00:20:32,920
that can accommodate all of the corner cases and uncertainty in a noisy system it is hard and it's

224
00:20:32,920 --> 00:20:40,040
like bespoke and I you know I don't have any data to this but my sense is that a lot of organizations

225
00:20:40,040 --> 00:20:47,480
have plot a lot of time and money trying to handcraft this over and over again and this idea of

226
00:20:47,480 --> 00:20:55,240
you know weekly system for weak supervision provides a you know a kind of an elegant you know

227
00:20:55,240 --> 00:21:02,600
framework for a side stepping needing to make the rules piece bullet proof and still getting to

228
00:21:02,600 --> 00:21:09,240
the ultimate outcome that you're trying to to get to your intuition is absolutely correct I mean

229
00:21:09,240 --> 00:21:15,400
we've seen this time and time again where an organization will attempt to build the world's best

230
00:21:15,400 --> 00:21:23,720
set of rules and it always turns out that it's not the world's best set of rules like it's never good

231
00:21:23,720 --> 00:21:30,360
you know it's it's maybe good enough but like we as humans are like notoriously bad at being able

232
00:21:30,360 --> 00:21:35,880
to articulate edge cases like it's it's if we can articulate it very easily it's no longer an

233
00:21:35,880 --> 00:21:41,720
edge case you know it's something that we can write a rule for but that's where highly generalizable

234
00:21:41,720 --> 00:21:47,320
like deep learning models come into play that's the whole reason why you would train a model out

235
00:21:47,320 --> 00:21:52,840
of a weak supervision like process as opposed to just using the weak supervision process itself

236
00:21:52,840 --> 00:21:59,320
as the model it's because of that added level of generalization generalizability you would train

237
00:21:59,320 --> 00:22:06,360
a larger like you know deep learning model that has no concept of the rules of the heuristics that

238
00:22:06,360 --> 00:22:13,880
went into creating the labels all it sees are the label and the input data and ideally then it learns

239
00:22:13,880 --> 00:22:19,480
a very rich relationship between the input and and the projected output such that it you know

240
00:22:20,360 --> 00:22:24,120
it's not going to reverse engineer the rules because it has no concept of how many rules it

241
00:22:24,120 --> 00:22:28,920
took to create those labels it's going to learn something else and we hope that the thing it learns

242
00:22:28,920 --> 00:22:34,120
is actually more generalizable than the human input rules that went into that to begin with.

243
00:22:34,120 --> 00:22:43,400
Is there a an in-between kind of traditional rules and you know something more sophisticated

244
00:22:43,400 --> 00:22:49,480
that's like rule functions you know functions or that you were thinking of when you made that

245
00:22:49,480 --> 00:22:58,920
comment like how how far can you you know have you seen folks go before introducing the machine

246
00:22:58,920 --> 00:23:04,680
learning model part beyond just kind of rules engine maybe that's a way to get at it.

247
00:23:04,680 --> 00:23:10,440
Interestingly we actually have customers that are using watchful as their model in production

248
00:23:11,080 --> 00:23:16,920
despite us telling them hey like you know you should probably consider training a model

249
00:23:16,920 --> 00:23:22,680
and then using that model in production here here kind of like the properties to think about right

250
00:23:22,680 --> 00:23:28,600
it really depends on your use case and this is sort of you know my my co-founder hates it when

251
00:23:28,600 --> 00:23:35,960
I say this but it holds true it's like every data science problem the answer is it depends like

252
00:23:35,960 --> 00:23:39,960
it doesn't matter what the problem is the answer is always it depends so the answer here is it

253
00:23:39,960 --> 00:23:49,480
depends so if your data changes frequently you know from from time to time or if you find that

254
00:23:49,480 --> 00:23:54,360
the variability that you captured in the samples not truly representative the variability that you

255
00:23:54,360 --> 00:24:02,840
see in the total population of your data or if you need you know sort of like an additional layer

256
00:24:02,840 --> 00:24:08,840
of nuance that you find very difficult to capture by way of heuristics you know where perhaps you

257
00:24:08,840 --> 00:24:13,960
actually do want something that's pre-trained that you're then fine tuning and so on that's

258
00:24:13,960 --> 00:24:20,280
we're having a deep learning model trained after the fact is so helpful you know it it provides

259
00:24:20,280 --> 00:24:25,480
that additional level of granularity that additional level of generalizability that would be

260
00:24:25,480 --> 00:24:31,080
notoriously difficult for a human to articulate in heuristics again these are these are functions

261
00:24:31,080 --> 00:24:36,760
right these are functions that a human is writing in some capacity now there's obviously a lot of

262
00:24:36,760 --> 00:24:40,520
like cleverness that you can add on top of this to make sure that these functions are expressive

263
00:24:40,520 --> 00:24:49,320
and and very powerful and so on but at some point you start almost converging on like model

264
00:24:49,320 --> 00:24:55,720
activations you know a sufficiently complex function is just like you know almost no different

265
00:24:55,720 --> 00:25:00,520
from just like modeling a single activation in a model or something like that you know so

266
00:25:00,520 --> 00:25:08,280
there's also a trade-off between explainability and power as we've seen in in AI in general but

267
00:25:08,280 --> 00:25:14,280
you know obviously the same goes for weekly supervised systems it's as your functions become more

268
00:25:14,280 --> 00:25:20,360
and more powerful as they become more and more general they also lose some degree of explainability

269
00:25:21,480 --> 00:25:29,240
so again the answer is always it that it depends but here especially it depends because

270
00:25:29,240 --> 00:25:34,760
in some use cases you can get away with just creating a couple of heuristics that are really

271
00:25:34,760 --> 00:25:40,120
really good at labeling your data and all you really needed was that like additional layer of

272
00:25:40,120 --> 00:25:47,000
fuzziness almost that that embrace of noise in these heuristics as opposed to rules right and

273
00:25:47,000 --> 00:25:53,400
and that that might be good enough in other cases you might definitely rely on having some like

274
00:25:53,400 --> 00:25:59,240
you know pre-trained quote-unquote common sense in your model that is then used to sort of like

275
00:25:59,240 --> 00:26:06,280
boost overall like system performance so it kind of depends on your use case so we've had

276
00:26:06,280 --> 00:26:13,080
customers go the entire you know 10 miles that they had to go with just pure watchful we've

277
00:26:13,080 --> 00:26:17,800
had other customers trained very sophisticated models after the fact and get you know really really

278
00:26:17,800 --> 00:26:25,960
good results so yeah again it depends you mentioned previously this idea machine teaching

279
00:26:26,920 --> 00:26:31,800
can you elaborate on that is that kind of a you know an umbrella terminology that you're

280
00:26:31,800 --> 00:26:37,320
using for all of the things that we've discussed from active learning to week supervision or

281
00:26:38,520 --> 00:26:44,440
is it is it more or something different so machine teaching was actually a term coined by

282
00:26:44,440 --> 00:26:52,040
Microsoft Microsoft research specifically and so the best way to frame it relative to data center

283
00:26:52,040 --> 00:26:58,680
AI is that data center AI is very much a movement machine teaching is sort of a mentality coupled

284
00:26:58,680 --> 00:27:05,960
with like a set of strategies or proposed strategies so it becomes slightly more concrete machine

285
00:27:05,960 --> 00:27:15,720
teaching is a part of the data center AI movement it's just one layer of granularity deeper so the

286
00:27:15,720 --> 00:27:21,640
idea behind machine teaching is kind of simple it's that classical machine learning research

287
00:27:22,200 --> 00:27:29,160
has largely focused on building bigger and better machine learning models right it's like

288
00:27:29,160 --> 00:27:34,440
focusing on the student if you want to call it that and the idea is that you want to create a

289
00:27:34,440 --> 00:27:41,720
student that requires very little data to learn how to do the thing the problem is that is in

290
00:27:41,720 --> 00:27:47,800
large part very difficult to predict you know machine learning invention true like machine learning

291
00:27:47,800 --> 00:27:53,320
architecture sort of like innovation happens in almost like step wise motions so it's very

292
00:27:53,320 --> 00:27:59,080
difficult to predict when the next innovation is going to happen on the other hand

293
00:27:59,080 --> 00:28:05,400
the whole idea of machine teaching is if you could flip the coin and instead of focusing so much on

294
00:28:05,400 --> 00:28:11,800
the student focus more on the teacher and make it so that the person training this model or teaching

295
00:28:11,800 --> 00:28:17,960
it how to do the thing is several orders of magnitude more effective at doing that what does that

296
00:28:17,960 --> 00:28:21,800
world look like you know where it doesn't matter what student you're training you know it could be

297
00:28:21,800 --> 00:28:27,400
a very sophisticated deep learning model or it could be you know something more classical it shouldn't

298
00:28:27,400 --> 00:28:33,000
really matter because the same tool sets should be applicable in both cases where we're focusing a

299
00:28:33,000 --> 00:28:39,960
lot on making the teacher much more effective so techniques like weak supervision and active learning

300
00:28:39,960 --> 00:28:45,240
go into this as well as things like you know transfer learning and and even like

301
00:28:47,240 --> 00:28:53,080
like simulated data approaches and you know synthetic data rather like all of these things kind of

302
00:28:53,080 --> 00:29:01,640
belong under that umbrella where it's a very strong focus on taking what's in the head of an

303
00:29:01,640 --> 00:29:07,560
expert and applying that programmatically as much as possible to data which is then used to train

304
00:29:07,560 --> 00:29:13,400
models and not requiring you know 40 doctors to sit in a room for a month labeling data

305
00:29:14,200 --> 00:29:21,400
that's that's sort of the idea yeah I really appreciate the what I think I was the broadening

306
00:29:21,400 --> 00:29:29,160
of the term machine teaching I know very well the folks said bonsai which was a company that was

307
00:29:29,160 --> 00:29:35,480
acquired by Microsoft and they were kind of the spark of this machine teaching idea applied to

308
00:29:37,640 --> 00:29:43,160
reinforcement learning and industrial AI we collaborated on an industrial AI ebook and

309
00:29:43,160 --> 00:29:48,040
they were a long time client of mine and it's great to see that term resurface here in the context

310
00:29:48,040 --> 00:29:55,080
of data and data center AI yeah we think it's criminally underused we like we we stumbled upon

311
00:29:55,080 --> 00:29:59,800
the terminology in one of the research papers out of the Microsoft research lab after they got

312
00:29:59,800 --> 00:30:05,000
acquired and it just really resonated with us like that is exactly what we're building that's

313
00:30:05,000 --> 00:30:12,440
exactly what needs to be built for AI to make its way into most organizations so yeah absolutely

314
00:30:12,440 --> 00:30:16,680
I mean we're we're really excited about all the work that's happening in this space and it's

315
00:30:16,680 --> 00:30:20,840
it's becoming more like the concept of machine teaching is becoming more and more popular so

316
00:30:20,840 --> 00:30:29,080
I'm glad to see it's it's you know slow vital resurgence how do you see this fitting into the overall

317
00:30:29,960 --> 00:30:35,160
model development workflow when an organization you know has a problem you know they think machine

318
00:30:35,160 --> 00:30:41,720
learning is the solution to this you know do they start with rules do they start with

319
00:30:41,720 --> 00:30:47,720
do they need to think totally differently about the the way that they approach building models today

320
00:30:47,720 --> 00:30:53,960
yeah as I mentioned before it's it's more of like a mentality shift so it's less about like what

321
00:30:53,960 --> 00:31:00,120
technique they start about they start with it's more about matching the data to the architecture

322
00:31:00,120 --> 00:31:08,440
and and here's here's what I mean by that especially these days like machine learning coding

323
00:31:08,440 --> 00:31:12,840
for the most part is like kind of a solve problem you know I'm going to hand wave like a lot here

324
00:31:12,840 --> 00:31:19,160
so bear with me uh it's like you know with with with the very frameworks that exist there's Keras

325
00:31:19,160 --> 00:31:25,160
there's you know there's torch and and so I'm like uh there's several just like very nice

326
00:31:25,160 --> 00:31:30,520
commoditized ways to build almost any neural architecture you can imagine and even more than that

327
00:31:30,520 --> 00:31:35,720
they're like very nice architectures that you can just get off the shelf that will work for most

328
00:31:35,720 --> 00:31:42,520
problems so a lot of the time organizations don't really have to innovate on machine learning code

329
00:31:42,520 --> 00:31:49,880
itself uh so for all intents and purposes like a data centric AI perspective you can kind of hold

330
00:31:49,880 --> 00:31:57,640
the code static and you can iterate on the data to get better yield and that's where the shift

331
00:31:57,640 --> 00:32:04,680
and mentality comes you know it's yes I could have like an army of humans on standby and just give

332
00:32:04,680 --> 00:32:10,280
them data and like if if the data I get back is not good I tweak the rules or tweak the description

333
00:32:10,280 --> 00:32:16,040
just slightly to get you know perhaps better yield and so on but it's very much like again stepwise

334
00:32:16,040 --> 00:32:21,560
motion and each time you do that you might have a lag of like two weeks before you get your data back

335
00:32:21,560 --> 00:32:28,440
it's not a very efficient process so data centric AI is about not only focusing very carefully

336
00:32:28,440 --> 00:32:33,480
on what data and how you're getting it labeled and that sort of thing but it's also about

337
00:32:33,480 --> 00:32:39,080
the user experience or at least the experience around acquiring that labeled data how do I shorten

338
00:32:39,080 --> 00:32:47,320
the period of time between hey I need data to okay I've got good data and that process could

339
00:32:47,320 --> 00:32:52,120
you know utilize things like weak supervision depending on what your data looks like and that sort

340
00:32:52,120 --> 00:32:56,680
of thing it could use things like active learning it could use things like synthetic data it really

341
00:32:56,680 --> 00:33:02,760
depends on what you have at your disposal both from like an infrastructure and like machine learning

342
00:33:02,760 --> 00:33:08,440
ops perspective as well as from like a data distribution perspective like does your data lend itself

343
00:33:08,440 --> 00:33:12,520
to active learning particularly well does it lend itself to weak supervision particularly well does

344
00:33:12,520 --> 00:33:20,440
it lend itself perhaps better to synthetic data so really this is just about like talking about the

345
00:33:20,440 --> 00:33:27,320
data specifically as opposed to the model because good data can be used to train almost any model

346
00:33:27,320 --> 00:33:32,520
and again lots of hand waving here depending on the model architecture you might need more data

347
00:33:32,520 --> 00:33:38,200
you might need less data you know it depends on so many variables but the idea is that for the

348
00:33:38,200 --> 00:33:44,600
most part the hard thing about machine learning and the enterprise today is no longer the actual

349
00:33:44,600 --> 00:33:49,480
modeling component and it's actually on the data and it's just about thinking holistically about

350
00:33:49,480 --> 00:33:56,840
that now and do you find that that's the case uh you know hand waving notwithstanding kind of

351
00:33:56,840 --> 00:34:04,680
across the the most valuable of an organization's problems you know I've you know talked about this

352
00:34:04,680 --> 00:34:11,000
idea of model driven enterprise and like it's the anti commoditization of machine learning like yeah

353
00:34:11,000 --> 00:34:16,280
you can you know use machine learning in your organization and get some advantage if it's

354
00:34:16,280 --> 00:34:20,920
you know just built into all the stuff that you use but it's not really your models you know

355
00:34:20,920 --> 00:34:27,240
solving your problems for your data that's where the real values going to be uh is is that in line

356
00:34:27,240 --> 00:34:32,360
with what you're saying or yeah it's it's absolutely in line I don't know that this is a hot

357
00:34:32,360 --> 00:34:38,040
take anymore but I I firmly believe that in the next 10 years every company will have some sort

358
00:34:38,040 --> 00:34:43,640
of AI functionality internally um it's just an inevitability and the question ask is like

359
00:34:44,840 --> 00:34:50,040
why hasn't that happened yet you know like modeling techniques have again become largely

360
00:34:50,040 --> 00:34:55,640
commoditized already like why haven't we been able to bring AI into the fold in a meaningful way

361
00:34:55,640 --> 00:35:02,120
across all enterprises so that's sort of where our conviction is around the data piece it's that

362
00:35:02,120 --> 00:35:07,800
the hard part is not actually on the model architecture or the code aspect of this if you think

363
00:35:07,800 --> 00:35:15,240
about AI is just like code plus data it's exactly as you said it's like my hardest problems are around

364
00:35:15,240 --> 00:35:21,720
like is one part of the business where I got like one subject matter expert who knows how to perform

365
00:35:21,720 --> 00:35:27,400
this particular task no one else knows how to perform it except for this person I want to build a

366
00:35:27,400 --> 00:35:33,080
model to augment that person but very quickly I end up in a catch 22 where I would need that person

367
00:35:33,080 --> 00:35:38,600
to be in the process of labeling that data for that model but they're already a critical bottleneck

368
00:35:38,600 --> 00:35:44,280
bottleneck to my business so I can't have them sit in a room for like a month labeling data I

369
00:35:44,280 --> 00:35:49,640
really do need them to be productive on top of labeling data and that it just becomes very very

370
00:35:49,640 --> 00:35:54,360
hard to build a system that will actually help there so that's why a lot of the time if you do

371
00:35:54,360 --> 00:36:02,200
see enterprises kind of like trying AI they're usually trying it for use cases that are not the most

372
00:36:02,200 --> 00:36:07,800
needle moving you know that's why like the typical hello world for machine learning is like a

373
00:36:07,800 --> 00:36:13,080
Twitter sentiment analysis bot it's because it's like the most accessible data and it's not that

374
00:36:13,080 --> 00:36:17,720
the architecture is particularly simple a lot of the time the sentiment analysis bots like

375
00:36:17,720 --> 00:36:22,840
they use off-the-shelf architectures that are fine you know a lot of it is just around like the

376
00:36:22,840 --> 00:36:26,840
mechanics of acquiring the data cleaning it like going through process of training the model

377
00:36:26,840 --> 00:36:32,600
iterating on it and so on it's just availability of data so our conviction is that if we can make it

378
00:36:33,320 --> 00:36:40,040
dead simple for your expert to label your data in your environment as quickly as humanly

379
00:36:40,040 --> 00:36:46,280
possible without requiring them to like take time away from the things that they're already a bottleneck

380
00:36:46,280 --> 00:36:52,200
day-to-day for then we can price organizations in to actually using AI for solving their hardest

381
00:36:52,200 --> 00:36:57,720
problems that's that's kind of like the laser focus we have you know maybe an insight you know

382
00:36:57,720 --> 00:37:03,640
out of this question in your responses that model is kind of an overloaded term and

383
00:37:03,640 --> 00:37:15,720
you can use an off-the-shelf model architecture off-the-shelf code but the thing that makes you know

384
00:37:15,720 --> 00:37:24,280
after you apply your kind of proprietary data to those things you have a proprietary high-value

385
00:37:24,280 --> 00:37:31,640
model at the you know coming out of the process that may be based on a commodity architecture

386
00:37:31,640 --> 00:37:37,560
yeah that that's exactly right generally speaking again like I'm I'm very excited about like

387
00:37:37,560 --> 00:37:43,080
these massive foundation models that are being built and I still think that we need more of these

388
00:37:43,080 --> 00:37:50,040
foundation models across other data modalities and other tasks so I am certainly not pushing against

389
00:37:50,040 --> 00:37:55,240
the idea of these foundation models that use lots of generic label data to produce you know

390
00:37:55,240 --> 00:38:05,080
generally available good insight the hard part is marrying that to specific use cases and a lot

391
00:38:05,080 --> 00:38:12,040
of the time you can use a GPT-3 for like very interesting use cases but in order to match that

392
00:38:12,040 --> 00:38:18,520
with enterprise value you have to marry it to data or a use case that is unique to that enterprise

393
00:38:18,520 --> 00:38:23,480
most of the time and that's where like that whole data centric movement becomes very interesting

394
00:38:23,480 --> 00:38:28,280
because then you can marry the foundation model with very specific very curated data that you

395
00:38:28,280 --> 00:38:33,240
can then combine to fine tune a model that again becomes proprietary it could be based on this

396
00:38:33,240 --> 00:38:37,800
like massive architecture that you kind of like repurpose from somewhere else but the piece that

397
00:38:37,800 --> 00:38:42,680
makes it yours is the fact that it was trained on your data for your task by your people what what

398
00:38:42,680 --> 00:38:53,080
do organizations need to be successful in making this mindset shift I think like the big thing is

399
00:38:53,080 --> 00:38:57,720
just like this is kind of again going to be hand wave but just like being open to it that's like

400
00:38:58,440 --> 00:39:04,520
like the biggest hurdle it's like a lot of the time folks get entrenched in the way things have

401
00:39:04,520 --> 00:39:09,400
been done for a very long time and it's very difficult to break out of it one of the things that we

402
00:39:09,400 --> 00:39:17,800
find like very common is the idea that hand labeled data is ground truth um and and this is sort of

403
00:39:17,800 --> 00:39:24,680
like an interesting idea where like it's it's just not the case in the real world you know a lot

404
00:39:24,680 --> 00:39:31,640
of the time people have uh subjectivity that comes to play or or they have biases that manifest in

405
00:39:31,640 --> 00:39:38,840
their labeled data and and try as hard as you might it's impossible to get a 100% accurate and

406
00:39:38,840 --> 00:39:46,280
fair data set labeled by an army of humans uh so that's like one of the biggest hurdles that

407
00:39:46,280 --> 00:39:51,240
we've had to face in like communicating what we do to the rest of the market it's getting people

408
00:39:51,240 --> 00:39:57,080
to acknowledge that the way they've gone about doing things so far has been leveraging

409
00:39:58,120 --> 00:40:03,000
words that they don't actually mean do you go as far as you know trying to convince people to stop

410
00:40:03,000 --> 00:40:09,720
using the term ground truth when yes yeah it's not absolutely ground truth it's just their labeled

411
00:40:09,720 --> 00:40:14,040
data yeah that's exactly right I mean like we we we try and educate them on life hey you're like

412
00:40:14,040 --> 00:40:18,520
hey this actually means something and it's not what you're what you're thinking yeah words matter

413
00:40:18,520 --> 00:40:23,800
you know like like you can't go around saying that your data is ground truth when it's like

414
00:40:23,800 --> 00:40:28,840
clearly riddled with bias and like incorrect labels and assumptions like we've seen this time and

415
00:40:28,840 --> 00:40:35,400
time again so first we educate them that like look hand labels themselves are noisy in some

416
00:40:35,400 --> 00:40:44,360
unknowable capacity they are by nature weak you know perhaps not as weak as functions but they

417
00:40:44,360 --> 00:40:51,800
are still weak in some way and now the question is how weak right how much can we trust the hand

418
00:40:51,800 --> 00:40:56,520
labels that you already have like how do we leverage them in in a more robust system how do we

419
00:40:56,520 --> 00:41:02,360
combine them with other sources of signal to then boost or reduce the signal introduced overall

420
00:41:02,360 --> 00:41:08,840
those become very very interesting questions and part of our like overall conviction is that

421
00:41:09,480 --> 00:41:16,760
you know as we move towards a more mature nl ops workflow overall you know data centric AI

422
00:41:16,760 --> 00:41:21,160
notwithstanding right if we're talking about just like the maturity of machine learning as a whole

423
00:41:21,160 --> 00:41:27,320
from the enterprise explainable AI has been a very very important thing for several years now

424
00:41:28,280 --> 00:41:34,440
but the interesting thing to us is that a lot of the techniques there are all around explaining

425
00:41:35,320 --> 00:41:39,720
your model and like why your model is making certain decisions and how they could play out

426
00:41:39,720 --> 00:41:44,120
but the reason why your model is making a bias decision is because it was trained on bias data

427
00:41:44,120 --> 00:41:49,000
right like right especially if we're holding yeah potential exceptions to that naturally you know

428
00:41:49,000 --> 00:41:56,680
I saw a good one where someone was using distilbert to predict sentiment on certain phrases so it

429
00:41:56,680 --> 00:42:04,440
was like if a movie was like the term was like this movie was produced in India that would have

430
00:42:04,440 --> 00:42:10,520
a very high sentiment then there's another where it was like this movie was produced in Germany

431
00:42:10,520 --> 00:42:16,280
and that had a very low sentiment and the reason why that happened there's several reasons

432
00:42:16,280 --> 00:42:20,680
obviously distilbert was trained on like internet data there's obviously imbalance and in that

433
00:42:20,680 --> 00:42:25,000
data because it's probably overfitting on like this pre-trained notion of like World War 2 and

434
00:42:25,000 --> 00:42:30,920
you know several mentions of Germany that are like not very favorable but even more interestingly

435
00:42:30,920 --> 00:42:36,120
the model architecture is such that like on the sentiment analysis side there's no notion of

436
00:42:36,120 --> 00:42:41,240
neutrality it's either positive or negative so the model architecture actually ended up biasing

437
00:42:41,240 --> 00:42:46,920
the entire outcome because there's like those are inherently neutral statements but it was forced

438
00:42:46,920 --> 00:42:51,960
to either make a positive or negative assumption that's such a clear example of this thing that

439
00:42:51,960 --> 00:42:57,560
we've been fighting about on Twitter for years like that bias is only in the data and not in the

440
00:42:57,560 --> 00:43:03,400
models no it's everywhere it's everywhere you can't escape it it's it's it's literally everywhere

441
00:43:03,400 --> 00:43:09,240
and but you just have to be smart about like where you're limiting the possible sources of bias

442
00:43:09,240 --> 00:43:14,440
you know there's human-generated bias both in the way of data as well as in the way of model

443
00:43:14,440 --> 00:43:20,200
architectures there's bias in the way labels or predictions are used you know downstream there's

444
00:43:20,200 --> 00:43:28,040
so many different ways that you could sort of like capture bias unintentionally in a system but

445
00:43:28,760 --> 00:43:34,840
as part of that like we should be building systems to find where this bias might be being introduced

446
00:43:34,840 --> 00:43:39,000
you know we should we should be building more robust systems exactly all the way up and all the way

447
00:43:39,000 --> 00:43:44,360
down the stack so it's part of this like because we touch the data a lot our notion is that you

448
00:43:44,360 --> 00:43:47,800
should be able to explain that data you should be able to explain the data that goes into your

449
00:43:47,800 --> 00:43:52,920
models otherwise how are you going to reason about it so like again the point I'm trying to make

450
00:43:52,920 --> 00:43:56,440
is that you just have to be like open to a lot of these things you have to be willing to shift

451
00:43:56,440 --> 00:44:01,160
your mentality and willing to shift again as a side effect your techniques in producing some of

452
00:44:01,160 --> 00:44:06,440
these models and then naturally it also helps if you've taken the first couple of steps in your

453
00:44:06,440 --> 00:44:10,760
machine learning journey if you know what status quo looks like if you know why this is different

454
00:44:10,760 --> 00:44:16,120
if you know how this plays into the rest of your anal ops stack that's helpful but not required

455
00:44:16,120 --> 00:44:21,080
I think the biggest requirement is very much just like a willingness to have your mind changed

456
00:44:21,080 --> 00:44:28,840
you talked about the noisiness of hand label data and you know to some degree we've

457
00:44:28,840 --> 00:44:35,240
recognized that and sophisticated organizations have you know employed a number of strategies

458
00:44:35,240 --> 00:44:42,760
to try to mitigate that you know quorum among laborers and managing labor labor labor

459
00:44:42,760 --> 00:44:48,280
quality and all this kind of stuff is it data centric AI or some of the things that we've

460
00:44:48,280 --> 00:44:56,200
been talking about under that banner week supervision etc is that a substitute for all that do you

461
00:44:56,200 --> 00:45:03,800
need kind of these you know advanced labeling strategies less if you're employing some of the

462
00:45:03,800 --> 00:45:09,480
things that we've been talking about our argument is yes however that's not necessarily the argument

463
00:45:09,480 --> 00:45:16,040
shared across the board yeah it depends exactly so so here's a deal like you've got you've got

464
00:45:16,040 --> 00:45:21,480
things like cap of values right so like measuring how good of a label or someone is and

465
00:45:21,480 --> 00:45:25,960
and that sort of thing you've got inter annotator disagreement as a side effect of that you've got

466
00:45:25,960 --> 00:45:30,840
as you mentioned quorum between hand labelers you've got all these different things that play in

467
00:45:30,840 --> 00:45:36,440
I think they're ways to do that correctly I just haven't seen them yet so for instance quorum

468
00:45:36,440 --> 00:45:41,960
amongst hand labelers like let's say you have like seven out of 10 people say that this thing is

469
00:45:41,960 --> 00:45:47,960
toxic and three people say that it's not toxic like it's a tweet or something like that like

470
00:45:47,960 --> 00:45:54,920
my conviction is that every single one of those people was correct in some capacity right there's

471
00:45:54,920 --> 00:45:59,640
some amount of subjectivity right like what what exactly is toxicity toxicity is a thing that

472
00:45:59,640 --> 00:46:04,120
is very difficult to actually capture the nuance of especially if you're trying to write up a

473
00:46:04,120 --> 00:46:11,400
description so you have to acknowledge that there's like cascading systems that go into these labels

474
00:46:11,400 --> 00:46:18,680
like you have to have the world's most perfect rules that describe how to label a thing in order to

475
00:46:18,680 --> 00:46:24,440
get inherent consistency in the labels that come out the other end and we're human you know we're

476
00:46:24,440 --> 00:46:28,440
not going to write the world's most perfect descriptions it's just not going to be possible we're

477
00:46:28,440 --> 00:46:33,160
not going to write the world's most perfect criteria for labeling it's just not possible

478
00:46:33,160 --> 00:46:41,720
so this kind of speaks to you know we talked about bias and data and how there's also bias and

479
00:46:42,520 --> 00:46:48,120
models you know here we're pointing specific to there's also kind of inherent bias in

480
00:46:48,840 --> 00:46:54,280
labeling systems and labeling tools and the way you're you know labeling instructions

481
00:46:54,280 --> 00:46:59,960
exactly I mean it's it's all over the place right like bias can be introduced in a multitude of

482
00:46:59,960 --> 00:47:07,480
places so you have to be like very cognizant about that and I think that like trying to articulate

483
00:47:07,480 --> 00:47:13,000
whether someone is a good or bad labeler is fundamentally the wrong approach the reason for

484
00:47:13,000 --> 00:47:20,120
this is because you might be trying to categorize them overall as a bad labeler but they might be

485
00:47:20,120 --> 00:47:24,200
very very good at labeling certain parts of the data because of their context because of their

486
00:47:24,200 --> 00:47:30,520
expertise but very bad at labeling other parts of the data and I think it's more accurate to

487
00:47:30,520 --> 00:47:34,920
indicate whether the expertise they're bringing to the table in particular segments of the data

488
00:47:35,560 --> 00:47:39,960
is actually valuable or not so that's sort of like point number one point number two is I believe

489
00:47:39,960 --> 00:47:47,720
that these labels should capture the rich context behind what you know sort of the one or zero

490
00:47:47,720 --> 00:47:52,760
output is so for instance in the earlier case if seven out of ten people said this thing is

491
00:47:52,760 --> 00:47:58,440
toxic I'd want to know that I'd want my model to acknowledge the nuance that went into this so that

492
00:47:58,440 --> 00:48:03,320
it can actually learn a richer relationship between the input space and the output value so

493
00:48:04,520 --> 00:48:08,200
that is actually very very important and again that's why I'm saying that I think that there

494
00:48:08,200 --> 00:48:13,800
there are ways that you could build a system that does a lot of this stuff but I haven't seen it yet

495
00:48:14,520 --> 00:48:19,640
and so like we take kind of like just a different approach to it our system is not a hand labeling

496
00:48:19,640 --> 00:48:25,320
system you know we we have hand labeling as part of our process but a we acknowledge that there's

497
00:48:25,320 --> 00:48:30,920
no such thing as ground truth and b all of our labels are inherently probabilistic by nature

498
00:48:31,560 --> 00:48:36,920
so you can have probabilities that are 100% or 0% so you can have extreme confidence on either side

499
00:48:36,920 --> 00:48:41,640
but you can also have confidence in the middle you know like you can have that 70% likelihood

500
00:48:41,640 --> 00:48:49,480
probability or or or something else right but that allows for 101 degrees of freedom in your

501
00:48:49,480 --> 00:48:54,920
in your label space which then hopefully yields a model that will learn a richer relationship

502
00:48:54,920 --> 00:48:59,560
between the input and the output so it's a long way of saying yeah I think you could you know

503
00:48:59,560 --> 00:49:03,720
build better hand labeling systems I just haven't seen it yet you referred to

504
00:49:03,720 --> 00:49:12,040
data centric AI and and the the kind of stuff we're talking about is part of the ml ops stack

505
00:49:12,040 --> 00:49:18,280
historically and in a lot of ways there's been kind of a separation between all of the data stuff

506
00:49:18,280 --> 00:49:25,720
the data stack and even you know data ops and data prep and all the stuff and kind of ml ops

507
00:49:25,720 --> 00:49:31,400
you kind of it does data centric AI kind of bring those together yeah it does again the whole

508
00:49:31,400 --> 00:49:38,600
concept here is that in today's world as it relates to AI you can broadly consider the code

509
00:49:39,320 --> 00:49:44,920
to be like held static and again AI is sort of like code plus data and if you're holding the

510
00:49:44,920 --> 00:49:49,160
code static then the thing you're iterating on is data which means that by nature if you want

511
00:49:49,160 --> 00:49:54,920
that to go back through an ml ops system it has to be connected in some way so the argument here

512
00:49:54,920 --> 00:50:01,160
is data should be a part of the stack in some meaningful way perhaps not the entire like process

513
00:50:01,160 --> 00:50:05,160
of procuring the data and so on like you know maybe some of that is a little bit more

514
00:50:05,160 --> 00:50:14,200
bespoke or you know a little less framework but really the idea is that AI is code plus data

515
00:50:14,200 --> 00:50:19,400
and we have very very robust ways of managing that code due to decades of innovation on the

516
00:50:19,400 --> 00:50:26,520
software engineering side and we haven't seen quite the same thing in an ml concept or context

517
00:50:26,520 --> 00:50:32,840
on the data side and that's I think the part that data centric AI really aims to help

518
00:50:32,840 --> 00:50:38,200
sort of like push it's the idea that data really should be a part of the ml ops stack

519
00:50:38,200 --> 00:50:47,880
and do you find that these ideas apply equally well across media type modality text audio

520
00:50:47,880 --> 00:50:53,640
video yeah yeah absolutely um the obviously the only real place where it starts breaking down

521
00:50:53,640 --> 00:50:58,120
is the moment you start leaving like supervised machine learning because then it you might not

522
00:50:58,120 --> 00:51:03,240
need data right which is totally reasonable but but broadly speaking when we're talking about

523
00:51:03,240 --> 00:51:08,440
machine learning the enterprise we are generally talking about some sort of supervised system

524
00:51:08,440 --> 00:51:15,400
and some capacity and yeah we have seen that this applies like roughly the same across every

525
00:51:15,400 --> 00:51:23,160
modality across every task now devil's always in the details right like managing things like

526
00:51:24,360 --> 00:51:29,800
like 3d point cloud data is very different to managing things like you know just single

527
00:51:29,800 --> 00:51:35,640
dimensional text or something like that like um the way you go about interfacing with this data

528
00:51:35,640 --> 00:51:40,760
and the way you go about applying your subject matter expertise to it will differ like quite a lot

529
00:51:40,760 --> 00:51:49,240
from from modality modality um now like a whole purpose to be for for my company is finding the

530
00:51:49,240 --> 00:51:55,240
common threads across these modalities and across these these tasks such that you can reuse the

531
00:51:55,240 --> 00:51:59,800
same skill set the same workflow no matter what data you're bringing to the table and no matter

532
00:51:59,800 --> 00:52:04,440
what task you're bringing to the table not all things will be similar not all things will be the

533
00:52:04,440 --> 00:52:10,040
same but a lot of them will at least the core will and that's kind of like where we focus almost

534
00:52:10,040 --> 00:52:14,840
all of our effort it's it's finding that common ground well cyan has been wonderful having you on

535
00:52:14,840 --> 00:52:21,960
the show and chatting about your take on data centric AI lots of great stuff in here um thank you

536
00:52:21,960 --> 00:52:49,080
thank you so much Sam this is a lot of fun I really appreciate it

