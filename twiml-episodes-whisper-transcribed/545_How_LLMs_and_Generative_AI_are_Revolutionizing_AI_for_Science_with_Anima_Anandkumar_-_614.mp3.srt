1
00:00:00,000 --> 00:00:04,960
All right, everyone. Welcome to another episode of the Twimmil AI podcast. I am, of course,

2
00:00:04,960 --> 00:00:10,800
your host, Sam Charrington. Today, I'm joined by Anima Anam Kumar. Anima is

3
00:00:10,800 --> 00:00:16,080
Bren Professor of Computing and Mathematical Sciences at Caltech and Senior Director of AI

4
00:00:16,080 --> 00:00:21,280
Research at NVIDIA. Of course, before we get going, take a moment to hit that subscribe button

5
00:00:21,280 --> 00:00:26,000
wherever you're listening to today's show. You can also follow us on TikTok and Instagram

6
00:00:26,000 --> 00:00:32,160
at Twimmil AI for highlights from every episode. Anima, welcome back to the podcast. It's been a bit.

7
00:00:32,160 --> 00:00:38,480
Yeah, thank you, Sam. It's such a pleasure to be back and so great to see where how Twimmil

8
00:00:38,480 --> 00:00:45,920
has expanded its audience. It's now even on TikTok. And when we started back then in the beginning

9
00:00:45,920 --> 00:00:52,880
of the AI revolution to where we are now, it's so great to see Twimmil be part of the journey and

10
00:00:52,880 --> 00:00:58,880
helping our viewers and audience to really be up to date with the latest and greatest machine

11
00:00:58,880 --> 00:01:05,760
learning tools. Thanks so much. I'm really excited about catching up with you and digging into

12
00:01:05,760 --> 00:01:11,200
all the work you're doing around AI for science. Since it's been such a while since we last spoke,

13
00:01:11,200 --> 00:01:18,800
I'd love to have you just reintroduce yourself to our audience and share how you came into the field.

14
00:01:18,800 --> 00:01:25,440
Yeah, certainly. It's been such an exciting journey to be part of the AI revolution starting

15
00:01:25,440 --> 00:01:33,200
all the way back when AI was considered more a theoretical concept. People didn't think in our

16
00:01:33,200 --> 00:01:39,200
lifetimes it would take off like the way it has done now. But that also meant I could hone in

17
00:01:39,200 --> 00:01:46,160
on the theoretical foundations from statistics, signal processing, machine learning, probabilistic

18
00:01:46,160 --> 00:01:54,400
models, and ask questions like, how do we extract hidden or latent variables or phenomena

19
00:01:54,400 --> 00:02:01,200
from data without labels? What we call unsupervised learning. And I was working on tensor methods

20
00:02:01,200 --> 00:02:09,200
which conceptually looks at higher order moments of data when people were mostly limiting to

21
00:02:09,200 --> 00:02:16,560
second order moments or linear algebraic techniques. We talked about that back then,

22
00:02:16,560 --> 00:02:23,600
and a lot of my work was saying, no, no, don't just limit to linear models. Don't just limit to

23
00:02:23,600 --> 00:02:31,360
spectral methods that use matrices go beyond, think bigger. And we have the computational

24
00:02:31,360 --> 00:02:37,360
abilities to do that now. We can go to tensors. And that kind of non-linear transformation

25
00:02:37,360 --> 00:02:43,760
is what we also see with neural networks. So we were early in that journey to go from matrices

26
00:02:43,760 --> 00:02:49,920
to tensors. And from tensors now we have all kinds of non-linear mappings learned through

27
00:02:50,480 --> 00:02:59,200
all the neural networks and related tools. And so to me to see that journey and now last year

28
00:02:59,200 --> 00:03:05,520
was the year of Generative AI. And so to me who's been working on generative modeling since my

29
00:03:05,520 --> 00:03:13,440
earliest days in graduate school, it's so great to see that. Come into fruitition and see how

30
00:03:13,440 --> 00:03:19,840
it's having a lot of really important practical implications. That's awesome. And it's been

31
00:03:19,840 --> 00:03:26,240
quite a quite a year for for generative AI. And I'm sure we'll be talking a little bit about that.

32
00:03:27,280 --> 00:03:33,040
But one of the things we wanted to dig into a bit is the work that you've been doing around AI

33
00:03:33,040 --> 00:03:38,720
for science. Tell me a little bit about how you got involved in that. Yeah, certainly,

34
00:03:38,720 --> 00:03:45,600
you know, at Caltech here we founded AI for Science as a campus wide initiative in 2018.

35
00:03:46,160 --> 00:03:53,760
Way before researchers were thinking of it as a mainstream area in AI, right?

36
00:03:54,640 --> 00:04:00,480
Indeed, a lot of the AI development has been driven by industry, by big tech,

37
00:04:00,480 --> 00:04:06,560
and focuses on natural images and text, because that's where we can easily get

38
00:04:06,560 --> 00:04:14,640
web scale data. And they also have a lot of commercial value for building the next generation

39
00:04:14,640 --> 00:04:21,360
internet. You could argue. But when it comes to scientific domains, you know, there's even a

40
00:04:21,360 --> 00:04:25,760
hard question of where do we get started. You know, there's so many challenges that we don't

41
00:04:25,760 --> 00:04:34,240
see in our standard AI domains like Image and Text. I know that starts with the lack of data,

42
00:04:34,240 --> 00:04:40,800
right? You may not have large scale data that's really necessary for deep learning. And you

43
00:04:40,800 --> 00:04:47,680
would not only think about a very specific task, you most of the time required zero-shot

44
00:04:48,640 --> 00:04:53,760
generalization. So you want to go beyond the training domain, you know, a lot of the

45
00:04:53,760 --> 00:05:00,240
sciences extrapolation, right? Not only the data I've seen, what is the possibility beyond?

46
00:05:00,240 --> 00:05:06,080
What are new drugs? What are new molecules? How do I create those? You know, you don't have data

47
00:05:06,080 --> 00:05:13,360
on that because all the design and discovery is about finding something new that doesn't exist

48
00:05:13,360 --> 00:05:19,920
so far in your training. And so that's, to me, that extrapolation and generalization beyond

49
00:05:19,920 --> 00:05:26,160
the training data is such an important aspect that standard machine learning wasn't capable of

50
00:05:26,160 --> 00:05:34,240
doing. But that's where I think now in the era of generative AI, so much of that is becoming

51
00:05:34,240 --> 00:05:41,040
much more possible. You know, just like we can generate avocado chairs, can we generate better drug

52
00:05:41,040 --> 00:05:47,920
molecules? You know, for the viewers, what I'm referring to is like a stable diffusion or

53
00:05:47,920 --> 00:05:56,480
dolly, these models that once they learn text to image mapping can now create entirely new

54
00:05:56,480 --> 00:06:02,800
images that were not in training data, right? Entirely new scenarios, like a chair that looks like

55
00:06:02,800 --> 00:06:08,320
an avocado. You know, you probably didn't see it in training data, but if you learn the concept

56
00:06:08,320 --> 00:06:14,240
of what is a chair and what is an avocado, you can try and mix that together. Do this compositional

57
00:06:14,240 --> 00:06:21,840
generation. And I think these are concepts now that we are coming a full circle is aspects that

58
00:06:21,840 --> 00:06:28,080
can have huge implications in the sciences. And yeah, so a lot of unique challenges, but also

59
00:06:28,080 --> 00:06:32,560
I think opportunities that lie ahead when it comes to AI for science.

60
00:06:32,560 --> 00:06:41,040
I think one of the, one of the areas of AI for science that's received a lot of publicity over

61
00:06:41,040 --> 00:06:48,320
the past year, a couple of years has been around the various protein folding research.

62
00:06:49,040 --> 00:06:53,760
Can you talk a little bit about that and why that's interesting and important?

63
00:06:53,760 --> 00:07:01,200
Yeah, absolutely. Protein folding has already been making such a big impact in our ability

64
00:07:01,200 --> 00:07:07,680
to quickly generate the full three-dimensional structure of proteins, right? So how do you map

65
00:07:07,680 --> 00:07:14,560
from sequences to now three-dimensional structures? And this folding is so important because

66
00:07:14,560 --> 00:07:20,960
that's what will determine how other molecules or other proteins interact and bind.

67
00:07:20,960 --> 00:07:25,920
So if you now have a drug molecule, which is typically a small molecule, I mean, not all the time,

68
00:07:25,920 --> 00:07:31,520
but most of the time it's simpler to design a small molecule. And then the question is how does

69
00:07:31,520 --> 00:07:37,920
it want bind in a certain pocket of the protein? You know, what are the active areas in this protein,

70
00:07:37,920 --> 00:07:44,800
right? So all that's determined by this three-dimensional structure. And that's why for biologists,

71
00:07:45,760 --> 00:07:50,640
you know, this was really critical because you can all use all the knowledge and intuitions

72
00:07:50,640 --> 00:07:57,440
and say, oh, this could be a good binding candidate. But until you have the three-dimensional

73
00:07:57,440 --> 00:08:03,760
structure easily available, how do you determine if that's good computationally? And that's

74
00:08:03,760 --> 00:08:09,120
where you have to go to the lab and do those experiments, which are really expensive.

75
00:08:10,160 --> 00:08:18,000
And now we are going even beyond the folding, right? Even just the protein by itself,

76
00:08:18,000 --> 00:08:23,280
what is the structure? To now, how do the protein and ligands, which are small molecules,

77
00:08:23,280 --> 00:08:29,760
bind together? So in many cases, it's even more complicated, as always, biology is quite

78
00:08:29,760 --> 00:08:36,240
complicated, right? The protein, when it's binding to this small molecule, in fact, changes

79
00:08:36,800 --> 00:08:42,800
its structure. So it's not even a static process. And so that's one of the work we've been

80
00:08:42,800 --> 00:08:50,720
exploring using these generative AI diffusion models is not only to model the static

81
00:08:50,720 --> 00:08:56,640
behavior of how a protein looks in three dimensions, but how does that three-dimensional structure

82
00:08:56,640 --> 00:09:04,160
change when it binds to the small molecules? And so to model that process, and with that,

83
00:09:04,160 --> 00:09:10,880
we can predict better about the contacts. How well does it bind? And that's so crucial for

84
00:09:10,880 --> 00:09:15,840
coming up with much more realistic predictions. And that's just one example. You know,

85
00:09:15,840 --> 00:09:23,360
we're talking a lot about structure. And this is, quoting Frances Arnold, who is Nobel Prize winner

86
00:09:23,360 --> 00:09:30,560
here at Caltech, for directed evolution of protein. So she is quoted in New York Times on

87
00:09:30,560 --> 00:09:36,480
Generative AI for proteins that appeared, I think, about two weeks ago. And so she's asking,

88
00:09:36,480 --> 00:09:42,480
that's great. We're solving all these structure problems, right? We can speed up our prediction of

89
00:09:42,480 --> 00:09:48,800
structure. We can greatly increase the accuracy of our structure prediction. But what about function?

90
00:09:49,680 --> 00:09:55,040
Because ultimately, what we want to understand is, what is the functionalities of different proteins?

91
00:09:55,760 --> 00:10:02,960
And can we create like better proteins, which have functional implications? Because, you know,

92
00:10:02,960 --> 00:10:10,240
if you can create better targets and create drugs for it, we can cure a lot of diseases and

93
00:10:10,240 --> 00:10:18,080
issues that so far have not been able to do through traditional therapeutics. And I think that's

94
00:10:18,080 --> 00:10:24,720
where a lot of the work that, for instance, my group is doing as well as others, is to not just

95
00:10:24,720 --> 00:10:29,920
limit to the structure. Because if you only look at proteins and their structure and create the

96
00:10:29,920 --> 00:10:36,320
foundation models, you know, it's not very easy to talk about the function. But if you go to the level

97
00:10:36,320 --> 00:10:42,400
of the genome, you know, the DNA, right, that determines the function. If you can learn the

98
00:10:42,400 --> 00:10:47,840
relationship between different genes, you know, what is the relationship and how does it relate

99
00:10:47,840 --> 00:10:52,960
to different functionalities of the proteins? Because genes can be mapped to the proteins that

100
00:10:52,960 --> 00:10:59,120
generate. And by understanding relationship between the genes, can we generate better proteins

101
00:10:59,120 --> 00:11:04,720
that are functionally meaningful for different tasks? And that's what we've been doing now. And

102
00:11:04,720 --> 00:11:12,640
and it has so many applications. So in the largest biological language model that we built

103
00:11:12,640 --> 00:11:20,880
of about 25 billion parameters, what we showed was the ability to learn at the genome level,

104
00:11:20,880 --> 00:11:28,960
so long sequences of bacteria and viruses, more than 110 million such sequences. So we consumed

105
00:11:28,960 --> 00:11:35,440
all that into a language model. So in a way, you're understanding the language of the genome. And then

106
00:11:35,440 --> 00:11:42,880
from that, we can generate now new gene sequences, right. And with that, we could in fact predict

107
00:11:42,880 --> 00:11:50,160
new variants of coronavirus, as well as, you know, the ones we had held out, the existing variants that

108
00:11:50,160 --> 00:11:56,800
appeared like Delta, Omicron, right, all those we could predict, even though the model had never

109
00:11:56,800 --> 00:12:03,200
seen that before. What was the text that that model was trained on? Genome sequences. Got it.

110
00:12:03,200 --> 00:12:10,400
So think of now the language is the genome, right. So that's your now alphabet. That's your sentences.

111
00:12:11,280 --> 00:12:18,160
It's longer and that's a challenge. In fact, we used a combination of language model like GPT,

112
00:12:18,160 --> 00:12:23,760
but with the diffusion backbone in the latent space. So it's a hierarchical model that has

113
00:12:23,760 --> 00:12:30,400
both diffusion and GPT components, which is necessary for these long sequence modeling.

114
00:12:31,360 --> 00:12:37,520
And, you know, and that really helped us to understand at the genome level these relationships.

115
00:12:38,320 --> 00:12:44,320
And you could also see organization in the latent space of different latent genes, right. So genes

116
00:12:44,320 --> 00:12:52,320
that are closing that latent space are also functionally similar or related. And I think those

117
00:12:52,320 --> 00:12:57,440
also help us now to design better proteins. This reminds me a little bit of

118
00:12:58,320 --> 00:13:04,320
work that I spoke with Richard Socher about when he was, I think he did this at Salesforce

119
00:13:04,320 --> 00:13:10,800
of all places. Are you familiar with that at all? Yeah, certainly. That was some of the early work on

120
00:13:10,800 --> 00:13:19,840
understanding protein. I think sequence models, if I recall right, or like molecule models, right.

121
00:13:19,840 --> 00:13:25,040
So a lot of work in the last few years has gone first at the molecular level. You know,

122
00:13:25,040 --> 00:13:30,560
how do we generate better molecules? How do we give like condition on various properties and

123
00:13:30,560 --> 00:13:35,520
generate better molecules? And then we said, we can't just look at a molecule in isolation.

124
00:13:35,520 --> 00:13:40,160
We have to look at how it binds to the protein, right. That's the important interaction.

125
00:13:40,160 --> 00:13:45,280
And that's why we went into the protein structure prediction, which is the alpha-fold and

126
00:13:45,280 --> 00:13:51,440
the meta-ESM and all open-fold and all these tools. But then we said, we can't look at the protein

127
00:13:51,440 --> 00:13:57,840
in isolation. We have to look at how the protein and this molecule bind. And that's the joint

128
00:13:57,840 --> 00:14:03,520
prediction. And we also do that dynamically because the protein structure can change as it's

129
00:14:03,520 --> 00:14:09,440
finding. So that's modeling the dynamics. And now we are saying, all this is great, but this

130
00:14:09,440 --> 00:14:15,760
still doesn't tell me the function. You know, what are these proteins doing in our body or in

131
00:14:15,760 --> 00:14:21,040
organism? And that's where we need to go to the DNA level. And so we need to understand the

132
00:14:21,040 --> 00:14:27,440
language of the genome and not only learn those relationships between genes because they map

133
00:14:27,440 --> 00:14:34,000
two proteins, but also potentially generate new gene sequences and through that new proteins and

134
00:14:34,000 --> 00:14:40,240
new targets. And I think that's exciting because we're really going up the hierarchy and then

135
00:14:40,240 --> 00:14:46,640
essentially understanding the code of life, right, life on Earth. So I think it's very exciting times.

136
00:14:47,200 --> 00:14:54,000
Yeah. If the genome encodes the protein structure, how does that get, how does understanding

137
00:14:54,000 --> 00:15:00,640
that get you to function? Because it's at the genome level that you're determining a lot of

138
00:15:00,640 --> 00:15:05,760
functionalities of the organism, right? So it's encoding more than just protein structure.

139
00:15:06,320 --> 00:15:13,280
Yeah, absolutely. Because that's what evolution has endowed with, you know, all the fitness,

140
00:15:13,280 --> 00:15:18,960
right? Like which one survive, which ones are good proteins? You know, that's what is encoded

141
00:15:19,680 --> 00:15:27,200
in all our DNA as well as, you know, the DNA from all the way from viruses to all the higher organisms.

142
00:15:27,200 --> 00:15:34,560
And it's complicated, right? You can't just directly map one to one in higher organisms and that's

143
00:15:34,560 --> 00:15:42,080
why we started from virus and bacteria where it's clearer and simpler to analyze, but I think we can

144
00:15:42,640 --> 00:15:48,880
as these models get bigger as we create foundation models on long genome sequences and

145
00:15:49,440 --> 00:15:56,560
really get into understanding what in the latent space we can encode, right? And what does the

146
00:15:56,560 --> 00:16:02,240
generation of new sequences mean? And, you know, what kind of evolutionary gaps or other things

147
00:16:02,240 --> 00:16:06,400
they're filling? I think we can do a lot more than what we've done today.

148
00:16:07,840 --> 00:16:14,000
You mentioned diffusion a couple of times in the couple of works that you've discussed. Can you

149
00:16:14,000 --> 00:16:19,280
talk a little bit about the relationship between how are you using diffusion and how it's used in

150
00:16:19,280 --> 00:16:24,720
the context of stable diffusion? Yeah, absolutely. You know, last year has been exciting for

151
00:16:24,720 --> 00:16:34,480
generative AI and stable diffusion has really, you know, been a trial blazer in the sense that we

152
00:16:34,480 --> 00:16:42,400
have an open source model, right? Where has found a whole range of an ecosystem of applications

153
00:16:42,400 --> 00:16:48,320
and it can generate seamlessly new image candidates based on the text instruction.

154
00:16:48,320 --> 00:16:56,000
And so what the diffusion model at its core is a generative model that can sample new

155
00:16:56,720 --> 00:17:03,920
candidates, new realizations from a distribution it has learned. And the way it does is to go from,

156
00:17:03,920 --> 00:17:09,680
let's say, a Gaussian distribution, which is simple to sample from to the distribution of images,

157
00:17:10,400 --> 00:17:16,000
which would be hard to directly learn. But by learning this mapping, like think of it as a

158
00:17:16,000 --> 00:17:22,880
slow diffusion, you go from Gaussian noise and you, you know, progressively make it look more

159
00:17:22,880 --> 00:17:27,760
and more like the image. It'll be a noisy image and ultimately at the very end, all the noise

160
00:17:27,760 --> 00:17:35,760
would be filtered out and it would be the true image. So that slow process, how can we model that

161
00:17:35,760 --> 00:17:41,280
through learning these mappings, right? Learning this transformation from a Gaussian distribution to

162
00:17:41,280 --> 00:17:47,840
the image. And similarly, we can do that for scientific domains as well because it can, you know,

163
00:17:47,840 --> 00:17:54,240
the concept is you can learn any arbitrary distribution, probability distribution. And so we can

164
00:17:54,240 --> 00:18:01,360
learn now the probability distribution of how does the pros, you know, how do we create new genome

165
00:18:01,360 --> 00:18:06,720
sequences, right? What is the probability of them occurring, right? Because there is, you know,

166
00:18:06,720 --> 00:18:11,920
we know a lot of like there's high correlation between different gene sequences in a genome.

167
00:18:11,920 --> 00:18:17,680
You know, there's just like a code where some combinations are just not valid or they're not

168
00:18:17,680 --> 00:18:23,680
going to survive. They're not that, you know, relevant for the real world. And that's encoded in

169
00:18:23,680 --> 00:18:32,400
that data set of all known genome sequences that people have collected. And so by encoding that,

170
00:18:32,400 --> 00:18:40,080
we can learn now a better way to sample. And that's true for also so many other applications.

171
00:18:41,040 --> 00:18:48,720
Another setting we're thinking about over the last few years has been to look at all kinds of

172
00:18:49,920 --> 00:18:56,320
spatial temporal processes and the phenomena in science and engineering, right? Think about how

173
00:18:56,320 --> 00:19:04,640
fluids move, how does our weather forecasting change? How does our ability to predict earthquakes

174
00:19:04,640 --> 00:19:11,440
develop? And so so that's another area as well where uncertainty quantification is very important.

175
00:19:12,000 --> 00:19:16,480
And I think tools like this could be very effective when that's something we're working on.

176
00:19:16,480 --> 00:19:25,280
Are you for seeing kind of the diffusion applying diffusion to all the things? Is it a tool that we

177
00:19:25,280 --> 00:19:31,120
kind of stumbled upon for, you know, generating cool images, but it's going to have much broader

178
00:19:31,120 --> 00:19:36,960
implications over the coming years? I think it's one of the tools, right? I mean, the question of

179
00:19:36,960 --> 00:19:43,200
which specific architecture and which specific framework can evolve over time. It was GANS

180
00:19:43,200 --> 00:19:49,680
a few years ago. And we saw like challenges in optimization. And maybe one day will overcome that

181
00:19:49,680 --> 00:19:55,920
optimization challenges, right? And I do think that's important for the scientific applications too

182
00:19:55,920 --> 00:20:02,400
because in sciences, we have a lot of constraints. You know, for instance, one setting we've been

183
00:20:02,400 --> 00:20:08,000
extensively working on is solving partial differential equations. And so they're incorporating the

184
00:20:08,000 --> 00:20:13,840
physics constraints, incorporating, you know, the fact that you should satisfy this equation. So

185
00:20:13,840 --> 00:20:20,720
you penalize if you don't satisfy that equation, right? Our constraints. And so these kind of

186
00:20:20,720 --> 00:20:27,600
constrained optimization frameworks are similar to again, right? Because they're primal dual

187
00:20:27,600 --> 00:20:34,000
optimization. And so we still haven't solved those fundamental optimization issues. In deep learning,

188
00:20:34,000 --> 00:20:39,040
we've kind of swept it under the rug and said, okay, this problem is hard. So let's just develop

189
00:20:39,040 --> 00:20:45,040
a different methodology where we don't have to tackle that at all. And I think in the short run

190
00:20:45,040 --> 00:20:49,840
that gives us some gains, but at the same time in the long run, I think that comes with its own

191
00:20:49,840 --> 00:20:56,080
issues. You know, for instance, diffusion models are slow to sample because they have to

192
00:20:56,080 --> 00:21:02,960
gradually go from a Gaussian distribution to the distribution of images. And one of the frameworks

193
00:21:02,960 --> 00:21:09,520
we've developed to overcome that, you know, to do a decoding in parallel. So instead of sequentially

194
00:21:09,520 --> 00:21:16,320
sampling going from Gaussian to the image distribution, can we directly jump? And can we do it in

195
00:21:16,320 --> 00:21:22,960
a generalizable manner, not just overfitted one setting and then, right, it doesn't work in other

196
00:21:22,960 --> 00:21:30,080
settings. And the frameworks we are using in all these settings has been what we're called neural

197
00:21:30,080 --> 00:21:37,200
operators, which are, I think, a really important concept when it comes to scientific domain

198
00:21:37,200 --> 00:21:43,760
applications. They can solve differential equations like the one used in diffusion models for

199
00:21:43,760 --> 00:21:50,800
sampling. But they can also solve all kinds of other differential equations like in fluid dynamics,

200
00:21:50,800 --> 00:21:59,040
you know, the ability to model seismic waves underground to predict earthquakes to model carbon

201
00:21:59,040 --> 00:22:05,120
capture and storage, you know, as we mitigate climate change. How do we pump carbon dioxide

202
00:22:05,120 --> 00:22:10,800
deep underground? And how does it interact with water? These what we call multi-phase flow systems.

203
00:22:10,800 --> 00:22:18,480
And I think that to me conceptually is different from the standard neural networks. And that becomes

204
00:22:18,480 --> 00:22:24,480
a really important tool for scientific domains. Can you dig into that a little bit more? What does a

205
00:22:24,480 --> 00:22:30,720
neural operator look like and how did you arrive at that? Yeah, absolutely. I think this has been

206
00:22:31,360 --> 00:22:38,000
such an exciting journey because we went about asking, you know, let's say I have the problem of

207
00:22:39,040 --> 00:22:45,600
saying how does the fluid dynamics evolve, right? I start the fluid and I shake it a little.

208
00:22:46,320 --> 00:22:52,160
How does the flow change over time? You know, it sounds very similar to a video prediction problem.

209
00:22:52,160 --> 00:22:58,320
So why not just use all the tools we've already developed to do this prediction? So it turns out

210
00:22:58,800 --> 00:23:05,760
the main challenge is that here the fine scales matter a lot, right? We know the turbulent

211
00:23:05,760 --> 00:23:12,240
phenomena like this. You can't just like filter out or smooth out the fine scales or the high

212
00:23:12,240 --> 00:23:18,560
resolution information. If you do that, you will get come up with bad predictions. On the other

213
00:23:18,560 --> 00:23:24,400
hand, in the natural image domain, we want to all the time filter out, right? All the pixel

214
00:23:24,400 --> 00:23:30,160
is just too high dimensional and mostly useless information. We want to go from pixels to object

215
00:23:30,160 --> 00:23:38,240
localization. And so that means we all the processes to remove all this irrelevant information

216
00:23:38,240 --> 00:23:43,680
and filter out smooth out. And that's what like convolutional filters do because locally,

217
00:23:43,680 --> 00:23:49,600
you're learning these ability to smooth out and extract only the relevant features like edges.

218
00:23:50,160 --> 00:23:56,080
But that's different in so many of the scientific simulations like fluid dynamics

219
00:23:56,080 --> 00:24:03,040
because you cannot just filter out or smooth out the higher resolution information, right?

220
00:24:03,040 --> 00:24:11,760
You have to keep that. And the consequence of that is you cannot just learn your input to

221
00:24:11,760 --> 00:24:18,320
output mapping in one resolution. So think of any kind of image generation or image prediction.

222
00:24:19,120 --> 00:24:25,760
You always specify, pre-specify, what is the resolution of the image you want to either generate

223
00:24:25,760 --> 00:24:30,640
or predict on, right? It doesn't work at a different resolution. People train and the train only

224
00:24:30,640 --> 00:24:37,360
at that one resolution, they use the model only at that resolution. Whereas for the scientific

225
00:24:37,360 --> 00:24:43,520
domains, that would break down. And that's why until now, until these neural operators,

226
00:24:45,280 --> 00:24:50,640
we proposed until that point, people never thought about replacing the full traditional

227
00:24:51,440 --> 00:24:55,920
numerical solvers, right? So the whole point was, let's keep the solvers, let's

228
00:24:56,560 --> 00:25:00,400
get measurements from them and then try to do super resolution on top of it.

229
00:25:00,960 --> 00:25:06,320
Still mapping from one fixed resolution to another fixed resolution. But on the other hand,

230
00:25:06,320 --> 00:25:11,920
we couldn't completely get rid of the numerical methods and the traditional solvers.

231
00:25:12,640 --> 00:25:19,760
And so the way we went about overcoming this fundamental drawback is to come up with now

232
00:25:20,240 --> 00:25:27,280
framework called neural operator. So once you've trained the model using some training data

233
00:25:27,280 --> 00:25:33,120
at certain resolution, at testing time or at inference time, you can test it at any resolution.

234
00:25:33,120 --> 00:25:38,640
So you can give it an input at a different resolution, even a higher resolution than what it

235
00:25:38,640 --> 00:25:45,280
has seen during training. And still, it can make valid predictions. And that's the concept that

236
00:25:46,080 --> 00:25:52,320
makes this both, first of all, important for scientific simulations because people want

237
00:25:52,320 --> 00:25:57,840
the flexibility to choose different measures, different sampling techniques, right? They don't

238
00:25:57,840 --> 00:26:03,360
want to limit to one resolution, one grid. And the other is it also gives it the superior ability

239
00:26:03,360 --> 00:26:08,480
to capture the fine scales, which is so important for the simulation frameworks.

240
00:26:09,040 --> 00:26:19,040
And you mentioned and explaining this convolutional operator is the idea here that the

241
00:26:19,040 --> 00:26:30,800
the neural operator is a kind of an abstraction of that idea and the exact relationship between

242
00:26:30,800 --> 00:26:35,760
the thing that you apply it to and the output is learned.

243
00:26:35,760 --> 00:26:41,200
Absolutely. You know, at a conceptual level, it's very similar to the current neural networks,

244
00:26:41,200 --> 00:26:46,880
right? Could be convolutional neural network transformers. You know, it does also try to learn

245
00:26:46,880 --> 00:26:53,440
a mapping from the input to output. But the difference here is that it doesn't just accept your

246
00:26:53,440 --> 00:26:58,960
input at one resolution. It has the flexibility to accept input at different resolutions.

247
00:26:59,760 --> 00:27:05,760
And which is lacking in our current standard networks. You know, if you take convolutional

248
00:27:05,760 --> 00:27:11,440
neural network, it learns filter at one resolution. So you give it input at a different resolution,

249
00:27:11,440 --> 00:27:19,600
right? It completely fails. And so this fundamentally says that we can now learn

250
00:27:20,080 --> 00:27:27,520
an operator, which is mapping between function spaces. So we are changing our input to a fix

251
00:27:27,520 --> 00:27:33,520
size, right? Could be a vector or matrix like image, which is of a fix size to one that is a

252
00:27:33,520 --> 00:27:39,360
function. And that function, you could sample anywhere in a domain, usually the continuous domain.

253
00:27:39,360 --> 00:27:45,600
So if it's a fluid flow, right? And I tell you, what is the domain? What is its boundary?

254
00:27:46,800 --> 00:27:51,520
You know, you could make your resolution finer and finer, right? Because it's a continuous domain.

255
00:27:51,520 --> 00:27:58,400
So you don't just limit to one resolution. And that's the ability we provide in these neural

256
00:27:58,400 --> 00:28:04,240
operators. Of course, the next question is, how do I make this practical, right? I mean, sure,

257
00:28:04,240 --> 00:28:11,200
this is a wish list. So far, what I said is, this is what I want out of a learnt mapping

258
00:28:11,920 --> 00:28:18,720
that is not present in the standard networks. Now, how do I make neural operator actually possible

259
00:28:18,720 --> 00:28:25,920
that can handle these different resolutions? And so the way we go about doing this is through

260
00:28:25,920 --> 00:28:31,840
Fourier domain operations. So what we call Fourier neural operator. And you can operate the Fourier

261
00:28:31,840 --> 00:28:40,480
transform at any resolution, even any grid, right? So conceptually, it has the expressivity to handle

262
00:28:40,480 --> 00:28:45,760
different grids, different resolutions. And is this where the traditional numerical methods come in?

263
00:28:45,760 --> 00:28:51,600
It is inspired by traditional methods, which also use Fourier transform. But the differences,

264
00:28:51,600 --> 00:28:58,080
we are marrying that with non-linear transformations, like in standard neural networks. That's the

265
00:28:58,080 --> 00:29:04,320
power that deep learning has, right? It's not just linear. You have non-linear activations.

266
00:29:04,320 --> 00:29:11,760
So we are combining the principles from signal processing, signal representation theory that

267
00:29:11,760 --> 00:29:20,240
using Fourier transforms can represent signals at any resolution. But now, you know, if I learn,

268
00:29:20,240 --> 00:29:25,200
like in the frequency domain weights, and I go back to the standard domain, that's not enough,

269
00:29:25,200 --> 00:29:31,600
right? That's just a linear transformation. And so I now do non-linear activations and keep doing

270
00:29:31,600 --> 00:29:39,360
these series of operations again and again, this way I can capture scales at different frequencies.

271
00:29:39,360 --> 00:29:45,440
So I can capture a big spectrum, even though like each set of operations is just linear.

272
00:29:46,320 --> 00:29:52,640
But I have non-linear activations in between. And so in a way, it's a marriage of the old and the new,

273
00:29:52,640 --> 00:30:00,480
right? The old being properties that Fourier transforms have that, you know, people in numerical

274
00:30:00,480 --> 00:30:07,280
methods have been using that as a way to express signal more efficiently. But then they assume

275
00:30:07,280 --> 00:30:12,560
linearity that, you know, you should be able to capture the spectrum of your input

276
00:30:13,280 --> 00:30:19,360
compactly through Fourier transform. And that's usually not true, right? Many of this have high

277
00:30:19,360 --> 00:30:25,600
frequencies. So just doing Fourier transform once isn't very efficient. But if you combine it with

278
00:30:25,600 --> 00:30:34,080
non-linear transformations of multiple such blocks, it is expressive. And in fact, we show that

279
00:30:34,080 --> 00:30:42,480
this becomes a universal approximator for functions basis. Meaning just as theoretically a standard

280
00:30:42,480 --> 00:30:47,520
neural network can universally approximate any function in a fixed dimension,

281
00:30:47,520 --> 00:30:55,760
we can now use neural operators and approximate any operator in a function space. So it can,

282
00:30:55,760 --> 00:31:01,840
it has the expressivity to handle any non-linear operators, like what we see as solutions

283
00:31:02,480 --> 00:31:06,960
of fluid dynamics and other partial differential equations, as an example.

284
00:31:06,960 --> 00:31:11,520
So you've gone from wish list to something that's more tangible, but you haven't,

285
00:31:12,160 --> 00:31:16,960
you know, yet demonstrated practicality. Can you talk a little bit about practicality?

286
00:31:16,960 --> 00:31:23,360
Oh, yeah, yeah, absolutely. And that's where, you know, the last year is where we really took this off

287
00:31:23,360 --> 00:31:30,000
to very practical and real-world large-scale applications. So one of them has been in the

288
00:31:30,000 --> 00:31:36,480
realm of weather forecasting, you know, our ability to predict what's going to happen in the next

289
00:31:36,480 --> 00:31:42,480
week or next two weeks, right? It is so critical. And especially extreme weather events, which are

290
00:31:42,480 --> 00:31:48,160
increasing in their intensity and scale as climate change, you know, intensifies.

291
00:31:48,960 --> 00:31:57,840
And so what we showed is that these framework using these kind of concepts from neural operator,

292
00:31:57,840 --> 00:32:04,960
we can predict whether as good as the current numerical weather models for as long as two weeks,

293
00:32:04,960 --> 00:32:14,000
but be about 45,000 times faster. Wow. And so that's, you know, really exciting. And a hard

294
00:32:14,000 --> 00:32:21,360
benchmark, because currently numerical weather models use numerical solvers sometimes over

295
00:32:21,360 --> 00:32:27,760
thousands of variables of partial differential equations. And, you know, that can take our

296
00:32:27,760 --> 00:32:35,280
some CPU clusters, right? Whereas our model works within a second, in fact, a quarter of a second

297
00:32:35,280 --> 00:32:40,720
on a single GPU. And we've open sourced this model, we've made this available to the community.

298
00:32:41,360 --> 00:32:47,200
And I think that's very exciting to, you know, see this kind of democratization, not just in the

299
00:32:47,200 --> 00:32:52,640
generative AI that's so much in the news, but in these scientific domains, you know, enabling

300
00:32:52,640 --> 00:32:58,320
everybody to run their own weather model and build all kinds of downstream applications, whether it's

301
00:32:58,320 --> 00:33:05,680
for agriculture, you know, can we come up with accurate regional predictions to, you know,

302
00:33:05,680 --> 00:33:12,640
that interest new sensor data, new information, and that can refine the scale locally to provide better

303
00:33:12,640 --> 00:33:20,480
estimates. We can also ask how this information could be used in conjunction with a climate model,

304
00:33:20,480 --> 00:33:25,280
because not just looking at the weather of today, but what about the weather of the future?

305
00:33:26,160 --> 00:33:32,160
How well does it extrapolate? And these are aspects that we are right now working closely with

306
00:33:32,800 --> 00:33:40,480
meteorologists and climate scientists, both at NVIDIA, as well as the broader community,

307
00:33:40,480 --> 00:33:46,080
and that's been very exciting. And that's just one of the examples. The other one that I've mentioned

308
00:33:46,080 --> 00:33:53,520
before has been in carbon capture and storage, you know, so climate change is upon us, and I think

309
00:33:53,520 --> 00:34:00,800
a lot of scientists believe that the only way to completely, you know, tackle this is to mitigate it

310
00:34:00,800 --> 00:34:06,880
through frameworks like capturing carbon, right? Whether it's directly from that atmosphere,

311
00:34:06,880 --> 00:34:11,760
what is known as direct-air capture, you know, people are also trying that from the ocean,

312
00:34:11,760 --> 00:34:19,280
or it's like as it's being produced, you kind of isolate and capture it and pump it deep underground.

313
00:34:19,280 --> 00:34:24,880
And so they're the phenomena that we need to model is how does the carbon dioxide pressure

314
00:34:24,880 --> 00:34:30,880
build up deep underground? You know, as we interacted with water, it's a highly non-linear

315
00:34:30,880 --> 00:34:37,680
gas-plume evolution. And so how do we contain the pressure over several decades? And even there,

316
00:34:37,680 --> 00:34:45,360
we can see benefits as much as 700,000 times faster than numerical solvers. Because here,

317
00:34:45,360 --> 00:34:51,200
it's a very, again, fine-scale phenomena in this wells underground. You need to model how both

318
00:34:51,200 --> 00:34:55,920
carbon dioxide and water, you know, this is called multi-phase because you have both liquid and gas

319
00:34:56,480 --> 00:35:02,400
interact. And so modeling that non-linear phenomena. But also, it's not just in a single well,

320
00:35:02,400 --> 00:35:08,240
it's multiple such wells, but there's some porosity, right? There's some permeability. They're still

321
00:35:08,240 --> 00:35:15,200
interact. And so having a multi-grid approach and the ability to capture both the cores and the

322
00:35:15,200 --> 00:35:21,680
fine-skills is so important in this application. And so that's just another application of

323
00:35:22,560 --> 00:35:29,600
these methods. And yeah, it's been very exciting. We have now, whether forecasting, carbon capture

324
00:35:29,600 --> 00:35:35,040
and storage, you know, modeling deformation in materials, you know, how much can I stretch

325
00:35:35,040 --> 00:35:42,800
this material, right? How plastic it is is also a very non-linear phenomena. And we can, again,

326
00:35:42,800 --> 00:35:48,240
show hundreds of thousands of times speed up over traditional solvers. We're able to

327
00:35:49,360 --> 00:35:57,040
do modeling of lithography process. So how do I go from, right, a mask designed to? What is the

328
00:35:57,040 --> 00:36:04,240
resist to make like what is finally being shown on the silicon wafer? And we also show the inverse

329
00:36:04,240 --> 00:36:09,360
problem, which is really critical in many of these applications because you want to design a better

330
00:36:09,360 --> 00:36:17,680
mask to be able to create the wafer of like desirable properties, right? So inverse problems are

331
00:36:17,680 --> 00:36:23,040
especially hard with numerical methods because if you keep running this forward simulation,

332
00:36:23,040 --> 00:36:29,200
which is very expensive. And but again, this is not data driven. So it's very hard to

333
00:36:30,080 --> 00:36:35,440
make changes or explore the design space. But what we showed with our

334
00:36:36,480 --> 00:36:42,560
neural operator-based methods is that we can progressively self-trainer improve. You know,

335
00:36:42,560 --> 00:36:48,960
we can keep creating better masks and we can train on them. And with that, it can generate better

336
00:36:48,960 --> 00:36:54,640
candidates. And so all of this, what you see with, you know, reinforcement learning or progressive

337
00:36:54,640 --> 00:37:01,440
self-training, all these phenomena, in other general AI applications, we can bring all those tools

338
00:37:01,440 --> 00:37:08,720
here as well. And marry it with neural operators because that gives the right foundation to capture

339
00:37:08,720 --> 00:37:13,200
all the fine scales, which are very important in many of these scientific domains.

340
00:37:13,200 --> 00:37:22,560
It sounds like the various applications you've described all involved kind of research efforts

341
00:37:22,560 --> 00:37:30,720
to apply neural operators to a particular domain. You know, I imagine what you eventually want

342
00:37:30,720 --> 00:37:35,680
is a tool that you take off the shelf and, you know, assuming you know your, you know,

343
00:37:35,680 --> 00:37:42,480
underlying PDE models, you can more, you can readily apply this tool. Can you talk a little bit

344
00:37:42,480 --> 00:37:48,160
about, you know, A, is that a fair characterization, and B, kind of, how do you think you get there?

345
00:37:48,160 --> 00:37:54,160
Yeah, I mean, as all these efforts, right, in the beginning, it's been very important to work with

346
00:37:54,160 --> 00:37:59,200
domain scientists, and that's been the goal of also the AI for Science Initiative when I found

347
00:37:59,200 --> 00:38:05,120
it at Caltech. But now, it's even broader than just the Caltech community, right? We are working

348
00:38:05,120 --> 00:38:11,760
with national labs, we are working with other universities, closely working, with also,

349
00:38:11,760 --> 00:38:17,360
NVIDIA engineers who are enabling and helping scale up all these frameworks and open sourcing

350
00:38:17,360 --> 00:38:22,080
it to the community. And I think that kind of building trust in the beginning is important,

351
00:38:22,800 --> 00:38:29,200
because, you know, Sally Benson at Stanford is considered a leader in carbon capture and storage,

352
00:38:29,200 --> 00:38:37,920
right? And we are collaborating with ECMWF, the European weather agency that, you know, you,

353
00:38:37,920 --> 00:38:42,960
that, in fact, created the data sets for these, you know, historical weather. And so working with

354
00:38:42,960 --> 00:38:48,800
them and asking, what are the metrics that matter here, right? You know, it's not just the short term,

355
00:38:48,800 --> 00:38:54,720
what is the long term behavior? What are the uncertainties? And I think that aspect of the deep domain

356
00:38:54,720 --> 00:39:01,760
expertise being married with AI and fully integrated is important. But you're perfectly right,

357
00:39:01,760 --> 00:39:09,360
as we show that these proof of concepts are really solid, they've already, right, been making

358
00:39:09,360 --> 00:39:15,120
impact in these domains. The next question remains, how do we generalize? You know, how do we create

359
00:39:15,120 --> 00:39:22,160
foundation models for science and engineering? And yeah, and that's a mission that I'm undertaking

360
00:39:22,160 --> 00:39:28,480
now. And I should say it's not straightforward, it's because one is we don't have the data,

361
00:39:28,480 --> 00:39:35,360
like what we do for text or images, right? And even there, it required quite a bit of curation

362
00:39:35,360 --> 00:39:42,160
effort. And here we need to think what that is. The other aspect we've been working really

363
00:39:42,160 --> 00:39:46,960
well on is how to incorporate the right physics constraints. What are the meta-learning and other

364
00:39:46,960 --> 00:39:53,520
approaches to generalize beyond one domain? And we've been seeing really good success. So I do think

365
00:39:53,520 --> 00:39:59,040
it's a great time to scale up, but we need to bring all these pieces together. And that's what I

366
00:39:59,040 --> 00:40:07,440
hope to do this year. Yeah, how different does the application to whether look relative to the

367
00:40:07,440 --> 00:40:13,520
application to carving capture? Yeah, that's a great question. You know, there's some architectural

368
00:40:13,520 --> 00:40:18,880
details that could be different. For instance, one of the newest architectures we are

369
00:40:18,880 --> 00:40:25,200
are experimenting in the weather case involves spherical geometry, right? Because we know we want to

370
00:40:25,200 --> 00:40:30,160
predict the weather on the surface of the earth that's a sphere. So by incorporating that

371
00:40:30,160 --> 00:40:36,800
geometric information, how much better can you do, right? So, and that's always this balance

372
00:40:36,800 --> 00:40:43,120
between, you know, knowing more about the domain incorporating the inductive bias versus

373
00:40:43,120 --> 00:40:49,680
via a generalist model that may know less about it, but with more data could do well.

374
00:40:50,400 --> 00:40:56,560
But the underlying concepts are still same. And I think there is a way to still capture the aspect

375
00:40:56,560 --> 00:41:05,120
that, you know, the underlying, right, if you only look at say the turbulence, right, there is

376
00:41:05,120 --> 00:41:11,680
similarity. But then on the carbon capture and storage, that interaction with water is very

377
00:41:11,680 --> 00:41:16,720
important to model. Whereas in the weather, there's certainly water, we have the oceans,

378
00:41:16,720 --> 00:41:23,040
but that's usually considered a longer timescale evolution compared to the shorter timescale,

379
00:41:23,040 --> 00:41:29,680
wind surface wind and other variables. And so people conveniently kind of ignore some of the

380
00:41:29,680 --> 00:41:37,200
phenomena or simplify it for different domains, which I think is very important for tractability.

381
00:41:37,200 --> 00:41:46,080
But yeah, I do think that, you know, there is a broader range of phenomena that could be

382
00:41:46,080 --> 00:41:52,880
captured through a common model. And it need not be a model that's learning everything all at once,

383
00:41:52,880 --> 00:42:00,240
right? It could kind of provide the seed or initialization for other models to further hone in and

384
00:42:00,240 --> 00:42:10,480
find you and on a very specific domain. When you're referring to your results with weather,

385
00:42:10,480 --> 00:42:19,200
for example, and you have the significant speed up, is that operating under the same

386
00:42:20,480 --> 00:42:26,800
sets of variables, or are you, is the model that you're referring to simplified relative to

387
00:42:26,800 --> 00:42:35,120
the model that you're comparing against. And likewise, one of the big motivations here is the

388
00:42:35,120 --> 00:42:43,040
ability to handle, you know, different resolutions. Are you, you know, when you are comparing those

389
00:42:43,040 --> 00:42:49,360
models, are you looking at the same resolutions, different resolutions? Yeah, absolutely. First of all,

390
00:42:49,360 --> 00:42:56,480
we're even not even currently taking all the information that's available in the historical data,

391
00:42:56,480 --> 00:43:01,520
right, and all the variables. So, you know, in the beginning, when we started this project,

392
00:43:01,520 --> 00:43:06,640
we wanted to do just a rough cut. We thought, okay, let's start with a sub sample of data and

393
00:43:06,640 --> 00:43:13,040
a selection of variables just to see what we get. And we didn't expect it to be producing such

394
00:43:13,040 --> 00:43:19,760
good results with even just that subset. And that's where I think, you know, there is a potential,

395
00:43:19,760 --> 00:43:26,400
and we, you know, see evidence that we can even beat the current weather predictions by training

396
00:43:26,400 --> 00:43:32,720
bigger models on all of the data available. And the weather is a great use case because

397
00:43:32,720 --> 00:43:38,320
we have wealth of historical data available, you know, that's collected since the 1970s,

398
00:43:38,320 --> 00:43:45,280
collected hourly, right, whereas these numerical weather models hardly use any of that data. So,

399
00:43:45,280 --> 00:43:52,480
they're using it to just calibrate a few coefficients or parameters in their numerical methods.

400
00:43:53,120 --> 00:43:58,000
And so, in the sense, they're redoing these computations again and again without having the

401
00:43:59,520 --> 00:44:07,040
a wealth of data being understood and incorporated into the calculations. And I think that's where

402
00:44:07,040 --> 00:44:13,760
the, you know, impressive speed up comes, right? One of the important reasons is because these

403
00:44:13,760 --> 00:44:22,720
AI-based methods can learn from data and they can learn better ways to do these numerical iterations,

404
00:44:22,720 --> 00:44:30,480
whereas the standard PDE solvers are forced to go to a very fine grid. As I said, we cannot ignore

405
00:44:30,480 --> 00:44:37,840
the fine scale phenomena. So, they have to like for guarantees for convergence be operating at a

406
00:44:37,840 --> 00:44:44,400
very fine grid and that makes them expensive. And so, our ability to learn from data better

407
00:44:44,400 --> 00:44:49,840
nonlinear transformations where computations become quicker I think is a key to the speed up.

408
00:44:50,560 --> 00:44:55,920
And regarding your question of, you know, can we predict at higher resolutions? So,

409
00:44:55,920 --> 00:45:03,280
currently the data that's available we've trained on is 25 kilometers spatial resolution, right?

410
00:45:03,280 --> 00:45:10,960
So, it's 25 kilometers proofed across the globe. And so, we are working with regional

411
00:45:11,600 --> 00:45:16,320
weather agencies to see if we can get now, you know, we are in fact getting some of the higher

412
00:45:16,320 --> 00:45:20,960
resolution data. And then we can ask, you know, how do the predictions look? And is there even

413
00:45:20,960 --> 00:45:28,080
potential to further refine those? And I think that's why this kind of a foundation model

414
00:45:28,080 --> 00:45:35,040
is valuable because you can go from being able to predict all across the globe to regional models

415
00:45:35,040 --> 00:45:41,280
where especially countries that don't have a lot of computing capabilities or

416
00:45:41,280 --> 00:45:46,800
technical abilities, right? Could start with a reasonably good model and hone in

417
00:45:47,360 --> 00:45:54,160
on their region and refine in a much more cost effective manner rather than running these

418
00:45:54,160 --> 00:45:59,200
numerical methods which are extremely expensive as you get to the fine grids.

419
00:45:59,200 --> 00:46:06,640
So, just to give you an intuition of how expensive it gets as you refine the grid for these kind

420
00:46:06,640 --> 00:46:13,600
of calculations, my colleague, Tapio Schneider here at Caltech Hours in Climate Sciences

421
00:46:14,160 --> 00:46:20,160
estimates that you need about 100 billion times more computing than what we have today.

422
00:46:20,160 --> 00:46:27,360
If we have to go to the actual finest resolution to be able to capture all these

423
00:46:27,760 --> 00:46:33,280
turbulence in low-lying clouds, which for climate models is the biggest source of uncertainty.

424
00:46:34,000 --> 00:46:38,480
And so that, you know, requires computing at the resolution of one meter.

425
00:46:39,200 --> 00:46:43,920
You know, I'm talking 25 kilometers, right? We need to go all the way to one meter,

426
00:46:43,920 --> 00:46:49,840
spatial resolution all across the globe and one second in time temporal resolution.

427
00:46:50,800 --> 00:46:57,280
And so that's, you know, the more slow is no longer upon us and even all this

428
00:46:57,280 --> 00:47:03,120
acceleration and scaling will not get us to 100 billion times, right? And even if they say,

429
00:47:03,120 --> 00:47:08,720
oh, that's too much, what if we get somewhere intermediate? It's still too large and that's where

430
00:47:08,720 --> 00:47:14,960
I think machine learning is a necessity, otherwise there's no way we can tackle these deepest

431
00:47:14,960 --> 00:47:23,920
questions in scientific domains. With these models, you've referred to the relative speed up.

432
00:47:25,200 --> 00:47:31,280
In the case of the weather, for example, in terms of performance, predictive performance,

433
00:47:31,280 --> 00:47:35,440
how did they, this method compare to traditional numerical models?

434
00:47:35,440 --> 00:47:42,640
Yeah, absolutely. In the case of the weather we are doing, as well as the current weather models

435
00:47:42,640 --> 00:47:50,160
for as much as two weeks, which is considered where, right, it's very predictive. So after that

436
00:47:50,160 --> 00:47:55,440
and a bit longer, you kind of get to the subsisinal and seasonal scales where it's no longer

437
00:47:56,000 --> 00:48:03,200
predictable. So it becomes chaotic and, you know, you can kind of say, right, statistical measures

438
00:48:03,200 --> 00:48:09,440
on average, what it would look like, but not the actual trajectory, the actual weather, what

439
00:48:10,240 --> 00:48:15,600
it is because it's just not predictive. And that's another challenge too with many of these

440
00:48:15,600 --> 00:48:23,760
models, right? So for short term, we can predict, but for the longer term, we can only simulate and

441
00:48:23,760 --> 00:48:29,440
on average get to what's known as the invariant measure, which is the attractor or the right

442
00:48:29,440 --> 00:48:36,400
distribution, right? Because you cannot predict exactly where the trajectory evolves. And we are

443
00:48:36,400 --> 00:48:43,040
also working on now using these neural operators and we've shown their ability not only to capture

444
00:48:43,040 --> 00:48:49,440
short term phenomena, but also the long term behavior by capturing the nature of chaotic

445
00:48:50,800 --> 00:48:55,200
systems and hence, through that, be able to simulate them again effectively.

446
00:48:55,200 --> 00:49:02,160
Do you characterize the failure or error of these models and like compare that to the weight

447
00:49:02,160 --> 00:49:10,320
traditional error models fail, thinking about, you know, whether it's something that, you know,

448
00:49:10,320 --> 00:49:15,840
we, there's kind of this inherent, you know, we joke about how bad weather models are, right? And,

449
00:49:15,840 --> 00:49:22,320
and how, and how, you know, difficult it is to predict the weather, but then a lot of people

450
00:49:22,320 --> 00:49:32,080
count on those predictions in very important ways, thinking about like if we, you know, start

451
00:49:32,080 --> 00:49:37,840
predicting the weather in different ways and those predictions are, you know, generally better,

452
00:49:37,840 --> 00:49:44,400
but occasionally much worse, like how does, how does, do we communicate that to the users of

453
00:49:44,400 --> 00:49:51,120
the predictions? And I'm just curious, you know, how that, you know, factors into the way you think

454
00:49:51,120 --> 00:49:57,520
about the problem, right? And that is a big challenge, right? Like as you said, you know, we need

455
00:49:57,520 --> 00:50:02,320
the uncertainty quantification as well, you know, are we getting to the right, uh,

456
00:50:03,120 --> 00:50:09,680
probabilities? So for instance, if we slightly perturb the initial condition, how does my prediction

457
00:50:09,680 --> 00:50:16,000
change? And so that's an important notion of stability and, and, and also because we have some

458
00:50:16,000 --> 00:50:20,880
uncertainty on our input, right? Like the, what we are measuring from our satellites and how

459
00:50:20,880 --> 00:50:28,000
that is all assimilated has, uh, certainly errors, right? It has uncertainties. And so this kind

460
00:50:28,000 --> 00:50:34,000
of what we call unsombling because you're not just feeding in one fixed initial condition,

461
00:50:34,480 --> 00:50:40,080
but you perturb it and then you ask, even with those perturbations, what is the output now?

462
00:50:40,080 --> 00:50:45,200
And through that, can I get a measure of the uncertainty or probabilities? And so we can also

463
00:50:45,200 --> 00:50:53,280
calibrate our weather models, AI-based ones, um, similar and even better many times than the current

464
00:50:53,280 --> 00:50:58,560
numerical weather models because we are much cheaper, right? So these numerical weather models

465
00:50:59,200 --> 00:51:06,000
will be surprised to use only about 50 ensemble members. So what kind of statistical

466
00:51:06,000 --> 00:51:13,120
averaging do we get out of 50, right? So 50 samples. And but that's so expensive. So, you know,

467
00:51:13,120 --> 00:51:19,280
people are using large clusters even for one single prediction and takes a few hours.

468
00:51:19,840 --> 00:51:26,480
So they are bound by the cost. And that's where this is another big benefit that we are seeing from

469
00:51:27,440 --> 00:51:33,200
AI-based models because it is so cheap. You can now run thousands of ensemble members like we

470
00:51:33,200 --> 00:51:40,160
are doing now. And we're carefully testing how does it do in all kinds of scenarios, right? We

471
00:51:40,160 --> 00:51:46,160
show, for instance, it can nicely capture the uncertainty around hurricane evolution. So many

472
00:51:46,160 --> 00:51:51,840
of the famous hurricanes that didn't appear in the training data, but we are testing them on,

473
00:51:51,840 --> 00:51:58,080
you know, how not just like the single trajectory, right? Because people care about uncertainties.

474
00:51:58,080 --> 00:52:03,920
And that has all kinds of downstream implications, you know, does hurricane hit Florida or go into

475
00:52:03,920 --> 00:52:11,200
the ocean, right? That makes all the difference. And so being able to get to the right uncertainties

476
00:52:11,200 --> 00:52:18,160
is such a big aspect. And that's where AI models are already proving to be superior. And by

477
00:52:18,160 --> 00:52:25,040
honing in all that and giving the right ensemble level estimates very cheaply, I think will make

478
00:52:25,040 --> 00:52:30,480
this so valuable. And that's what we are seeing now and working with weather scientists.

479
00:52:30,480 --> 00:52:37,520
Now, all these applications that you've been talking about have traditionally been, we talked

480
00:52:37,520 --> 00:52:44,160
about kind of the traditional numerical approaches. Those of all driven kind of the development of

481
00:52:44,160 --> 00:52:55,280
high performance computing as a field. And I guess I'm curious what, you know, if you thought of

482
00:52:55,280 --> 00:53:03,680
HPC as a, you know, a pie or whatever, like how much of that pie is, you know, being eaten up by

483
00:53:03,680 --> 00:53:13,040
AI today and in time, you know, over time, does AI, you know, eat all of that pie or their elements,

484
00:53:13,040 --> 00:53:17,600
are the things that we're doing with HPC that, you know, we don't think or you don't think will be

485
00:53:18,800 --> 00:53:24,960
that are incompatible with AI in some way or does AI, you know, just come to be a standard tool

486
00:53:24,960 --> 00:53:30,720
that apply to that type of problem. Yeah, no, that's a great question. I mean, so many now

487
00:53:30,720 --> 00:53:38,240
national labs and other centers are rebranding us HPC AI, right, combination, which is great. And,

488
00:53:38,240 --> 00:53:44,960
you know, that's been our experience too, like kind of to think of that as an integrated approach

489
00:53:44,960 --> 00:53:52,560
to begin with, right? And that doesn't mean like keep the numerical methods as they are, keep an AI

490
00:53:52,560 --> 00:53:58,080
tool as a standard hammer, you know, take something that works on natural images, bring it here,

491
00:53:58,080 --> 00:54:05,520
right? That's not the, that to me is not a winning strategy. When I say hybrid HPC AI, it's really

492
00:54:05,520 --> 00:54:11,840
thinking conceptually at an algorithmic level and say what does an integrated algorithm look like?

493
00:54:11,840 --> 00:54:16,720
And that's how we came up with Fourier neural operator, right? So we know numerical methods

494
00:54:16,720 --> 00:54:22,640
compute in the spectral domain that's efficient and that's also a nice way to create a basis that

495
00:54:22,640 --> 00:54:32,320
works in any dimensions and any resolution because it is having the ability to learn in a function

496
00:54:32,320 --> 00:54:40,320
space itself. So those, you know, thinking and that intuition, we don't need to reinvent from

497
00:54:40,320 --> 00:54:45,520
scratch. We already have so much that's been dealt in numerical methods, right? On the other hand,

498
00:54:45,520 --> 00:54:51,200
we know what's also the downside of numerical methods. They can't be data driven. So you can't

499
00:54:51,200 --> 00:54:56,720
think that computation based on if it's an easier or a harder sample, right? And you can't use all

500
00:54:56,720 --> 00:55:02,800
the existing computations you've done to learn and improve. And that's what AI provides. The other

501
00:55:02,800 --> 00:55:10,000
is all these non-linear transformations, which is right also an effect of being data driven

502
00:55:10,000 --> 00:55:16,160
that we have the ability to learn these non-linear transformations. So I think to me,

503
00:55:16,160 --> 00:55:21,120
ultimately, they'll all come together. But I don't want to get into the debate of what was

504
00:55:21,120 --> 00:55:28,560
numerical method, what was AI? You know, in that I worked like baby, you shouldn't be able to tell

505
00:55:28,560 --> 00:55:34,640
them apart. That's kind of the whole point that they just seamlessly integrate together. That's

506
00:55:34,640 --> 00:55:41,600
my hope and that's what we are working towards. Are there other ways that you see the

507
00:55:42,560 --> 00:55:48,640
kind of advances around, you know, that are happening broadly in AI, kind of driving AI for

508
00:55:48,640 --> 00:55:55,280
scientific applications? Yeah, as we've been talking quite a bit last year has been the

509
00:55:55,280 --> 00:56:02,000
era of generative AI and foundation models, right? So these big models having the ability,

510
00:56:02,000 --> 00:56:07,920
not just to do now one targeted task like we were doing for the past decade, but really be general

511
00:56:07,920 --> 00:56:13,440
purpose. You know, they're not all the way, but if zero short, you can't get a good answer,

512
00:56:13,440 --> 00:56:18,160
you could do few short, right? And you can give some examples. You could even fine tune.

513
00:56:18,800 --> 00:56:24,880
And what we've been very excited is taking one step further and asking how we can use these

514
00:56:24,880 --> 00:56:30,720
foundation models for decision making, you know, using with reinforcement learning,

515
00:56:30,720 --> 00:56:38,080
limitation learning, all the tools, which are conceptually the right thing to adapt to new

516
00:56:38,080 --> 00:56:44,880
environments and make decisions, but are very, very expensive, right? We really haven't broken

517
00:56:44,880 --> 00:56:54,720
beyond standard game settings and still fairly low dimensional action space to one now that is

518
00:56:54,720 --> 00:57:00,720
open world setting. And that's where I think there's been a lot of exciting work and one of the

519
00:57:00,720 --> 00:57:08,320
works that we've provided as a benchmark to the community is called Mind Audio. So it's a suite of

520
00:57:09,200 --> 00:57:17,120
thousands of tasks in Minecraft along with internet scale information about YouTube videos of how

521
00:57:17,120 --> 00:57:24,080
people are playing Minecraft, how they're building structures, wiki, ready articles. So all kinds

522
00:57:24,080 --> 00:57:30,000
of like text, image, tabular data, all the information that you can clean. And from that,

523
00:57:30,000 --> 00:57:37,440
can you now solve not just one task, but thousands of tasks, right? So what we call as open-ended

524
00:57:38,240 --> 00:57:45,040
task solving. And so, you know, if I give the instruction in text-based prompts and say go

525
00:57:45,040 --> 00:57:51,520
minor diamond or go build me a castle, right? So it should figure out how to do it, but it's not

526
00:57:51,520 --> 00:57:57,920
doing that from scratch, like what we saw in Alpha Go, right? Or Alpha Zero, rather we said either it's

527
00:57:57,920 --> 00:58:05,360
some limited imitation data or it is from scratch, both of which are still way too expensive in these

528
00:58:05,360 --> 00:58:12,000
kind of domains. And that's where I think Minecraft is different from other games because it's not

529
00:58:12,000 --> 00:58:19,280
one goal, right? It's all about unleashing creativity and solving all kinds of new tasks coming up with

530
00:58:19,280 --> 00:58:26,000
new structures. You can create a castle, you can create a flying dragon, all kinds of new structures.

531
00:58:26,560 --> 00:58:32,480
But you still need to understand the environment and its complexity is huge. It's not one tool,

532
00:58:32,480 --> 00:58:39,440
it's not killing somebody, it's not trying to negotiate and imitate what we saw in training data,

533
00:58:39,440 --> 00:58:47,040
right? It is having to solve and learn something new. But all the information that's available as

534
00:58:47,040 --> 00:58:53,600
foundation models through videos, through text will help us towards that goal. And that's

535
00:58:53,600 --> 00:59:00,800
where I think that benchmark, you know, we got the outstanding paper awarded Nureps, which I'm

536
00:59:00,800 --> 00:59:06,880
very proud of what the team did. Congrats on that. Thank you. But it also paves the way for the

537
00:59:06,880 --> 00:59:13,280
community to take this as a new challenge, right? And this is in a way much, much harder than any of

538
00:59:13,280 --> 00:59:21,520
the game playing bots we've seen solve. But at the same time is to me very timely and relevant

539
00:59:21,520 --> 00:59:26,800
because all these foundation models are becoming so powerful. But it's challenging them beyond

540
00:59:26,800 --> 00:59:32,240
their current capabilities, right? Because it's not just learning what they've already seen,

541
00:59:32,720 --> 00:59:38,240
but pushing the envelope to create new worlds, new structures using all that knowledge.

542
00:59:38,240 --> 00:59:44,560
And is it a formal challenge in the sense that there's a competition and a leaderboard and all that?

543
00:59:44,560 --> 00:59:51,920
We have the website where all the information is available. We haven't launched a leaderboard

544
00:59:51,920 --> 00:59:57,360
because this ultimate goal is very hard, right? So people are solving there is there was a deep

545
00:59:57,360 --> 01:00:05,600
mind paper that's all only mining diamonds, for instance. And we are building to end our first

546
01:00:05,600 --> 01:00:13,520
work showed that you can, you know, do solve multiple tasks by using what we call a mine clip,

547
01:00:13,520 --> 01:00:19,920
meaning you look at Minecraft videos, you connect it with text. So you create a clip like model

548
01:00:20,480 --> 01:00:26,960
and use that as a way to get dense rewards, right? And so you make progress towards the goal.

549
01:00:26,960 --> 01:00:32,400
And that is just the first step. And so we are working on it. We have no others in the community

550
01:00:32,400 --> 01:00:38,640
are. So as people start solving it, it'll be easy to create a leaderboard. So it's still a bit

551
01:00:39,520 --> 01:00:44,880
you know, out of reach, I would say from what the community is capable. So that's why it's

552
01:00:45,760 --> 01:00:55,280
the challenge for the next decade. Do you expect that the models that perform, you know, best in

553
01:00:55,280 --> 01:01:00,720
here will be based on kind of these foundation models, language models, you know, in some way,

554
01:01:00,720 --> 01:01:09,040
or is that still, you know, a way to open ended a question or I mean the foundation models are a

555
01:01:09,040 --> 01:01:15,760
necessity to begin with, right? Because we are providing text instructions on what to do in Minecraft.

556
01:01:15,760 --> 01:01:21,600
So you're describing the task in text. And it could be entirely new things that, right,

557
01:01:21,600 --> 01:01:27,440
this agent hasn't seen before. And so you need to go to now the foundation models to understand

558
01:01:27,440 --> 01:01:32,720
the text, but also understand what this image means, right? And go to the wiki and see,

559
01:01:32,720 --> 01:01:38,320
oh, this is what a hammer is. Now, how do I go and pick up a hammer where do I find it?

560
01:01:38,320 --> 01:01:45,360
So even to just get started, all this is necessary, right? And so that because we are not pre-programming

561
01:01:45,360 --> 01:01:51,760
all of this, whereas if it's one fixed environment, you just give all this rules beforehand. You set

562
01:01:51,760 --> 01:01:57,760
what the objective is. You design the reward function beforehand, right? So then you let reinforcement

563
01:01:57,760 --> 01:02:04,160
learning do its magic, which can take very long and can be very challenging to do, but it's all

564
01:02:04,160 --> 01:02:11,280
preset. Whereas in mind, Ojo, none of this is given beforehand. So to even get started, you need this,

565
01:02:11,280 --> 01:02:17,280
but it's a great test for foundation models because what can you really learn from it? You know,

566
01:02:17,280 --> 01:02:24,240
I give you everybody playing Minecraft before. You know, what can you lean from it, but still we

567
01:02:24,240 --> 01:02:29,760
are asking for something new beyond that. You know, how well can you do problems solving? And I think

568
01:02:29,760 --> 01:02:35,520
it's exciting to see how that develops. Awesome. Awesome. And I feel like there's a whole interview

569
01:02:35,520 --> 01:02:44,080
just on on that topic. Absolutely. And in fact, with new reps, there was a workshop on foundation

570
01:02:44,080 --> 01:02:51,280
models for decision making, and there's just so much excitement, because I think that'll provide

571
01:02:51,280 --> 01:02:58,000
in my view the right starting point, the right initialization for some of really challenging

572
01:02:58,000 --> 01:03:03,520
the reinforcement learning problems, because I would argue that reinforcement learning has shown,

573
01:03:04,080 --> 01:03:09,680
you know, really fantastic gains, but in highly focused and limited domains, right? It hasn't

574
01:03:09,680 --> 01:03:16,320
become the generalist agent like we see with, say, generative modeling, you know, you can sample

575
01:03:16,320 --> 01:03:24,320
from distribution through diffusion models or GPT-like models very easily, but that's not task-oriented,

576
01:03:24,320 --> 01:03:30,800
that's not, you know, learning online with the reward function. And so these two have to marry

577
01:03:30,800 --> 01:03:37,520
together to bring that generality while being able to learn online, improve, adapt, and make better

578
01:03:37,520 --> 01:03:44,720
decisions. Yeah. Well, clearly, I mean, three plus years is too long between our conversations,

579
01:03:44,720 --> 01:03:51,360
because there's so much to cover. Certainly been an exciting time, and I'm privileged to have

580
01:03:51,360 --> 01:03:58,320
a great team that is enabling me to, you know, think about such a wide range of problems. So that's

581
01:03:58,320 --> 01:04:03,520
been very exciting. Awesome. Absolutely. Well, thanks so much for joining us and sharing a bit about

582
01:04:03,520 --> 01:04:09,120
what you've been up to. Thanks a lot Sam. Well, it's been a pleasure and I hope you have a great

583
01:04:09,120 --> 01:04:39,040
rest of 2023. Absolutely. You too. Thank you.

