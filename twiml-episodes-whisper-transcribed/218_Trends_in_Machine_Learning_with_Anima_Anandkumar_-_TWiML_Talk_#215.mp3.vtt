WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

00:35.920 --> 00:40.640
to present to you our first ever AI rewind series.

00:40.640 --> 00:45.040
In this series I interview friends of the show for their perspectives on the key developments

00:45.040 --> 00:49.720
of 2018 as well as a look ahead at the year to come.

00:49.720 --> 00:54.360
We'll cover a few key categories this year, namely computer vision, natural language

00:54.360 --> 00:59.200
processing, deep learning, machine learning and reinforcement learning.

00:59.200 --> 01:03.840
Of course we realize that there are many more possible categories than these, that there's

01:03.840 --> 01:08.840
a ton of overlap between these topics and that no single interview could hope to cover

01:08.840 --> 01:12.240
everything important in any of these areas.

01:12.240 --> 01:17.120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

01:17.120 --> 01:25.840
by commenting on the series page at twimbleai.com slash rewind 18.

01:25.840 --> 01:31.280
In this episode of our AI rewind series we're back with Anima Anand Kumar, brand professor

01:31.280 --> 01:36.560
at Caltech and now director of machine learning research at NVIDIA to discuss her take on

01:36.560 --> 01:41.000
trends in the broader machine learning field in 2018 and beyond.

01:41.000 --> 01:45.880
In our conversation we cover not only technical breakthroughs in a field but also those around

01:45.880 --> 01:47.960
inclusivity and diversity.

01:47.960 --> 01:48.960
Enjoy.

01:48.960 --> 01:56.880
All right everyone, I am on the line with Anima Anand Kumar, Anima is the brand professor

01:56.880 --> 02:02.560
at Caltech in the CMS department and as of a couple of months ago the director of machine

02:02.560 --> 02:08.360
learning research at NVIDIA as well as being a good friend of the show you can hear her

02:08.360 --> 02:13.960
previous conversation at twimble talk 142, Anima welcome back to this week of machine learning

02:13.960 --> 02:15.960
and AI.

02:15.960 --> 02:22.760
Thanks a lot Sam, it's great to be back the past few months have been so exciting and so

02:22.760 --> 02:27.880
happy to see you do this show reviewing the year.

02:27.880 --> 02:33.120
It is going to be a fun conversation, I am sure, I am sure.

02:33.120 --> 02:38.640
Before we get too far in maybe you can kind of share a little bit about what you're up

02:38.640 --> 02:40.120
to at NVIDIA.

02:40.120 --> 02:48.640
Yeah, it's been an exciting couple of months at NVIDIA as we have really so many new frameworks

02:48.640 --> 02:57.560
and we have many new researchers and engineers and others join us and my role is director

02:57.560 --> 03:04.080
of machine learning research there and that means I want to focus on core algorithmic

03:04.080 --> 03:10.800
problems, how do we optimize neural networks in a principled way or generalize, think about

03:10.800 --> 03:17.400
reinforcement learning, generative modeling, so a broad class of machine learning questions

03:17.400 --> 03:24.720
but focusing on the core algorithms, analyzing them, running careful scientific studies and

03:24.720 --> 03:30.800
at the same time connecting with all the really interesting applications that we are seeing

03:30.800 --> 03:31.800
today.

03:31.800 --> 03:38.800
So we're also speaking right on the tail end of NURP's, NURP's being a new name for the

03:38.800 --> 03:45.240
neural information processing systems conference, a new name which you helped initiate.

03:45.240 --> 03:50.840
Maybe can you maybe share a little bit about your experience at NURP's this year as well

03:50.840 --> 03:54.280
as your thoughts on the name change?

03:54.280 --> 04:05.880
Yes, I'm so happy to see NURP's take important stance on diversity and inclusion.

04:05.880 --> 04:13.120
In addition to having technical advancements, this is so critical because I feel like

04:13.120 --> 04:21.760
we are at this cusp where we can either have a democratization of AI and have diversity

04:21.760 --> 04:28.760
and creativity from all the groups coming in or we could have it closed off to only a certain

04:28.760 --> 04:36.880
kind of individuals and the latter would be a sad thing for the world and for AI.

04:36.880 --> 04:43.080
So I'm happy this year that there was so much focus not only on diversity and inclusion

04:43.080 --> 04:53.960
but also on societal impacts, AI fairness, ethics and we have people coming from those expertise

04:53.960 --> 05:01.440
with those expertise into the conference and that just makes it such an amazing place.

05:01.440 --> 05:03.400
I think it's changed for the better.

05:03.400 --> 05:10.400
There is so much awareness now about the issues affecting the underrepresented communities

05:10.400 --> 05:15.880
and more allies coming to these diversity events, right?

05:15.880 --> 05:21.400
It's not an event that's only for those groups but it's more of understanding the issues

05:21.400 --> 05:30.960
those groups face and how the rest of the community can help bring in people from diverse

05:30.960 --> 05:39.800
backgrounds and to me the name change, the whole experience was part of this push for

05:39.800 --> 05:50.480
diversity and inclusion when we saw last year there was many issues where the name was

05:50.480 --> 05:58.640
used as a way to make fun and have jokes that followed on sexual harassment, that is

05:58.640 --> 06:06.800
a problem and to me some people may argue it's just a name or it's just a joke, right?

06:06.800 --> 06:14.600
But there are deeper issues that this name change helped us explore, I mean I was personally

06:14.600 --> 06:21.240
very surprised at the amount of trolling I received when I spoke up about it and that's

06:21.240 --> 06:27.480
why so many women cannot speak up about it because they feel threatened and so there

06:27.480 --> 06:33.960
are broader issues of how do we make this a safe and an inclusive environment for everybody

06:33.960 --> 06:40.400
and I'm so happy that there are so many male allies who strive to understand the issues

06:40.400 --> 06:45.760
better, the kept asking me throughout this conference, how do we make this better for

06:45.760 --> 06:49.600
everybody, what can I do, how do I help, right?

06:49.600 --> 06:55.640
And that's why this has been just a unique experience, I have never experienced a conference

06:55.640 --> 06:57.160
like this before.

06:57.160 --> 07:05.440
Yeah, it was a really amazing conference for me as well, on a couple of levels first I

07:05.440 --> 07:13.640
think there's been kind of a blooming of these different interest groups at the conference.

07:13.640 --> 07:21.560
So women and machine learning has been around for quite some time and has kind of met

07:21.560 --> 07:28.440
in conjunction with the conference but last year for the first time I attended the black

07:28.440 --> 07:34.480
and AI workshop which was the first time for that workshop the second time this year,

07:34.480 --> 07:44.360
this year there was added Latinx and AI as well as several others.

07:44.360 --> 07:52.680
I would add queer in AI and that's there was also the town hall on diversity and inclusion

07:52.680 --> 07:59.040
where all these groups came together and discussed how there were unique problems for each groups

07:59.040 --> 08:04.040
but also common ones right and then there is the whole issue of intersectionality that

08:04.040 --> 08:09.280
a person may not just belong to one group but several groups and how do we you know think

08:09.280 --> 08:14.800
about this issue and for a technical community so much of this is new and we are having

08:14.800 --> 08:19.400
experts from other areas you know really educators on these issues.

08:19.400 --> 08:26.760
Yeah, what I thought was really interesting was the way at least in the black and AI workshop

08:26.760 --> 08:33.760
both the technical issues as well as these intersectionality issues were all kind of weaved

08:33.760 --> 08:34.760
together.

08:34.760 --> 08:41.440
I don't know it just you know as an African American in technology I'm often the only person

08:41.440 --> 08:47.080
in the room you know for much of my career and to you know be in a room where you know

08:47.080 --> 08:54.840
there are so many not only African Americans but you know Africans and black folks from

08:54.840 --> 09:02.080
all over the world talking about cutting edge machine learning in AI as well as the broader

09:02.080 --> 09:07.600
issues you know surrounding not just our communities but other communities is you know I

09:07.600 --> 09:18.960
left really inspired and then beyond that to see in the broader Nureps conference the extent

09:18.960 --> 09:25.560
to which you know some of these issues like not just diversity inclusion but fairness

09:25.560 --> 09:32.000
accountability transparency you know they really took a kind of a top line played a top

09:32.000 --> 09:37.440
line role I think in this year's conference unlike last year or I didn't notice it to

09:37.440 --> 09:43.200
that extent last year so it's very very nice to see.

09:43.200 --> 09:50.920
I agree yes and in the way I've been meeting lawyers policy makers you know people with

09:50.920 --> 09:59.440
expertise in ethics and you know you couldn't imagine Nureps having these kind of people

09:59.440 --> 10:03.920
you know attend a few years ago and that's I think great for the field.

10:03.920 --> 10:12.880
So that was certainly some an important development for 2018 and so maybe we can use that as

10:12.880 --> 10:20.880
a jumping off point to talk about some of the things on the research side that you are

10:20.880 --> 10:25.840
most that most culture interests in the in the past year.

10:25.840 --> 10:33.200
Sam you know there are being a lot of developments so I'll just pick a few doesn't mean that

10:33.200 --> 10:39.440
you know that the others are not important there is just so vast right and that's what's

10:40.080 --> 10:48.240
exciting to be an AI researcher today because we have so many domains and so many new for

10:48.240 --> 10:55.760
a kind of perspectives coming into AI. So some of the highlights personally for me one paper

10:55.760 --> 11:03.680
I would pick is this video to video synthesis that appeared here at Nureps with a group of

11:03.680 --> 11:13.120
NVIDIA researchers and what's really novel here is how to take this idea of image to image

11:13.120 --> 11:20.800
synthesis to videos right so over the past few years synthesizing images through GANS has

11:20.800 --> 11:26.320
been very popular and then the next step was if you already have an image can you transform it

11:26.320 --> 11:33.600
in a certain way to output another image right like the idea of cycle GANS and other frameworks

11:33.600 --> 11:40.000
to do that well but when you think about the videos this is much more challenging because you

11:40.000 --> 11:46.160
have to maintain temporal coherence and you have to generate it for that whole length of time

11:46.160 --> 11:53.280
in a nice smooth manner and so there were many new advances in this paper the main one I would

11:53.280 --> 11:59.120
call out is the spatial temporal adversarial objectives so it's not just the space it's not

11:59.120 --> 12:05.200
just the time but the two have to come together and this achieves good quality synthesis as well

12:05.200 --> 12:11.440
as coherence so you need to think of these multiple objectives being achieved here and they

12:11.440 --> 12:18.240
show that it's possible to generate two k resolution videos of street scenes up to 30 seconds long

12:18.240 --> 12:26.000
using only the input semantic segmentation maps and these segmentation maps were obtained from

12:26.000 --> 12:34.160
a renderer right so the idea is now you can think that you know you only get very quick simple

12:34.160 --> 12:41.680
inputs from a renderer and then you can actually use the GAN or learned frameworks to generate

12:41.680 --> 12:49.520
the output videos well and so there's a lot of promise in this area videos are always much harder

12:49.520 --> 12:56.960
than images in so many ways and I'm happy to see this development. So in their approach did they

12:56.960 --> 13:04.880
use it sounded like you're saying they used just multiple objectives as opposed to a second

13:06.080 --> 13:11.920
adversary or critic were they like doing adversarial in multiple dimensions time and space or did

13:11.920 --> 13:20.240
they somehow combine those two objectives with the same adversary. It was a single adversary but

13:20.240 --> 13:27.440
it's spatio temporal objective so you combine both of them together. You mentioned something about

13:27.440 --> 13:33.440
a renderer can you elaborate on that? So in this case the renderer was used only to get the

13:33.440 --> 13:40.560
semantic segmentation maps right like you know what are the what is the area of the pixels that

13:40.560 --> 13:46.880
has the car what is the area of the pixel that has the background right and then the whole filling in

13:46.880 --> 13:55.040
was done using the GAN framework and you could think in future you could have like a mix of

13:55.040 --> 14:01.520
renderer doing some of the rendering task and then GAN helping it and kind of having

14:02.400 --> 14:10.000
combines free system. There's been so much interesting work happening in the that happened in

14:10.000 --> 14:19.200
the GAN space this year I remember another one that was really interesting was the photo realistic

14:19.200 --> 14:28.000
images I think also from Nvidia researchers that really pushed the bar very far for what these

14:28.000 --> 14:33.840
GAN these generated images could look like. That's right that's right there have been progressive

14:33.840 --> 14:41.040
GANs there has been the big GAN paper right like how do we really push to very high resolution

14:41.040 --> 14:49.840
and ultra realistic images and I think these are exciting developments and applied deep learning

14:49.840 --> 14:58.880
and can be applied in a lot of interesting areas. Do you see GANs being applied beyond the image

14:58.880 --> 15:06.960
domain to other types of information? I think you know the idea of having a competitive

15:06.960 --> 15:12.400
optimization right at the core of it that is the generator and discriminator but you could think

15:12.400 --> 15:19.520
of other kinds of competition that's applicable in so many problems so many if you want robustness

15:19.520 --> 15:25.040
you now have this competition if you have multi agent systems you know you this framework

15:25.040 --> 15:31.200
could be applicable so I would say at the core algorithmic side yes there are a lot of connections.

15:31.840 --> 15:35.840
So video to video synthesis what else has caught your eye this year?

15:36.400 --> 15:42.880
Yeah so on more the theoretical front I will talk about some of the works we've been doing

15:44.480 --> 15:50.640
looking from a different perspective right so I just said I've been excited about GANs

15:50.640 --> 15:58.960
and GANs are great for now generating these photo realistic images but what about applications

15:58.960 --> 16:05.280
where it's not good right so so water applications where GAN is not that easy to use and that's

16:05.280 --> 16:12.320
for semi supervised learning and other frameworks where you need to use the

16:12.320 --> 16:21.680
generator process to help a supervised task and in this case these are two separate systems the

16:21.680 --> 16:28.640
GAN is separate from your convolutional neural network which does the prediction and it's very

16:28.640 --> 16:34.240
messy to try to combine both of them instead we came up with a framework called neural rendering

16:34.240 --> 16:41.520
model that tries to build a generator process right into your convolutional network rather than having

16:41.520 --> 16:49.520
a separate system and its goal is not to generate good looking images but to ask what are the relevant

16:50.720 --> 16:56.720
hidden variables and you know what is the relevant variability I need to capture in the data

16:57.360 --> 17:04.480
into this convolutional network such that I can do semi supervised learning well and I can also

17:04.480 --> 17:11.200
regularize my supervised learning in a nice way so intuitively what it's doing is it's taking

17:11.200 --> 17:18.240
this convolutional network which is a feed forward system and tries to reverse it but then there

17:18.240 --> 17:22.720
is information loss when you're going through this network so you're not able to perfectly

17:22.720 --> 17:28.800
reverse the system instead what it's doing is it's saying what is this uncertainty in reversing

17:28.800 --> 17:35.440
the process can I capture that well can I capture it across all the scales and that gives me

17:36.240 --> 17:42.240
measure of variability and uncertainty of each image and we get state of the art results semi

17:42.240 --> 17:48.800
supervised learning in this case what are the evaluation criteria or what are the experiments that

17:48.800 --> 17:55.040
you're running and what are you comparing it to yes in semi supervised learning the task is you

17:55.040 --> 18:01.680
have both labeled and unlabeled data right and you can do that with benchmarks like C4

18:01.680 --> 18:08.000
ImageNet there are a few other data sets and the key is you have very little amount of label data

18:08.000 --> 18:14.080
and a lot of unlabeled samples can you still get good accuracy and that's why this is a much harder

18:14.080 --> 18:22.880
task on a classifier that's right on the on the held out test set right where you have to go

18:22.880 --> 18:27.840
unlabeled that but you only have few labeled examples in your training data set

18:29.520 --> 18:37.520
how does the performance compare between supervised and semi supervised how much of the

18:38.160 --> 18:42.480
you know gap have we've been able to make up you know for not having the label training data

18:43.600 --> 18:49.680
I would say this is still very much an unsolved problem you know the gap is significant

18:49.680 --> 18:56.480
especially when you think of ImageNet scale or on a large number of categories right and

18:56.480 --> 19:05.040
I think that's why a lot of new works will be looking into this in ultimately in many other

19:05.040 --> 19:12.720
domains we cannot have so many labeled examples and how do we use all the unlabeled data to help

19:12.720 --> 19:22.880
the supervised task I would say this is a hard problem to tackle and so the this kind of reverse

19:23.600 --> 19:31.920
past that you were describing that is trying to capture the variability in these images how does

19:31.920 --> 19:42.400
that variability lead to and lead to performance on the forward pass side so I would say a primary

19:42.400 --> 19:48.880
challenge in machine learning is capturing uncertainty well right so if you look at the

19:48.880 --> 19:54.800
softmax at the very end of these convolutional networks they don't do a good job and their

19:55.600 --> 20:02.480
series of works are saying why and try to fix that right but instead we are saying you don't

20:02.480 --> 20:07.440
try to capture the uncertainty only in the last layer you want to capture it at all layers

20:07.440 --> 20:15.520
okay in each layer the processing is progressively losing information and at what rate is this

20:15.520 --> 20:22.160
happening for instance you are distinguishing a cat and a dog at which layer is it like more

20:22.160 --> 20:30.080
confusing than the others right and this is a hard problem and we are filing away to automatically

20:30.080 --> 20:38.080
capture that across the scales all right so another interesting one what's what's next on your list so

20:38.080 --> 20:46.000
another paper that's coming out of another paper that came out of my group at Celtic has been

20:46.000 --> 20:55.280
on Sinus TD which looks at gradient compression so you know right now when most of the training

20:55.280 --> 21:02.320
happens in a centralized way right like if there are devices like IoT devices they send to the

21:02.320 --> 21:08.000
cloud all the machine learning happens there and then the data sends back but you can think in the

21:08.000 --> 21:14.640
future there'll be more of the demand for learning on the edge especially if you have a lot of

21:14.640 --> 21:21.760
video streams it is becoming impractical to send everything to the cloud you just don't have

21:21.760 --> 21:28.480
enough bandwidth to do that and so in these settings in low bandwidth distributed training settings

21:28.480 --> 21:36.320
how do we do good machine learning and in this case you need to compress the gradients so each

21:36.320 --> 21:44.240
device does its own pool of data computes the gradients but instead of sending it in full precision

21:44.240 --> 21:50.560
in our paper what we asked was if you only send the sign of the gradient right so you it appears

21:50.560 --> 21:55.840
you're losing a lot of information if you send only the sign what is the impact can we study that

21:56.640 --> 22:01.600
and what we did was to study both from the theoretical perspective as well as the empirical one

22:01.600 --> 22:08.640
and bridge this gap and that's why I'm emphasizing this paper because we need more that bridge the

22:08.640 --> 22:17.200
theory and practice together and the theoretical side what we asked was what is the rate of convergence

22:17.200 --> 22:25.520
on the training data compared to the full precision SGD right first of all you know is it even

22:25.520 --> 22:33.680
clear that taking just the sign will converge will it go to zero training error and it turns out yes

22:33.680 --> 22:41.280
you can get zero training error and that's because intuitively when the gradient is large even just

22:41.280 --> 22:49.920
taking the sign means that you're descending in the right direction and and so we in fact also

22:49.920 --> 22:57.440
argue that there are certain quantities like density of gradients and smoothness and other

22:57.440 --> 23:02.880
properties under which we can expect it to have the same rate of convergence as the full precision

23:02.880 --> 23:12.320
SGD and then we went to empirical studies and asked okay now can we see that happen and we saw this

23:12.320 --> 23:21.440
converge you know very fast and we also built efficient compression algorithms so that really

23:21.440 --> 23:30.000
could increase the throughput and get the gains of compressed gradients and and what we found was on

23:30.000 --> 23:36.400
the generalization side we were getting almost similar accuracies as the full precision SGD

23:36.960 --> 23:41.360
right and that's why this interface between systems and machine learning is so hard to

23:42.000 --> 23:48.720
characterize because there can be a lot of free lunches we are compressing gradients but still

23:48.720 --> 23:55.840
maintaining accuracy and we can also now ask theoretically why you know this is the case

23:55.840 --> 24:03.440
and I would love to see more works like this that try to bridge theory and practice and systems

24:03.440 --> 24:11.680
and machine learning. You mentioned building compression I don't recall if you said compression

24:11.680 --> 24:19.440
algorithm or some compression aspect of this can you elaborate on that the sign is easy to get

24:19.440 --> 24:27.280
what what do you need to specifically do to to kind of support this model.

24:28.160 --> 24:32.240
That's right Sam you know I wasn't talking of a sophisticated compression algorithm

24:34.000 --> 24:41.840
but more the details of you know so if you now take the sign how do you like kind of combine across

24:41.840 --> 24:50.560
all the parameters and use the right CUDA primitives to make this efficient right like you

24:50.560 --> 24:57.360
need to also worry about you know when you know in what form you're keeping this and how you're

24:57.360 --> 25:04.000
broadcasting it into the system. Okay okay so more the distributed components of it and how

25:04.000 --> 25:09.600
everything gets pulled together. That's right I mean the system's details but it's important to

25:09.600 --> 25:18.080
also be about that right and many papers stop at either doing the theoretical analysis or like

25:18.080 --> 25:23.360
asking what happens to accuracy but not coming up with efficient implementations that also

25:23.360 --> 25:35.520
get the hardware gains. Okay great so what's next? Yeah so the next paper is one that's a negative

25:35.520 --> 25:42.320
result so this is coming again from my group at Caltech and why I want to emphasize it is

25:43.120 --> 25:49.360
you know it's very hard to publish negative results right and people don't want to talk about

25:49.360 --> 25:57.200
negative results and you know we tried for a long time to of course get a positive result that

25:57.200 --> 26:03.680
wasn't our goal in the beginning to write a negative results paper and it also took me a lot of

26:03.680 --> 26:11.200
convincing my students to say no you know it's okay we've done a lot of work and this is the

26:11.200 --> 26:18.080
honest results let's get this out there and in this case what we were trying to do was to combine

26:18.080 --> 26:25.040
model-based and model-free RL reinforcement learning so you know the popular AlphaGo and other

26:25.040 --> 26:31.440
examples what you have is not just the deep Q network but you also have the Monte Carlo

26:31.440 --> 26:38.080
tree search right so meaning you have this tree of all possible actions and you're growing this

26:38.080 --> 26:45.360
to a large tree depth in the case of AlphaGo this tree was of depth in the hundreds and so you know

26:45.360 --> 26:52.160
these trees grow exponentially so you can imagine this is a very at the end at the number of leaves

26:52.160 --> 27:00.240
is very huge and this takes a lot of time more importantly this requires all these environmental

27:00.240 --> 27:05.840
interactions right like you need to ask the environment what happens under each action and keep

27:05.840 --> 27:11.840
growing this tree and in the real world this would be impossible like we could never roll out to

27:11.840 --> 27:18.320
such a large tree depth and if you're trying to simulate a more realistic real world example

27:18.320 --> 27:23.680
you could possibly do this only for short tree depths right because for the longer ones there

27:23.680 --> 27:29.840
would be two higher error propagation so that's the other motivation to ask what we can do with

27:29.840 --> 27:37.120
short tree depths so when we started out thinking about the paper our idea was you know we will

27:37.120 --> 27:45.840
use a GAN-LEC framework to learn the dynamics and predict it for a few time steps and then like

27:45.840 --> 27:54.240
two tree search and DQN combinations and argue that this will give us a good result that was our

27:54.240 --> 28:01.200
initial idea because then that's a motivation of like oh you can now take this to a more real

28:01.200 --> 28:07.600
ultimately a realistic example where you can simulate and instead of interacting with the real

28:07.600 --> 28:14.320
environment you do this in simulation and we were really worried about the model mismatch between

28:14.320 --> 28:21.120
what's being simulated and the real world right and those issues but as we went through it it

28:21.120 --> 28:27.760
turned out to be a very different project and that's where you know I encourage other researchers

28:27.760 --> 28:34.400
to also adapt right like you know we may go with an original idea and if that doesn't work out

28:34.400 --> 28:42.000
there may be so many other interesting things you find as you go through the experiments and in

28:42.000 --> 28:48.800
this case what we found was for the class of Atari games it was actually quite easy to simulate them

28:48.800 --> 28:56.480
yes they are simple images but we were still surprised in terms of forecasting up to like 10

28:56.480 --> 29:02.080
time steps you know they did almost perfectly like you couldn't tell the real image from the

29:03.120 --> 29:11.360
generated one you know visually you couldn't tell them apart and so okay that was easy so we thought

29:11.360 --> 29:19.360
the gun part would be hard but that was easy and what was now surprising was even with this perfect

29:19.360 --> 29:25.040
simulation we were not able to get good results so in fact I'll take it back in the first game the

29:25.040 --> 29:31.920
pong which is a simple game we got really good results we got you know we showed that with just

29:31.920 --> 29:40.160
half the number of samples you could like beat the baseline dqn or the ddqn network and so that's

29:40.160 --> 29:44.880
promising now we're like okay we'll scale up to more games and we will write the paper right

29:44.880 --> 29:55.920
but as we started doing these experiments and it was performing worse than ddqn it was performing

29:55.920 --> 30:01.520
worse than random policy and we're like okay what's going on you know like is there a bug

30:01.520 --> 30:07.280
come in like you know what else can we do it's so many two weeks the students did and you know

30:07.280 --> 30:13.760
saw Kamiar and Brandon who were mainly doing the experiments and still it did not work out

30:14.240 --> 30:22.000
and so then we took a step back and asked okay why is this the case so it turns out that if you're

30:22.000 --> 30:31.280
using these short tree depths with your tqn model it can be worse because you're not getting the

30:31.280 --> 30:38.880
negative experiences very well right so so think of it the analogy I would give is like a helicopter

30:38.880 --> 30:44.720
parent like you know not getting these kids to make mistakes always like hovering around

30:46.080 --> 30:51.600
right hovering about the kids and making sure they're okay and like kind of protecting them

30:52.240 --> 30:56.560
so these short tree depth the Monte Carlo tree search is doing the same

30:56.560 --> 31:04.240
let's say this agent is near a fire where you know if you went into the fire you'd get killed you'd

31:04.240 --> 31:10.480
get a very large negative reward and that would happen with the dqn because there is no protection

31:10.480 --> 31:15.600
but it would learn okay I should avoid the fire because I'll get killed but you have the

31:15.600 --> 31:22.080
Monte Carlo tree search it'll look out a little bit into the future and realize oh no I shouldn't

31:22.080 --> 31:28.160
go to the fire because I will get killed so I will avoid the fire right so it takes a long time

31:28.160 --> 31:35.360
just avoiding the fire never getting killed but not making good progress either and that's why

31:35.360 --> 31:41.520
you need large tree depths because then you have a global view right out then you can do really good

31:41.520 --> 31:47.440
decisions but having this myopic short tree depth actually hurts you more than helps you

31:47.440 --> 31:54.000
are you planning to extend this based on the success that you had with the prediction part

31:54.800 --> 32:00.240
yeah so I mean certainly on the simulation side you know how we can

32:01.520 --> 32:07.280
simulate the dynamics in different applications you know even real applications involving control

32:07.280 --> 32:14.720
systems that we are very interested in and also you know now the question is how can we fix this

32:14.720 --> 32:20.560
right if it's not this classical Monte Carlo tree search with dqn what are other reinforcement

32:20.560 --> 32:26.320
learning algorithms that could help us combine the model based with the model free reinforcement

32:26.320 --> 32:32.720
learning cool so I think you had like seven papers we've got through four of them what else

32:33.360 --> 32:39.520
what else have you found interesting this year yes the other two are more I would say the

32:39.520 --> 32:47.920
applications of AI to the sciences okay this is gonna this has to be the future right they we

32:47.920 --> 32:54.800
need to go and impact the scientific questions you know the very fundamental discoveries I think

32:54.800 --> 33:03.040
that's really important and I'm happy to see two papers that I think have been great one of them

33:03.040 --> 33:10.880
is called the phase link it's a deep learning approach to seismic phase association and this is

33:11.920 --> 33:19.360
from my colleague isong you at Caltech who is also a machine learning professor and other seismic

33:19.360 --> 33:28.160
researchers at Caltech and here the goal was to ask can we link the unknown phases from different

33:28.160 --> 33:34.320
sensors for a common earthquake apparently like these phases are not known and this is something

33:34.320 --> 33:40.240
you need to do when you have so many different sensors that are trying to sense the same common

33:40.240 --> 33:47.840
earthquake and in this case it's challenging because you don't know the you know how many sources

33:47.840 --> 33:54.320
like is it just one earthquake shutter or is it happening in different places and these events

33:54.320 --> 33:59.360
could be happening frequently in time so it's not just one shutter but multiple ones and there's

33:59.360 --> 34:08.640
overlap and how do we disentangle all this well right and the challenge here was that there is

34:08.640 --> 34:16.560
little real data right so I mean I think at Caltech we have the longest recordings of seismic

34:16.560 --> 34:25.520
activities at least in the United States goes back almost a century but this is still not enough

34:25.520 --> 34:32.080
if you want to come up with a very good model and especially for deep learning which is data hungry

34:32.080 --> 34:39.840
so instead what the researchers did was a simple and an effective approach where they simulated

34:39.840 --> 34:45.200
synthetic sequences so there's something known as the p wave and the s wave these are different

34:45.200 --> 34:53.440
kind of waves in earthquakes and so they simulated both these kinds they arrival times were generated

34:53.440 --> 35:00.000
using just a very simple one-dimensional velocity model so no this was nothing complicated you know

35:00.000 --> 35:05.920
we're always when we are generating synthetic data we are worried is this realistic enough

35:05.920 --> 35:13.040
but what was surprising in this case was this simple synthetic model when tens of millions

35:13.040 --> 35:21.360
were sequences were used combined with real data was very effective and to me the intuition is

35:21.360 --> 35:26.960
this is encoding all the physics of the waves in an effective way right so the basic physics is

35:26.960 --> 35:34.320
encoded well and then with some amount of real data any additional variations it's able to also

35:34.320 --> 35:43.600
capture and you know so to me this is a good example of going into a different field understanding

35:43.600 --> 35:51.120
the problems in that domain well through the domain experts and you know coming up with the

35:51.120 --> 35:57.280
fairly simple machine learning solutions but that have a big practical impact I've certainly

35:57.280 --> 36:05.840
seen in my conversations with folks on the applied side that they can accomplish a lot with

36:06.720 --> 36:16.000
some fairly basic tools that the you know ML and AI have provided so with with deep learning

36:16.000 --> 36:25.440
there are tons of applications that can dramatically simplify some of some of their previous workflows

36:25.440 --> 36:34.240
as researchers that aren't particularly complex that are you know fairly off the self applications

36:34.240 --> 36:41.200
of you know for example image classification or you know deep learning classifiers or predictors

36:42.480 --> 36:48.640
so so I agree to the extent that the machine learning principle is simple but at the same time

36:48.640 --> 36:53.840
this required close collaboration between the machine learning expert and the domain experts

36:53.840 --> 36:59.600
right like it's not a plug-and-play approach it and I believe in most applications it won't be

36:59.600 --> 37:05.600
right and just taking the standard deep learning you usually don't have enough data and you have to

37:05.600 --> 37:10.880
see how to augment the data or how you should do semi-supervised learning well and that's why

37:11.600 --> 37:18.880
you know this is usually requires close collaboration and not just the domain experts using

37:18.880 --> 37:27.120
some of the shelf machine learning tools yeah that's fair and I think that you know there's often

37:27.120 --> 37:32.720
that the the challenges come up in the the data side getting enough labeled data or

37:33.680 --> 37:41.440
coming up with tricks to improve sample efficiency or I've also talked to folks that have to

37:41.440 --> 37:48.160
kind of customize their objective functions to do interesting things but it's not like they're

37:48.160 --> 37:55.680
using you know exotic things like GANs very often at this point that's right and I would always

37:55.680 --> 38:01.840
suggest people to start with the simplest things you know I was in this mentoring session with

38:01.840 --> 38:06.800
for reinforcement learning the topic was reinforcement learning and there were many people coming

38:06.800 --> 38:12.960
from the applied side and asking when they should use reinforcement learning and my answer was

38:12.960 --> 38:21.280
only when someone puts a gun to your head I mean if you run out of everything else and there is

38:21.280 --> 38:27.680
no nothing else that works then try reinforcement learning because reinforcement learning is expensive

38:27.680 --> 38:33.200
right so first one asks you know how much of knowledge do I already have what kind of pre-trained

38:33.200 --> 38:40.080
models are effective and you know do I really need to explore here or do I have enough data to

38:40.080 --> 38:44.800
begin with I mean these are all things you want to think about rather than blindly applying

38:44.800 --> 38:51.040
reinforcement learning and you mentioned the next paper on your list is is kind of in similar vein

38:52.320 --> 38:59.360
an application on the science side that's right and also on the system side so this is the paper

38:59.360 --> 39:06.000
on exescal deep learning for climate analytics and this one the Gordon Bell prize this year

39:06.000 --> 39:12.560
at the supercomputing conference and to me this is very exciting because for the first time this

39:12.560 --> 39:19.600
broke the Excel ops barrier right so to me it's mind boggling the scale of computations we're

39:19.600 --> 39:27.520
able to do now this had the scaling up to 27,000 GPUs and this is on the new summit supercomputer

39:28.160 --> 39:35.520
and traditionally supercomputing has been very CPU based and now taking this to the next level

39:35.520 --> 39:43.680
with GPUs is very exciting and the other question is now what kinds of applications can we do on

39:43.680 --> 39:50.560
these supercomputing traditionally it has been HPC you know kind of climate modeling so there are

39:50.560 --> 39:56.080
like you know these known climate models and how do we simulate this right but in this case the

39:56.080 --> 40:03.520
unique thing was to combine those HPC models with deep learning architectures specifically they

40:03.520 --> 40:08.880
applied segmentation architectures to climate datasets and they achieved state-of-the-art

40:08.880 --> 40:17.520
weather pattern masks and they had both the FP16 course and the traditional HPC operations

40:17.520 --> 40:25.600
right so this kind of mixed precision and mixed operations of both HPC and deep learning together

40:25.600 --> 40:32.480
I think will be the future in the sciences it's interesting it's my understanding from conversations

40:32.480 --> 40:41.120
that for quite a long time maybe I don't know if it's machine learning or GPU based architectures

40:41.120 --> 40:48.480
weren't really appreciated in the supercomputing community and for example I think the anecdote

40:48.480 --> 40:55.600
that I heard was that that was really the reason why Jensen and Nvidia created GTC because they couldn't

40:55.600 --> 41:01.360
you know the researchers that were working on GPUs couldn't get any papers accepted at super

41:01.360 --> 41:10.480
computing you know that that community is hard to break into so many other I can go into the

41:10.480 --> 41:18.160
sociological factors right and that's because you know they are in the big labs the big national

41:18.160 --> 41:25.200
labs and the people who trained on those traditional supercomputing for decades and they don't

41:25.200 --> 41:32.960
want to move away so I still see these issues but this is where we need like especially the

41:32.960 --> 41:39.600
anger people to say like okay no there is so many new developments that this is inevitable

41:39.600 --> 41:45.200
right like we have to move it from those traditional supercomputing and same with machine learning

41:45.200 --> 41:51.600
they you know demanded these high precision operations you know 64-bit operations right and that's

41:51.600 --> 41:59.600
like no in machine learning everything is 32 bits or lower and that's because what you know in

41:59.600 --> 42:06.160
machine learning the idea is what we are computing anyways inherently uncertain and we don't need

42:06.160 --> 42:14.480
high precision whereas in traditional HPC the idea is no will precisely maintain the uncertainty

42:14.480 --> 42:20.080
quantification and in some cases this is super important right there are certain cases where this

42:20.080 --> 42:27.200
is needed but in many others the model itself they're using has a big mismatch right like if you're

42:27.200 --> 42:33.680
using a linear model and you want this high precision you could argue is this really needed and so

42:33.680 --> 42:40.640
what I believe is we should revisit and ask when do we need these very high precision computations

42:40.640 --> 42:46.080
and are there other parts of the pipeline where we could relax now so maybe let's switch gears

42:46.080 --> 42:55.200
a little bit and talk a little bit about your kind of your predictions for 2019 and more broadly

42:55.200 --> 43:02.160
the the near future in machine learning and AI and in particular I want to start with you know

43:02.160 --> 43:09.040
what are you most excited about when you kind of think about where we are today all that you

43:09.040 --> 43:15.840
know we've accomplished in 2018 the hand of the struggles of 2018 that you're intimately familiar with

43:15.840 --> 43:22.640
you know what what are you most excited about going forward or looking forward

43:23.600 --> 43:33.360
I think 2019 will be a great year for really expanding the horizons and the way we think of

43:33.360 --> 43:40.400
machine learning right I think there'll be hopefully less of leaderboard chasing and myopic gains

43:40.400 --> 43:48.800
in deep learning and more serious and more of a holistic approach and integration with the other

43:48.800 --> 43:57.120
sciences with the humanities in a stronger way you know going beyond standards supervised learning

43:57.120 --> 44:05.120
you know we know very well that the you know scenarios where the train machine learning models

44:05.120 --> 44:10.720
are used will differ so much from the training data can we come up with new frameworks to you

44:10.720 --> 44:15.920
know detect these domain shifts you know especially in safety critical applications can we

44:15.920 --> 44:23.120
come up with ways to learn online you know as we are getting this new non stationary data

44:23.120 --> 44:29.120
right can we do lifelong learning and never ending learning and keep getting new knowledge

44:30.080 --> 44:35.760
and when we go to these new domains can we infuse that domain knowledge for instance can we

44:35.760 --> 44:40.640
ask what is the knowledge of physics we can already build in if especially we want to do

44:40.640 --> 44:46.000
learning on our robots or drones you know there's so much that's already the control

44:46.640 --> 44:52.240
so people have developed right we can't throw all that away there is no way instead there

44:52.240 --> 44:58.000
you know the question of do we bring in learning in a principled way because for control systems for

44:58.000 --> 45:04.960
instance you want stability you cannot throw that away right whereas neural networks by themselves

45:04.960 --> 45:14.240
are not stable how do we enable that and we had some initial work this year on enabling a stable

45:14.240 --> 45:21.120
landing of drones with stability guarantees so those kinds of collaborations across different

45:21.120 --> 45:28.560
expertise and different groups that's what I hope to see more in 2019 I'm curious you mentioned

45:28.560 --> 45:33.920
one of the papers that you highlighted was reinforcement learning and of course you your comment

45:33.920 --> 45:42.240
about it being a tool of last resort do you expect to see that change much in 2019 how close do you

45:42.240 --> 45:52.560
think we are to the point where RL becomes a useful kind of general purpose tool for folks

45:52.560 --> 45:58.480
not in the research community I like that somehow you know put me at a spot because

46:01.120 --> 46:06.080
you know yes I work in reinforcement learning to clarify you know I have a great student on the

46:06.080 --> 46:12.720
market come here as is are they who is an amazing reinforcement learning work and we certainly

46:12.720 --> 46:18.800
should do the principled approaches ask what would reinforcement learning do in different scenarios

46:18.800 --> 46:25.120
what are the algorithms and how can we analyze them and when it come to the practical setting my

46:25.680 --> 46:31.200
comment was don't blindly apply right is it what you need here is a reinforcement learning

46:31.200 --> 46:38.080
and to me some of the most interesting work will be in the robotics side so I talked about my

46:38.080 --> 46:44.480
Celtic project for landing of drones and autonomous flying of drones so this is where we need

46:44.480 --> 46:50.880
the domain experts but we also need precise measurements so in this case in the cast facility

46:50.880 --> 46:57.520
there we have a unique wind testing facility for drones where we have 1300 programmable

46:57.520 --> 47:04.640
pans that can generate all kinds of wind conditions and we can get precise data on how the drone

47:04.640 --> 47:10.960
would react right so now armed with this data we can ask what kinds of reinforcement learning

47:10.960 --> 47:18.000
and machine learning we can do very well because without this you know there is no hope of

47:18.000 --> 47:25.280
trying to improve these systems and so data collection is important and when you have less of

47:25.280 --> 47:32.320
this real data that's when we need very physically precise simulations and this is where at NVIDIA

47:32.320 --> 47:40.720
I believe we have this unique capabilities to do simulations extremely well so there are two

47:40.720 --> 47:48.640
frameworks that is flex and physics physics is a framework we made it open source recently earlier

47:48.640 --> 47:57.600
this week here at new reps which is simulating real world physical behavior very precisely

47:57.600 --> 48:03.840
right like think about how if you have a damage to a building or you have like movement of the

48:03.840 --> 48:13.200
cloth they have how do we make them realistic and flex is a system that is also you know high

48:13.200 --> 48:22.320
fidelity GPU based physics simulator and it simulates these rigid body dynamics very well

48:22.320 --> 48:32.320
and the detail fox at NVIDIA and his group has been using flex very effectively to close the gap

48:32.320 --> 48:40.160
from simulation to real so the main idea is you may not even know what to simulate in the

48:40.160 --> 48:45.520
simulations right you need the real world experience rather than trying to manually tune what to

48:45.520 --> 48:51.200
simulate what they're doing now is doing a little bit of a real world rollout and based on that

48:51.200 --> 48:57.360
experience learning what to simulate next and so going back and forth between simulation and

48:57.360 --> 49:07.120
reality and I believe in 2019 we'll see a lot more of such simple real applications and that'll

49:07.120 --> 49:16.800
lead to more realistic robotic impact more realistic impact in robotics do you have some specific

49:16.800 --> 49:24.640
predictions for kind of what you expect to see in your area and in ml more broadly in the next year or

49:24.640 --> 49:34.160
so so the main one is moving beyond just supervised learning right so semi supervised learning

49:34.160 --> 49:40.320
asking what are the relevant generator modeling how do I capture uncertainty in a relevant way

49:40.320 --> 49:46.720
you know the base neural networks how do I design them in the right way and through that how do I

49:46.720 --> 49:54.480
do better active learning how do I do domain adaptation better how do I scale up the algorithms

49:54.480 --> 50:00.320
while understanding these tradeoffs between systems and machine learning I think we'll see a lot

50:00.320 --> 50:06.960
more of those kind of interdisciplinary questions as well as the core machine learning questions

50:07.920 --> 50:16.080
so any parting thoughts before we finish up no I think I should say like 2018 has

50:17.120 --> 50:22.880
been personally a challenging year because you know I have had I think the time to

50:22.880 --> 50:30.880
introspect more on the societal impacts of machine learning and how we can improve diversity

50:30.880 --> 50:39.600
and inclusion in machine learning and I think the year has now ending in a good note especially

50:39.600 --> 50:45.200
at new reps as I'm seeing so much overwhelming support and hopefully we can improve that and with

50:45.200 --> 50:52.720
that come up with new creative approaches to machine learning hopefully less of the media hype

50:52.720 --> 51:01.840
more of serious podcasts such as what I'm doing right now and yeah and I've been very

51:01.840 --> 51:08.880
vocal in dispelling some of the hype and the myths and I hope to continue to do that fantastic

51:08.880 --> 51:14.480
well Anima thank you so much for all of your work in the space and for taking the time to chat

51:14.480 --> 51:24.960
with us about it thanks a lot Sam this is a pleasure all right everyone that's our show for today

51:24.960 --> 51:31.040
for more information on Anima or any of the topics covered in this episode visit twimmalei.com

51:31.040 --> 51:39.040
slash talks slash 215 you can also follow along with our AI rewind 2018 series at twimmalei.com slash

51:39.040 --> 51:46.400
rewind 18 as always thanks so much for listening and catch you next time happy holidays

