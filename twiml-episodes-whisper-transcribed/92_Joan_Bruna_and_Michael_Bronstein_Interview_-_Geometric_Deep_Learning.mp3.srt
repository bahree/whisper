1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,400
I'm your host Sam Charrington.

4
00:00:23,400 --> 00:00:27,920
This week on the podcast, we're featuring a series of conversations from the Nips conference

5
00:00:27,920 --> 00:00:30,240
in Long Beach, California.

6
00:00:30,240 --> 00:00:34,120
This was my first time at Nips and I had a great time there.

7
00:00:34,120 --> 00:00:37,360
I attended a bunch of talks and of course learned a ton.

8
00:00:37,360 --> 00:00:43,520
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful

9
00:00:43,520 --> 00:00:47,680
people, including some former Twimble Talk guests.

10
00:00:47,680 --> 00:00:52,480
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

11
00:00:52,480 --> 00:00:59,160
take a second right now to subscribe to at twimblei.com slash newsletter.

12
00:00:59,160 --> 00:01:04,840
This week through the end of the year, we're running a special listener appreciation contest

13
00:01:04,840 --> 00:01:09,560
to celebrate hitting one million listens on the podcast and to thank you all for being

14
00:01:09,560 --> 00:01:11,480
so awesome.

15
00:01:11,480 --> 00:01:16,200
Tweet to us using the hashtag Twimble1Mill to enter.

16
00:01:16,200 --> 00:01:20,840
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

17
00:01:20,840 --> 00:01:22,880
other mystery prizes.

18
00:01:22,880 --> 00:01:29,640
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

19
00:01:29,640 --> 00:01:32,120
for the full rundown.

20
00:01:32,120 --> 00:01:36,480
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

21
00:01:36,480 --> 00:01:39,680
of this podcast and our Nips series.

22
00:01:39,680 --> 00:01:44,720
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

23
00:01:44,720 --> 00:01:50,480
sessions, their big news this time was the first public viewing of the Intel Nirvana

24
00:01:50,480 --> 00:01:54,200
neural network processor or NNP.

25
00:01:54,200 --> 00:01:59,160
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

26
00:01:59,160 --> 00:02:04,240
primitives while making the core hardware components as efficient as possible, giving

27
00:02:04,240 --> 00:02:09,720
neural network designers powerful tools for solving larger and more difficult problems

28
00:02:09,720 --> 00:02:14,360
while minimizing data movement and maximizing data reuse.

29
00:02:14,360 --> 00:02:20,200
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

30
00:02:20,200 --> 00:02:30,200
Nirvana.com and now on to the show.

31
00:02:30,200 --> 00:02:31,560
All right, everyone.

32
00:02:31,560 --> 00:02:37,160
I am here in Long Beach, California at the Nips conference and I am seated with Juan

33
00:02:37,160 --> 00:02:43,400
Bruna, Assistant Professor at the current Institute at NYU and also at the Center of Data

34
00:02:43,400 --> 00:02:49,080
Science at NYU and Michael Bronstein, who is currently on sabbatical at Harvard.

35
00:02:49,080 --> 00:02:53,000
Juan and Michael, welcome to this week a machine learning and AI.

36
00:02:53,000 --> 00:02:54,000
Thank you.

37
00:02:54,000 --> 00:02:58,440
But why don't we get started by having each of you introduce yourselves and tell us a

38
00:02:58,440 --> 00:03:02,920
little bit about how you got involved in machine learning and artificial intelligence?

39
00:03:02,920 --> 00:03:10,560
Yes, so I did my masters and my PhD in France and actually at the beginning my training

40
00:03:10,560 --> 00:03:17,120
was more on the single processing and applied math and I was not so much into machine learning

41
00:03:17,120 --> 00:03:26,400
up until I started to mutate during my PhD and enter machine learning through the ideas

42
00:03:26,400 --> 00:03:28,800
and tools from single processing.

43
00:03:28,800 --> 00:03:33,640
So that was five years ago and ever since I have been getting more and more interested

44
00:03:33,640 --> 00:03:39,800
in ways and problems in which we can maybe leverage some of the more techniques and ways

45
00:03:39,800 --> 00:03:45,640
of mathematical tools from applied math into some of the current modern problems in

46
00:03:45,640 --> 00:03:46,640
deep learning.

47
00:03:46,640 --> 00:03:49,960
Yeah, that's kind of how I arrived here.

48
00:03:49,960 --> 00:03:50,960
Awesome.

49
00:03:50,960 --> 00:03:56,160
And Michael, so I did my studies in Israel and the Technion and my main background is in

50
00:03:56,160 --> 00:04:01,480
geometry, so different types of geometry, metric geometry, spectral geometry, mostly applications

51
00:04:01,480 --> 00:04:07,360
to computer vision and computer graphics problems and in recent years, well, probably with

52
00:04:07,360 --> 00:04:12,200
like many other people, I wouldn't say everybody, but the following a little bit, the

53
00:04:12,200 --> 00:04:16,800
hype of deep learning, we are trying to apply some of deep learning methods to geometric

54
00:04:16,800 --> 00:04:22,400
data, which comes with many of its challenges that are, there are many similarities and

55
00:04:22,400 --> 00:04:29,040
many dissimilarities from the more traditional Euclidean soul to say data, okay?

56
00:04:29,040 --> 00:04:33,600
And I mentioned you're on sabbatical at Harvard, where are you otherwise affiliated?

57
00:04:33,600 --> 00:04:37,920
So otherwise I explain, I spend most of my time between Switzerland and Israel, full

58
00:04:37,920 --> 00:04:42,600
professor at the University of Lugano in Switzerland, Tel Aviv University in Israel, and I also have

59
00:04:42,600 --> 00:04:48,280
a position as a principal engineer at Intel, perceptual computing.

60
00:04:48,280 --> 00:04:56,200
You sound like a very busy guy, don't tell the way wife.

61
00:04:56,200 --> 00:04:59,480
The two of you delivered a tutorial, was it today, right?

62
00:04:59,480 --> 00:05:00,480
Yes.

63
00:05:00,480 --> 00:05:03,680
You did a tutorial, tell us a little bit about the tutorial you did.

64
00:05:03,680 --> 00:05:09,360
Okay, so the tutorial is called geometric deep learning on graphs and manifolds, okay?

65
00:05:09,360 --> 00:05:17,160
So the, in one sentence, the goal of what the tutorial is about is how can we leverage

66
00:05:17,160 --> 00:05:24,760
the successful techniques that the deep learning developed to process images, text and speech

67
00:05:24,760 --> 00:05:31,160
into data and tasks where you might have, where your input might be a bit more exotic.

68
00:05:31,160 --> 00:05:37,840
For example, proteins, or it could be data from a social network, or it could be data captured

69
00:05:37,840 --> 00:05:42,920
from a, let's say, a Kinect, where you have a bunch of point clouds in, in, in 3D.

70
00:05:42,920 --> 00:05:47,680
And so what the tutorial is trying to do is to say, basically give a picture of our current

71
00:05:47,680 --> 00:05:53,720
understanding of how deep learning models can be used and developed in this regime and

72
00:05:53,720 --> 00:05:59,800
also try to describe some of the future and current open directions.

73
00:05:59,800 --> 00:06:05,800
And Michael, in your introduction, you drew one of the key distinctions here, which is

74
00:06:05,800 --> 00:06:09,160
Euclidean space versus other geometric spaces.

75
00:06:09,160 --> 00:06:10,960
Can you elaborate on that distinction?

76
00:06:10,960 --> 00:06:11,960
Sure.

77
00:06:11,960 --> 00:06:16,000
So basically, the difference between Euclidean and non-Euclidean data, one of the main

78
00:06:16,000 --> 00:06:19,920
differences is that you don't have the possibility of vector space operations.

79
00:06:19,920 --> 00:06:25,000
So in Euclidean space, you can take your sample, your point, and you can add or subtract

80
00:06:25,000 --> 00:06:26,000
two points.

81
00:06:26,000 --> 00:06:29,440
You cannot do the same thing on a graph, for example, you cannot add or subtract two vertices

82
00:06:29,440 --> 00:06:30,440
on a graph.

83
00:06:30,440 --> 00:06:36,240
So basically, you have more general structures, but also less operations that you, you can

84
00:06:36,240 --> 00:06:37,240
do with these structures.

85
00:06:37,240 --> 00:06:42,840
So you need to reinvent many of the building blocks that, basically, such as convolutions

86
00:06:42,840 --> 00:06:48,800
or pudding, or that are commonly used in deep learning architectures when you need

87
00:06:48,800 --> 00:06:51,120
to deal with these data.

88
00:06:51,120 --> 00:06:55,280
So Zohan, why don't you walk us through the general structure of the tutorial?

89
00:06:55,280 --> 00:06:56,320
How did you set the context?

90
00:06:56,320 --> 00:06:57,560
How did you get started?

91
00:06:57,560 --> 00:07:03,040
Well, I mean, historically, yeah, so there's a little clique of researchers that we started

92
00:07:03,040 --> 00:07:07,040
to look into this problem a few years ago, maybe like three, four years ago.

93
00:07:07,040 --> 00:07:12,280
So that was at the time where I was a postdoc in Yalekunz Lab in New York, like at the

94
00:07:12,280 --> 00:07:13,600
time of the postdoc.

95
00:07:13,600 --> 00:07:18,280
So there we, and together with Arthur Slam, who is another organizer of the tutorial,

96
00:07:18,280 --> 00:07:25,760
we came up with a very simple, I would say, first model that was trying to set up the

97
00:07:25,760 --> 00:07:31,520
techniques that later on we realized, and Michael and his group also helped us understand

98
00:07:31,520 --> 00:07:35,640
that they were far too naive, right, that there were a lot of things that could be improved

99
00:07:35,640 --> 00:07:37,200
upon this first idea.

100
00:07:37,200 --> 00:07:41,720
So I guess that since the very beginning, we kind of saw that there was a very interesting

101
00:07:41,720 --> 00:07:47,520
exchange of ideas, and also leveraging the fact that we came from slightly different backgrounds,

102
00:07:47,520 --> 00:07:48,520
right?

103
00:07:48,520 --> 00:07:52,840
Michael's group, they have a lot of expertise in geometry and understanding manifolds,

104
00:07:52,840 --> 00:07:57,880
and we came up with an expertise maybe where we had been working hands-on with convolutional

105
00:07:57,880 --> 00:08:03,360
neural networks and some of the ideas that I also worked under in my PhD that involve

106
00:08:03,360 --> 00:08:07,400
more the harmonic analysis of this convolution neural networks.

107
00:08:07,400 --> 00:08:09,800
And so the tutorial came quite naturally.

108
00:08:09,800 --> 00:08:16,640
We have been also collaborating in a variety of different projects, yeah, I mean, that's

109
00:08:16,640 --> 00:08:17,840
how it came out.

110
00:08:17,840 --> 00:08:22,800
So for us, basically, the paper that Ron mentioned that he also, as a postdoc, was actually

111
00:08:22,800 --> 00:08:27,920
an inspiration, and that's where we started looking into these kind of problems.

112
00:08:27,920 --> 00:08:31,360
So we've been working, of course, on spectral geometry for a long time, and in computer

113
00:08:31,360 --> 00:08:36,120
graphics, geometry, processing, community, these or similar ideas have been around for

114
00:08:36,120 --> 00:08:41,360
a while, but basically combining it with learning and redefining operations, for example,

115
00:08:41,360 --> 00:08:45,960
in the spectral domain, as they did in their influential paper, was, to some extent,

116
00:08:45,960 --> 00:08:48,160
eye-opening, and basically we took it from there.

117
00:08:48,160 --> 00:08:53,560
So we rolled together a review paper that appeared in a triple-esignal processing magazine

118
00:08:53,560 --> 00:08:58,240
just this summer, that for us, it was also doing some homework.

119
00:08:58,240 --> 00:09:03,400
We discovered many works in other communities that existed for quite some time, or maybe

120
00:09:03,400 --> 00:09:08,080
we're almost forgotten, or not given sufficient attention.

121
00:09:08,080 --> 00:09:12,880
And maybe even in different communities, people calling the same things with different

122
00:09:12,880 --> 00:09:19,160
names, or to some extent even, I wouldn't dare say reinventing approaches just without

123
00:09:19,160 --> 00:09:22,320
probably being aware of what is done in other communities.

124
00:09:22,320 --> 00:09:23,320
So that never happens.

125
00:09:23,320 --> 00:09:29,600
Well, of course, it always happens, but in this way, because it's new, it's a new field.

126
00:09:29,600 --> 00:09:33,840
We can probably already call it a new field, or a new trend, or a new niche in machine

127
00:09:33,840 --> 00:09:39,520
learning, and because it's new, then basically it's an effort of, there are several seeds

128
00:09:39,520 --> 00:09:46,600
that exist in other domains, and I think now people start getting, maybe, a uniform

129
00:09:46,600 --> 00:09:50,480
picture of what exists, and what's the relation between different methods.

130
00:09:50,480 --> 00:09:53,360
So I think the tutorial is very timely.

131
00:09:53,360 --> 00:09:55,720
So you mentioned spectral analysis.

132
00:09:55,720 --> 00:09:59,800
When I think of that, I think of things like fast freer transforms, and the like, how

133
00:09:59,800 --> 00:10:06,680
does that fit into graphs and manifolds, and non-uclidean spaces, and all these things?

134
00:10:06,680 --> 00:10:10,120
One way you can think about it is that it's like your dictionary, right?

135
00:10:10,120 --> 00:10:14,600
And we should probably even take a step back and explain what an FFT is for a folks who

136
00:10:14,600 --> 00:10:16,800
might not be familiar with that.

137
00:10:16,800 --> 00:10:21,960
Yes, so maybe, if we take a step sufficiently back, we can maybe start with physics, right?

138
00:10:21,960 --> 00:10:28,560
So, so, so, yes, so there's a very fundamental equation in physics that governs how things

139
00:10:28,560 --> 00:10:29,560
oscillate, right?

140
00:10:29,560 --> 00:10:33,560
But like in a guitar, or you know, in a domain, like if you take, you know, like a drum,

141
00:10:33,560 --> 00:10:37,480
and you hit it, there's some, some equation that is going to determine how things are going

142
00:10:37,480 --> 00:10:38,480
to oscillate.

143
00:10:38,480 --> 00:10:42,800
And so the modes of oscillation, as you, as you might suspect, they are related to the

144
00:10:42,800 --> 00:10:44,040
Fourier analysis, right?

145
00:10:44,040 --> 00:10:49,960
So, so the, and the equation that, and the operator that governs this behavior is called

146
00:10:49,960 --> 00:10:50,960
Laplacian, right?

147
00:10:50,960 --> 00:10:56,840
So, so the Laplacian is the, the mathematical, it's a differential operator that in the

148
00:10:56,840 --> 00:10:59,360
occlusion domain might look very inoffensive, right?

149
00:10:59,360 --> 00:11:03,320
It's just the, you take the partial derivatives with respect to all directions, and you just

150
00:11:03,320 --> 00:11:04,760
sum everything.

151
00:11:04,760 --> 00:11:09,440
But it turns out that, that from this operator, you can use this as a vehicle to generalize

152
00:11:09,440 --> 00:11:10,440
things, right?

153
00:11:10,440 --> 00:11:15,280
Because now, if you want to generalize convolutions from the occlusion domain to an

154
00:11:15,280 --> 00:11:17,800
unequally in domain, you will have a hard time, right?

155
00:11:17,800 --> 00:11:21,120
Because they are defined through something that doesn't exist in the nodule domain.

156
00:11:21,120 --> 00:11:25,480
But if you, if you take one step back and they say, okay, maybe I cannot directly use

157
00:11:25,480 --> 00:11:29,960
the convolution, but maybe I can step back and, and use the, this Laplacian operator as

158
00:11:29,960 --> 00:11:33,080
a tool to maybe go from one world to the other.

159
00:11:33,080 --> 00:11:37,640
And as it turns out, the Laplacian is a, is a operator that is intrinsically defined with

160
00:11:37,640 --> 00:11:38,640
very weak assumptions.

161
00:11:38,640 --> 00:11:43,200
In it, what it means is that even on, on a graph or on a manifold, it's an operator that,

162
00:11:43,200 --> 00:11:49,080
that exists, and it's, well, it's, it has a very complete and rich structure and status.

163
00:11:49,080 --> 00:11:54,520
And these, the application of the Laplacian to these different domains, this is all work

164
00:11:54,520 --> 00:11:58,280
that, you know, was done with regard to that domain, independent of what we're trying

165
00:11:58,280 --> 00:11:59,960
to do here with machine learning.

166
00:11:59,960 --> 00:12:00,960
Yeah, absolutely.

167
00:12:00,960 --> 00:12:04,400
I mean, I think that, I mean, I, I would not claim that I know exactly when these things,

168
00:12:04,400 --> 00:12:10,360
but I'm sure that many famous physicists from even like the 90th century were aware of,

169
00:12:10,360 --> 00:12:13,800
you know, the distance and the generality of this operator, right?

170
00:12:13,800 --> 00:12:19,720
And this is way, I think I believe that when people refer to spectrograph theory, it's

171
00:12:19,720 --> 00:12:25,280
most of it, or a big part of it, is in developing the properties of Laplacian and related operators

172
00:12:25,280 --> 00:12:26,280
in graph.

173
00:12:26,280 --> 00:12:28,560
As applied to graphs, yes, okay.

174
00:12:28,560 --> 00:12:35,360
And so, so in that respect, the Fourier transform is now a concept that you can, of course,

175
00:12:35,360 --> 00:12:40,440
if you analyze it and the occlusion domain, it has a very, again, a very rich structure

176
00:12:40,440 --> 00:12:43,360
and it's useful beyond our imagination, right?

177
00:12:43,360 --> 00:12:47,200
It's going to be used to do fast, mostly thanks to the fast Fourier transform.

178
00:12:47,200 --> 00:12:53,480
But it's also an object that exists in general graphs, perhaps not with the only detail

179
00:12:53,480 --> 00:12:57,280
that is like a single detail, but might be very important, is that it doesn't come with

180
00:12:57,280 --> 00:12:58,760
a fast algorithm, right?

181
00:12:58,760 --> 00:13:04,200
That's the, I would say that this is the main, or one of the main technical differences.

182
00:13:04,200 --> 00:13:10,760
So, when I think of the Fourier transform or the FFT, I think of, you have some input

183
00:13:10,760 --> 00:13:15,880
signal and you pass it through this Fourier transformation and it basically decomposes

184
00:13:15,880 --> 00:13:18,400
it into its frequency parts.

185
00:13:18,400 --> 00:13:20,200
How does that apply to a graph?

186
00:13:20,200 --> 00:13:22,120
What does that even mean?

187
00:13:22,120 --> 00:13:25,840
So, on the graph, basically, in the Euclidean case, you can, as you said, you can represent

188
00:13:25,840 --> 00:13:29,840
a function or a signal as a superposition of signs or co-science, right?

189
00:13:29,840 --> 00:13:31,600
Basically, some harmonic functions.

190
00:13:31,600 --> 00:13:36,720
So, these functions turn to be eigenfunctions or eigenvectors of the Euclidean operator,

191
00:13:36,720 --> 00:13:40,880
in the Euclidean case and the one-dimensional case, it's just the second order derivative.

192
00:13:40,880 --> 00:13:45,320
So, basically, if you replace this Euclidean operation with a non-Euclidean operation, with

193
00:13:45,320 --> 00:13:49,360
a graph operation, or in a manifold, that would be what is called differential geobotid,

194
00:13:49,360 --> 00:13:53,200
a plus-biltrami operator, basically, you get exactly the same thing.

195
00:13:53,200 --> 00:13:58,240
You get eigenfunctions of this operator that turn to be an orthogonal basis, basically,

196
00:13:58,240 --> 00:13:59,840
it's a self-adjoint operator.

197
00:13:59,840 --> 00:14:01,840
So, it has orthogonal eigen decomposition.

198
00:14:01,840 --> 00:14:07,400
The eigenvalues play a role of frequencies that are exactly, as Ron mentioned, vibration

199
00:14:07,400 --> 00:14:13,440
modes of, yeah, basically, the eigenvalues that you obtain in the heliports or the spatial

200
00:14:13,440 --> 00:14:17,840
part of the wave equation that governs vibration of a domain.

201
00:14:17,840 --> 00:14:22,760
Ron, you were saying, no, it's just that maybe one way where you can maybe picture this

202
00:14:22,760 --> 00:14:28,000
thing in your head is, yeah, if you take like a spherical plate that would correspond

203
00:14:28,000 --> 00:14:34,240
to a drum, as we understand it, right, just a drum, and then you could imagine like deforming

204
00:14:34,240 --> 00:14:39,960
somehow the drum and just playing it, you would, intrinsically, you would still be playing

205
00:14:39,960 --> 00:14:45,320
things that can be seen as a superposition of like fundamental waves that would look

206
00:14:45,320 --> 00:14:49,640
a bit more funny, right, that would probably adapt, and they would be like the formation

207
00:14:49,640 --> 00:14:54,840
of the original science and coscience, depending on how you have to form the original drums.

208
00:14:54,840 --> 00:14:55,840
Ron, interesting.

209
00:14:55,840 --> 00:14:57,240
This is interesting stuff.

210
00:14:57,240 --> 00:15:03,640
It's bringing me back to my DSP class and I don't think eigenvalues and eigenvectors

211
00:15:03,640 --> 00:15:07,720
has really come up much on the podcast either, but we're not going to go into the linear algebra.

212
00:15:07,720 --> 00:15:10,440
We'll assume a bunch of that.

213
00:15:10,440 --> 00:15:19,840
So basically just to kind of, to catch us up, this is all kind of background to the tutorial.

214
00:15:19,840 --> 00:15:25,080
Maybe let's take a moment to talk about applications of this to make it a little bit more concrete

215
00:15:25,080 --> 00:15:26,080
for everyone.

216
00:15:26,080 --> 00:15:30,400
How was the, some of the stuff that you're doing applied?

217
00:15:30,400 --> 00:15:34,960
So maybe, yeah, we can illustrate a couple of very different applications.

218
00:15:34,960 --> 00:15:39,000
So one that I'm personally involved in is Article Physics.

219
00:15:39,000 --> 00:15:44,080
So the sort of experiments that people do in a large collider and like in particle accelerators

220
00:15:44,080 --> 00:15:49,720
where they're there, the goal of the game is to say very precise things about how the

221
00:15:49,720 --> 00:15:53,120
standard model, like some very specific properties of the standard model.

222
00:15:53,120 --> 00:15:58,680
So the way it works is that you do like a very, an experiment where you clash two particles

223
00:15:58,680 --> 00:16:03,360
certain, you know, with very, very large speed and then they produce what they call a jet.

224
00:16:03,360 --> 00:16:08,400
And a jet is like a shower of little events that can be detected through a very expensive

225
00:16:08,400 --> 00:16:11,800
and very sophisticated detector, like looks like a cylinder.

226
00:16:11,800 --> 00:16:15,720
So and the goal of the game is to say, well, given this observation that looks like this

227
00:16:15,720 --> 00:16:21,360
point cloud in my detector, how can I infer what was the underlying physical theory, right?

228
00:16:21,360 --> 00:16:26,800
And so it's really a machine learning and their machine learning is a very, very fundamental

229
00:16:26,800 --> 00:16:30,080
step for this experimental physicist.

230
00:16:30,080 --> 00:16:34,960
And so there you use this techniques that we described in the tutorial to essentially

231
00:16:34,960 --> 00:16:41,920
learn a data driven model that looks as input a set of like a point cloud in model in the

232
00:16:41,920 --> 00:16:47,760
color limiter and tries to predict if it was due to theory A or theory B.

233
00:16:47,760 --> 00:16:52,640
Before we go to the other application, when I think about when I visualize this, I'm

234
00:16:52,640 --> 00:16:58,320
visualizing, you know, something that, you know, is very well within the bounds of Euclidean

235
00:16:58,320 --> 00:16:59,320
stuff, right?

236
00:16:59,320 --> 00:17:04,880
It's a three-dimensional point cloud, like why do we need to apply the stuff that you've

237
00:17:04,880 --> 00:17:07,600
done as opposed to the usual stuff?

238
00:17:07,600 --> 00:17:08,600
Very good question.

239
00:17:08,600 --> 00:17:11,960
So the answer is that you're not forced to apply what you do.

240
00:17:11,960 --> 00:17:18,480
So if you decide to stay in the Euclidean route, you would have to somehow quantify your

241
00:17:18,480 --> 00:17:21,240
measurements, such that they look like a regular grid.

242
00:17:21,240 --> 00:17:24,520
And so we thought there are two potential risks.

243
00:17:24,520 --> 00:17:30,040
The first one is that if the precision that you need, if the little blobs that the particles

244
00:17:30,040 --> 00:17:35,680
create are very small, you might need to sample at, you know, you will need a sampling rate

245
00:17:35,680 --> 00:17:38,760
that will make inputs incredibly large, right?

246
00:17:38,760 --> 00:17:44,600
That you will basically be paying a huge price in order to transform your input into something

247
00:17:44,600 --> 00:17:46,320
that is not regular grid.

248
00:17:46,320 --> 00:17:50,640
Maybe another way to put that is in order to solve this in Euclidean space, you have

249
00:17:50,640 --> 00:17:56,640
like a quantization noise that you have to sample more than otherwise if you were to use

250
00:17:56,640 --> 00:17:57,640
a different.

251
00:17:57,640 --> 00:18:02,760
So that's a trade-off, right, between, so once you choose a quantization step, there's

252
00:18:02,760 --> 00:18:07,560
a trade-off between how large and how sparse your input is going to look like versus how

253
00:18:07,560 --> 00:18:09,680
much resolution you are going to lose.

254
00:18:09,680 --> 00:18:15,560
So in that respect, the models that we propose here, they are an alternative that don't have

255
00:18:15,560 --> 00:18:16,720
this limitation, right?

256
00:18:16,720 --> 00:18:22,200
That can stay at the native, let's say they don't lose any information because we don't

257
00:18:22,200 --> 00:18:25,280
need to quantize, we don't need to go into this regular grid.

258
00:18:25,280 --> 00:18:31,000
And I would say that there's another potential advantage is that when you look at the experiment

259
00:18:31,000 --> 00:18:35,400
that comes from a detector, as if it was an image, right, in a regular grid, the underlying

260
00:18:35,400 --> 00:18:40,760
assumption is that the statistics, you know, the statistics of the input, they are following

261
00:18:40,760 --> 00:18:45,400
the same assumption as the statistics of natural images, namely that everything is stationary

262
00:18:45,400 --> 00:18:50,920
and that there's a, I would say, a canonical way to compare, like, to measure distance between

263
00:18:50,920 --> 00:18:52,720
points, right?

264
00:18:52,720 --> 00:18:59,000
Whereas in physicists, they have a very good and sophisticated notions of how one should

265
00:18:59,000 --> 00:19:00,000
be comparing particles.

266
00:19:00,000 --> 00:19:05,960
Okay, so there's a, in other words, there's a more physics-aware version of a neural network

267
00:19:05,960 --> 00:19:11,240
that, I mean, another way to say it is that for physicists, it's very important to have

268
00:19:11,240 --> 00:19:15,640
a model where you can infuse and you can incorporate, like, prior information about how things

269
00:19:15,640 --> 00:19:17,520
should be compared into the model.

270
00:19:17,520 --> 00:19:24,280
And so the graph formulation that we are working on is allowing you to incorporate this thing

271
00:19:24,280 --> 00:19:25,280
into the model.

272
00:19:25,280 --> 00:19:29,440
Whereas if you use just the quantization and the standard convolution of your network,

273
00:19:29,440 --> 00:19:33,760
it doesn't seem so easy to incorporate and to accommodate for this.

274
00:19:33,760 --> 00:19:34,760
Okay.

275
00:19:34,760 --> 00:19:40,960
So when I go from my picture of the three-dimensional Euclidean for this application to maybe

276
00:19:40,960 --> 00:19:46,160
something that's a little bit less traditional, I think, of, like, maybe looking at your

277
00:19:46,160 --> 00:19:49,000
points in some kind of spherical coordinate system.

278
00:19:49,000 --> 00:19:53,360
Is that kind of, is that what you're doing or is it, because we keep coming back to graph

279
00:19:53,360 --> 00:19:55,960
and I'm trying to wrap my head around where graph.

280
00:19:55,960 --> 00:19:56,960
Yes.

281
00:19:56,960 --> 00:20:03,240
So here, the graph enters the game as just like a data structure that you use to relate

282
00:20:03,240 --> 00:20:04,240
particles.

283
00:20:04,240 --> 00:20:05,760
Okay, so I think that your input is discreet.

284
00:20:05,760 --> 00:20:10,480
It contains a number of particles that can be variable, depending on each event.

285
00:20:10,480 --> 00:20:17,000
And what you learn is a network that learns how to relate, how to relate and propagate

286
00:20:17,000 --> 00:20:21,080
information across the set of particles.

287
00:20:21,080 --> 00:20:24,680
And so it doesn't see it into this extrinsic Euclidean space.

288
00:20:24,680 --> 00:20:30,680
So of course, you could say, well, you know, it's just a, I mean, yeah, I can see my input

289
00:20:30,680 --> 00:20:32,360
as being n particles.

290
00:20:32,360 --> 00:20:36,160
I can also see it as, you know, as you say, just put everything in the sphere and then

291
00:20:36,160 --> 00:20:39,200
I just see it as a function, like it's an image in that sphere, right?

292
00:20:39,200 --> 00:20:42,600
And then the number, the notion that I have in particles completely disappears, right?

293
00:20:42,600 --> 00:20:44,680
And then you can connect everything with everything.

294
00:20:44,680 --> 00:20:49,320
I would say that it's not that there's one approach that is systematically better than

295
00:20:49,320 --> 00:20:50,320
the other.

296
00:20:50,320 --> 00:20:53,680
It's really, there's context in which you might want to use one formulation versus the

297
00:20:53,680 --> 00:20:54,680
other.

298
00:20:54,680 --> 00:20:58,920
So what I see here is that we are just providing another tool that now our practitioners

299
00:20:58,920 --> 00:21:02,360
can use and maybe then they can combine with the other one.

300
00:21:02,360 --> 00:21:08,240
And we also have reasons to believe that in some contexts, one formulation might scale

301
00:21:08,240 --> 00:21:09,680
better than the other.

302
00:21:09,680 --> 00:21:15,320
For example, if you're having a moment of four dimensions, I had a moment of, you know,

303
00:21:15,320 --> 00:21:21,400
like eight dimensions, well, in our case, the architectural changes to the model model

304
00:21:21,400 --> 00:21:22,560
would be minimal, right?

305
00:21:22,560 --> 00:21:29,280
There's nothing in our scaling that depends on the, like it depends very quickly on the

306
00:21:29,280 --> 00:21:31,040
dimension of the input.

307
00:21:31,040 --> 00:21:36,760
Whereas if you had to do the quantization on the domain, you would pay a huge price for

308
00:21:36,760 --> 00:21:39,240
this small change.

309
00:21:39,240 --> 00:21:40,240
Interesting.

310
00:21:40,240 --> 00:21:42,080
So you're going to give a second example.

311
00:21:42,080 --> 00:21:45,280
I can give a second example or Michael can give an example as you prefer.

312
00:21:45,280 --> 00:21:46,800
Michael, why don't you give one?

313
00:21:46,800 --> 00:21:47,800
Yeah.

314
00:21:47,800 --> 00:21:50,640
So I can give you as an example, a paper that actually will be presenting tomorrow here

315
00:21:50,640 --> 00:21:51,640
at NIPS.

316
00:21:51,640 --> 00:21:52,640
Okay.

317
00:21:52,640 --> 00:21:54,080
And the example is recommended systems.

318
00:21:54,080 --> 00:21:58,560
So, you know, probably the most famous example is the Netflix problem, you know, the

319
00:21:58,560 --> 00:22:04,400
Netflix is a movie rental company and they have probably tens of millions of users and

320
00:22:04,400 --> 00:22:07,160
probably hundreds of thousands of different movies.

321
00:22:07,160 --> 00:22:11,360
So the users can give different scores to movies, whether they like the movie or not,

322
00:22:11,360 --> 00:22:14,040
let's say, on a scale between zero and nine.

323
00:22:14,040 --> 00:22:18,200
So basically yet, you can describe this information as a huge matrix that is very sparse

324
00:22:18,200 --> 00:22:23,040
example because, of course, even if you watch continuously the movies throughout your entire

325
00:22:23,040 --> 00:22:27,120
lifetime, you'll probably watch just a small percentage of what they have.

326
00:22:27,120 --> 00:22:30,560
So they want to feel in the missing entries of these huge metrics, basically they try

327
00:22:30,560 --> 00:22:31,960
to interpolate it.

328
00:22:31,960 --> 00:22:35,640
And the standard approach is user algebraic techniques, basically try to fit a low-dimensional

329
00:22:35,640 --> 00:22:41,960
model to these data, minimizing the matrix rank or more correctly, basically the convex

330
00:22:41,960 --> 00:22:45,320
proxy of the rank, the so-called nuclear norm.

331
00:22:45,320 --> 00:22:49,280
So this problem formation doesn't have any geometric structure, for example.

332
00:22:49,280 --> 00:22:53,040
You can shuffle the columns and the rows of the matrix.

333
00:22:53,040 --> 00:22:56,880
If you remove one of the columns, there are infinitely many ways you can fill it in.

334
00:22:56,880 --> 00:23:01,000
So if you have some either side information that you can construct it from the data, some

335
00:23:01,000 --> 00:23:05,840
notion of affinity between users or items or actually both, that you can represent as

336
00:23:05,840 --> 00:23:07,720
a graph, for example.

337
00:23:07,720 --> 00:23:12,280
So think of a social network and maybe a little bit naive model that friends have similar

338
00:23:12,280 --> 00:23:13,280
tastes.

339
00:23:13,280 --> 00:23:18,240
So this already allows you to think of the matrix as some kind of space with geometric

340
00:23:18,240 --> 00:23:19,240
structure.

341
00:23:19,240 --> 00:23:22,880
You can talk about notions like smoothness, so basically you can say that the values in

342
00:23:22,880 --> 00:23:28,120
these metrics vary significantly or insignificantly when you go from one vertex on a graph to

343
00:23:28,120 --> 00:23:32,520
another vertex on a graph, and you can actually learn optimal filters, spectral filters.

344
00:23:32,520 --> 00:23:36,760
For example, on these graphs, actually it's a product of two graphs, so you can, the best

345
00:23:36,760 --> 00:23:41,400
analogy from single processing will be a two-dimensional free-attent form, like free-attent form and

346
00:23:41,400 --> 00:23:42,400
image.

347
00:23:42,400 --> 00:23:44,400
Okay, two-dimensional free-attent form.

348
00:23:44,400 --> 00:23:45,400
Yeah.

349
00:23:45,400 --> 00:23:47,200
So think of, and the free-attent form of an image, right?

350
00:23:47,200 --> 00:23:50,200
You apply free-attent form to the columns and rows of the image.

351
00:23:50,200 --> 00:23:55,000
So here instead of Euclidean structured rows and columns have matrix that leaves on two

352
00:23:55,000 --> 00:23:56,000
different graphs.

353
00:23:56,000 --> 00:24:00,760
Right, so obviously the rows for example represent items or movies, and the columns represent

354
00:24:00,760 --> 00:24:01,760
users.

355
00:24:01,760 --> 00:24:06,120
So these are two different graphs, and you can apply filters in this frequency domain

356
00:24:06,120 --> 00:24:11,680
that is now basically characterized by the eigenvalues of the two operations of the rows

357
00:24:11,680 --> 00:24:13,960
and the column and the column graphs.

358
00:24:13,960 --> 00:24:18,560
And basically this way you can have a better way of feeling in the missing elements of

359
00:24:18,560 --> 00:24:23,920
the matrix that accounts for the similarities between different users and movies.

360
00:24:23,920 --> 00:24:31,120
How specifically does the frequency domain, the Fourier transform, tie into the graph itself

361
00:24:31,120 --> 00:24:33,760
and in this context?

362
00:24:33,760 --> 00:24:37,680
So it's very similar to what JoÃ£o described before, basically the spectral definition

363
00:24:37,680 --> 00:24:38,680
of convolution, right?

364
00:24:38,680 --> 00:24:41,440
You can do a convolution in the spectral domain, right?

365
00:24:41,440 --> 00:24:45,720
The classical convolution, this is what we call the conversion theorem in single processing

366
00:24:45,720 --> 00:24:50,880
that you can implement filtering or convolution in the frequency domain just as a product,

367
00:24:50,880 --> 00:24:52,960
point-wise product of free-attent forms, right?

368
00:24:52,960 --> 00:24:56,960
And this is what we usually do in standard single processing because we can efficiently

369
00:24:56,960 --> 00:24:59,840
compute the free-attent form using F50.

370
00:24:59,840 --> 00:25:01,600
And you can do the same thing for images, right?

371
00:25:01,600 --> 00:25:05,200
You can do two-dimensional filters in the frequency domain.

372
00:25:05,200 --> 00:25:09,240
So basically extend this analogy to a matrix data.

373
00:25:09,240 --> 00:25:12,800
Basically you can think of it as an image, but the domains where the rows and the columns

374
00:25:12,800 --> 00:25:19,320
of these image leave, they have graph structure, sort of standard Euclidean structure.

375
00:25:19,320 --> 00:25:26,120
If someone has a problem where they've got some set or sets of entities that have some

376
00:25:26,120 --> 00:25:33,080
inherent structure, one relative to the other, as opposed to some of the more traditional

377
00:25:33,080 --> 00:25:39,000
image, an image or other data types, that's an area where they could be looking at an

378
00:25:39,000 --> 00:25:42,640
approach like this and it might give them some advantage.

379
00:25:42,640 --> 00:25:43,640
Absolutely.

380
00:25:43,640 --> 00:25:50,720
So you mentioned a setting that is now approached and defined through different names, so now

381
00:25:50,720 --> 00:25:54,400
there's something that is becoming very popular recently called a meta-learning, right?

382
00:25:54,400 --> 00:26:01,360
Where is this idea that maybe rather than using a traditional supervised learning, where

383
00:26:01,360 --> 00:26:06,320
I have very few number of samples and then very few number of labels and then the environment

384
00:26:06,320 --> 00:26:09,440
is non-stationary, so it changes all again and again.

385
00:26:09,440 --> 00:26:15,280
Maybe what I can learn is a rule that by looking at how maybe the inputs relate to each other,

386
00:26:15,280 --> 00:26:20,680
even if the individual distribution of the labels and the images change, maybe the underlying

387
00:26:20,680 --> 00:26:26,040
relationship between labels and images that I need to learn is something that I can leverage

388
00:26:26,040 --> 00:26:28,160
by exploring more and more data.

389
00:26:28,160 --> 00:26:34,440
So once you start setting up problems using this formulation, you end up with a task where

390
00:26:34,440 --> 00:26:37,960
you have to learn how to relate different things, very different objects.

391
00:26:37,960 --> 00:26:43,360
And so this is a terrain where it's very natural to look at our models, so we have a recent

392
00:26:43,360 --> 00:26:49,160
paper that is currently under review where we think about what's called a few-shot learning

393
00:26:49,160 --> 00:26:52,360
problem using this model, using a graphical network.

394
00:26:52,360 --> 00:26:57,840
And so what is interesting is that somehow it generalizes and it includes, as particular

395
00:26:57,840 --> 00:27:02,640
cases, some of the models that people have been using and developing in the recent year

396
00:27:02,640 --> 00:27:05,080
or so to attack this problem.

397
00:27:05,080 --> 00:27:11,400
And so this is just to say that there are many tasks that you could imagine across AI

398
00:27:11,400 --> 00:27:16,040
and across sciences, the underlying thing that you need to learn is how to relate objects

399
00:27:16,040 --> 00:27:17,040
to each other.

400
00:27:17,040 --> 00:27:22,880
And so once you have to do this relational task, then the natural data structure for that

401
00:27:22,880 --> 00:27:25,440
is a graph, just a point cloud.

402
00:27:25,440 --> 00:27:30,480
So I expect that they will see more and more applications of these technology in the

403
00:27:30,480 --> 00:27:32,320
near future.

404
00:27:32,320 --> 00:27:37,440
So one of the things that this conversation brings to mind is a recent conversation I

405
00:27:37,440 --> 00:27:38,440
had.

406
00:27:38,440 --> 00:27:43,280
In fact, here it nips with Sri Ram Natharajan, who studies statistical relational AI.

407
00:27:43,280 --> 00:27:47,760
Are you familiar with that line of research and how that relates to this?

408
00:27:47,760 --> 00:27:48,760
Not really.

409
00:27:48,760 --> 00:27:49,760
Okay.

410
00:27:49,760 --> 00:27:50,760
Just check it out.

411
00:27:50,760 --> 00:27:58,640
Similar thinking he's also looking at graph-based approaches and applying them to the healthcare

412
00:27:58,640 --> 00:28:01,360
domain and other domains.

413
00:28:01,360 --> 00:28:07,560
So in the case of the, you know, any of the examples we've talked about, I can, you know,

414
00:28:07,560 --> 00:28:12,000
we've already talked about some of the advantages of this approach over traditional approaches.

415
00:28:12,000 --> 00:28:19,000
One of them is that in the case of the particle physics work, it's, you know, maybe more intuitive

416
00:28:19,000 --> 00:28:23,400
for the physicists because it maps more closely to the tools and the way they're used to thinking

417
00:28:23,400 --> 00:28:25,400
about the domain.

418
00:28:25,400 --> 00:28:27,720
What are the other advantages of this approach?

419
00:28:27,720 --> 00:28:33,240
Are there performance advantages in terms of either computational or model accuracy or

420
00:28:33,240 --> 00:28:34,240
things like that?

421
00:28:34,240 --> 00:28:35,240
Yeah.

422
00:28:35,240 --> 00:28:40,200
So what I would say that the main advantage is definitely the fact that it's more general

423
00:28:40,200 --> 00:28:44,760
so you can, there are problems in which it might be the only, your only choice, right?

424
00:28:44,760 --> 00:28:48,040
That there's no, there's no, including alternative to do so.

425
00:28:48,040 --> 00:28:53,840
I would say that whenever you're, I mean, you could ask the question, well, if I just forget

426
00:28:53,840 --> 00:28:57,920
about the grid structure of an image and just treat it as a graph, right?

427
00:28:57,920 --> 00:29:01,880
And I run my model, this model is it going to do better than the CNN?

428
00:29:01,880 --> 00:29:02,880
Right.

429
00:29:02,880 --> 00:29:03,880
I would say that the answer is no.

430
00:29:03,880 --> 00:29:04,880
Yeah.

431
00:29:04,880 --> 00:29:05,880
That's not really the point, right?

432
00:29:05,880 --> 00:29:06,880
It's not really the point, right?

433
00:29:06,880 --> 00:29:12,680
So I would say that the main strength of, of the model is really to, to respect the

434
00:29:12,680 --> 00:29:17,000
invariance of the data, for example, if you are treating a, you know, if you are just

435
00:29:17,000 --> 00:29:20,960
observing a, you know, a point cloud, you know, that the order in which I give you the

436
00:29:20,960 --> 00:29:23,240
point cloud is completely relevant, right?

437
00:29:23,240 --> 00:29:28,480
So therefore, you're, you're, if you can certify that your model is going to exactly, if

438
00:29:28,480 --> 00:29:33,760
exactly the same output independent of how the, the input are permitted, then you are kind

439
00:29:33,760 --> 00:29:36,760
of respecting that this kind of natural invariant of the data.

440
00:29:36,760 --> 00:29:41,640
So I would say that, that relative to models that are, for example, maybe using sequential

441
00:29:41,640 --> 00:29:46,840
networks, like, for example, Rekord neural networks, here, it could be that, that eventually

442
00:29:46,840 --> 00:29:51,400
these models based on graphs and sets are going to be more sample efficient and maybe

443
00:29:51,400 --> 00:29:56,480
they can get you better performance precisely because they are a bit more tailored to the

444
00:29:56,480 --> 00:30:00,440
data format, like to the input data format.

445
00:30:00,440 --> 00:30:04,720
Maybe a good example would be applications from the domain of graphics and computer vision

446
00:30:04,720 --> 00:30:07,960
and in particular analysis of deformable 3D shapes.

447
00:30:07,960 --> 00:30:12,440
So this is actually, I think it's a good illustration because you can treat such objects in two

448
00:30:12,440 --> 00:30:13,440
different ways.

449
00:30:13,440 --> 00:30:16,800
You can treat them as Euclidean objects, basically, they are things that live in three-dimensional

450
00:30:16,800 --> 00:30:17,800
space, right?

451
00:30:17,800 --> 00:30:22,600
You can apply standard, let's say, convolutional neural networks, maybe, on volumetric representations

452
00:30:22,600 --> 00:30:27,320
of, of these objects, or you can think of them intrinsically from the perspective of differential

453
00:30:27,320 --> 00:30:30,320
geometry, basically, model them as many folds.

454
00:30:30,320 --> 00:30:35,960
And what you gain in this way by resorting to these kind of architectures is, you gain

455
00:30:35,960 --> 00:30:37,480
a deformation invariance.

456
00:30:37,480 --> 00:30:41,480
So basically, your model is by construction invariant to inelastic deformations of the

457
00:30:41,480 --> 00:30:42,480
shape.

458
00:30:42,480 --> 00:30:45,720
And if you're a task, for example, is deformation invariant correspondence or deformation

459
00:30:45,720 --> 00:30:51,320
invariance similarity, it means that you can deal with way less training examples because

460
00:30:51,320 --> 00:30:55,120
you don't need to show to the network all the possible deformations that the shapes can

461
00:30:55,120 --> 00:30:59,040
undergo in order to learn these deformations from examples.

462
00:30:59,040 --> 00:31:02,160
You basically have these invariance built into the model.

463
00:31:02,160 --> 00:31:03,880
And the difference can be very dramatic.

464
00:31:03,880 --> 00:31:08,000
The difference can be in orders of magnitude, less training samples.

465
00:31:08,000 --> 00:31:16,640
So in other words, the approach you're taking to model, because it's more tailored to the

466
00:31:16,640 --> 00:31:21,360
problem domain, it kind of restricts your search space and you don't have to provide examples

467
00:31:21,360 --> 00:31:26,400
for things that wouldn't really exist in the domain itself, but do exist geometrically

468
00:31:26,400 --> 00:31:28,000
in Euclidean space.

469
00:31:28,000 --> 00:31:29,000
Exactly.

470
00:31:29,000 --> 00:31:34,080
Or maybe a better way to say it is that you try to model axiomatically as much as possible

471
00:31:34,080 --> 00:31:38,460
or as much as make sense in your specific problem and everything that cannot be modeled

472
00:31:38,460 --> 00:31:42,320
axiomatically because, of course, there is a limitation to what you can model basically

473
00:31:42,320 --> 00:31:48,320
in handcrafted way, everything that deviates from your model you learn.

474
00:31:48,320 --> 00:31:49,320
Okay.

475
00:31:49,320 --> 00:31:53,840
Other other topics that you covered in tutorial that we haven't touched on yet?

476
00:31:53,840 --> 00:32:00,400
So just very briefly, so one other potential area of application that we are currently exploring

477
00:32:00,400 --> 00:32:05,560
to what extent, if you now have a language to learn our graph-based structure, you can

478
00:32:05,560 --> 00:32:10,040
use it for a combinatorial optimization problems that are naturally defined over graphs.

479
00:32:10,040 --> 00:32:14,240
So this is a completely different domain of application, because there the goal is not

480
00:32:14,240 --> 00:32:19,240
so much to solve a task that you don't know how to solve, is more to solve or to approximate

481
00:32:19,240 --> 00:32:20,240
a task faster.

482
00:32:20,240 --> 00:32:23,480
We're talking like traveling salesmen and these kinds of graphs.

483
00:32:23,480 --> 00:32:24,480
Exactly.

484
00:32:24,480 --> 00:32:25,480
Exactly.

485
00:32:25,480 --> 00:32:29,160
So we briefly touched upon one of such problems in the tutorial, which is the quadratic

486
00:32:29,160 --> 00:32:30,160
assignment problem.

487
00:32:30,160 --> 00:32:31,160
But what?

488
00:32:31,160 --> 00:32:37,160
The quadratic assignment, which contains the travel statement problem is that you can think

489
00:32:37,160 --> 00:32:40,360
it as a particular case of that one.

490
00:32:40,360 --> 00:32:46,840
So there the general setup is really an instant of this trend that you can always have this

491
00:32:46,840 --> 00:32:51,680
analogy between an algorithm to solve a task and a neural network.

492
00:32:51,680 --> 00:32:56,080
And this analogy works by looking at the algorithm and then unraveling typically the algorithm

493
00:32:56,080 --> 00:33:01,720
involved a series of iterative steps, iterations, so you can just see these iterations as being

494
00:33:01,720 --> 00:33:04,400
different layers of a network.

495
00:33:04,400 --> 00:33:09,000
And then once you have this analogy, then you can try to study like a trade-offs between

496
00:33:09,000 --> 00:33:14,680
a computation and accuracy, by just replacing the guarantees that the algorithm gives you

497
00:33:14,680 --> 00:33:19,800
by just a data driven approach where you just feed the parameters of the network to a

498
00:33:19,800 --> 00:33:21,680
dataset of solved problems.

499
00:33:21,680 --> 00:33:28,880
So this is an interesting and potentially also useful because in many domains, especially

500
00:33:28,880 --> 00:33:33,080
when it comes to combinatorial optimization, there are problems in which it's still an

501
00:33:33,080 --> 00:33:38,680
open research area, how to come up with a vision approximation of intractable problems.

502
00:33:38,680 --> 00:33:44,760
So here, one of the potential uses of what we presented is, well, now we are providing

503
00:33:44,760 --> 00:33:50,880
a family of, let's say, trainable architectures that can be used to guide and to provide good

504
00:33:50,880 --> 00:33:55,120
trade-offs between accuracy and complexity for problems such as the travel experience,

505
00:33:55,120 --> 00:33:57,520
salesman, or other interesting things.

506
00:33:57,520 --> 00:33:58,520
Interesting.

507
00:33:58,520 --> 00:34:02,600
So the paraphrase is that what you've done is your research is kind of providing a way

508
00:34:02,600 --> 00:34:10,600
to express graph-oriented problems in terms of neural networks, traveling salesmen, map

509
00:34:10,600 --> 00:34:15,320
coloring, all these other combinatorial kind of graph problems.

510
00:34:15,320 --> 00:34:20,560
They're typically very difficult to solve exactly and so there are all kinds of approximations

511
00:34:20,560 --> 00:34:25,320
and heuristics, but for a certain level of complexity, those don't work very well.

512
00:34:25,320 --> 00:34:31,200
So now, your research applied to them gives you a way to solve these using neural networks.

513
00:34:31,200 --> 00:34:32,200
Well, yes.

514
00:34:32,200 --> 00:34:33,720
So I would not say potentially.

515
00:34:33,720 --> 00:34:34,720
Potentially.

516
00:34:34,720 --> 00:34:35,720
Potentially, exactly.

517
00:34:35,720 --> 00:34:39,720
It's a question mark and I think it's a question mark that it's worth exploring, right?

518
00:34:39,720 --> 00:34:40,720
That's correct.

519
00:34:40,720 --> 00:34:41,720
Sure.

520
00:34:41,720 --> 00:34:48,600
Because in some applications, it could be useful that, for example, not just have a single

521
00:34:48,600 --> 00:34:56,520
algorithm with a single aristic, but to have a toggle that you can select between how

522
00:34:56,520 --> 00:35:00,600
many cycles do you want to spend versus how much accurate do you want the solution to

523
00:35:00,600 --> 00:35:01,600
be, right?

524
00:35:01,600 --> 00:35:07,560
And be able to learn adaptive trade-offs and all these things are interesting.

525
00:35:07,560 --> 00:35:12,840
Then there's another declination of this area of research that is a bit more going into

526
00:35:12,840 --> 00:35:18,560
the theoretical computer science, namely to what extent the models that we learn could

527
00:35:18,560 --> 00:35:22,000
be interpreted as algorithms that we still don't know.

528
00:35:22,000 --> 00:35:27,240
This might be, it could well be that it doesn't work because it relies on this fact that

529
00:35:27,240 --> 00:35:31,080
can we interpret or can we uncover what the neural network is learning?

530
00:35:31,080 --> 00:35:33,320
And we know that this is typically a hard thing to do, right?

531
00:35:33,320 --> 00:35:37,440
Even for a convolutional neural network, we don't really know how the network figures

532
00:35:37,440 --> 00:35:39,000
out how to solve the problem.

533
00:35:39,000 --> 00:35:44,000
But what I'm saying is that in some context, it could be interesting to try to understand

534
00:35:44,000 --> 00:35:47,560
and analyze kind of things that the network learned.

535
00:35:47,560 --> 00:35:50,360
So we have these graph problems.

536
00:35:50,360 --> 00:35:53,560
There are graph problems in computer science as well.

537
00:35:53,560 --> 00:35:58,560
Your research allows us to express those as neural networks if we could then peer into

538
00:35:58,560 --> 00:36:02,560
the neural network that might give us some insight into these computer science problems

539
00:36:02,560 --> 00:36:04,960
that we're trying to model in a first place.

540
00:36:04,960 --> 00:36:05,960
Yes.

541
00:36:05,960 --> 00:36:06,960
Interesting.

542
00:36:06,960 --> 00:36:07,960
Interesting.

543
00:36:07,960 --> 00:36:08,960
How about implementation?

544
00:36:08,960 --> 00:36:12,760
Like I'm imagining, like at this point in time, you can't just do, you know, TensorFlow

545
00:36:12,760 --> 00:36:13,760
TF.

546
00:36:13,760 --> 00:36:16,800
You know, graph solver and do this.

547
00:36:16,800 --> 00:36:18,000
How does that work?

548
00:36:18,000 --> 00:36:22,520
So to some extent, we are trying to leverage existing tools not to reinvent the wheel.

549
00:36:22,520 --> 00:36:25,120
Basically, the underlying framework is a standard one.

550
00:36:25,120 --> 00:36:27,680
We use TensorFlow, for example.

551
00:36:27,680 --> 00:36:32,880
We just create some custom things that then boil down again to some standard operations

552
00:36:32,880 --> 00:36:35,080
like matrix multiplication.

553
00:36:35,080 --> 00:36:39,160
So basically the short answer is yes, it is that easy.

554
00:36:39,160 --> 00:36:42,040
It is built on top of some standard frameworks.

555
00:36:42,040 --> 00:36:43,040
Okay.

556
00:36:43,040 --> 00:36:51,040
Some minor but potentially profound differences in the fact that the scaling up, like using

557
00:36:51,040 --> 00:36:56,920
these models on large graphs or large domains involves, you know, matrix multiplication

558
00:36:56,920 --> 00:36:58,200
with matrix that are large.

559
00:36:58,200 --> 00:37:02,400
And so the structure that we have is that these matrices are sparse.

560
00:37:02,400 --> 00:37:08,800
And so hopefully we see more and more integration of sparse linear algebra into PyTorge and

561
00:37:08,800 --> 00:37:15,560
TensorFlow, etc. but there's a fundamental difference now that maybe the hardware, like

562
00:37:15,560 --> 00:37:22,720
GPUs, they are excelling at a specific form of operation that is not very friendly with

563
00:37:22,720 --> 00:37:24,800
sparse matrix multiplication.

564
00:37:24,800 --> 00:37:25,800
Okay.

565
00:37:25,800 --> 00:37:31,160
But again, I'm not an expert in this low-level implementation, but this, I would say,

566
00:37:31,160 --> 00:37:35,640
is one of the main differences between, you know, running a complement or running a

567
00:37:35,640 --> 00:37:37,640
graph convolution.

568
00:37:37,640 --> 00:37:44,000
So you've released some code that works on their TensorFlow, but it isn't necessarily

569
00:37:44,000 --> 00:37:46,880
amenable to scaling up just yet.

570
00:37:46,880 --> 00:37:49,240
There's stuff that needs to be figured out.

571
00:37:49,240 --> 00:37:54,640
Maybe a one way to get a sense for the complexity of this has anyone like beyond the, you know,

572
00:37:54,640 --> 00:37:58,360
the two of you and folks that are like deep in this research use this.

573
00:37:58,360 --> 00:38:04,520
Are you aware of any like external arms length applications?

574
00:38:04,520 --> 00:38:07,680
So people, different people from different communities try to use.

575
00:38:07,680 --> 00:38:13,640
I wouldn't say that it's extremely popular yet, but it probably, it starts to become.

576
00:38:13,640 --> 00:38:18,480
So many different domains, many different applications can be, the problems in these domains

577
00:38:18,480 --> 00:38:23,520
can be formulated using graphs, graphs at the end are very generic and very convenient

578
00:38:23,520 --> 00:38:28,720
representation of any kind of relations or interactions you can think of.

579
00:38:28,720 --> 00:38:33,360
So that's really, very generic framework of describing certain types of data.

580
00:38:33,360 --> 00:38:38,520
So yeah, people that are even not experts in machine learning that come from different

581
00:38:38,520 --> 00:38:44,200
domains that bring certain applications, they try to basically, they see that graphs allow

582
00:38:44,200 --> 00:38:49,640
to formulate their problems in a natural way and they are curious to try out these approaches.

583
00:38:49,640 --> 00:38:50,640
Okay.

584
00:38:50,640 --> 00:38:51,640
Great.

585
00:38:51,640 --> 00:38:52,640
Great.

586
00:38:52,640 --> 00:38:52,640
This is really exciting stuff.

587
00:38:52,640 --> 00:38:59,240
Where can folks go to learn more about it, download the TensorFlow code or read some

588
00:38:59,240 --> 00:39:00,240
of the papers?

589
00:39:00,240 --> 00:39:06,200
So we have a dedicated website that is easy to remember, it's geometricdiplearning.com.

590
00:39:06,200 --> 00:39:07,720
geometricdiplearning.com also.

591
00:39:07,720 --> 00:39:11,520
Where, yeah, where I think the idea is to have all the tutorial material.

592
00:39:11,520 --> 00:39:14,400
We have the review paper that Michael mentioned.

593
00:39:14,400 --> 00:39:20,880
We have also the recent literature by not just us, but other groups that use the tools.

594
00:39:20,880 --> 00:39:24,800
And then we are also going to have in two months, we're going to have an IPAM workshop here

595
00:39:24,800 --> 00:39:30,760
in Los Angeles, where I think I'm also looking forward to it because it's what you are

596
00:39:30,760 --> 00:39:36,040
saying that people from different domains and people from different disciplines will

597
00:39:36,040 --> 00:39:42,560
come together and essentially present their data problem or their information.

598
00:39:42,560 --> 00:39:48,000
And what I'm expecting is that we are going to see more and more this realization that

599
00:39:48,000 --> 00:39:55,480
actually the models and the tools can be used across more domains than maybe we are expecting.

600
00:39:55,480 --> 00:39:56,480
Okay.

601
00:39:56,480 --> 00:39:57,480
Awesome.

602
00:39:57,480 --> 00:39:58,480
Great.

603
00:39:58,480 --> 00:40:01,320
Well, Joanne, Michael, thank you so much for taking the time to chat.

604
00:40:01,320 --> 00:40:02,320
I enjoy the conversation.

605
00:40:02,320 --> 00:40:05,320
Thank you very much.

606
00:40:05,320 --> 00:40:10,720
All right, everyone, that's our show for today.

607
00:40:10,720 --> 00:40:15,840
Thanks so much for listening and for your continued feedback and support.

608
00:40:15,840 --> 00:40:22,800
To follow along with the NIP series, visit twimmelai.com slash NIPS 2017.

609
00:40:22,800 --> 00:40:29,440
To enter our Twimmel 1 Mill contest, visit twimmelai.com slash Twimmel 1 Mill.

610
00:40:29,440 --> 00:40:35,160
Of course, we'd be delighted to hear from you either via a comment on the show notes page

611
00:40:35,160 --> 00:40:40,360
or via a tweet to add Twimmelai or add Sam Charrington.

612
00:40:40,360 --> 00:40:44,880
Thanks once again to Intel Nirvana for their sponsorship of this series.

613
00:40:44,880 --> 00:40:49,600
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

614
00:40:49,600 --> 00:40:53,840
the AI arena, visit intelnervana.com.

615
00:40:53,840 --> 00:40:58,600
As I mentioned a few weeks back, this will be our final series of shows for the year.

616
00:40:58,600 --> 00:41:03,840
So take your time and take it all in and get caught up on any of the old pods you've been

617
00:41:03,840 --> 00:41:05,440
saving up.

618
00:41:05,440 --> 00:41:07,840
Happy holidays and happy new year.

619
00:41:07,840 --> 00:41:10,120
See you in 2018.

620
00:41:10,120 --> 00:41:14,240
And of course, thanks once again for listening and catch you next time.

