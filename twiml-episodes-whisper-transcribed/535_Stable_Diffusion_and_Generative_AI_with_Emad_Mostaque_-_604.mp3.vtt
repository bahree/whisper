WEBVTT

00:00.000 --> 00:07.000
.

00:07.000 --> 00:14.000
.

00:14.000 --> 00:21.000
.

00:21.000 --> 00:28.000
.

00:28.000 --> 00:34.000
.

00:34.000 --> 00:42.000
All right everyone this is Sam Charrington host of the Tumel AI podcast and today I'm coming to you live from the future frequency podcast studio at the AWS

00:42.000 --> 00:52.000
Reinvent Conference here in Las Vegas and I am joined by Amad Mostak. Amad is founder and CEO of Stability AI. Amad welcome to the podcast.

00:52.000 --> 01:11.000
Thanks so much Harmi. Super excited to talk to you. You are of course the founder and CEO of Stability. Stability is the company behind stable diffusion, which is a large multimodal model that has been getting a lot of a lot of fanfare I think.

01:11.000 --> 01:15.000
And I'd love to jump in by having you share a little bit about your background.

01:15.000 --> 01:25.000
Yeah, I think it's been super interesting. I think several diffusions kind of a specific text image model. I think it's that large, but we can talk about that a bit later, which is one of the fun parts.

01:25.000 --> 01:35.000
As for me, I started off mass computer science at uni and enterprise developer and then we came a hedge fund manager and one of the largest video game investors in the world and then artificial intelligence.

01:35.000 --> 01:54.000
I was doing that. There was a lot of fun and then my son was diagnosed with autism and they said there was no cure treatment. So I quit switched to advising hedge funds and built an AI team to do literature review all the autism literature and then buy molecular pathway analysis of neurotransmitters to repurpose drugs to help them out.

01:54.000 --> 02:00.000
And it kind of worked. He went to mainstream school and super happy. It's awesome. It's kind of cool. Good trade. Good trade.

02:00.000 --> 02:12.000
Then I went back to the hedge fund world once more wars as I was boring. So then decided to make the world a better place. So first off took the global ex price for learning. That was 15 million dollar prize from Elon Musk and Tony Robin.

02:12.000 --> 02:22.000
So the first app to teach kids literacy and numeracy about internet. My co-founder and I have been deploying that around the world and now we're teaching kids and refugee camps literacy and numeracy in 13 months and one hour a day.

02:22.000 --> 02:33.000
And about AI the craft out of that in 2021. I designed and led the United Nations one of the United Nations AI initiatives as COVID-19.

02:33.000 --> 02:40.000
Collective and mental intelligence against COVID-19 launched a Stanford that by the WHO in ESCO and the World Bank.

02:40.000 --> 02:53.000
And that was really interesting because we're trying to use the world's made the world's knowledge free on COVID-19. So there's a 500,000 paper data set freely available to everyone and use AI to organize it because it's really confusing.

02:53.000 --> 03:03.000
During that lots and lots of interesting tech kind of came through but I realized these foundation models are super powerful. You can't have them control by any one company.

03:03.000 --> 03:15.000
It's bad business and it's not the correct thing ethically. So I thought let's widen this and create open source foundation models for everyone because I think it can really advance humanity. And again, I think it'll be great to see these things collaborate.

03:15.000 --> 03:22.000
So we can have an open discussion about it and also have the value created from just these brand new experiences.

03:22.000 --> 03:26.000
That's awesome. And when did you get started down that that part of the journey?

03:26.000 --> 03:38.000
About two years ago, stability has been going for about 13 months now. Yeah, when I think about the, you know, a lot of stable diffusion goes back to this latent diffusion paper, which was, you know, not even a year ago.

03:38.000 --> 03:44.000
It's not only a year ago. I think the whole thing kind of kicked off with the clip released by open AI in January of last year.

03:44.000 --> 03:47.000
So I said COVID during that time while doing my COVID thing. Okay.

03:47.000 --> 03:59.000
My daughter came to me and said, Dad, you know, all that stuff you do, taking all that knowledge and squishing it down to make it useful for everyone. Can you do that with images like we can. So a bit of system for her based on VQ gang and clip.

03:59.000 --> 04:08.000
So image generating model and then clip is an image to text model where she created like a vision board of everything she wanted a description what she wanted to make.

04:08.000 --> 04:17.000
There is 16 different images and then she said how each one of those is different and change the latent. And then generate another 60 another 60 another 60 and then eight hours later.

04:17.000 --> 04:22.000
She made an image that she went on to sell as an NFT for three and a half thousand dollars.

04:22.000 --> 04:25.000
Wow. And donated the proceeds to India code relief. Okay.

04:25.000 --> 04:28.000
I thought it was awesome. She's seven years old. Wow.

04:28.000 --> 04:32.000
And then I was like, this is transformative technology.

04:32.000 --> 04:41.000
The image is the one it's out language. You're already at 85% we're going to get a 95% image. We're at 10%. We're not visual species like the easiest way for us to communicate is what we're doing now.

04:41.000 --> 04:48.000
We're having a nice chat, you know, then text is the next part of an image like the images or PowerPoints are impossible.

04:48.000 --> 04:55.000
Let's make it easy. This tech can do that. So we started funding the entire sector, Google call up notebooks models, all these kind of things.

04:55.000 --> 05:03.000
Late in diffusion was done by the conference lab at the University of Munich, who are led on the stable diffusion one as well.

05:03.000 --> 05:11.000
Amazing lab led by BjÃ¶rn Jomrod and led by Robin Rombach, who was one of our lead developers here at Stability.

05:11.000 --> 05:18.000
And then there was work by Catherine Krausen, Rivershave Wings, a Twitter handle on clip condition models and things like that.

05:18.000 --> 05:20.000
And the whole community just came together and built really cool stuff.

05:20.000 --> 05:25.000
Then he had entities like mid-journey where we just gave grants for the beta that started operationalizing it.

05:25.000 --> 05:30.000
And it's all come together now to the finality of stable diffusion that was released on August 3rd.

05:30.000 --> 05:39.000
So that was led by the conference lab and then we ourselves a stability runway ML, a Luther AI community that we kind of helped run and lie on.

05:39.000 --> 05:49.000
We came together to put out 100,000 gigabytes of image lab text pairs, two billion images turned into a two gigabyte file that runs natively on your Macbook.

05:49.000 --> 05:54.000
That can create anything. It's kind of insane. I think it's yeah.

05:54.000 --> 05:57.000
And the speed of which it all came together is mind modeling.

05:57.000 --> 06:05.000
Yeah, like our model was to have a core team and then site contributors and partners from academia and then these communities that we kind of built and accelerated.

06:05.000 --> 06:13.000
So like tens of thousands of people from open by ML were doing protein folding work to a Luther with language models to harmonize with audio.

06:13.000 --> 06:18.000
And it turned out that's a really good system to just iterate and experiment with these things and exactly the right time.

06:18.000 --> 06:27.000
And now it's progressed. So like when we started with stable diffusion and launched it in August, 5.8 seconds for a generation on an A 100.

06:27.000 --> 06:31.000
As of yesterday, 0.86 seconds.

06:31.000 --> 06:36.000
As of two weeks from now, it'll be 20 times faster with our new still models.

06:36.000 --> 06:43.000
So you're getting to 24 frames a second high resolution image creation from basic blobs a year ago.

06:43.000 --> 06:54.000
I don't think we've ever seen anything that fast. And the uptake has been crazy. So I believe on Monday, the number of GitHub stars was stable diffusion overtook Ethereum and Bitcoin.

06:54.000 --> 07:00.000
So we're taking Kafka, everything else thing. I'll overtake kind of PyTorch and TensorFlow and like a month or two.

07:00.000 --> 07:07.000
And that's since inception, like over the last month, I think Master Don has had 6000 GitHub stars over the last week.

07:07.000 --> 07:13.000
Stable diffusion to has had 6000. Yeah, and stable diffusion to was just released this month.

07:13.000 --> 07:19.000
Yeah, we were last month. It was a week ago. Yeah, last month. These times I lay stable. So stable diffusion one.

07:19.000 --> 07:26.000
We kind of use the Lyon data set to create the image model and then we use open a eyes clip L 14 to kind of condition it.

07:26.000 --> 07:28.000
So we combine the text model and the image model.

07:28.000 --> 07:41.000
We're stable diffusion to we instead use something called open clip run by kind of the Lyon charity, whereby we had an open data set for both because open added amazing work open sourcing clip.

07:41.000 --> 07:46.000
But we didn't know what data was inside it. So it learned all these concept. I'm like, how does it know that?

07:46.000 --> 07:54.000
And so when we launched able to fusion kind of as a collaboration, we had all these questions about attribution about what's in the data set, say for work, not say for work.

07:54.000 --> 08:02.000
But you can't control that if you don't control half the data set right off the learning. So still diffusion to have that, but I also had a better texting code a model.

08:02.000 --> 08:08.000
So now basically it's heading towards photorealism. Yeah, you can get photorealistic outputs from it if you can process.

08:08.000 --> 08:16.000
Yeah, and again, kind of insane. Like you just see these things generated in a second. You're like, it can be completely like artistic or completely photorealistic.

08:16.000 --> 08:28.000
These people do not exist. This landscape or this interior does not exist. I don't think we've ever actually seen anything like this because the majority of him out he doesn't believe they can visually create.

08:28.000 --> 08:33.000
Just like before the Gutenberg press, you couldn't write or read.

08:33.000 --> 08:48.000
But now hundreds of thousands of developers think we've had like 380,000 developers sign up for a thing on hugging face. And now using this to create ridiculous things. And now that it gets to real time, what does that even look like when people can just seem to see communicate visually.

08:48.000 --> 08:56.000
Like we could literally in a few months a year, definitely this podcast, you could generate a live video almost on it.

08:56.000 --> 09:08.000
But all the topics that we're talking about, which is insane. One of the examples that you like to use is killing PowerPoint. So we've got the text that's where you usually start and you go through this long process to make it pretty or engaging aesthetic, right?

09:08.000 --> 09:18.000
Yeah, because you know what these models do, like this attention based models, like it's interesting. So with my son with his autism, autism is kind of a social interaction disorder.

09:18.000 --> 09:26.000
It's caused, in my opinion, largely by a gap of glutamate imbalance in the brain. So Gabba calms you down when you pop a value and glutamate excites you.

09:26.000 --> 09:35.000
And obviously in our industry, a lot of people kind of have people, they know on the spectrum or they're just highly there because it lends itself sometimes where there's a dual wedge thing.

09:35.000 --> 09:42.000
Because of all that stuff, what happens is that there's too much glutamate. It's like, you know when you're tapping your legs, there's too much going on in your brain.

09:42.000 --> 09:45.000
Imagine that was like that all the time. You couldn't think straight.

09:45.000 --> 09:52.000
Yeah. And so you can't form the connections of like a cup means cup your hands or a cup or a world cup in your brain.

09:52.000 --> 09:55.000
That's why there's a lot of cases where they can't communicate properly.

09:55.000 --> 10:01.000
Addressing those factors and calm it down, then you basically start teaching them just like when you have a stroke.

10:01.000 --> 10:06.000
A cup means it's a cup means that a cup means that and they can start talking or you know progress.

10:06.000 --> 10:18.000
With these attention-based models, you've moved from kind of giant extrapolation of data to paying attention to the most important parts between words and pixels, which is kind of crazy for the denoising process of diffusion.

10:18.000 --> 10:26.000
The latent is that it built up there where it has all the concepts of a cup means that if you have a cup in a sentence, it understands what that is in that context, a world cup or cup in the hands.

10:26.000 --> 10:31.000
And then can do these images, which is kind of insane. So it works like that part of the human brain.

10:31.000 --> 10:36.000
I think that's what's so exciting. That's what lets you have the compression of knowledge.

10:36.000 --> 10:43.000
Like so 100,000 gigabytes into two gigabytes is like we're pie-pype from that Silicon Valley HDO show, right?

10:43.000 --> 10:47.000
I think it doesn't make sense. Yeah. You know, like a lot, but that's because...

10:47.000 --> 10:51.000
A hundred thousand gigabytes. A hundred terabytes.

10:51.000 --> 10:54.000
Was our input data and the output files two gigs. Yeah.

10:54.000 --> 10:58.000
And it's not optimised yet. We reckon we can get that to 400 megabytes.

10:58.000 --> 11:06.000
Oh wow. A 400 megabyte file that now works on an iPhone that can generate any image in seconds by description.

11:06.000 --> 11:07.000
Yeah.

11:07.000 --> 11:10.000
And you can go the other way as well. You can take an image and turn it into text.

11:10.000 --> 11:16.000
And that text encoding is only a few lines that can generate a high resolution masterpiece.

11:16.000 --> 11:18.000
It's insane.

11:18.000 --> 11:19.000
That's nuts.

11:19.000 --> 11:30.000
And I think we were kind of a bit misguided by not mismigod, but you know, the focus was on scale is all you need 540 billion parameter, trillion parameter, large language models.

11:30.000 --> 11:34.000
Stable diffusers 890 million parameters.

11:34.000 --> 11:37.000
How does that kind of work about large earlier?

11:37.000 --> 11:40.000
It's not large. It's actually quite small.

11:40.000 --> 11:46.000
And this is kind of pointing something to the future because like, you know, OpenAI took GPT-375 billion parameters.

11:46.000 --> 11:52.000
And they instructed it. So reinforce with only human feedback by getting annotators to use it.

11:52.000 --> 11:55.000
And then seeing which neurons kind of lit up these kind of latent space things.

11:55.000 --> 12:02.000
Instruct GPT had equivalent performance. I think they probably use a larger version of that at 1.3 billion parameters.

12:02.000 --> 12:07.000
Because kind of you don't need all the information of the world completely to do stuff.

12:07.000 --> 12:09.000
You just need some of it.

12:09.000 --> 12:12.000
Image models though are surprisingly small.

12:12.000 --> 12:17.000
Like the largest we've seen was the 12 billion parameters. Are you Dali model?

12:17.000 --> 12:23.000
But now, like I said, we're 900 million parameters and we've had great success with our 400 million parameter models.

12:23.000 --> 12:29.000
Our 4 billion parameter models are better. Actually, the largest is party, which was from Google at 220 billion.

12:29.000 --> 12:36.000
We don't know what an optimal data set is, what optimal parameter size is for these particular non-text models.

12:36.000 --> 12:41.000
Text models themselves. Text is quite a dancing coding. I think we'll tend larger.

12:41.000 --> 12:44.000
But combining these models is going to be super interesting as we move forward.

12:44.000 --> 12:52.000
It's a lot of the efforts as far have been on shrinking the model to make the performance better, to make it smaller, faster.

12:52.000 --> 12:55.000
Do you see a pull towards larger models?

12:55.000 --> 13:02.000
Or do you think it's a different paradigm altogether where there's not going to be that kind of drive to make the model bigger and bigger?

13:02.000 --> 13:10.000
I think there'll be a mixture of things. Again, like what we saw with the deep-mind chiller paper was that the scaling laws weren't necessarily appropriate.

13:10.000 --> 13:18.000
So that showed that a 67 billion parameter model trained on 5 epochs would outperform 175 billion parameter model.

13:18.000 --> 13:23.000
But actually what it really showed if you dig into the details is that data is what you need.

13:23.000 --> 13:27.000
And what does that data look like? We haven't done the proper data augmentation in other studies.

13:27.000 --> 13:32.000
But this is also like, you can think of these models like, stable diffusion one was a pre-crecious kindergarten.

13:32.000 --> 13:37.000
And we talked about the whole internet, so it occasionally turned a little bit off in some of the outputs.

13:37.000 --> 13:40.000
Yeah, stable diffusion too, you're getting to like grade school now.

13:40.000 --> 13:45.000
Let's still see what we're pre-crecious. And you know, we made it safe for work and then it's safer for work and a whole bunch of other things.

13:45.000 --> 13:48.000
Did you do the data sets? We're still not feeding it the right information.

13:48.000 --> 13:51.000
Once we know what the information to feed it will make it even better.

13:51.000 --> 13:54.000
And I don't think that translates to large. I think it turns to more efficient.

13:54.000 --> 13:57.000
And I think one of these things is the accessibility.

13:57.000 --> 14:06.000
Because we optimize stable diffusion kind of as a group and collective to be available on low energy devices, not just like 3090s or A100s.

14:06.000 --> 14:09.000
You can download it on your MacBook right now.

14:09.000 --> 14:15.000
And MacBook M2, as of today, can generate an image in 18 seconds of any type.

14:15.000 --> 14:17.000
In a couple of weeks, it'll be less than a second.

14:17.000 --> 14:21.000
So you can have PyTorch, you can have jacks or whatever.

14:21.000 --> 14:23.000
And you can just stop coding.

14:23.000 --> 14:27.000
And so that opens it up to so many people. It's a new type of programming primitive.

14:27.000 --> 14:30.000
You know, this hashed file that can create anything.

14:30.000 --> 14:34.000
That went into the connection between programming and stable diffusion.

14:34.000 --> 14:37.000
So if you think about it, you're creating an experience for your programming, right?

14:37.000 --> 14:44.000
And so if you use the diffuses library from hugging phase, it's like a couple of lines you can be using stable diffusion in a code base.

14:44.000 --> 14:47.000
And again, it can run your MacBook with no internet.

14:47.000 --> 14:54.000
So what type of experiences can you do when you have this verifiable file, words go in, images come out.

14:54.000 --> 14:56.000
It opens up a whole world of its possibilities.

14:56.000 --> 14:59.000
It's like an ultra library in a way.

14:59.000 --> 15:02.000
Like the library condense in AI model.

15:02.000 --> 15:04.000
And we're not really used to that.

15:04.000 --> 15:07.000
Like, you know, we've had birds and kind of some of these other things.

15:07.000 --> 15:11.000
But nothing that has this massive range, shall we say.

15:11.000 --> 15:16.000
Like two billion images, a snapshot of the internet can press down.

15:16.000 --> 15:17.000
Yeah.

15:17.000 --> 15:19.000
You're kind of thinking more broadly like we.

15:19.000 --> 15:24.000
A lot of the conversation about stable diffusion today is about art.

15:24.000 --> 15:29.000
And kind of, you know, the creation part of that process.

15:29.000 --> 15:32.000
I'm thinking more broadly about practical applications.

15:32.000 --> 15:38.000
And this is maybe getting into something I wanted to speak about later, just where you see the company going.

15:38.000 --> 15:46.000
You know, talk about some of the other things that are disrupted beyond just, you know, making pretty pictures, arts and crafts, right?

15:46.000 --> 15:47.000
Yeah, man.

15:47.000 --> 15:51.000
I think art is like, we think about it as like, if my artist never make money, right?

15:51.000 --> 15:54.000
Unless they do, you know, like my seven-year-old daughter.

15:54.000 --> 15:57.000
She's obviously, you know, one of the OGs now in general is about.

15:57.000 --> 16:02.000
I actually asked her, why don't you make any more art anymore? And she's like, well, dad, there's this thing called supply and demand.

16:02.000 --> 16:10.000
If I reduce the supply and you can make this whole industry, the bond from my stuff will go up so the value can't be like, you're paying for your own university.

16:10.000 --> 16:14.000
Creative industry is worth hundreds of billions of dollars a year.

16:14.000 --> 16:18.000
Video games, 170 billion, like movies or 80 billion.

16:18.000 --> 16:21.000
This will all be disrupted by this technology.

16:21.000 --> 16:26.000
If you think about the creation process, like one of our directors, he was doing a shoot with a famous actress.

16:26.000 --> 16:29.000
And with a rating, it's going to be $113,000.

16:29.000 --> 16:33.000
It gets flyer out and do all this and get all these other people just for three days.

16:33.000 --> 16:37.000
He fine-tuned a stable diffusion model.

16:37.000 --> 16:40.000
Did it in three hours, 2,000 shots.

16:40.000 --> 16:42.000
Go to realistic.

16:42.000 --> 16:45.000
Meaning being entire shoot was generated as opposed to?

16:45.000 --> 16:46.000
Yeah, like all the shots.

16:46.000 --> 16:51.000
Because there's going to be a shoot to kind of put her in different things to go into the movie kind of process.

16:51.000 --> 16:54.000
So concept artists are using this to become more efficient.

16:54.000 --> 16:56.000
There's a group corridor digital.

16:56.000 --> 17:01.000
They created a Spider-Man everyone's home, which is like a two and a half minute trailer,

17:01.000 --> 17:06.000
in the spider-verse style by having a spider-verse model.

17:06.000 --> 17:08.000
But they train on like 100 images.

17:08.000 --> 17:11.000
And you can't tell it's like, wow, this is an amazing animation.

17:11.000 --> 17:12.000
No, it isn't.

17:12.000 --> 17:16.000
They just interpolated every single frame and used stable diffusion to kind of do image damage.

17:16.000 --> 17:17.000
It's the craziest thing.

17:17.000 --> 17:19.000
It would cost millions of dollars before.

17:19.000 --> 17:21.000
It did it like a few days.

17:21.000 --> 17:27.000
I think media is going to be the first to be disrupted here, because that creation process is hard.

17:27.000 --> 17:28.000
And now it's easy.

17:28.000 --> 17:31.000
I would think industrial design, for example, wouldn't be too far back behind.

17:31.000 --> 17:32.000
Like auto desk.

17:32.000 --> 17:36.000
You know, they spend a lot of time thinking about ways to use machine learning help designers.

17:36.000 --> 17:38.000
Yeah, they've got amazing kind of data sets.

17:38.000 --> 17:42.000
You know, you've got the cameras of the world that have every single click on design.

17:42.000 --> 17:46.000
It can make all of those easier, because the system learns.

17:46.000 --> 17:50.000
There's a foundational model in some ways, because also like a base foundation.

17:50.000 --> 17:52.000
You can then train on your own things.

17:52.000 --> 17:55.000
And it learns physics and all sorts of other stuff, which is a bit creepy.

17:55.000 --> 18:00.000
But it can learn about that specific type of design that you might want to do.

18:00.000 --> 18:05.000
We work with car manufacturers right now, who want to have custom models based on their entire back catalog.

18:05.000 --> 18:08.000
And they want to iterate and combine different concepts.

18:08.000 --> 18:11.000
And then they automatically stick together these cars and combines them.

18:11.000 --> 18:13.000
You know, we also didn't just release the model.

18:13.000 --> 18:15.000
We also released an in-painting model.

18:15.000 --> 18:20.000
So you can delete parts of a picture and have seamless edits based on your text conditioning on that.

18:20.000 --> 18:21.000
You've got an image to image model.

18:21.000 --> 18:22.000
We can define it into any style.

18:22.000 --> 18:25.000
We have a four soon to be eight times up scaler.

18:25.000 --> 18:29.000
That's like enhanced and enhanced on a TV show, you know.

18:29.000 --> 18:33.000
And all of these are going sub-second now in terms of the speed of iteration on them.

18:33.000 --> 18:35.000
So I think creative is the first.

18:35.000 --> 18:37.000
But then I said some of this design kind of things.

18:37.000 --> 18:40.000
Then it goes into more visual communication, like I said, slides.

18:40.000 --> 18:45.000
If you go to image model, combine with language model, combine with code model, you never need to make a presentation again.

18:45.000 --> 18:47.000
And understand what aesthetics are.

18:47.000 --> 18:55.000
Like one of the things we do with stable diffusion thing is that we create a discord bot where everyone rated the outputs of stable diffusion 1.1.

18:55.000 --> 19:03.000
And then we use that to filter down our giant 2 billion image data set into the most aesthetically pleasing things using clip conditioning on that.

19:03.000 --> 19:06.000
And then we trained on that and it became more aesthetic and pleasing.

19:06.000 --> 19:11.000
Bit weird in some ways, but again, these feedback leaves become very, very interesting.

19:11.000 --> 19:18.000
Because to get the wide range of viability on these image models, language models, audio models, others.

19:18.000 --> 19:20.000
The human and the loop factor is essential.

19:20.000 --> 19:23.000
Because your typical training data is quite diverse.

19:23.000 --> 19:29.000
But you want to customize it to the needs and wants of the humans or the sector or the specificness of that.

19:29.000 --> 19:31.000
There are other models out there.

19:31.000 --> 19:38.000
You mentioned the journey a few times, you mentioned Dali, we've talked about performance as a kind of a target differentiator.

19:38.000 --> 19:47.000
What are some of the other ways that you see stable diffusion kind of defining itself relative to the other things that are popping up?

19:47.000 --> 19:50.000
Open source will always lack closed source.

19:50.000 --> 19:54.000
Because they can always just take open source and upgrade it, especially foundation models, right?

19:54.000 --> 19:56.000
I think data is kind of a key thing.

19:56.000 --> 20:01.000
There's been a recurring theme that's come up in our conversation a lot.

20:01.000 --> 20:08.000
This is the idea of the human and the loop and data, refining the data versus evolving the model, the whole data-centric AI idea.

20:08.000 --> 20:14.000
Yeah, so it's kind of a data-centric thing where if you look at people adapt to these models right now, they're doing few shot learning.

20:14.000 --> 20:16.000
Or they're doing basic fine tuning.

20:16.000 --> 20:20.000
Because there's no point in training your own model because it's freaking moving so fast.

20:20.000 --> 20:28.000
We'll have stable diffusion version three in a few months. We had a 20 time speed up yesterday on the model.

20:28.000 --> 20:30.000
It's insane, these kind of moves.

20:30.000 --> 20:32.000
I don't think we've ever seen anything quite this exponential.

20:32.000 --> 20:38.000
But what happens then is that if you go via an API, there's only so much you can do.

20:38.000 --> 20:40.000
That's what all of these companies do.

20:40.000 --> 20:43.000
Or if you go via an interface like the mid-genius and like that or a Dali.

20:43.000 --> 20:46.000
If you've got the model yourself and you can play, you can experiment, you can adapt it.

20:46.000 --> 20:50.000
So the language models from the Elite, the community, GPT Neo, JNX.

20:50.000 --> 20:54.000
They're GPT level models, but only up to 20 billion parameters.

20:54.000 --> 20:58.000
They've been downloaded 20 million times by developers.

20:58.000 --> 21:02.000
And they need to tell anyone, they just get on with things.

21:02.000 --> 21:06.000
And so one of the interesting things for me is that the positioning is the tooling around this.

21:06.000 --> 21:14.000
Because once you've got those primitives, you can build stuff around just like you've seen loads of community web UIs and other interfaces to interact with stable diffusion.

21:14.000 --> 21:16.000
And for our own company, it's a very simple thing.

21:16.000 --> 21:18.000
This is like a database on steroids.

21:18.000 --> 21:20.000
You're thinking about it.

21:20.000 --> 21:22.000
Like it's a database that comes pre-filled with interesting stuff.

21:22.000 --> 21:24.000
And that's how much people are using it right now.

21:24.000 --> 21:28.000
But soon when we upgrade it a few bits and it comes mature.

21:28.000 --> 21:30.000
It's a database, it's a database, it's a kind of a magic bot.

21:30.000 --> 21:32.000
Bot's database of images and your query is your prompt.

21:32.000 --> 21:34.000
Exactly. It's a data store.

21:34.000 --> 21:36.000
Except for a super efficient data store.

21:36.000 --> 21:38.000
100,000 gigs to two.

21:38.000 --> 21:40.000
And it can do all sorts of wonderful things.

21:40.000 --> 21:42.000
So right now everyone's using like the pre-baked version.

21:42.000 --> 21:44.000
Like the Laura Mipson version, right?

21:44.000 --> 21:46.000
But then in a few years, everyone wants their own custom ones.

21:46.000 --> 21:48.000
So obviously, it's very simple.

21:48.000 --> 21:50.000
Take the exabytes of data from content companies.

21:50.000 --> 21:52.000
Convert them into these models and make them useful.

21:52.000 --> 21:54.000
Because we think content is turning intelligent.

21:54.000 --> 21:58.000
And it goes beyond media companies to bio, farmer, and others.

21:58.000 --> 22:02.000
And we're probably the only foundation model company building cutting edge AI.

22:02.000 --> 22:04.000
That's willing to work with people and go to the data.

22:04.000 --> 22:08.000
So models to the data, I think, are a very interesting thing.

22:08.000 --> 22:12.000
Based on open frameworks, so you don't have to lock in

22:12.000 --> 22:14.000
of some of these other ecosystems.

22:14.000 --> 22:16.000
You'll be like, I'll trade a model for you,

22:16.000 --> 22:18.000
but you have to be locked into my thing.

22:18.000 --> 22:20.000
Yeah. Yeah.

22:20.000 --> 22:22.000
One of the things that you mentioned in passing

22:22.000 --> 22:24.000
is that you've seen the model learn physics.

22:24.000 --> 22:26.000
What does that mean?

22:26.000 --> 22:30.000
So like, if you type in a lady looking across a still lake,

22:30.000 --> 22:34.000
it will do her reflection in the water.

22:34.000 --> 22:36.000
You know, raindrops, it gets correct and things like that.

22:36.000 --> 22:40.000
And as you train it more, it learns more and more concepts

22:40.000 --> 22:44.000
of how things interact, which again is a bit insane.

22:44.000 --> 22:46.000
Like, you can show it the sides.

22:46.000 --> 22:50.000
You can train it on like a experimental call like a cyber truck.

22:50.000 --> 22:52.000
You can see how much effort has gone into

22:52.000 --> 22:56.000
the visualization community trying to get that stuff right.

22:56.000 --> 23:00.000
Exactly. So like, you can show it parts of like a cyber truck.

23:00.000 --> 23:02.000
Yeah.

23:02.000 --> 23:04.000
And it doesn't know a cyber truck, say for instance.

23:04.000 --> 23:06.000
What the back of the cyber truck looks like and it will guess

23:06.000 --> 23:08.000
and it'll probably get it right.

23:08.000 --> 23:10.000
It knows the essence of truckness.

23:10.000 --> 23:12.000
Yeah. So rather than having these very specific models

23:12.000 --> 23:14.000
that learn stuff, you can now have something

23:14.000 --> 23:16.000
that can do just about anything in terms of lighting

23:16.000 --> 23:18.000
and, you know, they've got prompt to prompt

23:18.000 --> 23:20.000
where you can say, make this picture sadder

23:20.000 --> 23:22.000
or, you know, turn them out into a clown or a stormtrooper

23:22.000 --> 23:24.000
and automatically does that.

23:24.000 --> 23:26.000
Because it understands the nature of these things

23:26.000 --> 23:28.000
and the physics and balancing of that,

23:28.000 --> 23:30.000
which again is kind of insane.

23:30.000 --> 23:32.000
This has been implications for the rendering industry

23:32.000 --> 23:34.000
and other things because this is a far more efficient renderer

23:34.000 --> 23:36.000
that can do image-to-image

23:36.000 --> 23:38.000
and transform something into something else.

23:38.000 --> 23:40.000
Nobody's quite sure how it works.

23:40.000 --> 23:42.000
And I've got theories.

23:42.000 --> 23:44.000
And this is one of these things

23:44.000 --> 23:46.000
with these foundation models like

23:46.000 --> 23:48.000
they're just an alien type of experience

23:48.000 --> 23:50.000
when you first really start pushing it.

23:50.000 --> 23:52.000
Most people are surface level when you start pushing through it

23:52.000 --> 23:54.000
you're like, it's really curious that you can do this.

23:54.000 --> 23:56.000
It doesn't have agency.

23:56.000 --> 23:58.000
It's too key what file.

23:58.000 --> 24:00.000
But the fact that you can have that compression

24:00.000 --> 24:02.000
of those concepts

24:02.000 --> 24:04.000
is really interesting.

24:04.000 --> 24:06.000
Would that always be a fundamental limiter

24:06.000 --> 24:10.000
meaning, you know, if you want a quick and dirty approximation,

24:10.000 --> 24:12.000
use something like stable diffusion.

24:12.000 --> 24:14.000
But if you want a precise rendering, you know,

24:14.000 --> 24:16.000
you have to turn to traditional techniques.

24:16.000 --> 24:18.000
I think it's going to be, I always say it's part of a process.

24:18.000 --> 24:19.000
Yeah.

24:19.000 --> 24:21.000
Architecture, you shouldn't try to do zero sure everything.

24:21.000 --> 24:23.000
That's what people tend to pull into a travel.

24:23.000 --> 24:25.000
Like, yeah, I just wanted to know, like, have kind of

24:25.000 --> 24:27.000
K&Ns or knowledge graphs

24:27.000 --> 24:29.000
or retrieval augmented systems or kind of whatever.

24:29.000 --> 24:31.000
Put it as part of a process pipeline.

24:31.000 --> 24:33.000
But definitely a quick and dirty,

24:33.000 --> 24:35.000
it does very, very, very well.

24:35.000 --> 24:37.000
Better than anything.

24:37.000 --> 24:39.000
But then I think that also this way we have our

24:39.000 --> 24:41.000
impainting and all these other models.

24:41.000 --> 24:43.000
It's going to be part of multiple models

24:43.000 --> 24:45.000
doing multiple things for multiple purposes.

24:45.000 --> 24:47.000
Sometimes there might be a giant model

24:47.000 --> 24:49.000
once you get to a certain stage.

24:49.000 --> 24:51.000
Other times, you might just want to have a quick and dirty

24:51.000 --> 24:53.000
256x256 iteration loop.

24:53.000 --> 24:55.000
And so what we've seen as well.

24:55.000 --> 24:57.000
Like, we're stable diffusion too.

24:57.000 --> 24:59.000
We actually flattened the late in space through

24:59.000 --> 25:01.000
DDP and all bunch of other things.

25:01.000 --> 25:03.000
So it's more difficult to prompt.

25:03.000 --> 25:05.000
Stilt diffusion one was quite easy to prompt.

25:05.000 --> 25:07.000
Stilt diffusion two is more difficult,

25:07.000 --> 25:09.000
but it's got much more fine grain control.

25:09.000 --> 25:11.000
But where we're going, we're not going to use prompts.

25:11.000 --> 25:13.000
Because it will learn from...

25:13.000 --> 25:15.000
What do you see taking the place of prompts?

25:15.000 --> 25:17.000
Well, I think it will just be a case of like,

25:17.000 --> 25:19.000
you will have your own embedding store.

25:19.000 --> 25:21.000
That points to points in the late in space

25:21.000 --> 25:23.000
and then pulls up like the things that you like most commonly.

25:23.000 --> 25:25.000
So it learns and then kind of there's that interaction

25:25.000 --> 25:27.000
two things. So, you know, embedding is being

25:27.000 --> 25:31.000
a multiplier representation of kind of what's in there.

25:31.000 --> 25:33.000
So I think that people's own context is important.

25:33.000 --> 25:35.000
And AI models have, we understand people's

25:35.000 --> 25:37.000
AI person context in that.

25:37.000 --> 25:39.000
Or companies or other things.

25:39.000 --> 25:41.000
And again, this is fine tuning effect where you can

25:41.000 --> 25:43.000
with a two gigabyte file.

25:43.000 --> 25:45.000
Actually have your own model.

25:45.000 --> 25:47.000
And then why do you need to prompt training on

25:47.000 --> 25:51.000
art station 3D, obtain render and all these things.

25:51.000 --> 25:53.000
When it learns that that's what you want to have.

25:53.000 --> 25:54.000
That's this type of style that you like.

25:54.000 --> 25:55.000
Right.

25:55.000 --> 25:56.000
Having said that.

25:56.000 --> 25:58.000
I think prompting is just very difficult.

25:58.000 --> 26:00.000
Likewise, we've been trying to prompt me for 16 years

26:00.000 --> 26:02.000
if she has a quite managed.

26:02.000 --> 26:06.000
You've touched on a couple of things.

26:06.000 --> 26:08.000
Open source versus API.

26:08.000 --> 26:12.000
And very briefly.

26:12.000 --> 26:16.000
This idea of kind of customization.

26:16.000 --> 26:20.000
And I think, you know, based on stuff that I've heard you talk

26:20.000 --> 26:22.000
about in the past, like.

26:22.000 --> 26:26.000
You're very strongly opinionated around the.

26:26.000 --> 26:30.000
The model that through which you're kind of delivering the technology.

26:30.000 --> 26:32.000
Beyond just the technology itself.

26:32.000 --> 26:34.000
Can you talk a little bit about.

26:34.000 --> 26:36.000
Your thoughts there and kind of what's driving.

26:36.000 --> 26:38.000
You were driving that.

26:38.000 --> 26:39.000
So I think.

26:39.000 --> 26:41.000
This is incredibly powerful technology.

26:41.000 --> 26:43.000
I think it's one of the big epoch changes in humanity.

26:43.000 --> 26:45.000
Because you have a model that can do anything and approximate.

26:45.000 --> 26:47.000
There's two types of thing in type one and type two.

26:47.000 --> 26:49.000
Logical thinking and then principle based thinking.

26:49.000 --> 26:52.000
This kind of get to principle based thinking.

26:52.000 --> 26:55.000
We still don't have AI that does good fashion reasoning with logic.

26:55.000 --> 26:56.000
This can take leaps.

26:56.000 --> 26:57.000
That's what we said.

26:57.000 --> 26:59.000
Like quick and dirty approximation.

26:59.000 --> 27:00.000
You can do that.

27:00.000 --> 27:03.000
Your type of end you get like a hundred different images of.

27:03.000 --> 27:05.000
Like a book or a vase or something like that.

27:05.000 --> 27:07.000
You can then iterate and improve.

27:07.000 --> 27:09.000
Just very different experience.

27:09.000 --> 27:11.000
So I think was like, again, put this out as foundation models.

27:11.000 --> 27:14.000
Like again, benchmark models that people can then develop around.

27:14.000 --> 27:17.000
Because the pace of invasional outpace anything that's closed.

27:17.000 --> 27:20.000
But also addresses things like the digital divide.

27:20.000 --> 27:22.000
And my route is nothing.

27:22.000 --> 27:25.000
So like with open AI and Dali too.

27:25.000 --> 27:31.000
They introduced the anti bias filter, which automatically for non-gendered word added a gender and a race.

27:31.000 --> 27:35.000
So we type in suma wrestler or do Indian female suma wrestler.

27:35.000 --> 27:36.000
Yeah.

27:36.000 --> 27:38.000
Which I suppose could exist.

27:38.000 --> 27:39.000
Probably not many of them.

27:39.000 --> 27:40.000
That's probably not an intent.

27:40.000 --> 27:41.000
It's kind of limited.

27:41.000 --> 27:43.000
Whereas with our model, what happens we release it.

27:43.000 --> 27:46.000
And then a team manager pan created a Japanese texting code.

27:46.000 --> 27:47.000
That's our alternative.

27:47.000 --> 27:51.000
So salary man rather than meaning man with lots of salary, men and race have man.

27:51.000 --> 27:55.000
In a county's local context, these local elements, these local fine tunes.

27:55.000 --> 27:56.000
I think we're essential.

27:56.000 --> 27:58.000
And also widening the discussion.

27:58.000 --> 28:03.000
Because a lot of the stuff that occurs with these big powerful models is that we won't release them.

28:03.000 --> 28:04.000
Because we're scared about what's going to happen.

28:04.000 --> 28:06.000
Because the no, no, no, no, no, no.

28:06.000 --> 28:07.000
So that's fair.

28:07.000 --> 28:09.000
That's an opinion.

28:09.000 --> 28:13.000
That shouldn't mean that it shouldn't be available to other people because of the power of this technology.

28:13.000 --> 28:16.000
Because otherwise, they'll just go to corporate first and it won't be available.

28:16.000 --> 28:20.000
That's why the fact it could uplift them creatively and communicatively and other things.

28:20.000 --> 28:24.000
One way to think about it, if I'm really mean in some discussion sometimes, is like,

28:24.000 --> 28:27.000
why don't you want Indians or Africans to have this technology?

28:27.000 --> 28:28.000
Because there's no comeback.

28:28.000 --> 28:32.000
You can't say that more education is needed or it's too dangerous and they're not responsible.

28:32.000 --> 28:34.000
Because the reality is this is technology of the humanity.

28:34.000 --> 28:38.000
And there's an echo of what happened with cryptography.

28:38.000 --> 28:44.000
We can't let cryptography be open and the government pacified it as a weapon here in the US.

28:44.000 --> 28:46.000
Because bad guys might use it.

28:46.000 --> 28:49.000
But we use it now to protect ourselves as well.

28:49.000 --> 28:53.000
Open source will always be more secure than close source if the community rounds together.

28:53.000 --> 28:57.000
Because what do we run our infrastructure on here at AWS?

28:57.000 --> 28:58.000
It's not really Windows.

28:58.000 --> 29:00.000
It's Linux.

29:00.000 --> 29:03.000
Our databases are mySQL and things like that.

29:03.000 --> 29:07.000
Because the community can come together and build stronger systems and more effective systems.

29:07.000 --> 29:10.000
But it's crazy how far this is going.

29:10.000 --> 29:13.000
And so it's difficult lines to throw a toe down.

29:13.000 --> 29:14.000
Yeah.

29:14.000 --> 29:19.000
You mentioned that more recent versions of sable diffusion include, like, safe work filters, that kind of thing.

29:19.000 --> 29:26.000
So it sounds like something that you're thinking about and care about and not just putting out without any kind of controls.

29:26.000 --> 29:27.000
Yeah.

29:27.000 --> 29:31.000
So the original version, look, again, it was led by the Confis Lab.

29:31.000 --> 29:35.000
And we said very specifically, you guys get to decide and we will advise.

29:35.000 --> 29:42.000
Because it wasn't academic endeavor, you know, even if the people like one of them works for us and then works for runway and et cetera.

29:42.000 --> 29:44.000
Is the nature of the thing.

29:44.000 --> 29:49.000
And so we're very respectful of kind of entities that we collaborate with because it can be a minefield, right?

29:49.000 --> 29:51.000
You know, trying to whitewash anything.

29:51.000 --> 29:57.000
So it's released under a creative ML open rail license, which is a new type of license from Hungry Face that said you have to use ethically.

29:57.000 --> 30:02.000
Add a safety filter because the decision was made by the developers not to filter the data.

30:02.000 --> 30:06.000
So it could be a baseline from which we could then figure out biases and other things.

30:06.000 --> 30:11.000
And that removed a lot of nudity and kind of other things, especially because it was accidentally creating it.

30:11.000 --> 30:16.000
Sable diffusion too was trained on a highly safe for work data set.

30:16.000 --> 30:17.000
So it's massively more safe.

30:17.000 --> 30:20.000
It doesn't have a filter because of the need one.

30:20.000 --> 30:30.000
It has some drawbacks such as one of the things that we saw during the fine tuning after stele diffusion one is that people trained on not safer work images.

30:30.000 --> 30:33.000
Internet is for that whatever they fine tune it.

30:33.000 --> 30:36.000
They took lots of images that were not safe for work.

30:36.000 --> 30:37.000
Right.

30:37.000 --> 30:41.000
So obviously there was a standard effect of that because again, they're free to kind of use it as a community.

30:41.000 --> 30:47.000
But the side effect is that when you actually used it for safe for work prompts, it did amazing humans.

30:47.000 --> 30:53.000
Like photorealistic because it learned about anatomy from these not safer work images.

30:53.000 --> 30:54.000
It was quite funny.

30:54.000 --> 31:00.000
So stele diffusion two out the box is a bit less good at anatomy because we removed a lot of those things.

31:00.000 --> 31:01.000
Not much.

31:01.000 --> 31:03.000
And again, we're adding it back in safely.

31:03.000 --> 31:04.000
We really care about that.

31:04.000 --> 31:07.000
The other thing that we care about a lot is, you know, we read this community as big.

31:07.000 --> 31:10.000
We're creating millions, hundreds of millions of artists.

31:10.000 --> 31:12.000
So artists are part of the community.

31:12.000 --> 31:15.000
They were asking, can we opt out of the data sets?

31:15.000 --> 31:18.000
Some are actually asking, can we opt in because we're not in the data set.

31:18.000 --> 31:23.000
And so we worked with spawning and lying on others on opt in an opt out mechanisms.

31:23.000 --> 31:25.000
Because I think that's the right thing to do.

31:25.000 --> 31:29.000
Like, I think that it's ethical to use web scrapes to create models like this.

31:29.000 --> 31:34.000
Especially because the diffusion process doesn't create copies or photo mashes.

31:34.000 --> 31:35.000
It actually learns principles.

31:35.000 --> 31:36.000
It's like a human.

31:36.000 --> 31:40.000
For the same time, people don't want to have their data in the data set.

31:40.000 --> 31:41.000
They should opt out.

31:41.000 --> 31:42.000
If they want to end, they should opt in.

31:42.000 --> 31:45.000
In fact, we've had thousands of artists sign up for the system.

31:45.000 --> 31:48.000
It's been 50-50. Opt in an opt out.

31:48.000 --> 31:51.000
I think it's really interesting and not maybe what some people would expect.

31:51.000 --> 31:53.000
Yeah, interesting, interesting.

31:53.000 --> 32:00.000
Maybe shifting gives a little bit to stability as a company as an organization.

32:00.000 --> 32:04.000
I've heard it described as, you know, very, you know, variously an art studio.

32:04.000 --> 32:07.000
Kind of looks and feels a little bit like a research lab.

32:07.000 --> 32:12.000
Feels a little bit like a funder of things, a provider of GPUs and instances.

32:12.000 --> 32:14.000
How do you describe what it is?

32:14.000 --> 32:17.000
I mean, stability AI is a platform company.

32:17.000 --> 32:20.000
So we're trying to build a layer one for foundation model AI.

32:20.000 --> 32:22.000
And we think the future will be open source on this.

32:22.000 --> 32:27.000
So our research lab is, you know, researchers who have loads of freedom.

32:27.000 --> 32:30.000
And they can, in their contracts, open source and they create.

32:30.000 --> 32:33.000
And there's a revenue share for when we run the models on the API.

32:33.000 --> 32:36.000
Even if the researchers don't work at stability, they still get cut checks.

32:36.000 --> 32:38.000
Which thing is very interesting when we're doing things.

32:38.000 --> 32:39.000
Yeah.

32:39.000 --> 32:43.000
We've got a product team that takes the open source stuff, just like anyone can,

32:43.000 --> 32:45.000
and productizes it into things like Dream Studio.

32:45.000 --> 32:49.000
We have Dream Studio Pro coming up, which is a full enterprise level piece of software

32:49.000 --> 32:54.000
with like 3D keyframing, animation, video, audio, everything.

32:54.000 --> 32:57.000
We're going forward to deploy a team whereby for our top customers,

32:57.000 --> 33:00.000
with the most content that we've transformed in foundation models,

33:00.000 --> 33:03.000
we're basically embedding teams inside there and saying,

33:03.000 --> 33:05.000
you don't need to build a foundation model team.

33:05.000 --> 33:09.000
We're your team because we do all the modalities from the text to language to audio.

33:09.000 --> 33:11.000
I can add some of this super appealing to people that we've got infrastructure team

33:11.000 --> 33:15.000
that is supporting our five, six thousand to eight one hundred's

33:15.000 --> 33:21.000
and the infrastructure to scale API's to billions in support with Amazon and others as I am.

33:21.000 --> 33:25.000
Can you talk a little bit about some of the ways that you engage with enterprises?

33:25.000 --> 33:29.000
Like, what are the kinds of things that they want to help doing with these models?

33:29.000 --> 33:32.000
So, the pace of ML research is literally exponential,

33:32.000 --> 33:34.000
with a 23 month wobbling.

33:34.000 --> 33:35.000
It looked crazy.

33:35.000 --> 33:36.000
They can't keep on top of this.

33:36.000 --> 33:39.000
And there's very few people that publish papers on market.

33:39.000 --> 33:40.000
Yeah.

33:40.000 --> 33:41.000
Okay.

33:41.000 --> 33:43.000
It's always nice when you see a casual exponential.

33:43.000 --> 33:46.000
In AI to help with that.

33:46.000 --> 33:47.000
Yeah.

33:47.000 --> 33:48.000
Yeah.

33:48.000 --> 33:51.000
But like, when you look at this,

33:51.000 --> 33:53.000
they're realizing they need to be on top of this technology now.

33:53.000 --> 33:56.000
And they come to us as kind of almost consultants.

33:56.000 --> 33:57.000
They take a parentier type model.

33:57.000 --> 33:58.000
Yeah.

33:58.000 --> 34:00.000
Where we're like, we'll fine tune some models for you

34:00.000 --> 34:02.000
and we'll make them usable through Dream Studio.

34:02.000 --> 34:05.000
But you shouldn't train your own models now.

34:05.000 --> 34:07.000
Because the models aren't going to mature for another year.

34:07.000 --> 34:09.000
When that time comes, we will train the models for you.

34:09.000 --> 34:10.000
We will fine tune them for you.

34:10.000 --> 34:12.000
We will create custom models for you.

34:12.000 --> 34:15.000
That's our highest touch engagement with a couple of dozen entities.

34:15.000 --> 34:18.000
And when you're telling them, they shouldn't train models.

34:18.000 --> 34:20.000
Are you talking about from scratch from scratch?

34:20.000 --> 34:21.000
Sure.

34:21.000 --> 34:22.000
They should.

34:22.000 --> 34:23.000
Okay.

34:23.000 --> 34:24.000
They will be able to eventually.

34:24.000 --> 34:26.000
But right now, it's not a sensible thing to train a model from scratch.

34:26.000 --> 34:27.000
Yeah.

34:27.000 --> 34:31.000
I stable the fusion to 200,000 a 100 hours to like 600k you spent on it?

34:31.000 --> 34:32.000
Yeah, 600k.

34:32.000 --> 34:34.000
Well, we actually spent less because of our discounts.

34:34.000 --> 34:36.000
But I can't say what I discount.

34:36.000 --> 34:37.000
You know what I mean?

34:37.000 --> 34:38.000
You can figure out the retail.

34:38.000 --> 34:39.000
Yeah, retail.

34:39.000 --> 34:42.000
Retail-style diffusion to about 800,000 hours.

34:42.000 --> 34:44.000
Retail-open clip.

34:44.000 --> 34:47.000
Because we had to make the clip model about $5 million.

34:47.000 --> 34:49.000
So, you know, these things add up.

34:49.000 --> 34:50.000
Yeah.

34:50.000 --> 34:51.000
Quite a large bill.

34:51.000 --> 34:53.000
So, I think that when you kind of look at all of these,

34:53.000 --> 34:56.000
now's not the right time to do big trainings for big companies.

34:56.000 --> 35:01.000
Because again, the model architecture is just increasing on ridiculous rate.

35:01.000 --> 35:03.000
But there's going to level off.

35:03.000 --> 35:05.000
You can't keep improving forever.

35:05.000 --> 35:07.000
And then that's the right time to train up your own models.

35:07.000 --> 35:09.000
So, it'll be better than these fine-dune models.

35:09.000 --> 35:11.000
But then you have models with multimodalities.

35:11.000 --> 35:14.000
You know, this is part of the dear reason we've kind of partnered with SageMaker.

35:14.000 --> 35:17.000
Because people need to get used to this technology now.

35:17.000 --> 35:19.000
And they'll have all these different primitives,

35:19.000 --> 35:22.000
these different models they can mix and match to create brand new things going forward.

35:22.000 --> 35:24.000
And SageMaker makes it kind of easy to do that.

35:24.000 --> 35:26.000
And it makes it easy to address the tail.

35:26.000 --> 35:29.000
Because, apart from the top couple of dozen companies,

35:29.000 --> 35:32.000
we just want to have a SaaS solution for everyone else to be able to access,

35:32.000 --> 35:34.000
use and modify these models.

35:34.000 --> 35:38.000
And following up a little bit on the SageMaker and the AWS announcement,

35:38.000 --> 35:42.000
kind of red as, you know, you selected AWS.

35:42.000 --> 35:46.000
From my understanding, you've been using AWS for some extent all along.

35:46.000 --> 35:48.000
Yeah, so AWS built the core cluster.

35:48.000 --> 35:50.000
And now, you know, we reached this point.

35:50.000 --> 35:52.000
So, it was originally a 4,100 cluster,

35:52.000 --> 35:54.000
which on the public top 500 listed,

35:54.000 --> 35:58.000
about number 10, super computer in the world, which is kind of insane.

35:58.000 --> 36:00.000
So, AWS did a great job building that.

36:00.000 --> 36:02.000
But then we have to decide what's next,

36:02.000 --> 36:04.000
like the managing the resilience through some of these other things.

36:04.000 --> 36:06.000
We build our next cluster.

36:06.000 --> 36:08.000
Amazon came and they said,

36:08.000 --> 36:11.000
let's use the SageMaker service to offer a high level of resilience optimization.

36:11.000 --> 36:13.000
So the SageMaker crew, for example,

36:13.000 --> 36:17.000
took our language model, GP Neo X,

36:17.000 --> 36:19.000
again, 20 million downloads of this family.

36:19.000 --> 36:20.000
Yeah.

36:20.000 --> 36:23.000
They went and took the efficiency of a 512-A 100 trading

36:23.000 --> 36:26.000
from 103 telephlops with GPU to 163,

36:26.000 --> 36:29.000
by optimizing it for Amazon, EFA, Fabric,

36:29.000 --> 36:32.000
and pipeline powerism, and cost attention, and kind of all these things.

36:32.000 --> 36:34.000
And that was amazing things.

36:34.000 --> 36:36.000
So they're helping us optimize our entire stack,

36:36.000 --> 36:39.000
from inference to training, through to having resilience.

36:39.000 --> 36:41.000
So when GPUs fail, they come back up.

36:41.000 --> 36:44.000
And the final part of it was just how do you make this accessible?

36:44.000 --> 36:47.000
Through SageMaker and the services,

36:47.000 --> 36:49.000
the ecosystem they built around that.

36:49.000 --> 36:51.000
Now, we're going to make our models available on everything.

36:51.000 --> 36:52.000
Right.

36:52.000 --> 36:56.000
Right? So, like today, they became available on the MacBook M1,

36:56.000 --> 36:59.000
with native neural engine support, one of the first models ever to have that.

36:59.000 --> 37:01.000
You know, that's massive.

37:01.000 --> 37:02.000
We've got it working on Qualcomm.

37:02.000 --> 37:04.000
We're going to work on iPhones, all these things.

37:04.000 --> 37:05.000
But Amazon is a really great partner,

37:05.000 --> 37:07.000
because they're infrastructure players,

37:07.000 --> 37:09.000
one of the biggest cloud drivers in the world.

37:09.000 --> 37:11.000
And so that's why we kind of picked them as our preferred partner.

37:11.000 --> 37:13.000
Also, you know, we're super grateful in that.

37:13.000 --> 37:16.000
We wouldn't be here if they hadn't philosophically an enormous cluster,

37:16.000 --> 37:17.000
and really believed in us.

37:17.000 --> 37:19.000
Because we're only a 13-month-old startup.

37:19.000 --> 37:20.000
Yeah.

37:20.000 --> 37:21.000
So everything's been in the cloud the entire time?

37:21.000 --> 37:22.000
All the entire time.

37:22.000 --> 37:23.000
Yeah.

37:23.000 --> 37:27.000
So we had a machine learning ops team of four people,

37:27.000 --> 37:29.000
managing 4,000 A-100s.

37:29.000 --> 37:30.000
Wow.

37:30.000 --> 37:32.000
Now we're up to nearly 6,000.

37:32.000 --> 37:36.000
Was that team managing that cluster, you know,

37:36.000 --> 37:38.000
kind of bare metal with your own tooling,

37:38.000 --> 37:41.000
or how much of the Amazon tooling have you?

37:41.000 --> 37:43.000
So it was easy, too.

37:43.000 --> 37:46.000
And then the Amazon had a system called parallel cluster,

37:46.000 --> 37:48.000
with Slurm, that was used to kind of manage it.

37:48.000 --> 37:51.000
And so we've been working for the last four, five, six months,

37:51.000 --> 37:52.000
just constantly improving it together.

37:52.000 --> 37:53.000
And again, it's open source.

37:53.000 --> 37:54.000
Yeah.

37:54.000 --> 37:55.000
If you go to the stability AI GitHub,

37:55.000 --> 37:58.000
you can literally download all our configurations

37:58.000 --> 38:00.000
to run your own parallel cluster on that.

38:00.000 --> 38:01.000
Okay.

38:01.000 --> 38:02.000
And again, this is part of what we really like.

38:02.000 --> 38:04.000
The fact that the stack is open source.

38:04.000 --> 38:06.000
And anyone can take it and they can build their own clusters.

38:06.000 --> 38:09.000
Maybe not quite to the size that we did,

38:09.000 --> 38:11.000
unless you're feeling really punchy.

38:11.000 --> 38:12.000
Yeah.

38:12.000 --> 38:14.000
But still, I think these knowledge and these things should be shared.

38:14.000 --> 38:18.000
Because you find that large model training isn't really an art.

38:18.000 --> 38:19.000
Is it really science?

38:19.000 --> 38:20.000
It's more of an art.

38:20.000 --> 38:22.000
Like one of the most interesting reads you can do

38:22.000 --> 38:26.000
is the Facebook OPT175 logbook for the 175 billion parameter model.

38:26.000 --> 38:28.000
They just try stuff and it often fails.

38:28.000 --> 38:30.000
And there's the occasional weird thing,

38:30.000 --> 38:33.000
like really was the Azure kind of customer support

38:33.000 --> 38:35.000
on the 23rd of December.

38:35.000 --> 38:37.000
It deleted the entire cluster.

38:37.000 --> 38:40.000
And you're like, man, I feel for you guys.

38:40.000 --> 38:41.000
It's kind of that.

38:41.000 --> 38:44.000
But like I said, this is not just an easy click and play kind of thing.

38:44.000 --> 38:46.000
These models are difficult to train.

38:46.000 --> 38:47.000
Yeah.

38:47.000 --> 38:49.000
The smallest hardware thing can throw it out.

38:49.000 --> 38:50.000
They can be just weird stuff.

38:50.000 --> 38:53.000
We're making it up and figuring out as we go along.

38:53.000 --> 38:55.000
Because remember, transformer architectures

38:55.000 --> 38:57.000
are literally only five years old.

38:57.000 --> 38:58.000
Yeah.

38:58.000 --> 39:02.000
It's thinking about open source and that direction broadly

39:02.000 --> 39:03.000
that the company is taking.

39:03.000 --> 39:08.000
One of the challenges that comes up in open source

39:08.000 --> 39:11.000
as it matures is this idea of governance.

39:11.000 --> 39:14.000
How do you think about, maybe it's early,

39:14.000 --> 39:17.000
talking about governing a community that's just months old.

39:17.000 --> 39:21.000
But do you have thoughts on how the community,

39:21.000 --> 39:23.000
you know, governs itself over time?

39:23.000 --> 39:24.000
Yeah.

39:24.000 --> 39:25.000
So again, it's complicated one, right?

39:25.000 --> 39:26.000
AI governance.

39:26.000 --> 39:27.000
And does it policy led?

39:27.000 --> 39:28.000
Is it community led?

39:28.000 --> 39:30.000
Who are the voices at the table?

39:30.000 --> 39:31.000
Because there's some important things.

39:31.000 --> 39:32.000
This is such powerful technology.

39:32.000 --> 39:35.000
It's going to be essential, I believe, to the future of humanity.

39:35.000 --> 39:37.000
So like, for example, Luther AI is two and a bit years old.

39:37.000 --> 39:39.000
That's our language model community.

39:39.000 --> 39:42.000
15,000 people and 12 of us.

39:42.000 --> 39:44.000
We're kind of incubating at the moment.

39:44.000 --> 39:47.000
We're going to spin it out into its own separate 5 and 1 C3.

39:47.000 --> 39:49.000
Because it shouldn't be else influencing the direction

39:49.000 --> 39:51.000
of open source large language models, right?

39:51.000 --> 39:53.000
It should be a collective effort.

39:53.000 --> 39:55.000
But now we're really going through the governance thing

39:55.000 --> 39:56.000
and looking at different examples.

39:56.000 --> 39:58.000
And the Linux Foundation is an excellent example of that.

39:58.000 --> 40:01.000
So PyTorch has just been given to the Linux Foundation.

40:01.000 --> 40:04.000
And so Rin talks with them, a whole bunch of others to say,

40:04.000 --> 40:05.000
what are best practices here?

40:05.000 --> 40:07.000
And what should really look like given the power

40:07.000 --> 40:09.000
of these, some of the decisions need to make about that?

40:09.000 --> 40:12.000
As stability itself, we're setting up a subsidiaries in every country

40:12.000 --> 40:15.000
such that, first off, a temp set of equity in those

40:15.000 --> 40:17.000
goes to the kids using our tablets.

40:17.000 --> 40:19.000
Because I think they should influence it.

40:19.000 --> 40:20.000
Because that's the next generation.

40:20.000 --> 40:22.000
And this AI will be important to them.

40:22.000 --> 40:24.000
But then we want this to be independent entities that run the AI

40:24.000 --> 40:29.000
for India or Vietnam or kind of Malawi, et cetera.

40:29.000 --> 40:31.000
Because we need to train up a next generation of people

40:31.000 --> 40:33.000
to make those decisions for their own country.

40:33.000 --> 40:35.000
Because right now, what we have is a situation

40:35.000 --> 40:38.000
where you've got a few people in San Francisco

40:38.000 --> 40:40.000
making decisions in the most powerful infrastructure

40:40.000 --> 40:42.000
of the world for everyone.

40:42.000 --> 40:44.000
Because that's not to deny ourselves.

40:44.000 --> 40:46.000
This AI is infrastructure.

40:46.000 --> 40:48.000
It's essential for where we're going to go.

40:48.000 --> 40:51.000
And it shouldn't be controlled by any person or entity.

40:51.000 --> 40:53.000
Like, I'm very supportive of the whole ecosystem.

40:53.000 --> 40:56.000
The one time I, by almost the very direct.

40:56.000 --> 40:58.000
So I spoke out against open AI.

40:58.000 --> 41:01.000
Because for Dali too, they banned Ukrainians from using it.

41:01.000 --> 41:04.000
They removed any Ukrainian entities from that as well.

41:04.000 --> 41:08.000
And this is doing the time when they're being oppressed.

41:08.000 --> 41:12.000
I said, basically, you have excluded and removed and deleted

41:12.000 --> 41:13.000
and oppressed people.

41:13.000 --> 41:15.000
And that is ethically and morally wrong.

41:15.000 --> 41:18.000
But is their prerogative as a private company?

41:18.000 --> 41:21.000
And if it wasn't for us, there would be no alternative.

41:21.000 --> 41:24.000
And so I literally took Ukrainian developers,

41:24.000 --> 41:27.000
and houses were destroyed, and brought them to the UK.

41:27.000 --> 41:29.000
And so this is part of the thing as well.

41:29.000 --> 41:31.000
If you have control of this artificial intelligence

41:31.000 --> 41:34.000
given to an unregulated entity like these big companies,

41:34.000 --> 41:37.000
they can't help themselves but behave in certain ways

41:37.000 --> 41:38.000
and they can't release it.

41:38.000 --> 41:40.000
More than that, they tend to optimize.

41:40.000 --> 41:42.000
So I did a lot of counter-extremism work,

41:42.000 --> 41:43.000
advising multiple governments.

41:43.000 --> 41:45.000
The YouTube algorithm got hijacked by extremists

41:45.000 --> 41:47.000
because the most engaging content was extreme.

41:47.000 --> 41:49.000
Again, that's not YouTube's fault.

41:49.000 --> 41:51.000
That's full of great people.

41:51.000 --> 41:53.000
Add driven AI companies.

41:53.000 --> 41:55.000
They will use this technology to create the most amazing

41:55.000 --> 41:56.000
manipulative ads.

41:56.000 --> 41:58.000
Against this not their fault, it's kind of what they are.

41:58.000 --> 42:00.000
So regulation needs to come in appropriately.

42:00.000 --> 42:01.000
Governance needs to come in appropriately.

42:01.000 --> 42:04.000
But we need to educate and widen the discussion on this.

42:04.000 --> 42:06.000
And the only way to do that is open source.

42:06.000 --> 42:08.000
Otherwise, it will never happen.

42:08.000 --> 42:11.000
And so you will have AI basically being a colonial tool

42:11.000 --> 42:14.000
in some ways with very Western norms.

42:14.000 --> 42:16.000
When this is essential infrastructure,

42:16.000 --> 42:18.000
like I said, I believe for everyone.

42:18.000 --> 42:22.000
And I think the common retort to that

42:22.000 --> 42:25.000
is it needs to be controlled because it's so powerful,

42:25.000 --> 42:26.000
so dangerous.

42:26.000 --> 42:28.000
Yeah, so who are you to control it?

42:28.000 --> 42:29.000
I mean, this is a thing.

42:29.000 --> 42:31.000
Like, I've had it, it likes into a nuclear weapon.

42:31.000 --> 42:34.000
I'm like, it's a nuclear weapon that can allow humans

42:34.000 --> 42:36.000
to create visually.

42:36.000 --> 42:37.000
And so you're restricting it.

42:37.000 --> 42:38.000
And when you get, come and answer a question.

42:38.000 --> 42:40.000
Like, I've asked this, I've never had a question.

42:40.000 --> 42:43.000
Why don't you want Indians to have this for Africans?

42:43.000 --> 42:45.000
And the only answer is, because they need more education,

42:45.000 --> 42:47.000
so educate them more.

42:47.000 --> 42:49.000
Because they can't use it responsibly,

42:49.000 --> 42:51.000
and you can, it's racist.

42:51.000 --> 42:54.000
Like I think fundamentally, if you think about digital divide,

42:54.000 --> 42:56.000
we've seen this with technology being restricted

42:56.000 --> 42:59.000
from minority groups and from the rest of the world frequently.

42:59.000 --> 43:01.000
It's fundamentally racist because we think we know better

43:01.000 --> 43:02.000
in the West.

43:02.000 --> 43:04.000
When it's reality, we don't, because people can take this

43:04.000 --> 43:05.000
and extend it.

43:05.000 --> 43:07.000
And people are generally good.

43:07.000 --> 43:09.000
People are not bad, and if people are bad,

43:09.000 --> 43:11.000
as a society, we build systems to regulate that.

43:11.000 --> 43:13.000
So even if they create deep fakes,

43:13.000 --> 43:16.000
we build our social networks and those type of curation mechanisms.

43:16.000 --> 43:19.000
You know, we build authenticity schemes like content

43:19.000 --> 43:21.000
or authenticity.org that we back.

43:21.000 --> 43:23.000
That sounds like the core of your answer

43:23.000 --> 43:25.000
is that the ecosystem will solve the problem.

43:25.000 --> 43:26.000
The bad actors come in.

43:26.000 --> 43:29.000
They use these tools to cause whatever

43:29.000 --> 43:32.000
having their cause, and then, you know, we'll find faces.

43:32.000 --> 43:34.000
The bad actors have the tools already.

43:34.000 --> 43:37.000
They have tens of thousands of A-100s and people.

43:37.000 --> 43:39.000
I mean, since you're the proof point of this, right?

43:39.000 --> 43:42.000
Open AI was keeping Dolly closed behind, you know,

43:42.000 --> 43:44.000
APIs and wait lists and things like that.

43:44.000 --> 43:46.000
And, you know, you came up, I don't know where,

43:46.000 --> 43:48.000
and released something.

43:48.000 --> 43:50.000
And look, 4chan has had this technology for three months.

43:50.000 --> 43:52.000
What if they created nothing, right?

43:52.000 --> 43:53.000
Yeah.

43:53.000 --> 43:55.000
You know, like this isn't going to topple humanity,

43:55.000 --> 43:56.000
and have more and more people know about it,

43:56.000 --> 43:57.000
so we can bring this discussion.

43:57.000 --> 43:59.000
You know, we took a lot of flak.

43:59.000 --> 44:00.000
We had a lot of benefits.

44:00.000 --> 44:01.000
Yeah.

44:01.000 --> 44:02.000
We brought this discussion into the open,

44:02.000 --> 44:04.000
into policy and other fields as well.

44:04.000 --> 44:06.000
Like, again, it's my hope that now,

44:06.000 --> 44:07.000
we access a forcing function.

44:07.000 --> 44:09.000
So I reckon Dolly III will be open sourced.

44:09.000 --> 44:11.000
You know, it's like the open source whisper.

44:11.000 --> 44:12.000
And I think this will be fantastic.

44:12.000 --> 44:13.000
Let's bring it out into the open,

44:13.000 --> 44:15.000
because, again, this is foundational infrastructure

44:15.000 --> 44:17.000
for extending our abilities.

44:17.000 --> 44:18.000
It should not be closed.

44:18.000 --> 44:19.000
Yeah.

44:19.000 --> 44:22.000
But I don't believe that it should be free forever,

44:22.000 --> 44:25.000
and, like, even, actually, it's not open source,

44:25.000 --> 44:28.000
because it doesn't confirm with rule zero of open source

44:28.000 --> 44:29.000
in a pure open source way.

44:29.000 --> 44:31.000
The creative analysis is not,

44:31.000 --> 44:33.000
because we say you must use it ethically.

44:33.000 --> 44:36.000
You know, do we have to move it to open source?

44:36.000 --> 44:37.000
Yes.

44:37.000 --> 44:39.000
Under CC by a five, or MIT license,

44:39.000 --> 44:40.000
just like our other models,

44:40.000 --> 44:42.000
like our Korean language model,

44:42.000 --> 44:43.000
the polygote one from the Luther

44:43.000 --> 44:45.000
or open-clapal things like that.

44:45.000 --> 44:46.000
Yeah.

44:46.000 --> 44:47.000
But, again, this needs to be an open discussion, I think,

44:47.000 --> 44:49.000
rather than who is deciding it.

44:49.000 --> 44:50.000
I don't know.

44:50.000 --> 44:51.000
Right.

44:51.000 --> 44:52.000
If regulators want to come and regulate it,

44:52.000 --> 44:54.000
again, that's a democratic decision.

44:54.000 --> 44:56.000
And so I'm a big supporter of democracy

44:56.000 --> 44:57.000
and, kind of, these things.

44:57.000 --> 44:59.000
But let's use our institutions on processes

44:59.000 --> 45:01.000
rather than trying to make these decisions ourselves

45:01.000 --> 45:02.000
in closed rooms.

45:02.000 --> 45:03.000
Mm-hmm.

45:03.000 --> 45:04.000
Awesome.

45:04.000 --> 45:05.000
Awesome.

45:05.000 --> 45:07.000
Well, Emma, thanks so much for taking the time to chat.

45:07.000 --> 45:08.000
It's been wonderful.

45:08.000 --> 45:10.000
Speaking with you, I'm learning a bit more about what you have to.

45:10.000 --> 45:12.000
It's a pleasure, and I hope you have a seat as well.

45:12.000 --> 45:13.000
Nearly done.

45:13.000 --> 45:14.000
Nearly done.

45:14.000 --> 45:15.000
Thanks so much.

45:15.000 --> 45:27.000
Take care.

