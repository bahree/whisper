WEBVTT

00:00.000 --> 00:04.800
Hi, it's Sam, a quick story before we get going.

00:04.800 --> 00:10.480
A few weeks ago at the Twomel AI Summit, I met a listener, Nicholas, who works for a leading

00:10.480 --> 00:12.200
energy company.

00:12.200 --> 00:16.160
We spent a bit of time chatting, and he mentioned a few of the things he'd learned about on

00:16.160 --> 00:20.360
the podcast that he went ahead and implemented at his company.

00:20.360 --> 00:24.720
He was particularly excited about the semantic folding technology he learned about on Twomel

00:24.720 --> 00:26.280
Talk Number 10.

00:26.280 --> 00:32.600
Then, a couple of weeks later, I spoke with Alex, who works at a leading German career

00:32.600 --> 00:33.600
site.

00:33.600 --> 00:38.200
Alex mentioned that he'd recently kicked off a project there to explore applying zero-shop

00:38.200 --> 00:39.600
machine learning.

00:39.600 --> 00:42.880
Once again, an idea he learned about on the podcast.

00:42.880 --> 00:45.840
I'd really like to hear more stories like this.

00:45.840 --> 00:50.640
If the podcast has helped you at work or school, if it's educated or connected to you

00:50.640 --> 00:55.320
to resources or techniques you've found useful or valuable, please take a moment to share

00:55.320 --> 01:02.000
them over at twomelai.com slash 2av, the number 2av.

01:02.000 --> 01:05.800
The occasion, of course, is the podcast's second birthday.

01:05.800 --> 01:08.640
So go ahead, give it this birthday present.

01:08.640 --> 01:13.160
Okay, Nick, hit it.

01:13.160 --> 01:24.720
Hello, and welcome to another episode of Twomel Talk.

01:24.720 --> 01:29.480
The podcast why interview interesting people, doing interesting things in machine learning

01:29.480 --> 01:31.800
and artificial intelligence.

01:31.800 --> 01:42.200
I'm your host, Sam Charrington.

01:42.200 --> 01:47.560
In this episode of our train AI series, I sit down with Anima Anon Kumar, brand professor

01:47.560 --> 01:52.920
at Caltech and principal scientist with Amazon Web Services to discuss the research she's

01:52.920 --> 01:56.000
doing out of her tensor lab.

01:56.000 --> 02:00.680
In our conversation, we review the application of tensor operations to machine learning and

02:00.680 --> 02:06.280
discuss how an example problem, document categorization, might be approached using three-dimensional

02:06.280 --> 02:10.400
tensors to discover topics and relationships between them.

02:10.400 --> 02:15.240
We touch on multi-dimensionality, expectation maximization, and the Amazon products Sage

02:15.240 --> 02:17.240
Maker and Comprehend.

02:17.240 --> 02:22.080
Anima also goes into how to tensorize neural networks and apply our understanding of tensor

02:22.080 --> 02:27.240
algebra to perform better architecture searches.

02:27.240 --> 02:31.040
Before we start, though, a shout out to our friends over at Figure 8 for their sponsorship

02:31.040 --> 02:33.160
of this week's series.

02:33.160 --> 02:37.720
Figure 8 is the essential human and aloop AI platform for data science and machine learning

02:37.720 --> 02:39.000
teams.

02:39.000 --> 02:43.600
The Figure 8 software platform trains, tests, and tunes machine learning models to make

02:43.600 --> 02:45.920
AI work in the real world.

02:45.920 --> 02:51.280
Learn more at www.figure-8.com.

02:51.280 --> 02:57.240
Just a reminder, this episode was recorded on site at Train AI, so there is some unavoidable

02:57.240 --> 02:59.040
background noise.

02:59.040 --> 03:01.920
And now on to the show.

03:01.920 --> 03:04.880
All right, everyone.

03:04.880 --> 03:08.680
I am pleased to be seated here with Anima Anankumar.

03:08.680 --> 03:15.320
Anima is Bren Professor and Microsoft and Sloan Fellow in the Department of Computing

03:15.320 --> 03:20.760
and Mathematical Sciences at Caltech, as well as a principal scientist with Amazon Web

03:20.760 --> 03:21.760
Services.

03:21.760 --> 03:25.080
Anima, welcome to this weekend machine learning and AI.

03:25.080 --> 03:26.080
Thank you, Sam.

03:26.080 --> 03:27.600
It's a pleasure to be here.

03:27.600 --> 03:28.600
Yeah.

03:28.600 --> 03:31.360
It's absolutely a pleasure to finally get an interview going.

03:31.360 --> 03:35.480
It's not been that long since I saw you last, scaled ML, I think.

03:35.480 --> 03:36.480
Stanford.

03:36.480 --> 03:37.480
That's right.

03:37.480 --> 03:43.320
I mean, the scaled ML was a small and an intimate event, but just so many amazing speakers

03:43.320 --> 03:47.600
and the audience was also really cool to interact with.

03:47.600 --> 03:48.600
Absolutely.

03:48.600 --> 03:49.600
Absolutely.

03:49.600 --> 03:55.080
Can you tell us a little bit about your background and how you got interested and started down the

03:55.080 --> 03:57.040
path of ML and AI?

03:57.040 --> 03:58.040
Yeah.

03:58.040 --> 04:03.400
I guess you could say, like, even as a child, I was always interested in math, you know,

04:03.400 --> 04:09.080
like to me, like kind of just the aspect of understanding a lot of structure and explaining

04:09.080 --> 04:13.960
the world through math seem magical.

04:13.960 --> 04:18.600
And then through the years, you know, I did engineering and in the beginning of my PhD,

04:18.600 --> 04:27.240
I wanted to ask questions on how we can use mathematical techniques to understand data

04:27.240 --> 04:33.280
better, to process them better, scale that up and run them in real systems.

04:33.280 --> 04:38.160
Of course, my beginning was humble, you know, I was looking at purely the theoretical aspects

04:38.160 --> 04:41.600
I was asking questions and sensor networks.

04:41.600 --> 04:43.800
Now you might call them Internet of Things.

04:43.800 --> 04:44.800
IoT.

04:44.800 --> 04:45.800
Right.

04:45.800 --> 04:49.400
It didn't exist, which dates me.

04:49.400 --> 04:53.000
But the questions were still very relevant today.

04:53.000 --> 04:56.960
How much of learning do you do on the edge, on the devices?

04:56.960 --> 04:58.360
How do you transmit this?

04:58.360 --> 05:00.440
What about bandwidth constraints?

05:00.440 --> 05:01.520
How do you route them?

05:01.520 --> 05:06.040
How do you exploit the correlations of measurements between different sensors?

05:06.040 --> 05:09.400
So these were the questions that I started tackling.

05:09.400 --> 05:13.520
And to me, like solving these math problems was very fascinating.

05:13.520 --> 05:17.720
But I wanted to also get them working on real systems, right?

05:17.720 --> 05:22.240
So because back then, we didn't have the IoT revolution going at, I decided to switch

05:22.240 --> 05:26.160
to settings where I could just purely work with data.

05:26.160 --> 05:28.560
So I suppose we have all the data in one place.

05:28.560 --> 05:33.960
Can we do something interesting and can I try this out in different scenarios?

05:33.960 --> 05:38.920
And that's when I started working on probabilistic graphical models, understanding relationships

05:38.920 --> 05:44.680
between variables, how we can discover those graphical relationships at scale.

05:44.680 --> 05:49.160
And what techniques would allow us to provide guarantees?

05:49.160 --> 05:53.560
Like, when do we successfully discover them?

05:53.560 --> 05:59.920
And understanding really, when we can discover information about hidden variables.

05:59.920 --> 06:02.720
So as you can imagine, every model is wrong, right?

06:02.720 --> 06:08.880
So there's only better models which can approximate the real world.

06:08.880 --> 06:12.040
In some ways, you know, better than others.

06:12.040 --> 06:18.960
But the question is whether having hidden variables will allow us to have a more realistic model.

06:18.960 --> 06:23.160
Because so many times we cannot measure everything about the real world.

06:23.160 --> 06:28.120
So the ones that are hidden from the measurements can be incorporated them into the model

06:28.120 --> 06:30.040
and how do we discover them.

06:30.040 --> 06:35.520
As an example, you know, think about categorizing documents, right?

06:35.520 --> 06:37.960
So there's a whole bunch of news articles.

06:37.960 --> 06:42.920
But suppose there's no human annotation of what the documents are talking about,

06:42.920 --> 06:44.280
then that's a hidden variable.

06:44.280 --> 06:47.880
We don't know those categories for the documents.

06:47.880 --> 06:51.320
So suppose we can incorporate them as hidden variables

06:51.320 --> 06:54.920
and use the text as absurd variables.

06:54.920 --> 06:58.520
Can we now learn about these relationships from data?

06:58.520 --> 07:01.560
Even when the variables are not directly absurd?

07:01.560 --> 07:02.480
Okay, great.

07:02.480 --> 07:06.280
And so your lab at Caltech is called the tensor lab.

07:06.280 --> 07:11.000
And you spend a lot of your time focusing on researching tensors

07:11.000 --> 07:14.920
and tensor implementations of machine learning algorithms and things like that.

07:14.920 --> 07:20.240
Can you maybe walk us through that line of research and what it's all about

07:20.240 --> 07:25.240
and tie it back to this problem that you started with?

07:25.240 --> 07:26.240
Absolutely.

07:26.240 --> 07:28.400
I mean, it's been a very interesting journey.

07:28.400 --> 07:35.160
It's taken me about six years now to see the tensor field mature more, right?

07:35.160 --> 07:37.640
So I do want to add a caveat.

07:37.640 --> 07:41.800
You know, the research on tensor algebra is very different from tensor flow.

07:41.800 --> 07:45.640
You know, there is always this one person in the audience

07:45.640 --> 07:49.120
who will ask me about tensor flow when I give the talk on tensor.

07:49.120 --> 07:52.800
The common point there is the phrase tensor, which means what?

07:52.800 --> 07:56.720
Yeah, so tensor, you know, as a naive definition, right,

07:56.720 --> 07:58.720
is a multi-dimensional array.

07:58.720 --> 08:01.800
So now you don't just have two dimensions.

08:01.800 --> 08:06.760
It's not just rows and columns, you have many more dimensions in your array

08:06.760 --> 08:08.760
that can be thought of a tensor.

08:08.760 --> 08:13.800
But then there is a deeper algebraic meaning that I think many people

08:13.800 --> 08:16.160
who are not in the field may not be aware, right?

08:16.160 --> 08:19.880
So just as we treat the matrix not just as rows or columns,

08:19.880 --> 08:22.960
there's a whole host of linear algebraic techniques.

08:22.960 --> 08:25.760
So matrix really can be thought of as an operator.

08:25.760 --> 08:29.080
You're operating matrix on a field, you know,

08:29.080 --> 08:32.320
you're using that to change your vector, right,

08:32.320 --> 08:33.320
when you multiply a vector.

08:33.320 --> 08:34.600
The transformation of some sort.

08:34.600 --> 08:36.280
Exactly, exactly.

08:36.280 --> 08:40.960
And so tensors could be thought of as a richer class of such transformations.

08:40.960 --> 08:43.840
And so in that sense, the tensor flow term is apt

08:43.840 --> 08:47.880
because you have an input data, which can be thought of as a tensor.

08:47.880 --> 08:50.920
And there's an output that's also a tensor, right?

08:50.920 --> 08:54.200
I mean, if it's just a vector, that's a special case of tensor.

08:54.200 --> 08:57.120
So tensor flow is one that kind of flows through the network

08:57.120 --> 09:00.360
as a series of tensor transformations.

09:00.360 --> 09:03.000
But the shortcoming in most of the current networks

09:03.000 --> 09:06.480
is we are still using linear algebraic computations

09:06.480 --> 09:08.040
in different layers.

09:08.040 --> 09:09.840
Whether it's a fully connected layer

09:09.840 --> 09:12.440
where we are doing a matrix multiplication,

09:12.440 --> 09:14.160
it could be a convolutional layer

09:14.160 --> 09:16.440
where we have these set of filters.

09:16.440 --> 09:21.560
So can we now expand these set of operations to more dimensions?

09:21.560 --> 09:25.480
To give you an example, you know, can we now in a layer

09:25.480 --> 09:28.200
instead of doing just a matrix multiplication?

09:28.200 --> 09:31.640
Can we directly operate on higher dimensional tensors?

09:31.640 --> 09:35.960
If the input to a layer is now a three-dimensional set of activations

09:35.960 --> 09:41.880
from the previous layer, can we now set up a computation

09:41.880 --> 09:45.360
that doesn't just turn into a matrix multiplication?

09:45.360 --> 09:48.280
That directly operates on all the three dimensions.

09:48.280 --> 09:53.400
And so can exploit the information of different dimensions more effectively.

09:53.400 --> 09:56.800
Is it just me or as part of the challenge with this,

09:56.800 --> 10:04.880
the general trouble we have visualizing higher dimensions than three?

10:04.880 --> 10:07.880
Yeah, no, it's interesting that you ask.

10:07.880 --> 10:10.560
Actually, people, you know, this is not new, right?

10:10.560 --> 10:12.840
Like, thinking in high dimensions.

10:12.840 --> 10:17.360
There is a book I discovered a few years ago called Flatland from...

10:17.360 --> 10:18.240
Edwin Abbott.

10:18.240 --> 10:20.040
Yeah, exactly.

10:20.040 --> 10:21.720
Right? So it's been in arts.

10:21.720 --> 10:24.840
It's been in literature, the general theory of relativity

10:24.840 --> 10:27.120
thinks about many dimensions, right?

10:27.120 --> 10:34.000
So to tell the audience about what this book is about,

10:34.000 --> 10:38.960
the part that I found it fascinating is this is set up in a two-dimensional world.

10:38.960 --> 10:43.520
Like every person is a two-dimensional object like a square or a polygon.

10:43.520 --> 10:48.200
And then there's a three-dimensional monster that visits this world, right?

10:48.200 --> 10:52.400
So as the three-dimensional object is moving through the two-dimensional world,

10:52.400 --> 10:54.560
it's rapidly changing shapes.

10:54.560 --> 10:58.200
And so that's very scary to the two-dimensional people.

10:58.200 --> 11:00.800
So I would use this analogy that many people,

11:00.800 --> 11:02.560
when they cannot visualize something,

11:02.560 --> 11:07.320
they feel that's way too abstract or that's confusing.

11:07.320 --> 11:11.440
But then math is much richer than just the three dimensions that we live in.

11:11.440 --> 11:13.920
Or the four-dimension, if you use time, right?

11:13.920 --> 11:20.160
So they can formulate and solve problems in infinite dimensions.

11:20.160 --> 11:23.120
And that form of thinking and that form of algorithms

11:23.120 --> 11:27.600
will help us process all the multimodal data we are obtaining today.

11:27.600 --> 11:30.800
So it's not just visual data, it could be textual data.

11:30.800 --> 11:33.000
There's, again, to other domain knowledge.

11:33.000 --> 11:37.200
So how do we use all this in an integrated approach?

11:37.200 --> 11:40.200
And that's where tensors can have a big impact.

11:40.200 --> 11:44.160
And so maybe give us some concrete examples.

11:44.160 --> 11:49.760
I mean, one of the things that strikes me is that when we're talking about sensor data,

11:49.760 --> 11:54.640
we're talking about sensor data that, and this is perhaps maybe naive,

11:54.640 --> 11:59.840
but we're talking about sensor data that comes from an inherently three-dimensional world.

11:59.840 --> 12:04.640
I guess if you start layering on time and maybe channels and things like that,

12:04.640 --> 12:06.760
it gets more complex.

12:06.760 --> 12:10.120
Is that where the multi-dimensionality comes from?

12:10.120 --> 12:12.480
You know, when we're extracting fundamentally,

12:12.480 --> 12:15.040
we're extracting things from our real world.

12:15.040 --> 12:21.200
It's interesting that you ask, because indeed, one direct way of using tensors

12:21.200 --> 12:24.240
is when the data itself has many dimensions, right?

12:24.240 --> 12:30.480
But another more nuanced way of using it is to model the relationships between data.

12:30.480 --> 12:35.000
So let me go back to the example of document categorization.

12:35.000 --> 12:39.200
So I talked about how do we automatically discover

12:39.200 --> 12:43.400
the categories or topics in documents when there are no labels available, right?

12:43.400 --> 12:46.440
So unsupervised learning problems tend to be really hard,

12:46.440 --> 12:50.640
because we don't have a human label saying, example saying,

12:50.640 --> 12:53.600
where, what are the topics?

12:53.600 --> 12:57.880
So in this scenario, what you can do is intuitively look at frequency of words,

12:57.880 --> 13:01.280
right? This bag of words is the simplest technique that's out there.

13:01.280 --> 13:02.840
So you can just count words.

13:02.840 --> 13:06.360
So let's say the word apple, it appears a lot in the document.

13:06.360 --> 13:09.880
I mean, this by itself is not informative because it could be the fruit

13:09.880 --> 13:12.000
or it could be the company, right?

13:12.000 --> 13:14.520
So now you can say, okay, let me not just count one word,

13:14.520 --> 13:18.920
let me count occurrences, pairwise occurrences of words.

13:18.920 --> 13:22.680
So what if the word apple and orange occurs together, right?

13:22.680 --> 13:25.480
So even in this case, maybe it's likely to be fruit,

13:25.480 --> 13:27.840
but orange is also a company, right?

13:27.840 --> 13:29.400
So now what about three words?

13:29.400 --> 13:32.400
If I count like occurrences of triplets of words,

13:32.400 --> 13:36.760
if it's apple, orange, banana, then likely it's a fruit, right?

13:36.760 --> 13:40.440
But it's humanly impossible to go label every triplets of categories

13:40.440 --> 13:43.200
and say what topics they should belong to, right?

13:43.200 --> 13:47.160
So can there be algorithms that can automatically discover at scale

13:47.160 --> 13:49.840
these higher order relationships?

13:49.840 --> 13:53.680
Right, so if you think about pairwise relationships, they are correlations.

13:53.680 --> 13:55.840
So you can represent that as a matrix.

13:55.840 --> 13:58.800
So you list all the words as rows and columns,

13:58.800 --> 14:02.840
which count the core occurrences that can be represented as a matrix.

14:02.840 --> 14:03.840
But if you have to do that-

14:03.840 --> 14:08.440
So N-Y's relationships correlates to end-dimensional tensors.

14:08.440 --> 14:09.600
Precisely.

14:09.600 --> 14:13.640
And now the question is, what class of operations can you do on them

14:13.640 --> 14:18.240
to discover these hidden variables to discover the topics in documents?

14:18.240 --> 14:23.320
And what I showed in one of my first papers is how we can decompose

14:23.320 --> 14:26.280
a certain tensor, a three-dimensional tensor,

14:26.280 --> 14:29.720
and discover the topics and discover the relationship

14:29.720 --> 14:33.360
between those topics and the words in the documents.

14:33.360 --> 14:33.960
Interesting.

14:33.960 --> 14:42.720
And so prior to this work or prior to a tensor-oriented approach,

14:42.720 --> 14:45.440
we would first go through the step of mapping this all

14:45.440 --> 14:50.640
to linear algebra and matrix operations and things like that.

14:50.640 --> 14:54.600
And it sounds like the argument is that that's inefficient.

14:54.600 --> 15:01.120
Is it an inefficiency or are we missing out on key tools in doing that?

15:01.120 --> 15:03.000
So there is a bit of everything.

15:03.000 --> 15:07.640
So for this specific, as always, the problem is complicated.

15:07.640 --> 15:14.800
For this specific topic of modeling and other latent variable estimation,

15:14.800 --> 15:16.440
there were two sets of approaches.

15:16.440 --> 15:18.960
One is expectation maximization.

15:18.960 --> 15:21.040
So you have the likelihood function.

15:21.040 --> 15:25.160
So you do a sort of local search method and hope

15:25.160 --> 15:27.720
to reach globally optimal solution.

15:27.720 --> 15:30.160
But many times, it gets stuck, especially

15:30.160 --> 15:32.000
for high-dimensional problems.

15:32.000 --> 15:34.360
And it's also slow.

15:34.360 --> 15:36.960
And the other approach, these matrix approaches,

15:36.960 --> 15:41.000
tend to do simple linear dimensionality reduction techniques.

15:41.000 --> 15:44.480
The principle component analysis is the most common one.

15:44.480 --> 15:48.760
So if you try to do a matrix approach or a linear algebra approach

15:48.760 --> 15:53.160
for topic modeling, you would only discover the subspace

15:53.160 --> 15:56.360
of the topic vectors.

15:56.360 --> 15:59.120
So you won't discover the actual modeling.

15:59.120 --> 16:04.400
And also, there is the class of spectral clustering

16:04.400 --> 16:07.280
where if the documents have separated topics,

16:07.280 --> 16:09.600
you could try to do a clustering approach.

16:09.600 --> 16:11.600
But in many cases, they are not.

16:11.600 --> 16:15.760
There are news articles talking both about science and politics and so on.

16:15.760 --> 16:21.240
So there is no separation of documents into distinct categories.

16:21.240 --> 16:24.280
And in those problems, there is really

16:24.280 --> 16:29.000
no other approach other than expectation maximization

16:29.000 --> 16:31.920
that people have used in practice before.

16:31.920 --> 16:34.760
And what we found is in our experiments

16:34.760 --> 16:38.320
that these tensor methods can be scaled up very efficiently

16:38.320 --> 16:41.800
because they can build on current linear algebra techniques

16:41.800 --> 16:46.880
and be parallelized in an embarrassingly parallel way.

16:46.880 --> 16:49.640
And we have that actually now available

16:49.640 --> 16:53.760
with Amazon SageMaker and the Comprehend Service.

16:53.760 --> 16:57.040
So with that, we have benchmarks that we've released

16:57.040 --> 17:00.200
comparing it to open source topic modeling frameworks

17:00.200 --> 17:02.560
that use expectation maximization.

17:02.560 --> 17:04.880
And we see SageMaker and what?

17:04.880 --> 17:08.520
So SageMaker is the machine learning platform to run experiments.

17:08.520 --> 17:11.440
And the comprehend is the natural language processing service.

17:11.440 --> 17:12.320
OK.

17:12.320 --> 17:14.280
So with Comprehend, you can go and directly

17:14.280 --> 17:18.360
access the results of topic modeling with SageMaker.

17:18.360 --> 17:23.440
You can even play around with the algorithm and run benchmarks and so on.

17:23.440 --> 17:24.600
Tell me if this is right.

17:24.600 --> 17:29.080
It strikes me that your work is kind of operating in two dimensions.

17:29.080 --> 17:31.280
Well, now I'm overusing the term dimensions.

17:31.280 --> 17:36.120
But there's this one angle where there are different approaches.

17:36.120 --> 17:37.800
There's expectation maximization.

17:37.800 --> 17:41.200
There's PCA and others.

17:41.200 --> 17:48.240
And your work is not just that, or is it, the tenserization

17:48.240 --> 17:50.160
of expectation maximization.

17:50.160 --> 17:52.960
It's another approach, but it's an approach

17:52.960 --> 17:57.920
that is made possible because you can operate in this tensor domain.

17:57.920 --> 17:58.640
Is that correct?

17:58.640 --> 18:01.280
Or it really is another approach.

18:01.280 --> 18:03.680
It's looking at a different objective.

18:03.680 --> 18:06.160
So if you have a probabilistic model,

18:06.160 --> 18:09.280
the standard approach is maximized likelihood.

18:09.280 --> 18:11.360
But classically, there was another approach which

18:11.360 --> 18:13.680
was to match the moments.

18:13.680 --> 18:14.880
It goes back to spearmen.

18:14.880 --> 18:18.400
In fact, it goes back to even Gauss, if you go.

18:18.400 --> 18:20.280
If you only match the first two moments,

18:20.280 --> 18:24.160
that's when you get a Gaussian distribution

18:24.160 --> 18:27.200
as an approximation to your true distribution.

18:27.200 --> 18:30.040
And what this is doing is now expanding

18:30.040 --> 18:32.880
those two higher order moments and also

18:32.880 --> 18:37.160
to many coordinates, to multivariate distributions.

18:37.160 --> 18:42.160
Because the intuition is now, if you have a mixture of Gaussians,

18:42.160 --> 18:45.320
you can all just do with two second order moments.

18:45.320 --> 18:47.240
You need to go to higher order moments

18:47.240 --> 18:50.040
to try to separate the mixture components.

18:50.040 --> 18:52.840
And that applies to a broad class of models.

18:52.840 --> 18:55.360
Is tenser and tensor operations a tool

18:55.360 --> 18:58.480
that we can kind of naively go back and apply

18:58.480 --> 19:02.480
to these other things like expectation maximization PCA?

19:02.480 --> 19:07.040
And is it a magic wand that we can wave at other results

19:07.040 --> 19:12.560
and make them better or do we need a whole new set of approaches

19:12.560 --> 19:15.960
that are designed with this set of operations in mind?

19:15.960 --> 19:18.280
So I would say there's this whole spectrum of problems.

19:18.280 --> 19:21.640
Some are more low-hanging fruit than the other ones.

19:21.640 --> 19:24.480
So there are indeed some straightforward use of tensors

19:24.480 --> 19:26.560
where you say, like, oh, PCA.

19:26.560 --> 19:28.800
Now I can do PCA in more dimensions.

19:28.800 --> 19:31.520
Let me tenserize it.

19:31.520 --> 19:34.160
On the other hand, there are more sophisticated approaches

19:34.160 --> 19:37.640
where I can ask, can I now look at the problem

19:37.640 --> 19:41.120
in a new light because I can use tensor methods?

19:41.120 --> 19:44.640
Is this really the right approach there?

19:44.640 --> 19:46.680
And so yeah, so there is a whole spectrum.

19:46.680 --> 19:50.840
And the one that I'm recently been exploring a lot more

19:50.840 --> 19:54.000
on has been on tenserizing neural networks,

19:54.000 --> 19:57.760
but in interesting ways, because the question is,

19:57.760 --> 20:00.640
when can we win over simple linear algebraic techniques

20:00.640 --> 20:02.680
over different layers?

20:02.680 --> 20:06.640
Or can we even think of the overall deep layers

20:06.640 --> 20:10.080
as some kind of hierarchical tensor transformation?

20:10.080 --> 20:12.160
Because that's what it really is.

20:12.160 --> 20:18.160
And one of the works that some of the researchers

20:18.160 --> 20:21.000
have looked at, Nadok Cohen as the primary author,

20:21.000 --> 20:22.720
has been precisely to ask that.

20:22.720 --> 20:26.440
Can we look at algebraic techniques, the group theory,

20:26.440 --> 20:29.000
to understand neural networks better?

20:29.000 --> 20:31.480
So walk us through that because I think you presented that

20:31.480 --> 20:33.840
at scale.ml, and one of the examples

20:33.840 --> 20:35.560
you use if I'm remembering this correctly

20:35.560 --> 20:41.240
was looking at a computer vision task

20:41.240 --> 20:44.760
and convolutional neural nets and kind of mapping that

20:44.760 --> 20:48.080
to what that might look like in a tensor world.

20:48.080 --> 20:49.800
Am I remembering that correctly?

20:49.800 --> 20:51.160
A little bit.

20:51.160 --> 20:55.920
I mean, yes, I think so the basic problem there

20:55.920 --> 20:58.080
is we have these convolutional layers,

20:58.080 --> 21:01.000
and then you feed the output of the convolutional layers

21:01.000 --> 21:03.320
to the fully connected layers.

21:03.320 --> 21:05.280
So the output of the convolutional layers

21:05.280 --> 21:07.600
is actually a three-dimensional structure

21:07.600 --> 21:09.680
because there is the spatial dimensions

21:09.680 --> 21:11.840
and then there's the number of channels.

21:11.840 --> 21:14.560
But then when we are feeding them to the fully connected layers,

21:14.560 --> 21:16.680
we are flattening them and just turning it

21:16.680 --> 21:18.640
to a matrix multiplication.

21:18.640 --> 21:22.480
So we are all losing all the three-dimensional information.

21:22.480 --> 21:24.560
And the question we asked was a very simple one.

21:24.560 --> 21:27.720
What if we try to preserve this three-dimensional information

21:27.720 --> 21:29.440
all the way to the end?

21:29.440 --> 21:32.600
And the way to do it is instead of doing matrix multiplication,

21:32.600 --> 21:35.600
turn that into a three-dimensional tensor contraction.

21:35.600 --> 21:38.040
So multiply with weights on each of the dimensions

21:38.040 --> 21:40.080
of the three-dimensional tensor.

21:40.080 --> 21:42.840
And in the last layer, do a tensor regression

21:42.840 --> 21:46.960
and use a low-rank tensor weights as a way

21:46.960 --> 21:49.760
to reduce the number of parameters.

21:49.760 --> 21:50.800
And that's what we found.

21:50.800 --> 21:53.000
We found a huge amount of space savings

21:53.000 --> 21:56.040
compared to the standard neural networks

21:56.040 --> 21:59.200
up to 65% in these fully connected layers.

21:59.200 --> 22:00.640
Space savings in what sense?

22:00.640 --> 22:03.000
In terms of the number of parameters.

22:03.000 --> 22:05.720
So the idea is by exploiting all the dimensionality

22:05.720 --> 22:09.200
information, we can have more compact networks.

22:09.200 --> 22:10.240
OK.

22:10.240 --> 22:13.480
How else are you looking at applying this work?

22:13.480 --> 22:14.200
Yeah.

22:14.200 --> 22:15.920
And so that was one example.

22:15.920 --> 22:19.480
The other set of examples is, as I said,

22:19.480 --> 22:21.840
to investigate the ability of tensors

22:21.840 --> 22:26.960
to capture higher-order correlations better.

22:26.960 --> 22:29.280
And so we looked at time series.

22:29.280 --> 22:31.680
And so here, this can be very challenging

22:31.680 --> 22:35.080
because there can be all kinds of higher-order correlations

22:35.080 --> 22:38.760
that affect forecasting outcomes.

22:38.760 --> 22:40.280
And especially if you want to forecast

22:40.280 --> 22:43.920
way into the future, there can be much more important.

22:43.920 --> 22:48.080
And so what we did was we took RNNs and LSTMs

22:48.080 --> 22:49.800
and used a window of measurements

22:49.800 --> 22:52.560
and looked at tensorizations of them.

22:52.560 --> 22:55.600
And of course, if you naively tensorize that blows up

22:55.600 --> 22:58.600
the number of parameters, because that's a high-order

22:58.600 --> 22:59.480
tensor.

22:59.480 --> 23:00.960
But then you can do efficient tensor

23:00.960 --> 23:04.960
train approximations, a low-rank approximation.

23:04.960 --> 23:07.600
So what that yields is a succinct representation

23:07.600 --> 23:09.680
of higher-order correlations.

23:09.680 --> 23:12.280
And we found a lot of benefit compared

23:12.280 --> 23:15.880
to baseline LSTMs on a range of data sets,

23:15.880 --> 23:20.320
like forecasting, traffic, hour-saheads, forecasting,

23:20.320 --> 23:24.240
weather, hundreds of days into the future.

23:24.240 --> 23:29.520
So there is a much better performance in those settings.

23:29.520 --> 23:32.040
So I can foresee a lot of such cases

23:32.040 --> 23:34.440
where we have a diverse set of measurements.

23:34.440 --> 23:36.960
So in tensors become a natural framework

23:36.960 --> 23:38.400
to encode the input.

23:38.400 --> 23:40.600
But then even during their processing

23:40.600 --> 23:43.320
through all the layers, you kind of treat them

23:43.320 --> 23:45.040
as tensor operations.

23:45.040 --> 23:47.160
So you can exploit the information

23:47.160 --> 23:50.440
in different dimensions more effectively.

23:50.440 --> 23:53.720
Do you see this tensorization as it's applied

23:53.720 --> 23:54.760
to neural networks?

23:54.760 --> 23:57.440
Is this something that is automatable

23:57.440 --> 24:01.120
or is it akin to network architecture

24:01.120 --> 24:03.360
in its level of complexity?

24:03.360 --> 24:05.760
Did you do this via graduate student descent,

24:05.760 --> 24:08.960
or did you want it through some process?

24:08.960 --> 24:12.000
I mean, so I would say as a great starting point,

24:12.000 --> 24:15.480
would be to have a software framework that makes it easy

24:15.480 --> 24:17.520
to define these layers.

24:17.520 --> 24:20.840
So that's where John Kosofi, he

24:20.840 --> 24:24.880
interned with our group last year, and he's been working

24:24.880 --> 24:28.240
with me and also his advisors back in London

24:28.240 --> 24:31.200
on developing tensorly framework.

24:31.200 --> 24:34.560
So the tensorly software, you can think of it as Keras.

24:34.560 --> 24:36.640
It has a front end that makes it very easy

24:36.640 --> 24:39.760
to define these tensor operations.

24:39.760 --> 24:42.760
But you can now connect it to multiple different backends.

24:42.760 --> 24:47.040
TensorFlow, PyTor, TirmixNet, and the baseline NAMPI

24:47.040 --> 24:47.560
as well.

24:47.560 --> 24:49.360
So if you don't want to bother about deep learning,

24:49.360 --> 24:51.440
you can even just do basic NAMPI

24:51.440 --> 24:54.600
and do the operations on CPU.

24:54.600 --> 24:58.480
But the benefit with this is now different researchers

24:58.480 --> 25:02.600
and developers can go try out different architectures,

25:02.600 --> 25:06.320
see what works well in their domain for their data sets.

25:06.320 --> 25:08.760
And so I would say that's a good starting point.

25:08.760 --> 25:10.320
But indeed, the architecture design

25:10.320 --> 25:11.960
would be more complex because we

25:11.960 --> 25:15.400
have a broader set of operations.

25:15.400 --> 25:17.240
But on the other hand, I do think

25:17.240 --> 25:19.600
there's some nice research to be done

25:19.600 --> 25:21.800
in terms of how we can use group theories,

25:21.800 --> 25:24.920
how we can use our understanding of tensor algebra

25:24.920 --> 25:26.960
to do better architecture search.

25:26.960 --> 25:29.200
elaborate on that, what are you thinking there?

25:29.200 --> 25:31.560
I mean, this kind of, you know, has also

25:31.560 --> 25:36.000
been seen in quantum systems and other areas of physics

25:36.000 --> 25:41.040
on asking different forms of tensor representations,

25:41.040 --> 25:44.440
what kind of correlation structures to the end use,

25:44.440 --> 25:48.360
and what would make sense for a particular domain?

25:48.360 --> 25:52.320
And we're just basically touching the surface of this,

25:52.320 --> 25:55.760
so it's not still an algorithm that we can automate.

25:55.760 --> 25:57.520
But I think those kinds of understanding

25:57.520 --> 26:00.760
could help us design better architecture search.

26:00.760 --> 26:03.280
So again, I would say like, you know,

26:03.280 --> 26:05.760
we're talking about architecture search, right?

26:05.760 --> 26:07.720
And that is indeed a hard problem.

26:07.720 --> 26:10.680
I mean, right now, even simple, greedy search

26:10.680 --> 26:13.800
kind of approaches take up a lot of resources.

26:13.800 --> 26:16.400
So if we want to solve it at scale very efficiently,

26:16.400 --> 26:17.440
that's out there.

26:17.440 --> 26:18.280
OK.

26:18.280 --> 26:20.560
But on the other hand, there are some immediate things

26:20.560 --> 26:23.760
we can do to improve the current state of art.

26:23.760 --> 26:25.800
What are the things that you're working on?

26:25.800 --> 26:29.080
Yeah, it's been really a broad set of problems.

26:29.080 --> 26:33.000
I recently, I think I shared it online on slide share

26:33.000 --> 26:35.920
that talks about how to, you know,

26:35.920 --> 26:39.800
bring our thinking of all aspects of AI together, right?

26:39.800 --> 26:42.880
So I see three facets.

26:42.880 --> 26:45.240
There is data, there are algorithms,

26:45.240 --> 26:47.240
and then there's the infrastructure.

26:47.240 --> 26:50.880
And until now, we've been kind of thinking of each separately,

26:50.880 --> 26:52.840
but there's a need to bring them together

26:52.840 --> 26:57.440
to have the best efficiency, the best impact.

26:57.440 --> 26:59.560
To give an example, you know, typically,

26:59.560 --> 27:03.240
we first collect data, feed them into machine learning algorithms.

27:03.240 --> 27:05.080
But in most practical applications,

27:05.080 --> 27:07.440
data is such a crucial aspect.

27:07.440 --> 27:09.880
And what we found in a series of studies

27:09.880 --> 27:13.200
is you can drastically improve data collection, aggregation,

27:13.200 --> 27:16.200
and augment our current data better.

27:16.200 --> 27:21.520
To give an example, we looked at the name-density recognition

27:21.520 --> 27:26.280
task, and the on-to-notes is the biggest public benchmark there.

27:26.280 --> 27:27.600
The one is what's it called?

27:27.600 --> 27:28.480
On-to-notes.

27:28.480 --> 27:28.960
OK.

27:28.960 --> 27:29.640
On-to-notes.

27:29.640 --> 27:30.480
Yeah.

27:30.480 --> 27:36.520
And so then we ran a simple active learning heuristic

27:36.520 --> 27:40.440
which says, you know, feed the current model

27:40.440 --> 27:42.360
a set of labeled examples.

27:42.360 --> 27:45.880
And after it updates, look for the examples

27:45.880 --> 27:48.680
that have a high uncertainty in the current model.

27:48.680 --> 27:51.320
So this is the basic active learning framework.

27:51.320 --> 27:54.160
And we found that we could reach the state of the art

27:54.160 --> 27:57.960
with just 25% of the dataset.

27:57.960 --> 27:59.680
So there are two aspects to it.

27:59.680 --> 28:02.840
One is that active learning can drastically reduce

28:02.840 --> 28:04.440
our data requirements.

28:04.440 --> 28:07.240
And the other is, most of the time, we are collecting data

28:07.240 --> 28:08.560
that's really relevant.

28:08.560 --> 28:10.520
We don't need huge datasets.

28:10.520 --> 28:14.280
Many times, you can even do deep learning on small data.

28:14.280 --> 28:17.200
And that, to me, presents a lot of promise

28:17.200 --> 28:18.920
because in so many domains, we don't

28:18.920 --> 28:21.680
have the luxury of large datasets.

28:21.680 --> 28:26.040
So how do you map this to this idea of data algorithms

28:26.040 --> 28:28.720
and infrastructure?

28:28.720 --> 28:31.240
Because in this case, what we are doing

28:31.240 --> 28:33.240
is we're planning our data collection

28:33.240 --> 28:36.200
in the loop with our model training.

28:36.200 --> 28:40.320
So we can ask the same in terms of even aggregating data.

28:40.320 --> 28:43.080
You have a whole crowd of workers.

28:43.080 --> 28:47.920
How do you reconcile differences in their answers?

28:47.920 --> 28:50.640
The standard approaches, we do majority voting.

28:50.640 --> 28:52.400
We have a whole bunch of annotators.

28:52.400 --> 28:56.080
We try to clean up the input data very carefully.

28:56.080 --> 28:58.920
Then we feed it to the machine learning model.

28:58.920 --> 29:00.880
But what we show is you can, in fact,

29:00.880 --> 29:02.680
do them both in the loop.

29:02.680 --> 29:06.080
You can try to keep learning about the annotator quality

29:06.080 --> 29:09.160
as you go along to train the machine learning model.

29:09.160 --> 29:11.360
And the machine learning model can help you

29:11.360 --> 29:14.480
come up with better estimates of annotator quality.

29:14.480 --> 29:18.840
And with this, in fact, with the Microsoft Coco data

29:18.840 --> 29:21.520
sets, we took the raw annotations that were available

29:21.520 --> 29:22.880
for this data set.

29:22.880 --> 29:25.400
And we found that the optimal thing to do

29:25.400 --> 29:28.280
was to have just a single annotator per sample.

29:28.280 --> 29:30.880
So if you had a fixed budget of annotations,

29:30.880 --> 29:33.520
you're better off labeling more samples

29:33.520 --> 29:36.920
than to be very careful about labeling every sample

29:36.920 --> 29:39.240
and having a whole lot of annotators

29:39.240 --> 29:41.440
to reduce the noise on a single sample.

29:41.440 --> 29:42.280
Oh, interesting.

29:42.280 --> 29:43.880
Interesting.

29:43.880 --> 29:47.280
Of the three, you haven't mentioned infrastructure.

29:47.280 --> 29:48.880
How does that tie into the loop?

29:48.880 --> 29:50.560
Or how do we tie that more closely into the loop?

29:50.560 --> 29:51.560
Right.

29:51.560 --> 29:54.320
So this ties in with the work I've been doing

29:54.320 --> 29:56.280
at Amazon Web Services.

29:56.280 --> 30:00.160
So naturally, I think the answer will be the cloud

30:00.160 --> 30:04.720
because they're not just saying because I'm

30:04.720 --> 30:06.640
in the cloud division.

30:06.640 --> 30:09.240
I mean, as we scale up these models,

30:09.240 --> 30:12.400
as we have data requirements grow,

30:12.400 --> 30:18.560
the only answer will be to allow for elastic scaling

30:18.560 --> 30:24.400
and put computing at the hands of every individual.

30:24.400 --> 30:28.200
Anybody can go access large amounts of infrastructure.

30:28.200 --> 30:30.040
So that's the basic aspect, like making

30:30.040 --> 30:33.080
GPUs available to everyone is the starting point.

30:33.080 --> 30:35.920
But where I find a lot of interesting challenges

30:35.920 --> 30:39.160
is when we try to build machine learning services

30:39.160 --> 30:41.080
across all levels of the stack.

30:41.080 --> 30:44.720
So there's an expert developer who likes to maybe tweak

30:44.720 --> 30:47.480
with the backend to enterprises that just want

30:47.480 --> 30:51.920
to run application services and not even worry

30:51.920 --> 30:55.080
about any machine learning that's going on underneath.

30:55.080 --> 30:59.480
How do we satisfy the needs of these diverse customers?

30:59.480 --> 31:02.760
And I've been involved in building SageMaker.

31:02.760 --> 31:04.400
That's the machine learning platform

31:04.400 --> 31:07.160
that removes a lot of heavy lifting associated

31:07.160 --> 31:10.800
with DevOps work when it comes to productionizing AI.

31:10.800 --> 31:13.520
So how do we go from prototyping

31:13.520 --> 31:16.600
to a full-scale production machine learning service

31:16.600 --> 31:18.240
very easily?

31:18.240 --> 31:22.160
And the idea is we have built all these algorithms

31:22.160 --> 31:24.880
where there's a backend that automatically scales

31:24.880 --> 31:28.280
to multiple machines and gives high efficiency.

31:28.280 --> 31:31.760
And there's also the model serving aspect that helps you

31:31.760 --> 31:35.800
to manage different models, do A, B testing very easily,

31:35.800 --> 31:40.680
and deploy the models in a very scalable way.

31:40.680 --> 31:43.800
So I think all these aspects, so we are just

31:43.800 --> 31:46.360
beginning to think in an integrated way

31:46.360 --> 31:48.280
and there'll be more of it in the future.

31:48.280 --> 31:50.080
Awesome, awesome.

31:50.080 --> 31:52.360
You were just on a panel here at Train AI.

31:52.360 --> 31:54.120
What was the panel about?

31:54.120 --> 31:57.960
Yeah, so Rob Monroe, the CDO figure eight,

31:57.960 --> 32:00.640
you know, formerly crowd flower,

32:00.640 --> 32:03.680
was earlier in our group in Amazon AI.

32:03.680 --> 32:05.360
So we've interacted a lot.

32:05.360 --> 32:07.680
We've had a lot of discussions about data,

32:07.680 --> 32:09.680
about the state of machine learning,

32:09.680 --> 32:13.160
about his adventures around the world.

32:13.160 --> 32:16.760
He's, I don't know if you know, he's cycle large parts

32:16.760 --> 32:19.880
of India, Africa, like, you know, there is, you know,

32:19.880 --> 32:21.160
so Rob is great.

32:21.160 --> 32:25.120
And so it's very happy to be on a panel when he invited me

32:25.120 --> 32:30.960
that asks about taking somebody from PhD to products, right?

32:30.960 --> 32:34.720
So academia versus industry, and for me,

32:34.720 --> 32:37.920
being in both the world, I felt like, you know,

32:37.920 --> 32:39.960
this was a great fit.

32:39.960 --> 32:42.280
How would you summarize the panel,

32:42.280 --> 32:45.880
like, what were the main insights offered?

32:45.880 --> 32:49.320
Yeah, I think the main one is, like, you know,

32:49.320 --> 32:52.080
the question of like, why PhD, right?

32:52.080 --> 32:53.920
Like, it looks like, oh, there's so much happening

32:53.920 --> 32:55.840
in industry, why PhD?

32:55.840 --> 32:58.280
I think there is a lot of relevance for PhD,

32:58.280 --> 33:01.400
even today, in fact, more than before,

33:01.400 --> 33:05.400
because first of all, there's a whole set of unsolved problems

33:05.400 --> 33:08.480
and industry is looking at only a sliver of them.

33:08.480 --> 33:12.400
And in industry, sometimes things are so fast moving,

33:12.400 --> 33:15.000
you have to kind of get ship products

33:15.000 --> 33:18.480
and you don't have that opportunity for a deeper thinking.

33:18.480 --> 33:21.320
Right? So if you're kind of in a position to say,

33:21.320 --> 33:22.720
I want this adventure.

33:22.720 --> 33:24.480
I want to, like, explore on my own.

33:24.480 --> 33:26.480
I want to ask my own questions, right?

33:26.480 --> 33:28.320
Then PhD is the right fit.

33:28.320 --> 33:30.040
But it's not for everybody.

33:30.040 --> 33:32.840
So it's like, I would never advise somebody to do a PhD

33:32.840 --> 33:35.000
just to get a machine learning job, right?

33:35.000 --> 33:39.040
So that's like the novelism, yeah, exactly.

33:39.040 --> 33:41.000
And so, yeah, they were useful

33:41.000 --> 33:43.080
and very interesting discussions like that.

33:43.080 --> 33:44.440
Fantastic.

33:44.440 --> 33:48.080
Well, Nima, before we wind down any other thoughts

33:48.080 --> 33:49.840
that you'd like to share?

33:49.840 --> 33:53.240
No, I think I really appreciate all the great work

33:53.240 --> 33:54.080
that you're doing.

33:54.080 --> 33:58.200
I think it's very important to publicize all the efforts

33:58.200 --> 34:01.040
that are happening in AI and machine learning.

34:01.040 --> 34:04.040
And more importantly, like, kind of give a realistic view

34:04.040 --> 34:05.400
of the field, right?

34:05.400 --> 34:08.840
So I do think there is a lot of hype in the media on one hand

34:08.840 --> 34:11.560
and at the same time, a lot of fear mongering

34:11.560 --> 34:15.040
and sometimes by big and powerful names that I'm sure of.

34:15.040 --> 34:16.040
So we may name us.

34:16.040 --> 34:18.320
Yeah, but it's very easy to infer.

34:18.320 --> 34:20.240
And I completely disagree.

34:20.240 --> 34:23.040
I think it's a, I would call it an abuse of power

34:23.040 --> 34:26.960
to kind of use their positions to do this kind of a fear

34:26.960 --> 34:31.360
mongering because there's a lot of real world impact

34:31.360 --> 34:35.120
to be had, especially a lot of social impact to be had

34:35.120 --> 34:38.040
through the use of AI.

34:38.040 --> 34:40.920
And we are still very much at the beginning.

34:40.920 --> 34:43.040
There is so much development to be done.

34:43.040 --> 34:46.680
And investing in AI, investing in AI education

34:46.680 --> 34:51.840
should be the priorities of governments and organizations.

34:51.840 --> 34:55.160
So I'm so happy that, you know, you have this podcast

34:55.160 --> 34:59.680
and you're making efforts to provide a balanced view

34:59.680 --> 35:00.600
of the topic.

35:00.600 --> 35:00.880
Great.

35:00.880 --> 35:02.400
Well, thank you so much.

35:02.400 --> 35:03.800
It's great to chat with you.

35:03.800 --> 35:04.800
Thank you.

35:07.040 --> 35:08.200
All right, everyone.

35:08.200 --> 35:10.320
That's our show for today.

35:10.320 --> 35:13.320
For more information on Anima or any of the topics

35:13.320 --> 35:17.560
covered in this episode, head on over to twimlai.com slash

35:17.560 --> 35:20.640
talk slash 142.

35:20.640 --> 35:22.760
Thanks again to figure eight for their sponsorship

35:22.760 --> 35:23.840
of this show.

35:23.840 --> 35:26.400
To follow along with the train AI series,

35:26.400 --> 35:31.680
visit twimlai.com slash train AI 2018.

35:31.680 --> 35:35.720
Finally, show us some love for the podcast second anniversary

35:35.720 --> 35:37.880
and share how it's been helpful to you.

35:37.880 --> 35:42.640
You can do that over at twimlai.com slash 2AV.

35:42.640 --> 35:46.000
Thanks so much for listening and catch you next time.

