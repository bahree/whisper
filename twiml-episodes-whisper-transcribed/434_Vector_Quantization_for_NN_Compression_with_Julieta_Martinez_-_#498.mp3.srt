1
00:00:00,000 --> 00:00:16,400
All right, everyone, I am here with Hulieta Martinez. Hulieta is a senior research scientist

2
00:00:16,400 --> 00:00:22,880
at Wabi. Hulieta, welcome to the Twoma AI podcast. Thank you. Thanks for having me.

3
00:00:22,880 --> 00:00:27,840
I am really looking forward to diving into our conversation. We're going to talk all about

4
00:00:27,840 --> 00:00:34,400
some of your research into computer vision. To get us started, why don't you share a little

5
00:00:34,400 --> 00:00:39,680
bit about your background and how you came to work in the field? Yeah, of course.

6
00:00:40,960 --> 00:00:51,200
So I started, so I was born and raised in Mexico and around 2011, 2012, I finished my undergrad

7
00:00:51,200 --> 00:00:59,040
there, and I really wanted to go somewhere else for my for grad school. So I applied to a bunch

8
00:00:59,040 --> 00:01:05,280
of universities in Canada. Then I got accepted into the UBC, the University of British Columbia,

9
00:01:06,560 --> 00:01:13,120
and then I started when my master's there then eventually went on to do a PhD with the same

10
00:01:13,120 --> 00:01:22,640
institution, some supervisors. And then in 2018, I was grabbing my PhD, but I just had to defend,

11
00:01:22,640 --> 00:01:29,920
so I started looking for jobs, and I ended up moving to Toronto to Uber ATG, which was this

12
00:01:31,440 --> 00:01:39,440
lab that had just been founded and self-driving. Then in earlier this year, that lab shut down,

13
00:01:39,440 --> 00:01:46,320
and a few weeks later, a little bit later after that, I joined Wabi, which is what I'm working

14
00:01:46,320 --> 00:01:52,080
at right now. Awesome. Tell us a little bit about Wabi. What is Wabi doing?

15
00:01:53,760 --> 00:02:00,080
So Wabi is a company dedicated to build an ex-generation of self-driving.

16
00:02:01,680 --> 00:02:07,920
And so what would we believe is that self-driving is the most important and exciting technological

17
00:02:07,920 --> 00:02:17,600
innovation that we will see in the next few years. And Wabi is that Wabi's deal is trying to build

18
00:02:17,600 --> 00:02:23,280
upon the lessons of the last 20 years, and try to build an approach that is AI first. So not just

19
00:02:23,280 --> 00:02:28,720
something that is hand-engineered, and then we try to put AI to make it better, but something

20
00:02:28,720 --> 00:02:35,760
that right from the get-go can really put AI in the center stage. Awesome. And so when you say

21
00:02:35,760 --> 00:02:46,800
when you say kind of AI first versus bolt-on AI after, what are the implications of that approach?

22
00:02:46,800 --> 00:02:53,440
Where do you see it? Express itself. I imagine that everybody says that, but it means different

23
00:02:53,440 --> 00:03:01,680
things. So I guess it means that when you're building a system, you really have this choice of

24
00:03:01,680 --> 00:03:13,760
going for a more traditional approach, and then trying to say alter some parts of the stack

25
00:03:14,240 --> 00:03:22,160
with learned parts, I would say. But if you want to, it actually takes a lot of engineering

26
00:03:22,160 --> 00:03:28,320
effort, and there's a lot of overhead in trying to do this replication on a system that is

27
00:03:28,320 --> 00:03:35,600
in production. But if you build a system that from the start takes this into consideration,

28
00:03:36,480 --> 00:03:43,520
then it should be easier to say, for example, experiment on which parts should be learned,

29
00:03:43,520 --> 00:03:50,000
or which parts should not, or which parts should be trained and to end, and which parts should be

30
00:03:50,000 --> 00:03:55,360
trained separately. There are all these questions that arise when you are trying to decide where

31
00:03:55,360 --> 00:04:03,040
what the role of AI should be. And so I think it's really something that we're trying to do

32
00:04:03,040 --> 00:04:11,760
from the very start. That makes sense. Got it, got it. Yep. And so you, actually this week,

33
00:04:12,480 --> 00:04:22,160
as we're recording this, are presenting at CVPR. You've got a couple of papers at the conference

34
00:04:22,160 --> 00:04:29,440
that we'll talk about, and you're also doing a keynote talk at the Latinx and AI workshop.

35
00:04:31,120 --> 00:04:37,360
You know, let's start there. Tell us about the talk that you're doing at the workshop.

36
00:04:38,800 --> 00:04:43,520
Right. So first of all, I want to thank the organizers of the workshop for reminding me.

37
00:04:44,400 --> 00:04:50,080
It's funny. We have, so there's not a lot of people from Latin America in the field,

38
00:04:50,080 --> 00:04:56,800
let's say, ML and computer vision in particular. So we have been, there's a couple of us,

39
00:04:56,800 --> 00:05:03,760
like maybe five of us, six of us, that we have been trying to meet up every time there's a conference.

40
00:05:03,760 --> 00:05:07,920
We started with, like, you know, hanging out, the conference, like sometimes someone would come

41
00:05:07,920 --> 00:05:13,200
to your poster and be like, you notice that maybe they're like, Latin Americans, you'll be like,

42
00:05:13,200 --> 00:05:19,520
maybe it's big Spanish and so on. And then we would start, like, say, hanging out, like going for lunch,

43
00:05:19,520 --> 00:05:26,320
or trying to build a little bit of a community amongst ourselves. And then there's this Latinx

44
00:05:26,320 --> 00:05:33,360
in AI organization started. I think it was at Newrips where it started. And so then it started

45
00:05:33,360 --> 00:05:38,000
branching out to other conferences. And now there's a more formal presence, I would say,

46
00:05:38,000 --> 00:05:45,040
and more, and there's a workshop where, like, submissions from people from Latin America are

47
00:05:45,040 --> 00:05:54,320
encouraged. And where we want to also highlight tap speakers from Latin America, or Latinx living

48
00:05:54,320 --> 00:06:03,120
in the US or Canada, or, you know, another part of the world. So I think this is the first or

49
00:06:03,120 --> 00:06:08,160
second time that we are doing this official thing with official sponsors and official canals

50
00:06:08,160 --> 00:06:18,320
and posters and so on. Okay. Right. So that's how that happened. Got it. And the presentation that

51
00:06:18,320 --> 00:06:27,200
you're talking, what's the title of your talk? So I decided to name my talk what those large

52
00:06:27,200 --> 00:06:32,080
kill visual search and network compression have in common. Because I think the answer to that

53
00:06:32,080 --> 00:06:36,000
shouldn't be obvious. And I didn't encourage people to come to the talk.

54
00:06:38,400 --> 00:06:44,400
So let's dig into that. What do large-scale visual search and neural network compression

55
00:06:44,400 --> 00:06:55,600
have in common? So the argument that I tried to make there is that, so first of the motivation,

56
00:06:55,600 --> 00:07:02,240
is that we know that there's large neural networks that are that currently are achieving

57
00:07:02,240 --> 00:07:07,680
state-of-the-art performance on all sorts of tasks in computer vision, natural language processing

58
00:07:07,680 --> 00:07:16,080
and so on. And so they're the big elephant in the room. In particular, there is evidence,

59
00:07:16,080 --> 00:07:20,880
and I think this is starting in the NLP side, but now there's some papers that are showing this

60
00:07:20,880 --> 00:07:28,320
for computer vision tasks too, that the performance of these models scales log linearly with how

61
00:07:28,320 --> 00:07:35,600
many parameters they have, which eventually requires more compute and requires more data. So everything's

62
00:07:36,800 --> 00:07:42,240
we are at a moment where we have kind of known for a couple of years that this was going to happen,

63
00:07:42,240 --> 00:07:47,600
but now we have like very, very clear evidence, very clear and critical evidence that larger

64
00:07:47,600 --> 00:07:53,760
models are something that we're going to need. And so this means that we now have models that have

65
00:07:53,760 --> 00:07:59,760
billions of parameters and models that have trillions of parameters are coming. And so this means

66
00:07:59,760 --> 00:08:05,120
that if you want to deploy these models into a system, you have to compress them, like there is

67
00:08:05,120 --> 00:08:12,480
no going around that, unless you are in a very high-end lab where you have access to a super

68
00:08:12,480 --> 00:08:18,720
computer, even if you're trying to even if you have a powerful desktop or a powerful laptop or

69
00:08:18,720 --> 00:08:24,400
a powerful server for these models that have billions of billions of parameters, you're going to

70
00:08:24,400 --> 00:08:32,080
have to compress them anyway. So that is, you know, it sounds like it's a new problem that we are

71
00:08:32,080 --> 00:08:38,080
facing and it is kind of new in the context of neural networks, at no point in history how we

72
00:08:38,080 --> 00:08:48,400
ever build these trainable systems that are so large. But it's also true that we have seen

73
00:08:48,400 --> 00:08:54,480
similar problems in other areas. And we had similar problems before deep learning to

74
00:08:54,480 --> 00:09:04,560
center stage in the computer vision community. So in particular, myself, when I was doing my PhD,

75
00:09:04,560 --> 00:09:10,000
I worked on this problem called, it's called a large scale approximate in just neighbor search.

76
00:09:10,640 --> 00:09:17,200
Okay. And so this problem arises when, say, for example, you have a database of images

77
00:09:18,320 --> 00:09:22,240
and you want to find objects. And so you want to do, you want to match something in them.

78
00:09:24,800 --> 00:09:30,960
So the way that the way you do this and with the pipelines we had would be, say, from a

79
00:09:30,960 --> 00:09:36,000
from a 1080p image, you would run this through a key point detector and then you would find a bunch

80
00:09:36,000 --> 00:09:43,600
of features. And, and fairly high resolution image, but not super high, like, say, 720p 1080p,

81
00:09:43,600 --> 00:09:48,880
it would give you around 10,000 descriptors. And so then when, when an image comes, you have to

82
00:09:48,880 --> 00:09:52,800
extract these descriptors and then you have to find the nearest neighbors in these database.

83
00:09:52,800 --> 00:10:02,240
And so that means that if we have a data set of, say, like, like, say, a thousand images or, sorry,

84
00:10:04,240 --> 00:10:08,080
if you have a data set of a million images, then you're dealing with billions of descriptors.

85
00:10:09,840 --> 00:10:14,480
And so we have these database, these databases that are so large that we

86
00:10:15,440 --> 00:10:19,200
can't really keep them in memory. And then if we want to deploy them, we have to compress them.

87
00:10:19,200 --> 00:10:26,400
We have to somehow get around them. So I think what these two problems have in common is that

88
00:10:28,000 --> 00:10:33,360
they have very large computational requirements, particularly in terms of memory.

89
00:10:34,000 --> 00:10:38,080
And the fact that things are so big that we cannot even keep them in memory,

90
00:10:38,080 --> 00:10:43,760
we have to be clever about how we compress them such that we can still use them while they are

91
00:10:43,760 --> 00:10:49,360
compressed. It's a common team that I try to approach in this talk.

92
00:10:51,360 --> 00:10:58,400
It strikes me that while there's thematic commonality between these two problems,

93
00:10:59,840 --> 00:11:02,160
dealing with large databases and dealing with

94
00:11:04,640 --> 00:11:09,360
high dimensional many parameter neural networks are very different.

95
00:11:09,360 --> 00:11:17,840
And how do you, does the connection extend to technical approaches or is it more of this thematic

96
00:11:17,840 --> 00:11:26,640
connection? It's both. So there's definitely the thematic connection where the database,

97
00:11:28,800 --> 00:11:31,680
let's call them databases, but some of them call them data sets.

98
00:11:32,800 --> 00:11:37,120
They're so large, but they're also high dimensional, right? So usually these descriptors that

99
00:11:37,120 --> 00:11:43,120
we would extract from images, even if you go back 20 years, people were already using

100
00:11:43,120 --> 00:11:49,680
descriptors that had like hundreds of dimensions. So the sort of high dimensional problem was already

101
00:11:49,680 --> 00:11:59,200
there. Now, how do we approach now? Yeah, so I guess the talk kind of builds up upon to motivate

102
00:11:59,200 --> 00:12:07,440
that maybe we are facing the same problems, we can try to apply some of the same solutions. So

103
00:12:09,040 --> 00:12:14,800
in particular, in this talk, we are talking about this paper a little bit more in detail.

104
00:12:14,800 --> 00:12:19,680
The paper is being presented in the main conference where the workshop has been held on that

105
00:12:19,680 --> 00:12:29,520
CVPR. And so here we argue that we can use this particular approach called I guess vector

106
00:12:29,520 --> 00:12:37,440
quantization or some people might know it as product quantization. So product quantization is a

107
00:12:37,440 --> 00:12:44,160
very nice approach. It's not something I developed. It's an idea that originally came

108
00:12:44,160 --> 00:12:52,640
from a researcher that is currently at Facebook. I think his name is Erveje Gu. It's a paper that

109
00:12:52,640 --> 00:13:00,320
was published around 2011. And so it is motivated very much by this need to find nearest neighbors

110
00:13:00,320 --> 00:13:06,800
in high dimensional spaces very fast and with a compressed memory budget. And is it specifically

111
00:13:06,800 --> 00:13:12,880
in the context of neural networks or is it outside of that? Yeah, so this was 2011. This was dealing

112
00:13:12,880 --> 00:13:21,200
with the database problem before neural networks were widely used. So they actually have the

113
00:13:21,200 --> 00:13:27,200
they motivate this with the computer vision setting and experiments. It's a tip on the paper

114
00:13:27,200 --> 00:13:31,760
like transactions in pattern analysis and machine intelligence. It's one of the big computer vision

115
00:13:31,760 --> 00:13:40,160
journals. So the data sets are from computer vision and so on. So in particular, there was this one

116
00:13:40,160 --> 00:13:47,120
descriptor that everyone was using in the day called SIFT that describes a small patch of an

117
00:13:47,120 --> 00:13:51,920
image. And if you have a bunch of them, you can do a lot of interesting things with the database.

118
00:13:53,840 --> 00:13:59,360
So the approach is actually very, very simple. And I think that's part of what makes it so nice.

119
00:14:00,640 --> 00:14:05,360
So we're going to have, so after we extract all these descriptors,

120
00:14:05,360 --> 00:14:12,160
we are going to stack them on a big matrix that is going to have dimensionality D on one side.

121
00:14:12,160 --> 00:14:16,960
That's going to be the tiny side and then the size N where N is the size of the database. And the

122
00:14:16,960 --> 00:14:23,120
thing is that D can be like 100 or something like that, but N can be 100 million or so on.

123
00:14:26,320 --> 00:14:33,920
The simplest way I like to explain this is you can cut these data sets into say four smaller data

124
00:14:33,920 --> 00:14:42,080
sets. If the dimensionality is 100, then each sub data set is going to have dimensionality 25.

125
00:14:43,920 --> 00:14:49,600
And so then we're going to run k-means on each of these little data sets or a small representative set

126
00:14:49,600 --> 00:14:56,080
of that. That is going to give us two things. So the output of k-means is on the one hand the cluster

127
00:14:56,080 --> 00:15:01,920
centers or we're going to call that the codebook. And then it's also going to give us an assignment

128
00:15:01,920 --> 00:15:08,240
for each point in that sub data set to the cluster center that is closest to it.

129
00:15:09,920 --> 00:15:15,040
So now what we're going to do is we're going to run these k-means with a very low k,

130
00:15:15,040 --> 00:15:19,280
so say k equals something like 256 is like the standard value we use.

131
00:15:20,720 --> 00:15:26,240
So that means that the codes are going to be numbers between 0 and 255.

132
00:15:26,240 --> 00:15:30,400
And that means that we can store them with a single byte.

133
00:15:32,800 --> 00:15:38,640
Now the way, now the nice thing is that once we have these groups of codes in codebooks,

134
00:15:38,640 --> 00:15:42,240
we're going to throw away the regional database. The regional database that was so large,

135
00:15:42,240 --> 00:15:47,120
we couldn't fit in memory. And these codes that are small, we can load that in RAM.

136
00:15:49,040 --> 00:15:52,480
This is of course it's going to be an approximation of the regional data set.

137
00:15:52,480 --> 00:15:57,040
But because you can load it in RAM, then you can do clever things with it.

138
00:15:57,920 --> 00:16:01,920
So the first thing is now you have extra space and something that you can manage.

139
00:16:01,920 --> 00:16:07,120
So you can come up with certain data structures that allow you to say do filtering.

140
00:16:07,120 --> 00:16:11,360
Because when you have a query, even if it's compressed, even if it's very nice,

141
00:16:11,920 --> 00:16:16,560
easy to search, you still have hundreds of millions. And you don't want to compute hundreds of

142
00:16:16,560 --> 00:16:22,640
millions of distances. So you want to have some data structure that filters good candidates

143
00:16:22,640 --> 00:16:30,240
that are likely to be the nearest neighbor. But the other thing you can do is that you can use

144
00:16:30,240 --> 00:16:37,760
this code to approximate the distance to the regional vector. So when the query comes,

145
00:16:37,760 --> 00:16:43,840
you can also split it into four chunks. And then you can compute four tables from each little chunk

146
00:16:43,840 --> 00:16:50,960
to the code book, to the code book that we computed before. So that gives us four tables of

147
00:16:50,960 --> 00:16:59,200
255 entries each. And we can just do lookups. So if the element 2,000 in the database is represented

148
00:16:59,200 --> 00:17:09,440
by five numbers, called 5, 200, 120, and it's 255, then you can look up table one in

149
00:17:09,440 --> 00:17:15,680
index five, they will do an index 100, they will tree index, whatever number you had and so on.

150
00:17:16,640 --> 00:17:24,000
Just to make sure I'm following you, is it fair to compare this to or make the analogy of like

151
00:17:24,000 --> 00:17:28,880
a hashing table or hashing algorithm where your hashing algorithm is nearest neighbors

152
00:17:30,000 --> 00:17:35,040
or related to nearest neighbors? Yeah, so hashing is also an approach that will give you

153
00:17:35,040 --> 00:17:43,120
you it's also used to to make nearest neighbors faster. It often counts with nice theoretical

154
00:17:43,120 --> 00:17:49,040
guarantees. And so people like hashing because of that. One of the things that that

155
00:17:49,040 --> 00:17:54,080
product acquisition showed was that even though hashing hashing approaches tend to have

156
00:17:54,960 --> 00:18:04,720
nice theoretical guarantees in practice, they were very much outperformed by doing this

157
00:18:04,720 --> 00:18:13,520
quantization approaches. So yeah, this was 2011, now the literature has moved a lot on both sides.

158
00:18:14,880 --> 00:18:21,520
So, you know, I might be wrong now, I don't follow this closely, but back in the day, that was

159
00:18:23,440 --> 00:18:26,480
pretty much what the evidence was saying.

160
00:18:26,480 --> 00:18:37,520
And so we've got this methodology that we can use on the database side to it sounds like

161
00:18:40,560 --> 00:18:47,600
get more efficient ways to filter the data and you know, based on the reference to

162
00:18:47,600 --> 00:18:55,040
visual search, search as well. What's kind of the next step in your argument, how do we get

163
00:18:55,040 --> 00:19:03,920
closer to the application of neural network compression? Yeah, so I think this is the

164
00:19:03,920 --> 00:19:08,560
last obvious point and it's something that I was very proud of when these ideas started going.

165
00:19:09,760 --> 00:19:17,280
So again, to be fair, like other people had experimented with vector quantization of neural nets.

166
00:19:17,280 --> 00:19:26,160
There are papers on 2014, 2016 and so on, but it is considerably much less explored. So the standard

167
00:19:26,160 --> 00:19:32,240
approach to compressed neural nets is usually scalar quantization or things like network, there's

168
00:19:32,240 --> 00:19:42,000
a lot of focus on network pruning or say low rank approximations. Vector quantization, you can count

169
00:19:42,000 --> 00:19:47,600
the number of papers that do that for neural networks, it's one or two hands.

170
00:19:49,520 --> 00:19:56,640
Can you maybe talk a little bit about scalar quantization versus vector quantization and

171
00:19:56,640 --> 00:20:04,960
how those approaches are worked to set us up for your application?

172
00:20:04,960 --> 00:20:14,160
Yeah, so they're kind of similar. So scalar quantization is what happens when you try to look at each

173
00:20:15,520 --> 00:20:21,520
value in your network or in general in any data that you have and you're going to look at every

174
00:20:21,520 --> 00:20:28,400
number that you have and you're trying to map this real flooring point in practice values

175
00:20:28,400 --> 00:20:37,120
to a discrete subset. And so it's kind of similar to what we're doing in vector quantization.

176
00:20:37,120 --> 00:20:43,360
Just in scalar quantization, you're going to have one code for each scalar.

177
00:20:44,080 --> 00:20:51,040
In vector quantization, you're going to have one code for each vector and of course the question

178
00:20:51,040 --> 00:20:58,880
is how you define a vector, what is going to be grouped together and so on. So theoretically, scalar

179
00:20:58,880 --> 00:21:04,800
quantization is going to be limited in what compression ratios it can achieve because let's say that

180
00:21:04,800 --> 00:21:11,440
you're representing your numbers with 32-bit floating points. Even in the more extreme case,

181
00:21:11,440 --> 00:21:17,760
you're going to reduce that to a binary value, one or zero. So that's going to give you a 32-X

182
00:21:17,760 --> 00:21:24,240
compression rate. It's pretty extreme, though. Which is pretty good, right? It's pretty good.

183
00:21:24,240 --> 00:21:28,880
But when you have a network with trillions of parameters, that might still mean that you need a

184
00:21:28,880 --> 00:21:34,880
machine with 200 gives of RAM or something like that. So now you're saying if you represent

185
00:21:34,880 --> 00:21:44,080
a vector of eight quantities with a single binary, then you've multiplied that out.

186
00:21:44,080 --> 00:21:52,240
Yes, yes. Or if you take like eight values, 16 values, and so on, the bigger the vector,

187
00:21:54,080 --> 00:21:59,920
the more sort of bang for your buck you get. Yeah, exactly. Got it. Okay.

188
00:22:01,600 --> 00:22:10,880
And so then that is, you mentioned that there are only a handful of papers that have explored

189
00:22:10,880 --> 00:22:20,880
vector quantization. Any sense for why that is? Is it just hard? Did it not seem promising?

190
00:22:22,800 --> 00:22:26,960
Did other more promising ideas jump to the fore?

191
00:22:28,720 --> 00:22:34,800
Yeah, it's a good question, actually. I am not entirely sure why that is. I think

192
00:22:34,800 --> 00:22:43,120
a scalar quantization is very intuitive. One thing that might have happened is that

193
00:22:44,000 --> 00:22:52,240
when you have a scalar quantized network in particular, the way how you can use that to

194
00:22:52,240 --> 00:22:58,640
accelerate inference, for example, is more straightforward with current hardware.

195
00:22:58,640 --> 00:23:07,120
Whereas if you start mapping multiple values to a single code, then how you would accelerate that

196
00:23:07,120 --> 00:23:14,720
is not necessarily obvious. And so I think that might be. And so if you don't really have the

197
00:23:15,600 --> 00:23:21,200
the memory limitation being really the main constraint, which if your network has

198
00:23:22,400 --> 00:23:27,440
50 million parameters, 100 million parameters, you can probably get away with scalar quantization

199
00:23:27,440 --> 00:23:33,120
and still run that in, you know, decent desktops or fairly high end phones, for example.

200
00:23:34,720 --> 00:23:39,600
So it might be just now that we're seeing these networks, it's really, really huge

201
00:23:39,600 --> 00:23:42,400
that there's not going to be a way around it.

202
00:23:43,120 --> 00:23:53,520
Right. Right. And so kind of going back to the method that we discussed in the context of

203
00:23:53,520 --> 00:23:59,120
database, I forget you said product. Product quantization product quantization. Yeah. So the

204
00:24:00,960 --> 00:24:08,480
is a fair simplification to say that in scalar and vector quantization, you're only looking at

205
00:24:08,480 --> 00:24:13,680
the specific quantities that you're quantizing. Whereas with product quantization, now you're

206
00:24:13,680 --> 00:24:26,000
looking at the data that you're going to be quantizing it kind of at large. I think that is kind

207
00:24:26,000 --> 00:24:31,760
of fair to say. So the thing with vector quantization is that then you have to design, then you have

208
00:24:31,760 --> 00:24:38,560
to define, you really have to think about what your definition of a vector is going to be.

209
00:24:38,560 --> 00:24:43,840
Mm-hmm. Because for example, before we were saying we have this database that has a hundred

210
00:24:43,840 --> 00:24:50,800
dimensions and we were saying, let's just split it into four groups. But the performance of vector

211
00:24:50,800 --> 00:24:55,760
and sorry, and if you're doing something like scalar quantization, you can take any permutation

212
00:24:55,760 --> 00:25:00,400
of your values. You're going to get the same compression rates. It doesn't really matter for

213
00:25:00,400 --> 00:25:05,040
terms of compression in which order you look at your data. But if you're doing something like

214
00:25:05,040 --> 00:25:13,120
vector quantization, you might say, hey, maybe if I group the first five dimensions and then

215
00:25:13,120 --> 00:25:19,680
dimensions from 55 to 60 and then the last ten dimensions and I make that a single subgroup,

216
00:25:20,320 --> 00:25:25,040
that's going to be much easier to compress than to just take whatever dimensions happen to be

217
00:25:25,040 --> 00:25:37,040
together. Mm-hmm. Yeah, I think that what I was maybe trying to get at is when you're typically

218
00:25:37,040 --> 00:25:44,880
applying vector quantization, you're coming up with this map. Are you also considering the entirety

219
00:25:44,880 --> 00:25:51,920
of your data and using that to create your quantization scheme or is that unique to product quantization?

220
00:25:51,920 --> 00:25:57,280
It sounds like that that may not be. Oh, vector versus product. So I think,

221
00:25:58,320 --> 00:26:04,720
no, so in both cases you have to quantize everything. The gain that you get with product is that

222
00:26:04,720 --> 00:26:10,320
you have this freedom of having subgroups. Okay. Whereas if you're just doing like say more

223
00:26:11,120 --> 00:26:18,240
just taking your whole data set as each entry as a full vector, you wouldn't really have that.

224
00:26:18,240 --> 00:26:26,080
Okay. And so how do you characterize the advantages that you get with product quantization?

225
00:26:27,920 --> 00:26:34,160
So the nice thing is that because you have multiple subgroups in example that I was giving say for,

226
00:26:35,440 --> 00:26:42,480
you have, you can think of it as you're expressing for example, you're expressing your vector as

227
00:26:42,480 --> 00:26:50,960
four numbers. And those four numbers can take each 256 values. So in a sense, you can think of having

228
00:26:50,960 --> 00:27:00,160
a combinatorial sense of clusters to compress your data because you're going to have 256 times 256

229
00:27:00,160 --> 00:27:08,080
times 256 times 256 times four times. Whereas if you, so that allows you to have an implicit code

230
00:27:08,080 --> 00:27:13,840
book that is very, very large, which is good for compression. Whereas if you were looking at the

231
00:27:13,840 --> 00:27:20,960
data as a single vector, then having a code book of that size might not be even feasible for example.

232
00:27:22,320 --> 00:27:27,520
Okay. And then practically, how does this play out when you're,

233
00:27:29,680 --> 00:27:33,040
when you're using this to compress a neural network?

234
00:27:33,040 --> 00:27:38,000
You want to know how to efficiency and accuracy and all those kinds of things, right?

235
00:27:38,560 --> 00:27:44,160
Let's get into it. So the question of how we do it, I think that's a nice point.

236
00:27:45,040 --> 00:27:48,240
So like I was saying, the main question is, what are you going to group together?

237
00:27:48,240 --> 00:27:54,720
So let's say you have a matrix in your neural net that is of size a thousand by a thousand.

238
00:27:56,000 --> 00:28:01,760
You can choose to split this into subsets of four numbers each and then treat those of vectors,

239
00:28:01,760 --> 00:28:07,200
run k-means, and then get a compression that, get a representation that is much more compressed.

240
00:28:09,760 --> 00:28:15,840
So the first thing that we do is, that's why we call the paper permute, permute, then quantize

241
00:28:15,840 --> 00:28:23,280
and find you. So the permute step is, it's a nice, it's a nice observation. So neural nets have

242
00:28:23,280 --> 00:28:31,760
this nice property that if you have two adjacent layers, let's say a linear layer, some non-linear

243
00:28:31,760 --> 00:28:38,320
activation and another linear layer, some non-linear activation, you can express the same

244
00:28:39,280 --> 00:28:47,680
function by having different permutations of the weights. So let's say that you choose one

245
00:28:47,680 --> 00:28:54,160
permutation of this 1024 elements and on the first layer you're going to permute the rows according

246
00:28:54,160 --> 00:28:59,920
to that permutation and then in the second layer you're going to permute the columns. If you pass

247
00:28:59,920 --> 00:29:04,960
data through it, the first thing you're going to notice after the first layer is that your output

248
00:29:04,960 --> 00:29:11,520
is going to be permuted and then when you process it through the second layer because you

249
00:29:11,520 --> 00:29:18,720
permuted the rows, that is going to undo the permutation. The same thing is true for convolutional layers.

250
00:29:18,720 --> 00:29:26,240
So convolutional layers are parameterized by, say, a one-dimensional array of a lot of

251
00:29:26,240 --> 00:29:31,600
little three-dimensional arrays, which are the convolutional filters. The only thing that determines

252
00:29:31,600 --> 00:29:36,240
the order of your output, the order of the channels of the output of the first layer is in which

253
00:29:36,240 --> 00:29:43,920
order you apply these filters. So that is one dimension you can permute and just alter the order

254
00:29:43,920 --> 00:29:50,720
of the output. And then to the layer that's downstream you can just say, oh, you should switch

255
00:29:50,720 --> 00:29:57,600
the filters. You should switch the channels of your filters and then that will undo the permutation,

256
00:29:57,600 --> 00:30:02,960
right? So you're saying that you can permute, but you haven't said yet why you want to permute?

257
00:30:02,960 --> 00:30:09,520
Exactly, exactly, exactly. And that's what we've been hinting at, right? Because the key question

258
00:30:09,520 --> 00:30:15,920
for vector quantization to work and for product quantization to work is how am I going to form

259
00:30:15,920 --> 00:30:22,160
these little subgroups? If I just go and take whatever four numbers happen to be together

260
00:30:22,160 --> 00:30:27,760
because a stochastic gradient descent took these values there, that might not be very easy to

261
00:30:27,760 --> 00:30:35,040
compress. But if I have this other, this whole permutation, it gives me more degrees of freedom.

262
00:30:35,040 --> 00:30:40,160
I can, I can find, I can search over all the permutations of the data. Well, if you search

263
00:30:40,160 --> 00:30:44,000
over all of them, that's going to take you a while because there's a factorial number of permutations.

264
00:30:44,880 --> 00:30:50,880
But you can find efficient algorithms to say stochastically look at look at a subset of them

265
00:30:50,880 --> 00:30:55,760
and then try to find one permutation that is going to make your data easier to compress.

266
00:30:55,760 --> 00:30:59,680
In this case, the data is going to be the weights of the network itself.

267
00:31:01,840 --> 00:31:05,600
And so then the next question is if you have, if I give you to permutations, how do you know

268
00:31:05,600 --> 00:31:15,840
which one is better? That's a natural question. The thing is that vector quantization was a very

269
00:31:15,840 --> 00:31:23,680
hot topic. I'm talking about the 80s, late 80s, early 90s because there was this big push to

270
00:31:23,680 --> 00:31:29,360
digitize a lot of the infrastructure that we had to go from analog to try to make it more digital.

271
00:31:30,400 --> 00:31:38,240
And so the limitation that we had in doing that was that we had very little bandwidth.

272
00:31:40,720 --> 00:31:46,960
So we could only transmit so much binary data over the SL connections or whatever we had.

273
00:31:46,960 --> 00:31:58,960
So there is a lot of papers, usually older papers, that deal very neatly with satioretical guarantees

274
00:31:58,960 --> 00:32:08,080
of how to get good code books, how to get good compression rates. So one thing people studied

275
00:32:08,080 --> 00:32:14,800
in particular, because back then getting real data was really hard to get in. So there's a lot of

276
00:32:14,800 --> 00:32:20,720
papers on how to impress samples from a Gaussian, samples from a Laplacian, samples from other

277
00:32:21,680 --> 00:32:29,280
canonical say distributions. So for example, for a Gaussian, we know that there are well-known

278
00:32:29,280 --> 00:32:34,800
lower bounds, well-known upper bounds. Actually, upper bounds are harder to find. Lower bounds

279
00:32:34,800 --> 00:32:41,840
are lower bounds. And so if you look at expression for this, you will see that there is some constant

280
00:32:41,840 --> 00:32:47,440
numbers based on how big your code book is, how high-dimensional your data is. And then there is

281
00:32:47,440 --> 00:32:53,840
something, there is a term, a linear term based on the determinant of the covariance of the data

282
00:32:53,840 --> 00:33:00,320
that you have. And intuitively, this makes a lot of sense, because geometrically, the determinant

283
00:33:00,320 --> 00:33:04,880
of the covariance gives you a sense of how spread your data is in higher dimensions.

284
00:33:06,160 --> 00:33:10,880
So given two permutations of the data, you can compute your covariance,

285
00:33:10,880 --> 00:33:16,960
compute your determinant, and choose whichever one has lowest, lowest determinant of covariance.

286
00:33:17,680 --> 00:33:23,520
Got it, got it. And that's how you, that's how you create your mappings for your product quantization.

287
00:33:24,240 --> 00:33:28,080
Yes, that's how you create, that's how you decide exactly which things to compress together.

288
00:33:28,560 --> 00:33:37,920
Okay. Yes. And so, yeah, we're pretty proud of that idea, yeah. We're not the first to notice

289
00:33:37,920 --> 00:33:42,400
that, that this permutation invariance exists, like that has been known since the 90s or so on,

290
00:33:43,440 --> 00:33:47,520
probably since people started doing neural nets, I think it's kind of obvious and retrospect.

291
00:33:48,480 --> 00:33:56,160
And there's papers that try to use this to, to make optimization easier, or for example, there's

292
00:33:57,200 --> 00:34:02,000
there's a guy at Google Brain called Simon Cornbullet. He has some papers where

293
00:34:02,000 --> 00:34:08,960
he says, okay, if you have two networks, you train them same data, but different initializations.

294
00:34:09,520 --> 00:34:14,800
How can you tell if they learn the same features? And then the question that is that the features

295
00:34:14,800 --> 00:34:19,200
could be the same, but they're gonna be permuted differently because initialization is different.

296
00:34:19,920 --> 00:34:24,320
So you have to find some sort of canonical permutation to make these things comparable.

297
00:34:24,320 --> 00:34:28,800
And so they do a similar search to what we were doing with a very different end goal,

298
00:34:28,800 --> 00:34:37,280
what exploit in the same properties. Okay, okay, nice. And so then zooming out and looking broadly at

299
00:34:38,480 --> 00:34:45,200
approaches that folks are taking for neural compression or compressing deep neural nets,

300
00:34:45,200 --> 00:34:48,800
how does this approach compare in the grand scheme of things?

301
00:34:49,680 --> 00:34:55,440
Right, so in the paper we compare against a bunch of mostly scalar quantization baselines,

302
00:34:55,440 --> 00:35:02,880
because it's it's the dominant approach in the literature. Okay, and there was a paper that we

303
00:35:02,880 --> 00:35:11,040
are building upon that was doing vector quantization without say, without this permutation inside

304
00:35:11,040 --> 00:35:16,560
in particular. And when you plot this saying in the in the x-axis, you put the compression ratio,

305
00:35:16,560 --> 00:35:23,440
so 10x, 20x, 30x, and in the y-axis, you want to put how much accurate your network is on whatever

306
00:35:23,440 --> 00:35:30,400
task you have. You see that vector approaches sort of create an envelope around all the scalar

307
00:35:30,400 --> 00:35:36,560
ones. And our method just kind of pushes that I would say by a fairly large margin,

308
00:35:36,560 --> 00:35:40,000
especially for high compression ratios.

309
00:35:43,280 --> 00:35:51,200
In terms of implementation, difficulty or efficiency, did you look at any comparisons

310
00:35:51,200 --> 00:35:55,120
along those dimensions? Yeah, that is that is a question we get very often.

311
00:35:56,240 --> 00:36:05,040
So unfortunately not, so it's it takes a lot of engineering effort to to make these things

312
00:36:05,040 --> 00:36:14,560
actually run fast. We did some preliminary experimentation where we noticed that a lot of these

313
00:36:14,560 --> 00:36:22,560
libraries that people often use to do inference, for example, like Kula, Kudian, and so on, they are

314
00:36:24,160 --> 00:36:30,640
kind of hidden, but ones start poking them. They're very optimized for certain settings,

315
00:36:30,640 --> 00:36:35,280
and which means they're really bad for other settings. So they tend to be really good

316
00:36:35,280 --> 00:36:44,160
for particular batch sizes, for particular sizes of data that people tend to use a lot in practice.

317
00:36:45,840 --> 00:36:53,360
But once you take them out of that range, they tend to perform not so great.

318
00:36:54,240 --> 00:37:00,640
And so yeah, the question of how do we actually make this run fast? I think there's two ways.

319
00:37:00,640 --> 00:37:08,720
Either we find very talented engineers, and they do it for us, but another very promising

320
00:37:08,720 --> 00:37:15,760
approach is I think looking at automatic compilers, and compilers that can actually

321
00:37:16,480 --> 00:37:22,160
analyze the whole graph ahead of time and come up and maybe run some experiments, benchmark

322
00:37:22,160 --> 00:37:27,440
these things ahead of time for you, and then try to find a set of instructions that run nicely,

323
00:37:27,440 --> 00:37:36,400
such that the action implementation will be fast on actual hardware. That's not easy.

324
00:37:37,360 --> 00:37:41,920
There's a bunch of efforts, for example, Apache's TVM, I think, is very promising.

325
00:37:45,120 --> 00:37:51,840
And so the promises that there's potentially a greater engineering effort, but

326
00:37:51,840 --> 00:37:57,440
if you put in that effort, this is a potentially higher performance.

327
00:37:58,960 --> 00:38:06,800
Well, is it higher performance at a given compression level or greater compression at a given

328
00:38:06,800 --> 00:38:13,920
performance level or both? I get some thinking of what I'm trying to articulate is one objective

329
00:38:13,920 --> 00:38:24,640
could be to maximize your compression within some given performance bound. Another way you might

330
00:38:24,640 --> 00:38:35,200
approach the problem is to given a required compression level, what's the best performance you're

331
00:38:35,200 --> 00:38:40,240
going to get? If you can fit the model, if you have a hard limit into the model size that you need

332
00:38:40,240 --> 00:38:46,400
to get to, what's the best performance you can get? It's not clear to me that the same

333
00:38:46,960 --> 00:38:50,400
approach necessarily checks both boxes, and that's what I was trying to get at.

334
00:38:52,400 --> 00:38:57,600
Yeah, so I think in practice, you can definitely set the compression rate ahead of time.

335
00:38:58,720 --> 00:39:06,320
And what we see is that when we do that, then in this compression to performance chart,

336
00:39:06,320 --> 00:39:11,840
you tend to be a little bit, so that this would be a cross-section on the x-axis.

337
00:39:12,720 --> 00:39:18,960
So this method tends to be a little bit higher up. It's harder to say I want this performance,

338
00:39:18,960 --> 00:39:25,760
then give me the smallest model I can get, because yeah, a hair of time, networks are hard to

339
00:39:25,760 --> 00:39:28,880
predict. You have to train them, and then you have to hope for the best.

340
00:39:31,280 --> 00:39:35,600
So what we do is we just compress for a bunch of say code book sizes, and that gives us like a

341
00:39:35,600 --> 00:39:42,320
nice, a nice curve that tells you what the trade-offs are. And in practice, that happens to be an

342
00:39:42,320 --> 00:39:47,760
envelope that is pretty optimal with respect to other approaches.

343
00:39:50,560 --> 00:39:56,800
And so the paper that we've been discussing as far per mute quantize and fine-tune is one of a

344
00:39:56,800 --> 00:40:05,840
couple that you presented at or are presenting at CVPR. There's another deep multitask learning

345
00:40:05,840 --> 00:40:12,160
for joint localization, perception, and prediction. Can you give us a high-level overview of that one?

346
00:40:13,600 --> 00:40:21,280
Yeah, of course. So first of all, I want to say I am first author on the per mute quantize paper,

347
00:40:21,280 --> 00:40:26,640
but it was joint work with a bunch of other people. In particular, there's two interns,

348
00:40:27,520 --> 00:40:34,400
Joshan, Shabra Kamani, and Thingway Liu. They did a bunch of the of the work in this

349
00:40:35,360 --> 00:40:39,920
in this part, and you know, shout out to them and my other colleagues at UberHG.

350
00:40:41,520 --> 00:40:51,120
The joint localization paper is, I am not first author there, that is the work

351
00:40:51,120 --> 00:40:57,040
of John Phillips, who was also an intern with us. He's finishing his undergrad right now. He's

352
00:40:58,080 --> 00:41:02,160
he's one of those interns that are doing their undergrad in there, you know,

353
00:41:02,160 --> 00:41:08,400
already publishing at the venues. Oh wow. Yes, it's more common. It's just getting more and more common.

354
00:41:09,920 --> 00:41:16,400
So that's that's a paper actually like a lot too, because it's asking a question that doesn't get

355
00:41:16,400 --> 00:41:25,760
asked very often. So if you go to there's so localization is given a sensor reading for a bunch of

356
00:41:25,760 --> 00:41:32,080
sensor readings and a map, you want to know what in the map you are. And so this is if you want to

357
00:41:32,080 --> 00:41:37,200
build a robot that goes around your house or you want to build an autonomous driving car or

358
00:41:37,200 --> 00:41:42,400
anytime you want to build an autonomous system, there is always going to be some sort of map,

359
00:41:42,400 --> 00:41:46,240
some sort of map of the world that is going to make it easier for the agent to navigate.

360
00:41:47,440 --> 00:41:52,080
And I guess self-driving cars are just the prime example. Their whole thing is that they have

361
00:41:52,080 --> 00:41:58,880
to navigate and that they have to do it safely. So there are a lot of papers in localization,

362
00:41:58,880 --> 00:42:05,760
this has been a problem that has been going on for decades. And there's a lot of approaches,

363
00:42:05,760 --> 00:42:11,520
there's approaches that assume that the map has certain composition, the map is high definition,

364
00:42:11,520 --> 00:42:17,440
low definition, did you build it online, there's memory trade-offs, computational trade-offs,

365
00:42:17,440 --> 00:42:24,560
it's a huge area. And the way people motivate this work in that, they usually will throw on like

366
00:42:24,560 --> 00:42:30,000
say you want to localize and holiday image that you took three years ago and you don't remember

367
00:42:30,000 --> 00:42:36,720
what it was taken. Or you want to or say or you have an autonomous robot or a satellite car.

368
00:42:36,720 --> 00:42:45,040
But the thing that they're actually optimized for is they have this measure called localization error,

369
00:42:45,040 --> 00:42:53,840
which is basically how far you are from given some better sensors telling you, you know you are.

370
00:42:54,720 --> 00:42:57,920
Which makes a lot of sense, right? It's kind of the task that you're trying to solve.

371
00:42:57,920 --> 00:43:05,680
But when you are building say an autonomous system, an autonomous self-driving car,

372
00:43:07,680 --> 00:43:13,760
the thing you really care about is is this right comfortable and is this right safe?

373
00:43:15,920 --> 00:43:22,640
I don't really care if the car thinks that I am five centimeters ahead of what I'm supposed

374
00:43:22,640 --> 00:43:29,760
of where it actually is or 10 or 20 as long as it's safe, it's really fine. And so that was the

375
00:43:29,760 --> 00:43:38,000
question we asked, like how can we link these localization errors? How is that going to affect

376
00:43:38,000 --> 00:43:44,480
autonomous systems? In particular, how is that going to affect things like perception, prediction

377
00:43:44,480 --> 00:43:50,320
and motion planning? By perception, I mean object detection, so detecting the cars, the pedestrians,

378
00:43:50,320 --> 00:43:57,680
the bikes, the motorcycles around you. If my position is if I get that wrong, am I going to be

379
00:43:57,680 --> 00:44:07,520
unable or less good at detecting cars? And am I going to be worse at planning my way around them

380
00:44:07,520 --> 00:44:15,680
or trying to predict what they're going to do? What we notice is that if there is a small enough

381
00:44:15,680 --> 00:44:25,600
error, so say five, then below 20 centimeters, basically your perception system, your motion

382
00:44:25,600 --> 00:44:35,520
planning system doesn't care. It doesn't seem to matter. And then based on that, we're saying, well,

383
00:44:35,520 --> 00:44:44,080
maybe we can build a localization system that runs much faster, but it still gives us the accuracy

384
00:44:44,080 --> 00:44:51,520
that that is good enough for our autonomous system. So we end up designing a neural net that

385
00:44:52,160 --> 00:44:57,520
has a branch that is dedicated to localization, and that branch can be very, very tiny.

386
00:44:58,480 --> 00:45:03,120
And then we can leverage a big backbone that is doing the perception and the motion,

387
00:45:03,120 --> 00:45:09,440
and sorry, the perception and the prediction, and sort of kind of piggyback on top of those

388
00:45:09,440 --> 00:45:14,560
features, try to reuse that computation that's already there, so that we can have a very, very

389
00:45:14,560 --> 00:45:23,120
tiny network that runs in like two milliseconds, three milliseconds, so that and it's still acceptable.

390
00:45:23,120 --> 00:45:26,080
And so we have experiments to show that this is actually possible.

391
00:45:27,360 --> 00:45:35,120
Nice, nice. In the title of the paper is deep multitask learning, after you observe that you

392
00:45:35,120 --> 00:45:43,440
don't need your localization to be quite as accurate, then you've built a multitask network that

393
00:45:43,440 --> 00:45:48,480
has localization as one part of it, as opposed to localization being a standalone system.

394
00:45:50,800 --> 00:45:54,400
Yeah, so that will be the more traditional approach where like you will have some,

395
00:45:55,840 --> 00:46:00,320
either some part of the stock or its own network, or often this would reflect into having

396
00:46:00,320 --> 00:46:08,960
some group of engineers being a set task force or a group whose task is just to improve localization.

397
00:46:10,480 --> 00:46:16,560
So we're kind of doing this thing, joining, and another thing we know is besides the performance

398
00:46:16,560 --> 00:46:25,520
improvements is that if you are deploying this in the real world, this is we argue easier to

399
00:46:25,520 --> 00:46:31,360
deploy as well, because then your training times are much slower, so you can iterate faster

400
00:46:32,960 --> 00:46:37,600
and you don't have to think of two systems at the same time. You can just have

401
00:46:38,160 --> 00:46:42,960
sort of one system that you, after it's trained, you can push it to the car and just get going.

402
00:46:43,680 --> 00:46:46,080
Got it, got it. Awesome, awesome.

403
00:46:46,080 --> 00:46:55,280
I'm curious what you're, you know, when you look at everything that's happening and autonomous

404
00:46:55,280 --> 00:47:01,600
vehicles, just your personal take, what you're excited about, where you think the field is going.

405
00:47:04,880 --> 00:47:09,040
So I am really excited about why I am working here.

406
00:47:09,040 --> 00:47:21,680
I think, so I don't know if, so our co-founder, Professor Hurtason, she has 20 years of experience

407
00:47:21,680 --> 00:47:28,560
within AI, 10 years of experience in self-driving and four years working in industry and just in this

408
00:47:28,560 --> 00:47:36,880
problem. It's a lot of experience, a lot of lessons learned, and I think that having those

409
00:47:36,880 --> 00:47:43,360
insights and that belief that we can build something from scratch that is going to put AI center

410
00:47:43,360 --> 00:47:48,480
stage is not, we're not just saying that, like that's something that's backed up by a lot of

411
00:47:48,480 --> 00:47:55,280
experience and by seeing what limitations current approaches have.

412
00:47:56,640 --> 00:48:03,280
Awesome, awesome. Well, Hulieta, thanks so much for taking the time to chat with us and share

413
00:48:03,280 --> 00:48:07,920
a little bit about what you're up to and your recent presentations at CVPR.

414
00:48:07,920 --> 00:48:37,840
Thank you so much for having me. Thank you, bye bye.

