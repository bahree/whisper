WEBVTT

00:00.000 --> 00:20.000
All right, everyone. I am on the line with Patrick Heimbach. Patrick is a professor at the University of Texas. Patrick, welcome to the Twoma AI podcast.

00:20.000 --> 00:32.000
Thank you, Sam. Thank you very much. I'm excited to be on the show and I'm excited to have you on the show. You are a professor in the Department of Geological Sciences.

00:32.000 --> 00:55.000
But you are also speaking at a differential programming workshop at the NURPS conference. So you're doing a little bit of work in machine learning. Love to have you start us off with an overview of your background and how you came to work in machine learning applied to geological sciences.

00:55.000 --> 01:15.000
I'm an oceanographer by training a physical oceanographer and also a computational oceanographer. So what do we do as computational oceanographers? We don't go out to see. Unfortunately, instead what we do is we try to simulate the ocean circulation on large computers.

01:15.000 --> 01:32.000
The way you can think about this, you know, take your numerical weather forecast and these forecast models. And instead of simulating the atmospheric circulation on a computer, we're doing the same thing basically with the global ocean circulation.

01:32.000 --> 01:51.000
There's several, there's a lot of challenges between computing here. It's a very complex system. Some of the scales that we are trying to simulate and resolve are actually smaller than the corresponding scales in the atmosphere.

01:51.000 --> 02:03.000
For example, if you think about large scale weather systems, they can be on the order of scales of several hundreds to a thousand of kilometers.

02:03.000 --> 02:27.000
The corresponding systems in the ocean are much smaller. It can be on the order of 10 to 100 kilometers. We call those mesoscale eddies just as an example. So it is a surprisingly challenging problem to compute. It takes up. We can easily fill some of very large supercomputers to try to increase the process that we're trying to resolve in these simulations.

02:27.000 --> 02:44.000
That makes this problem very compelling for certain aspects of machine learning. We of course, being simulation models, everyone who knows about models, they come with certain uncertainties in terms of their inputs.

02:44.000 --> 03:01.000
So methods of trying to calibrate models and how we constrain models using observations. There are a lot of correspondences between algorithmic computational methods and how we do this with machine learning approaches.

03:01.000 --> 03:14.000
That is my interest here is really in making connections between these methods and trying where we can leverage both worlds of simulation based science and scientific machine learning.

03:14.000 --> 03:32.000
So there are specific areas that you're using machine learning in your research. This is still sort of spinning up and ongoing. So what the way how one is this is could be framed in the community is sort of physics guide machine learning.

03:32.000 --> 03:46.000
There is a recognition that in the in simulating the ocean or the atmosphere or broadly the climate system. There are certain governing equations that we know very well.

03:46.000 --> 03:57.000
So these are conservation laws. We know conservation of mass conservation of momentum conservation of heat and salt in the ocean. Those are exact conservation laws.

03:57.000 --> 04:20.000
So there are other aspects as we are trying to solve these governing equations. There are two types of uncertainties that come in the one is that are what are called constitutive equations constitutive laws, which relate to, you know, knowing the viscosity of the ocean circulation, the diffusion so ocean mixing is something where we don't have a first principle law.

04:20.000 --> 04:40.000
So we have to put in empirical relationships. A second one is what we call subgrid scale parameterizations, what does that mean that means that when we put the ocean on a computer, we basically look at, we discretize sort of the globe, right, we put it on localized

04:40.000 --> 04:59.000
and so one of these grid points or cells represents the average quantities in that in reality could be somewhere that's a 10 kilometer, everything that is smaller, that's just an example actually depending on the problem that you're trying to compute, you can go to

04:59.000 --> 05:14.000
we call higher resolution, so you can go to one kilometer resolution or even finer scale for very, very localized process models, but there are the other and there are also simulations that are even coarser result, right.

05:14.000 --> 05:39.000
So we, for example, interested in past climate over the last 1000 years or even 10,000 years as we compute sort of processes over the de-glaciation periods of time, we have to, we cannot afford run just computationally, it's just very expensive takes a lot of time, so we just cannot run at these very fine resolution, so we go to even coarser resolution.

05:39.000 --> 06:08.000
So the processes that are happening at these finer scales are, we are trying to find functional forms to express the impact of the small, these fine scales onto the resolved scale, and this is the process of a parameterization, it's a functional form that basically say, give me, give me the scales that are resolved and through this functional form, I can give you an estimate on how

06:08.000 --> 06:15.000
whether the non-resolved turbulence, what this property might mean.

06:15.000 --> 06:33.000
You can now take these subgrid scale, these parameterizations that are uncertain, and you can abstract those and say, well, this is, in some sense, just a functional, it's a surrogate model, because I do not know the exact equation, it's just a functional form of that model.

06:33.000 --> 06:53.000
It's out, because these are empirical functions, we're trying to, of course, have physical insights on how these parameterizations should be formulated, but they can, these functional forms can be very complex, we don't know their functional forms exactly.

06:53.000 --> 07:08.000
Over there, these come with some tunable parameters that we seek to calibrate, and again, there is uncertainty in how those parameters, what values they might have.

07:08.000 --> 07:34.000
So in, so machine learning now, we're exploring, the question is, can, can machine learning help us here? It turns out, and there has been a very nice paper by colleagues at Caltech, Tapio Schneider, who in 2017, you know, raised the issue, not in the context of the ocean, but in the context of the atmospheric model, the uncertainties and what we call cloud parameterizations,

07:34.000 --> 07:43.000
and the fact that some of these parameterizations are computationally very expensive, they take a significant fraction of a time step of the model.

07:43.000 --> 07:55.000
So as we, times that these model forward from time t to time t plus one, there is a lot of computation going on as we're solving the governing equations.

07:55.000 --> 08:09.000
The fraction that goes into these parameterizations can be fairly large, right, and at the same time, it's also the most uncertain part of our governing equations, unlike the conservation laws, which we know are exact.

08:09.000 --> 08:38.000
And so machine learning, so asks the question, can we find much more efficient surrogate models, which could be deep neural networks or neural ordinary differential equations that can be trained or calibrated, can we replace these empirical complex functional forms by some much more efficient surrogate models of other types.

08:38.000 --> 08:53.000
In order to, first of all, calibrate them better and also to then accelerate the computation of the overall model in which we insert these surrogate models.

08:53.000 --> 09:07.000
To try to replay that making sure I understand you've got the simulations that you're doing at these grid scales, they can be, you know, tens of kilometers or kilometer resolution.

09:07.000 --> 09:32.000
And as part of your simulations, you also want to have a sense for what's happening at the subgrid scale between points on the grid that you've averaged historically, you would use some type of parameterize function, you know, either one that you have a kind of a tight close form understanding of or one that's, you know, complex and harry and has a lot more parameters.

09:32.000 --> 09:45.000
And those are computationally expensive to figure out what those parameters are. And so an alternative is to use machine learning to learn.

09:45.000 --> 10:02.000
It sounds like, you know, in one sense, learn the parameters, but alternatively to learn alternatives to those functions as a whole and not necessarily try to map to the close form parameters, is that right?

10:02.000 --> 10:21.000
That's a great point. So there are different ways to look at this problem. Exactly one is to learn the parameters from separate data set that we have to generate some how and we should actually address that question, maybe later, how do we actually train those functional forms or surrogate models or deep neural networks.

10:21.000 --> 10:49.000
And alternative is indeed an approach is try can we learn the functional forms and, and there are efforts going on in that space as well. So here, you know, one of the leading figures in our field here to learn, sort of consistent physical parameterization as a colleague, Laura Zana at NYU, who's actually been looking in different directions and one of those is actually saying, OK,

10:49.000 --> 11:09.000
we want to be the deep neural networks are sort of functional forms that are that do not by themselves have physical properties that, for example, closing properties such as energy or momentum and so on.

11:09.000 --> 11:22.000
Instead, we can we can formulate other surrogate models or other functional forms where we build in from the outset certain derivative operators.

11:22.000 --> 11:51.000
There are versions operators, station operators and everything that we have in our toolbox on how we formulate these these differential equations and we try to we have we allow this functional form to have a number of expressions and machine learning would discover which one is the one to choose based on the data that we that we have to in order to learn which which of these

11:51.000 --> 12:08.000
operators, for example, that's, I mean, my work, I have not gotten anywhere near as far as that work, but this is, for example, work that, that, yeah, Professor Zana at NYU is pursuing.

12:08.000 --> 12:23.000
Speaking at the differential programming workshop and so that is related to what you were just describing you've got these differentiable operators that you're using even in the way that you formulated the problem.

12:23.000 --> 12:32.000
Can you talk a little bit about that workshop and the ways in which differential programming is expressing your work.

12:32.000 --> 12:50.000
Maybe take a one or even two step backs on how so for the for the central role of differential programming and related automatic differentiation plays and so we so we have these models and let's say even in a conventional form if you just

12:50.000 --> 13:04.000
talk about without any machine learning, let's just take the just the forward model. We the big challenges in our community is how do we so these model have uncertainties and these are coming from initial condition.

13:04.000 --> 13:11.000
So we have to in order to solve these governing equations or partial differential equations, we have to provide certain inputs.

13:11.000 --> 13:23.000
These could be the initial conditions or the state from which I begin my integration. They are uncertain because we don't have full observations of the ocean everywhere at a single point in time.

13:23.000 --> 13:44.000
The next type of uncertainty is we have the surface of the ocean plays in the atmosphere. What we call forcing the ocean circulation plays a very important sort of driver of ocean circulation. The winds exert forces on the ocean heat and freshwater.

13:44.000 --> 13:53.000
Determine you know the evolution of the ocean circulation so we call those surface boundary conditions and again these are uncertain.

13:53.000 --> 14:13.000
The third one is the one that we've already talked about the parameterizations and model parameters are key ingredients in our model right and so we need to calibrate those parameters and this parameter space can be potentially very very high dimensional 10 to this 5 10 to the 6 dimensional parameter spaces.

14:13.000 --> 14:25.000
And we need ways to basically find values and potential uncertainty associated with those values for each of these parameters and so on.

14:25.000 --> 14:31.000
So now we are that that's the one part of the sort of of the equation.

14:31.000 --> 14:43.000
The second part of the equation is that the ocean more so than the atmosphere but many problems in geosciences are actually not big data problems.

14:43.000 --> 14:55.000
We have a lot of data right so in the atmosphere we even have a lot of data in the ocean satellite data providers within unprecedented view of near surface properties of the ocean.

14:55.000 --> 15:06.000
Autonomous robots that are now probing the ocean interior which again give us an unprecedented view since around 2005 and so on.

15:06.000 --> 15:20.000
But even collectively compared to the complexity of the system and its time evolution those data are still have to be considered as isn't quite far sampling of the ocean circulation.

15:20.000 --> 15:38.000
So now the question is what do we do with that so we have this knowledge reservoir of observations they are very disparate they come from satellite measuring one property in situ like this profiling floats or ships measuring other properties they are heterogeneous.

15:38.000 --> 15:53.000
The model the physical model provides us with another knowledge reservoir namely it's sort of an numerical implementation of the equations of motions for which we do have information and that provide other very strong constraints.

15:53.000 --> 16:16.000
So how do we bring those two together and that the broad the general field of doing this is called inverse methods or in our fields atmosphere ocean science is called data assimilation right so we're we're bringing together we're trying to say given these observations can we find an algorithm that formally

16:16.000 --> 16:31.000
calibrates in you know we calibrates the parameters that we're looking for or in tells gives us the optimal initial conditions in order to simulate that get the best possible simulation of the ocean.

16:31.000 --> 16:59.000
The simulation in earth and ocean sciences is kind of a long held technique but you need to those simulations are parameterized and you also importantly need to have some kind of initialization and so this inverse modeling problem is starting from observed sparse data and figuring out what the right parameters are for your simulation.

16:59.000 --> 17:21.000
But it's starting from one way to look at the inverse problem is to basically find the right or optimal parameters that would be sort of a calibration problem that we're solving invert for the optimal parameters and related also to find the optimal initial conditions that will give us then for which the model will basically give us the best fit to the observations.

17:21.000 --> 17:50.000
And this method has been extremely successful in numerical weather prediction where it's called data assimilation and to do this problem well to do this one approach one very comprehensive and you know approach to do this is to find the gradient right so we formulate this by saying okay the miss bit between the simulated state of the ocean and the observed state of the ocean is a miss

17:50.000 --> 18:03.000
and we could call that a miss bit function we do a weighted these squares machine learning something very similar arises as the loss function right so there is a first there is a first tight connection between those two.

18:03.000 --> 18:19.000
The second tight connection is one way to do the optimization how do I find the optimal parameters is to find the sensitivity of all of the the miss bit function with respect to everything that you're allowed to vary.

18:19.000 --> 18:39.000
For example them all of these input parameters again they can be the model parameters like mixing they can be the initial condition a very powerful way to compute those sensitivity which is mathematically is a gradient very high dimensional gradient is through an adjoint model.

18:39.000 --> 19:03.000
And the next type correspondence we have here with machine learning is back propagation in fact you can mathematically those two are well if not equivalent very very closely connected in machine learning you are using back propagation to find the sensitivity of your loss function with respect to the weights that you're looking to train.

19:03.000 --> 19:20.000
In an adjoint method for for partial differential equations you're looking for the sensitivity of your miss bit function with respect to the parameters that you seek to calibrate and then in both cases basically through an optimization method.

19:20.000 --> 19:41.000
You're then trying to find the optimal weights for your for you machine learning algorithm for the optimal parameters for your calibration problem and this is where this connection becomes very apparent and this is where differentiable programming basically enters the enters the equation.

19:41.000 --> 20:10.000
Because in the in both cases the sensitivity right we're looking at a derivative right so we're asking the how does the output change when I change certain input in a certain manner and the power of back propagation or adjoints is that you can compute this gradient information you can compute all of the sensitivities for very very high dimensional problems.

20:10.000 --> 20:39.000
You can compute them in one step of a back propagation or an adjoint model in one adjoint integration and this is the central power of these derivatives right and the next question is how do I get those derivatives how do I get the adjoint model or how do I get the back propagation and this is now where automatic differentiation enters so we can show so the

20:39.000 --> 21:08.000
we can use automatic differentiation in the same manner that we use it in machine learning but we say well we have our our network and automatic differentiation gives us a form using the chain rule and using you know certain with methods that can write us down for us without without us having to write down a new sort of code for

21:08.000 --> 21:26.000
propagation operator the automatic differentiation can do this for us and it's the same we actually have done this in the non without using machine learning we actually have been doing this in the 4th round world sorry to

21:26.000 --> 21:43.000
say this to our readers right of course we can we can we'll come to Julia of course in the moment but we are with the group it's a NASA funded project called estimating the circulation and climate of the ocean we actually have been using differential programming

21:43.000 --> 21:55.000
methods in particular automatic differentiation within the 4th round world for almost two decades now and we've been very successful in doing global ocean data

21:55.000 --> 22:16.000
simulation using the technique of automatic differentiation computing these gradient information in order to then do these optimization problems and so moving forward we seek to take advantage of two

22:16.000 --> 22:36.000
emerging developments the one is of course maybe let's say three the one is Julia itself right as an emerging language that is fairly new it is it has this very nice properties that it solves or this two language problems you know on the one that is both a powerful

22:36.000 --> 23:00.000
language for high performance computing and it's at the same time sort of it's sort of mimics a little bit like a scripting language which is sort of the power of Python to some extent and so it's come it's a bit like a compiled language that comes in the disguise of a scripting language and so for that Julia is is really on sort of a rising star and in particular

23:00.000 --> 23:13.000
here towards high performance computing the second thing that Julia has that I'm excited I have to say I'm not a Julia person by training right so my work for the last two

23:13.000 --> 23:26.000
decades have been using 4th round but what is exciting about the development in Julia is that from the outset the Julia developers have really taken the notion of automatic differentiation

23:26.000 --> 23:49.000
on board and the way how Julia is being developed is to with this aspect of composability that you're trying to develop the language with the idea that derivative information especially for simulation base code is an important part of of the simulation

23:49.000 --> 24:08.000
basically implementing methods of automatic differentiation has been promoted in a very strong sense so this is these are this is on the Julia base on the on the earth science base or there's has been some recent exciting developments

24:08.000 --> 24:24.000
notably there's this climate modeling alliance that's led out of Caltech by Tapia Schneider and the group in both also colleagues at MIT and and maybe postgraduate school

24:24.000 --> 24:40.000
they're redeveloping a atmospheric model in Julia and also and then endowing this model with methods of exactly doing very targeted parameter calibration for cloud

24:40.000 --> 25:01.000
parameterization so the idea again is can you be very powerful at replacing existing extremely complicated very compute and sensitive and uncertain for our primatizations with much better calibrated surrogate models through machine learning so that's kind of a core piece

25:01.000 --> 25:29.000
and the MIT group is doing something similar in the on the ocean component and so in this part what's done right now there is no ad or differentiable programming in right now what we're coming to the into the into the play here is that we said look you know we will take advantage of the fact that this model is basically newly built

25:29.000 --> 25:43.000
and of ad capabilities to do two things extend ad capabilities that currently exist in Julia to what we might call really general purpose automatic differentiation so to be really

25:43.000 --> 26:01.000
agnostic about the type of problems that we're trying to solve with it whether highly targeted machine learning algorithms or to solve PDEs but basically develop this to a very versatile general purpose ad in order to be able to digest these very complex ocean circulation models

26:01.000 --> 26:21.000
and of course then apply this to the ocean circulation models to then have at the end of the day we have an infrastructure where we can seamlessly integrate the simulation based portion of the model which is the solving of the governing equations

26:21.000 --> 26:41.000
with the machine learning portion that we're basically all live within the same language within the same compute infrastructure within the same modeling infrastructure and potentially then also within the same derivative model as well sort of this this gradient information so we can do

26:41.000 --> 27:01.000
what's done now right now is basically people are taking out portions of a model they say well I take this functional form this parameterization I develop I calibrate this in isolation with this regard for the rest of the model and then I pluck this best in this back in and then I see how the model performs and so and this

27:01.000 --> 27:17.000
we think that expanding this to a much more integrated framework allows us to potentially perform better and and do more things that cannot be done in isolation sorry it was a very long

27:17.000 --> 27:35.000
innovation for pulling out those kind of modular components one based on computational cost and expense or is it more at the programming level and you know system architecture and trying to be able to conceive of the way the pieces fit together more cleanly

27:35.000 --> 27:53.000
a little bit of both but I think based mainly what you say with what's the idea here is that this parameterization often target very very specific processes like you know resolving certain mesoscale features turbulent features in the ocean or clouds in the atmosphere

27:53.000 --> 28:10.000
and we basically we want to have some very specific aspects of the physics physics be represented by these surrogate models and so what we do now is we pull these aspects out and now the question is how do we actually calibrate those

28:10.000 --> 28:30.000
models right so let's say and the one of the remaining challenges in oceanography and atmospheric sciences we we actually have not sufficient data even at the small scale to potentially constrain them at the level that is really satisfactory

28:30.000 --> 28:50.000
but one other thing we can do is because they are now we're in a very limited domain right so we're saying well we're we're doing simulation on in a box 10 by 10 kilometer but now within this localized simulation we're resolving the heck out of this simulation where we basically can go down to

28:50.000 --> 29:05.000
say a meter resolution or 10 meter resolution and we can we can computationally this becomes tractable to resolve certain scales that the let's say a global scale model would never be able to resolve

29:05.000 --> 29:29.000
and the hope is that by resolving those scales that provide us with sufficient data to then provide the required constraints in order to calibrate the parameters on the of the isolated surrogate model that we can then plug back into the full model that is one that is one way that is one approach of doing this

29:29.000 --> 29:52.000
description of kind of taking these modules and and it sounds like the the thing that you're trying to isolate is some set of physical properties you know these parameters or this surrogate model is related to you know some kind of physics based mechanism that is relatively isolated from other things

29:52.000 --> 30:12.000
and it prompted the thought I guess thinking about generalization and the machine learning sense and thinking about the you know the vastness of of oceans I'm wondering if you have situations where you might want to create

30:12.000 --> 30:27.000
models specific to a kind of physical region of an ocean because it's just so different from other regions of the ocean or do you find that kind of the governing physics mean that the models can be the same

30:27.000 --> 30:48.000
but that's a great question our ambition is that the governing equation provide sort of enough information to be applicable globally that is certainly our ambition and and to some extent that is being attempted and done but you're absolutely right there are regions and mixing is a perfect example in case

30:48.000 --> 31:07.000
so there in certain parts of the ocean are mixing is controlled by one set of processes let's say near the surface you know atmospheric input the input of the wind into the ocean turbulent mixing enhances mixing the near surface dramatically

31:07.000 --> 31:30.000
then again mixing near topography you can think of it like well you know lee wave so you come you know the ocean circulation basically encounters a topographic obstacle and then basically basically passes past it and it basically in its wake it basically spawns off these waves that are lee wave and those we know

31:30.000 --> 31:47.000
from localized regional measurements for example are enhanced mixing quite a bit and there and so there are a number of different processes that act locally in very different ways so they are triggered by different physical processes

31:47.000 --> 31:59.000
and ultimately of course the model we want to incorporate all of these processes in the global model but indeed so for for example one notion of taking local configurations

31:59.000 --> 32:19.000
to basically resolve these processes targets exactly what you're describing right so we say well in this region a certain set of physical processes interaction between the topography and the oceanic flow provides a certain set of turbulence of mixing that we are seek to constrain better that may not play a role in other parts

32:19.000 --> 32:44.000
another part is of course regions polar regions the Arctic were sea ice and how in of course in polar regions and for example in the Arctic the distinct processes of sea ice formation where we are growing sea ice by the freezing of the salty water and it implies the rejection of salt brine underneath the water column

32:44.000 --> 33:13.000
there's a very detailed processes that again of course pertinent only to regions where sea ice cover occurs and not in other regions and of course we can we have people are doing regional simulation that will target only the polar regions in order to simulate those regions and again it's another it's another theme which then itself to the question is of empirical models we have what we call sea ice biology

33:13.000 --> 33:40.000
so the material properties of sea ice is a phenomenological aspect there are no first principle governing equations of that and in fact there instead these are empirical relationships that come with empirical parameters that need to be calibrated that need to be tuned and again the question is how do we best take advantage of observations in order to calibrate those models

33:40.000 --> 33:56.000
the question is can we use machine learning to aid here or to complement some of the simulation based approaches with machine learning through the training again of surrogate models of or neural networks

33:56.000 --> 34:18.000
the way you put that prompted another question on my part and that is the we've talked about using ML to identify the parameters themselves we've talked about the using surrogate models are there other ways that machine learning can be used to support scientists in this simulation problem

34:18.000 --> 34:40.000
yeah I mean there's a number of different ways of course you know in if you were in the pure in the observation space there are issues with regard to classification detection of certain properties in the project that we have so we are looking at the ocean as a prime example

34:40.000 --> 35:08.000
we also look at ice sheet so there is a component where a colleague at Dartmouth College is developing an ice sheet model so that ultimately will simulate the green and an Arctic ice sheet and again there are questions for example regarding carving law so how do ice glaciers that basically spill out into the ocean how do they carve what are the processes can we detect the border between when these glaciers are in the water

35:08.000 --> 35:22.000
between when these glaciers so as long as they're grounded sitting below the sea surface some of them almost a thousand meter below the sea surface and then when do they go from grounded to floating

35:22.000 --> 35:32.000
and when do they carve right so there are a number of questions you know again how how can we use satellite data to detect first of all what are called these grounding lines

35:32.000 --> 35:48.000
how do we use can find empirical or improve the detection of empirical relationship between environmental processes and carving right so when does carving occur it's an it's an outstanding it's a grand challenge question

35:48.000 --> 36:16.000
and there may actually be progress we made because we can observe some of these processes from space but there's actually other processes for example below the surface of the oceans where we have no observational evidence for so it's a it's a it's a very very challenging problem where we're hoping to apply techniques or again I mean our goal in our project is really to incorporate to bring together techniques from

36:16.000 --> 36:36.000
inverse methods or data estimation which is kind of suits itself well for sparse data with machine learning methods which again suits itself very well for big data right so bringing those two worlds together in a unified framework is what we're after

36:36.000 --> 36:46.000
fantastic well Patrick thanks so much for joining us to share

