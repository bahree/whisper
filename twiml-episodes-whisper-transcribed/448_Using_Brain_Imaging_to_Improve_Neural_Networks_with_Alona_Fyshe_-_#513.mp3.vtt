WEBVTT

00:00.000 --> 00:24.000
All right, everyone. I am on the line with Alana fish. Alana is an assistant professor at the University of Alberta. Alana, welcome to the Twimal AI podcast. Yeah, cool. Thanks so much for having me. I'm excited to be here.

00:24.000 --> 00:37.000
Same. I'm really looking forward to our chat. I'd love to start by having you share a little bit about your background with our audience. How'd you come to work in kind of this intersection of computer science and psychology?

00:37.000 --> 00:42.000
Yeah, cool. So maybe I'll start way back when I was an undergrad.

00:42.000 --> 00:54.000
I was really interested in genetics. So I'll tell you about how I got into machine learning. I was really interested in genetics. But I was actually a computing science student. So I would take genetics classes for fun and also computer science for fun.

00:54.000 --> 01:03.000
And then one day I came across this book that told me that you could use computer science to do genetics. And I was like blown away. It was the most awesome thing.

01:03.000 --> 01:26.000
And I found a research group at the university. I actually did my undergrad at the University of Alberta. I did a found a research group at the University of Alberta and started working with them. And then I found out that you could actually build non deterministic programs with computers. I actually had never occurred to me, but you could like build a computer program that you didn't know what it would do because part of the program and comes from data.

01:26.000 --> 01:33.000
It was like also mind blowing. So I was just really excited about machine learning at that point.

01:33.000 --> 01:42.000
And I went off to do actually to work at Google for a few years, which was a lot of fun. And I learned a lot. But I felt like it wasn't quite as intellectually stimulating as I wanted it to be.

01:42.000 --> 01:45.000
So I went back to grad school.

01:45.000 --> 02:00.000
I had Carnegie Mellon, which was an amazing place to be. And it worked with Tom Mitchell. And he at the time had a, he still does have a research group working on brain imaging and comparing brain imaging to computer models of language.

02:00.000 --> 02:10.000
So we take brain images while people read. And we look at the representations we can detect with brain imaging and compare them to computer models of language.

02:10.000 --> 02:17.000
And we have a lot of neat things you can discover when you start to compare those things together. That was sort of the beginning.

02:17.000 --> 02:26.000
Awesome. I suspect we'll be talking about these brain images and some of the results that come out of your work with them in quite a bit.

02:26.000 --> 02:35.000
So maybe it might make sense to kind of dig into, you know, what the imaging process is and, you know, what the images look like, those kinds of things.

02:35.000 --> 02:46.000
Yeah, so typically we would have somebody come into, there's multiple ways you could do brain imaging. So FMRI EEG or MEG.

02:46.000 --> 02:56.000
But each one of those is just a machine that way you go into a room and we put it on your head. And it captures some aspect of the brain's response to whatever you're looking at, whatever you're hearing.

02:56.000 --> 03:15.000
So sometimes we have people read, sometimes we have people listen to text and their your brain responds as you do those tasks. And just like a neural network creates representations as it's doing whatever task it's doing has some input does some computation and there's some sort of mid points in those computation that we call hidden representations.

03:15.000 --> 03:41.000
The brain does the same thing has its input and it's doing some sort of computation and we can detect those mid points of the computation with brain imaging. So the big sort of discovery out of Tom Mitchell's lab, you know, a little over a decade ago now was that there's a relationship between the representations that we build in single word models of language and the brains representations to single words and isolation.

03:41.000 --> 03:49.000
And so from there, it's been expanded to, you know, the LSTM's and transformers that you're probably talking about on your on your podcast and also to computer models of vision.

03:49.000 --> 04:02.000
And people look at images, the representations that their brains make those intermediate representations actually map on to convolutional neural networks really well, which I think is a neat, a neat sort of fact.

04:02.000 --> 04:25.000
When you talk about the representations that the brain makes, what is, what does that mean? What does that look like? Are we talking about, you know, pixels or points in a 3D, you know, whatever the image scan format is or is there some higher level representation that is created kind of based on the knowing the structure of the brain.

04:25.000 --> 04:37.000
Yeah, so it depends on the modality. So if you're doing FMRI functional magnetic resonance imaging, then you would actually get a 3D volume of the brain. So it's like a 3D picture and we call each one of the pixels in that picture of voxel.

04:37.000 --> 04:49.000
It's a volumetric pixel and those voxels represent sort of the change in blood oxygen as your brain is doing work. And so different parts of your brain as you're doing different tasks will do different amounts of work.

04:49.000 --> 05:00.000
And we, so it's sort of like a byproduct of the work your brain is doing, we can detect with FMRI. So we can see which parts of your brain are working more or less depending on what you're doing.

05:00.000 --> 05:13.000
And it's sort of, it's a slow moving signal. So FMRI, we get about one sample every one to two seconds and it takes actually about six seconds for your brain to have its peak response to a stimuli.

05:13.000 --> 05:39.000
So if I show you a picture, I actually need to look at your FMRI image six seconds later to see the peak response to your that image EEG or electroencephalography is a different kind of brain imaging that gives me a time series from different sensor locations I put on your scalp to put a bunch of sensors over your scalp. And then I read it from those sensors as I show you images or have you read.

05:39.000 --> 05:52.000
And the sensors have much better time resolution than FMRI. So instantaneously as soon as your brain reacts to something EEG recognizes the electrical field caused by neurons firing.

05:52.000 --> 06:06.000
And so it's very good time resolution, but EEG has worse spatial resolution than FMRI. So sort of like a trade off. But if we're talking about FMRI, we're talking about voxels. And if we're talking about EEG, it's like a whole bunch of time series sensor readings that come up.

06:06.000 --> 06:19.000
And so it's with FMRI, we have a bit more of the anatomical characteristics of the brain that are visible with EEG is more of a general area of the brain that we think the signals coming from.

06:19.000 --> 06:36.000
And then kind of circling back to that question, when the representation is just the data coming out of the sensors, or is there some higher level thing that comes out of the processing of those signals that you call the representation.

06:36.000 --> 06:56.000
No, we actually, so that some of the simplest models are just straight up regression models with the data that comes out of the EEG, for example, it's not anything complicated. So it really is there is a linear relationship between what we can capture with FMRI and EEG and computer models of language of representations that they create.

06:56.000 --> 07:14.000
It's nothing, you know, magic or complicated. It's a pretty straightforward mapping. Nice. And so you kind of talk about your research broadly as, you know, studying the brain to better understand machine learning and studying machine learning to better understand the brain.

07:14.000 --> 07:22.000
I'd love to hear you talk a little bit about the relationship between those two worlds and, you know, how you've seen it evolve over the years.

07:22.000 --> 07:39.000
Sure. Yeah. So, Tom's research focus mainly on adults and fluent adults. And so I think there's actually a lot of directions that you can go and you take that as your starting off point, you can remove the fluency component.

07:39.000 --> 07:50.000
So like what happens if we start to do these experiments where we have people experiencing a language that they don't know at all. So they're learning a new language on the fly.

07:50.000 --> 08:01.000
What do the representations look like? Can we see the representations of their native language showing up as they learned that a new word maps on to their word and their native language.

08:01.000 --> 08:13.000
So we did an experiment with some of my colleagues at the University of Victoria, where we showed that people can, as they're doing a language learning task with a EEG us recording EEG.

08:13.000 --> 08:26.000
We can actually tell that they are learning the meanings of the words and they are actually calling up the English, for this case, it was English, the English meanings of the words when they see these symbols in in a new language.

08:26.000 --> 08:39.000
So on the fly, we can see these representations showing up. So you can move to, you can remove the fluency component. You could also say, instead of studying adults, let's try in children or infants and that's something I'm also working on now.

08:39.000 --> 08:54.000
So it's unpublished, you know, looking for the similar sorts of word representations in the infant brain, even before infants can talk, we can see, we have evidence that they understand language and so can we see the representations also.

08:54.000 --> 09:10.000
So that's sort of how we use computer models of language to understand the brain so that they allow us to see word representation in cases where it was not totally clear you'd be able to see it. So for example, in word in language learning and infants.

09:10.000 --> 09:19.000
And then how can computer models, how can the brain improve computer models, I think is actually a really interesting future direction.

09:19.000 --> 09:30.000
If we think of the brain as producing these intermediate representations along its way along the computational route that has to do, for example, when it when it sees an image.

09:30.000 --> 09:36.000
So you see an image, you do something with your brain and you get to the point where you understand within the image.

09:36.000 --> 09:49.000
And what we can measure with something like FMRI EEG are those intermediate stages along the way from the input to the understanding so we can we can understand those we can see those representations using brain imaging.

09:49.000 --> 09:57.000
And can we use those representations to improve computer models of vision, for example.

09:57.000 --> 10:07.000
So those are sort of the two directions.

10:07.000 --> 10:32.000
FMRI would be a big inhibitor to the kinds of things I'm hearing you do with that data, is it, is it a linear time lag like it's, you know, it's just not real time is just six seconds behind, but you know, otherwise from, you know, point to point their true representations of what happened about six seconds ago or does it get more complicated than that.

10:32.000 --> 10:50.000
So off to me like, well, so there's a couple ways you can handle that one is that you can show people a word give like a good lag in between the words you show people, so if you give a good lag in between the words you show people then he sort of self start part of that problem, but natural language comes at you around two words a second.

10:50.000 --> 11:02.000
So what are you going to do if you're, we call them a volume, one of your FMRI images is two seconds of data from six seconds ago, what are you going to do it's going to contain four words.

11:02.000 --> 11:12.000
And so, you know, there's lots of ways you can handle it, but one way is to just treat it as if all four of those words are happening at the same time during that time segment.

11:12.000 --> 11:24.000
Yeah, and that's often what what people do, but it's a downfall of FMRI, but it's also, I mean, you get the spatial resolution with FMRI, which is really nice.

11:24.000 --> 11:52.000
So you provide a little bit, a little bit of an overview of using models to kind of detect, identify word, meaning in a couple of different scenarios, you know, tell us a little bit more about the, you know, the models that we're talking about and, you know, how they're applied.

11:52.000 --> 12:02.000
That's where we were looking at word meeting as you're learning words, we actually just use word to that, which at this point is a little bit of an old model that it's, you know, tried and true.

12:02.000 --> 12:16.000
So it's just word to that so a neural network that's trained to predict context words, given essential word. So if I tell you the, the central word of a sentence, I want you to tell me the probability of seeing particular words nearby.

12:16.000 --> 12:26.000
Hidden representations of a neural network learn trained to do that task, actually correlate well to that we knew correlated well to adults who were fluent in the language.

12:26.000 --> 12:30.000
And so the question was, can we do the same thing for adults as they're learning any language.

12:30.000 --> 12:42.000
And then the method for going from that representation to the brain is actually just like a simple ridge regression. So it's like a regularized linear regression.

12:42.000 --> 12:46.000
It's actually, you know, a really simple technique.

12:46.000 --> 12:55.000
And so what are your, your labels in the case of you've got the basically your signals, your sensor readings.

12:55.000 --> 13:03.000
And you are trying to predict the word.

13:03.000 --> 13:14.000
So you know the word that's spoken and the sensor reading. So those are your features and your labels and the how are you using the word to back the embeddings.

13:14.000 --> 13:21.000
Yeah, so you can do it in two ways, but I usually the input to the regression model would be, for example, an EEG recording.

13:21.000 --> 13:32.000
And we would train an independent regression model to predict each one of the dimensions of the word to back vector. So like 300 independent regression models predicting the 300 dimensions of the word to back vector.

13:32.000 --> 13:43.000
And now I have a predicted vector predicted word vector for an EEG image. And I also know the true word vector. And so from there, we can do something like calculating the correlation.

13:43.000 --> 13:57.000
Or we can rank all of the words that we know will be could be shown and then see where the true word is ranked in, you know, when the list is ranked with respect to the predicted vector distance predicted vector.

13:57.000 --> 14:02.000
So sort of like how good is our prediction to the true word vector is the thing we're.

14:02.000 --> 14:10.000
And so we can get like fairly good accuracy on that. It sort of depends on the multiple factors, but.

14:10.000 --> 14:23.000
You know, depending on the experiments EEG, we found the accuracy is to be a little lower, but around 60% sort of rank accuracy, you would say. So like where in a list with this particular item show up if we ranked.

14:23.000 --> 14:44.000
All of the possible words, we see it around 60% from the bottom, you know, so it would be 50 would be at like chance. And so it's like slightly above chance, EEG, we found to be a little bit more noisy than something like FMRI or an EEG, which is a sort of a better EEG.

14:44.000 --> 14:53.000
And then you also mentioned that you've done similar experiments where you're incorporating vision into the mix. Yeah, talk a little bit more about that one.

14:53.000 --> 15:01.000
Yeah, so they're just like we had seen that there are correlations between computer models of language and the brain while it reads.

15:01.000 --> 15:21.000
There had been studies showing that there are correlations between the brain looking at images and computer models of vision. So this had been shown originally in monkeys. So a direct neural recordings from monkeys and then, but then later shown in humans using FMRI they showed that if you show, for example, an image from image net.

15:21.000 --> 15:33.000
And then look at the representations, the different sort of voxel readings you get out of the human brain at different visual areas of the brain. You can correlate those to the different layers of a neural network.

15:33.000 --> 15:54.000
I think they used Alex net, which is like a eight layer neural network, CNN, and they found that sort of as you move up through that hierarchy from input to prediction in the CNN, you get representations that correlate to the visual hierarchy in the visual system, the visual system also has sort of input.

15:54.000 --> 16:09.000
And then a hierarchy of areas that sort of read in from previous areas, just like the, just like a CNN. So the lower areas of the visual system correlate well to the lower layers, the layers closer to the input in a CNN.

16:09.000 --> 16:29.000
And as we move up through the visual system, we actually, we find representations that correlate better to hidden representations in the CNN that are closer to the prediction layer. So sort of like this, this computation that starts from images, either on the retina or in actual computer image.

16:29.000 --> 16:50.000
And then moves forward to do some sort of task, like in the brain or like in the CNN, those representations that computation is actually has a relationship, right, like there's even though, like the CNN doesn't know anything about how the brain does vision, it has some inspiration from the brain, but it doesn't know anything about the brain, it actually creates these representations that have a relationship to the brains representation.

16:50.000 --> 17:07.000
So that, I mean, in itself was a bit of a magical moment. And so we took that and, and tried to flip it on its head, we said, so when we are done training a computer vision model, the representations that it learns are more like a human brain than not.

17:07.000 --> 17:21.000
So similar to a human brain, after a strong training. So what does it mean? What would it mean if we said, if we took some representations of, in this case, it was actually monkeys viewing images, if we took those representations and use them to constrain the CNN.

17:21.000 --> 17:30.000
So if we take, if we say to the CNN, after you're done training, you would have a representation that looks like this, you should have a representation that looks like this.

17:30.000 --> 17:47.000
And so while we're training, we're trying to get the representations to look like what we observe in the brain. So we just, we just incorporated a new part of the lost function that says the representations should sort of create a space such that it looks like the brain.

17:47.000 --> 18:03.000
It's the way we did that is, if you take two images and pass them to the CNN, you'll get two representations, hidden representations, and you can compare the distance between those representations. So if there were two pictures of cats, those representations to be similar, pretty similar.

18:03.000 --> 18:19.000
It's a representative, if it's a picture of a house, a picture of cat, they'd be more different. Right. And the same thing you would expect to be true in the, in the monkey brain. So if they see a cat or they see a house, they have different representations, and they have different patterns of similarity.

18:19.000 --> 18:39.000
So what we are doing is introducing a constraint that says the space that you're learning, the hidden representations that the convolutional neural net is learning should respect those, those distance patterns. So you should, you should learn a representation for images of cats that are sort more similar to other images of cats than they are to images of houses.

18:39.000 --> 18:58.000
Sort of like, sort of learn to create a space that respects those distance patterns that we see in the brain. So when we did that, we saw a small, but consistent improvement in performance on object recognition in CNNs.

18:58.000 --> 19:15.000
And we saw it across several different CNN architectures. And it trains a little bit faster. But I think maybe one of the more interesting findings was that we also found that the mistakes that the CNN made. So it makes fewer mistakes when it has this additional brain constraint.

19:15.000 --> 19:31.000
The accuracy is better makes fewer mistakes, but it also makes better mistakes in that the mistakes that it makes if it says, oh, this isn't a cat. Well, what is the mistake that it makes it's going to be more likely to be within the same class of object.

19:31.000 --> 19:36.000
So it's less likely going to confuse a cat with a house than a cat with the dog.

19:36.000 --> 19:45.000
It's because stakes are better. It's mistakes are less bad. So they're more often within the super class, the same super class. Yeah. Yeah.

19:45.000 --> 19:54.000
And so some of the interesting stuff for that is, first of all, it's monkeys. So monkeys don't actually know really had the same conceptualizations of cats and houses that we do, right.

19:54.000 --> 20:01.000
And furthermore, the data we use was actually from a nephatized monkeys. So these monkeys are actually not even conscious of the time of the data recording.

20:01.000 --> 20:11.000
So I think there's, that's a, it was a sort of a proof of concept. And I think there's, there's tons more work to do there. And then we're collecting brain and gene data to do that now.

20:11.000 --> 20:16.000
Because really we should be using awake humans to do the data collection.

20:16.000 --> 20:19.000
Right.

20:19.000 --> 20:37.000
So it makes me think about the, the kind of work that's going into like adversarial attacks. And if this kind of me, it on like the headline is like using human intelligence to help networks, you know, avoid adversarial attacks.

20:37.000 --> 20:47.000
Absolutely. That was something I, so we didn't actually look at that from my recollection that paper, but that was absolutely something was on our radar. So like does it train better? Is it more robust to adversarial attacks?

20:47.000 --> 20:55.000
And so, before we started this interview, we were saying sort of like, well, we've been studying neural networks for a long time. Is that really considering how the brain works? And that's true.

20:55.000 --> 21:00.000
But when we come up against these weird things like adversarial attacks, I think.

21:00.000 --> 21:06.000
Those are the sorts of places where it might make sense to try to think about, try to use brain imaging to improve networks.

21:06.000 --> 21:12.000
So can we use them to be a little more robust to adversarial attacks? Can we use them to.

21:12.000 --> 21:21.000
To do is what we call the long tail, right? Like, so you can get a really long way in machine learning without without actually get a really long way with just rule based systems, right?

21:21.000 --> 21:32.000
But when we start to get into the more sort of edge case things, and it starts to get harder and harder, you have to look a little further, further out to get the improvements to get all the way to say 100%.

21:32.000 --> 21:47.000
And now what you were just describing was you created a neural network for object detection that's kind of conditioned on stuff that we know about the brain.

21:47.000 --> 22:07.000
I thought you said earlier, either before we started recording or since that independent of that kind of conditioning, you also find correlations between the spaces that, you know, again, uncondition vision network will create and what you would see in a human.

22:07.000 --> 22:19.000
Yeah, yeah, so right without, without knowing anything about the human brain, the computer vision model creates representations that correlate to the human brains, but the question is, well, they're not perfect correlations, right?

22:19.000 --> 22:33.000
And so is that extra little bit of push enough to improve accuracy? We saw the answer was yes, and enough to sort of improve it in these other ways, like against make better mistakes, for example.

22:33.000 --> 22:49.000
Also been looking at the relationship between the computer vision models and the representations that they create and the language models and the representations that those models create.

22:49.000 --> 22:51.000
Can you talk a little bit about that work?

22:51.000 --> 23:04.000
Yeah, so we had talked so far about how language models correlate to the brain and computer vision models correlate to the brain. So the next obvious question is, well, what about language models and computer vision models, right? They create representations, how could we compare them?

23:04.000 --> 23:16.000
This is actually a fun paper because it was born out of a miscommunication between my student and I so I asked him to do what in retrospect was a very boring thing and I won't even tell you what it was.

23:16.000 --> 23:23.000
And he came back to me with this result and I was like, that's not what we talked about, but wait a second, that worked.

23:23.000 --> 23:34.000
So he, I had asked him to do something boring, but what he did instead was correlated the representations from word to back with the layers of neural network.

23:34.000 --> 23:50.000
So we actually did like VGG 16 ResNet, Inception, V3 or whatever, something like that. So we looked at the hidden representations that are made through the processing of all of those networks and we compared them to where to back.

23:50.000 --> 24:08.000
So word to back is not a deep network and only creates one hidden representation per word, but it, as you move from input to prediction in several different neural networks, you actually see movement towards representations that look like word to back.

24:08.000 --> 24:26.000
And I say word look like again, it's this idea that if I show a cat and a host to a neural network and I look at the representations that distance between the representations becomes more similar as we move up through the network to the distances that we see in word to back.

24:26.000 --> 24:38.000
So VGG knows that dogs and cats are very similar, but dogs and trucks are different. And we see that the representations follow a similar pattern and that pattern gets stronger as we move through the neural network.

24:38.000 --> 24:47.000
So essentially neural networks are learning semantics, the sorts of semantics we observe in language, even though neural networks know nothing about language.

24:47.000 --> 24:57.000
They only understand concepts and they understand concepts in a discrete way, right, like they actually don't know that dogs and cats have anything to do with each other.

24:57.000 --> 25:07.000
But they do know that dogs and cats show up in images that look similar, right, like dogs and cats often seen sitting on laps often seen with leashes and suspect with.

25:07.000 --> 25:23.000
I guess when you mentioned this before, I said that I couldn't decide whether, you know, this was astounding or kind of what I would expect. And I'm still in the same place with regard to this result.

25:23.000 --> 25:33.000
But let me just try that home then, because I so words of act trained on text, trained to predict context words. I know I've already said this, but let me say it's your audience.

25:33.000 --> 25:43.000
Train on context words, trained to predict context words from a central word only understands words as discrete units and then learns representations to cost them properly.

25:43.000 --> 25:53.000
Computational neural networks take pixels as input and produce like discrete labels for those images and like trained on completely different data sets, completely different architectures.

25:53.000 --> 25:54.000
Yeah.

25:54.000 --> 26:01.000
Show up with the same representations and why you're right is it makes sense because why words are using objects.

26:01.000 --> 26:06.000
Right, there's some representation in the world in theory in the matrix.

26:06.000 --> 26:16.000
Yes, both of these are like smaller representations of the representation and the matrix, I guess. Right. Exactly. So if there is some true semantic model that existed in the world.

26:16.000 --> 26:17.000
Right.

26:17.000 --> 26:21.000
That's what we're measuring when we measure when we use language and when we use images.

26:21.000 --> 26:22.000
Yeah.

26:22.000 --> 26:23.000
Yeah.

26:23.000 --> 26:24.000
I don't know.

26:24.000 --> 26:25.000
It's still incredible.

26:25.000 --> 26:40.000
I think it's awesome. And it's also if you think of it. So we also saw that it's not like a linear trend. It's not like it goes continuously up as we go through a neural network. But rather, there's sort of ups and downs and in particular, some of the like a res net has those residual connections and we saw that.

26:40.000 --> 26:50.000
It seemed like in some cases, some of the residual connections didn't seem to be like the residual connection seemed to be the place where most of the information was coming from.

26:50.000 --> 26:56.000
And so maybe the stuff that was happening inside of the residual block, I think it's called seem maybe to not be adding anything.

26:56.000 --> 27:06.000
And so you could see it being used in a way to improve convolutional neural networks. Like, is this the right architecture choice to make? Maybe we should look at how it relates to computer models of language.

27:06.000 --> 27:17.000
Because, you know, conceptually, we should all be heading towards the same direction. And if we're seeing that parts of our network don't seem to be getting us towards that same representation. Maybe it's not the right architectural choice.

27:17.000 --> 27:21.000
That's an idea of not something we've actually explored.

27:21.000 --> 27:33.000
I think what I'm hearing in that is, you know, if we've established that, you know, these representations are related.

27:33.000 --> 27:42.000
Then, you know, when we're designing vision networks, we can, you know, think of, you know, the reference.

27:42.000 --> 27:55.000
We can think of the representation from the language network as a kind of reference or the representation from the brain, you know, as a reference and kind of constrain the things that the network is doing.

27:55.000 --> 28:04.000
To be improving relative to this reference. And we talked about that and the other research that you did, you've already kind of demonstrated some aspect of that.

28:04.000 --> 28:11.000
But this would be, you know, how do we use it to enhance architecture, for example.

28:11.000 --> 28:24.000
And so that when I talked about the I clear workshop this year was, you can think of what we measure with FMRI as these artifacts of the algorithm. The brain is doing something. We don't know what it is.

28:24.000 --> 28:30.000
But we can measure the points along that algorithm. You can think of it as being able to see the ticker tape and a turning machine.

28:30.000 --> 28:36.000
So I can read the state of the computer. I don't know what the computer is doing, but I can read something about the internal status. It's doing it.

28:36.000 --> 28:45.000
And then I can use those sort of internal state measurements to improve another model that I'm trying to build that should be doing the same thing.

28:45.000 --> 28:54.000
So it's sort of it releases you from this constraint that like I need to understand what the brain is doing in order to improve machine learning models. Actually, that's not true. And I've shown is not true.

28:54.000 --> 29:07.000
We don't need to know what the brain is doing because we can measure these artifacts of the algorithm and use those artifacts to constrain our algorithm, our model that is learning its own algorithm.

29:07.000 --> 29:35.000
The question that comes up for me and all this is, you know, how far do you take your interpretation of these results? You know, just, you know, you clearly were not saying that the representation of the, you know, the neural network is like fundamentally the same as the representation of the brain or something like that.

29:35.000 --> 29:41.000
Maybe there's something attractive to you, you kind of want to be able to say something like that, but it's probably not true.

29:41.000 --> 30:02.000
And I guess I'm curious, you know, as you engage with results like this, you know, how it's impacted your kind of mental model of like the brain and like, you know, do you kind of, you know, there lines that you draw on interpreting results like this, like, how do you think about it all?

30:02.000 --> 30:15.000
Well, so I mean, we definitely have to take a step back and consider that, of course, the brain is not what you talk about the CNN model is doing object recognition, like when a person looks at an image or doing so much more than object recognition, right.

30:15.000 --> 30:23.000
So there's a ton of other stuff going on in the brain and, you know, during language understanding, you're doing also all sorts of other stuff that the neural network is just not doing.

30:23.000 --> 30:41.000
So there's always that caveat and until we have, you know, computer models that are doing multiple tasks, like on the sort of scope that a human can do, I think we will, we will be very far from creating representations that are completely coherent to the human brain.

30:41.000 --> 30:56.000
And then of course, different people have different, I mean, different people have different experiences of the world. And of course, everybody's representation is a little bit different.

30:56.000 --> 31:11.000
So brain is doing lots of things. You've also done some work that kind of relates language models to the various tasks that we have been trying to use language models for.

31:11.000 --> 31:16.000
Can you talk a little bit about that work?

31:16.000 --> 31:28.000
Yes, sort of like moving forward. One of my inspirations is that the brain is doing lots of stuff, especially as you're reading, and it's doing more than what we're currently training our people that many language models are trained to do.

31:28.000 --> 31:34.000
So language models often predict the next word in sequence or predict a masked word in a sentence.

31:34.000 --> 31:37.000
People are doing much more than that while they're reading.

31:37.000 --> 31:52.000
So I am at the University of Alberta now, which is a great place to be for reinforcement learning. And I think if we think at a bigger level about what people are doing, while they do language, they have multiple sort of layers of tasks.

31:52.000 --> 32:06.000
They do have the task of generating the next word in a sequence, right? It's not the only thing they're doing. They have sort of these layers of rewards, right? And that can be similar to what reinforcement models do.

32:06.000 --> 32:17.000
And so one thing we had tried was, can we, in a multi task sort of a way multi task learning has been shown to create better representations and produce models that are more accurate.

32:17.000 --> 32:34.000
You use sort of a multi task framework, except we use reinforcement learning to constrain the representations and showed that the, if we constrain the representations to predict not only in the next word, but also the part of speech in the next word actually ended up working better.

32:34.000 --> 32:46.000
And so that was a very simple example of sort of incorporating an additional piece of prediction into a neural network in order to improve the sort of language generation process.

32:46.000 --> 33:07.000
But in the future, I'd like to do the same sort of thing with common sense reasoning. So can we incorporate into a model, the idea of common sense reasoning using a reinforcement framework, reinforcement learning framework to improve language generation and to think about the task of language generation, not just as the next word, but rather at a higher level.

33:07.000 --> 33:17.000
And can you talk a little bit about the setup for those problems from a RL perspective?

33:17.000 --> 33:34.000
Yeah, so really it was just that we replaced the the loss function with TD air. And so TD air has the nice benefit of having sort of rolling in information from the future back into previous predictions.

33:34.000 --> 33:39.000
So it was just sort of that idea that we could use information.

33:39.000 --> 33:58.000
Like future information to sort of update into the current representation in a different way than, of course, the like rack propagation through time also does a similar thing, but it was sort of a, it's a different way to incorporate that information ended up working better than just plain old least squares back propagation through time.

33:58.000 --> 34:20.000
And is reinforcement learning used in this case to train kind of from train the language model from ground up or is it used to supplement and existing train language model that's kind of traditionally trained using unsupervised learning or, you know, whatever.

34:20.000 --> 34:38.000
So we trained from scratch and it was used alongside the typical like LSTM predicting the next word in a sequence. So at the same time as the LSTM is learning to predict the next word in the sequence, we can strain one of the hidden representations to also be useful for predicting the part of speech of the next word in the sequence.

34:38.000 --> 34:58.000
So I think of that like if I was able to predict the part of speech of the next word, that would mean that my the normal task for language model, which is choose one of the of 10,000 words for the next word, that's a hard prediction task, but if you could, if you had instead information about the part of speech for the next word.

34:58.000 --> 35:11.000
The information could narrow down that, you know, prediction space from 10,000 to, you know, in some cases, like 20 words depending on the word category. So it's a big improvement in the prediction space.

35:11.000 --> 35:18.000
And, you know, also incorporating the prediction from that that model. So it was another piece of that we worked on was.

35:18.000 --> 35:37.000
Not just constrain the representation, but then also incorporating the prediction itself back into the representation, the hidden representation in the next layer, makes it more available for future predictions. So actually taking that label, what is the next part of speech in the in the sequence, putting that label in the hidden representation in the next layer.

35:37.000 --> 35:41.000
And then using it for prediction of the next word, we saw an improvement.

35:41.000 --> 35:47.000
And what are some other things that you might want to explore along the same lines.

35:47.000 --> 35:59.000
We're interested in incorporating common sense reasoning into language generation. So it had some work showing that even though.

35:59.000 --> 36:11.000
So like there's this data set called lama that has some sentences and with true or false sort of finishing, like the last word just either makes a sentence true or false.

36:11.000 --> 36:26.000
And we found that even when the when the sort of prediction of the network was to create a false sentence, it we could tell with above chance accuracy that from the representation that sort of it had a.

36:26.000 --> 36:34.000
It had a sort of a hint that it knew it was going to make a mistake it knew that the sentence was false.

36:34.000 --> 36:44.000
And so it sort of brings some mind the idea of metacognition that you have some monitoring process that's running at the same time as your.

36:44.000 --> 36:53.000
Your online like as I'm generating my language to you, I'm also monitoring my language and making sure I'm not making mistakes as I talk and sometimes this is working as sometimes it's not.

36:53.000 --> 37:11.000
But sort of a similar thing you could think of for bird so I guess it so in the representation there's a signal that it knows it's wrong and so could we create a sort of additional piece of an additional prediction piece that it's only job is to tell whether or not the prediction it's going to make is correct or not.

37:11.000 --> 37:19.000
So there's part of the network that's predicting the next word and then there's this little offshoot all is trying to do is say whether or not the prediction is going to be right or wrong.

37:19.000 --> 37:29.000
Using the hidden representation before the prediction layer is actually better than just using the entropy at the prediction layer so it's sort of like additional information that you can't just get from the probabilities the prediction itself.

37:29.000 --> 37:33.000
You are able to tell that it is actually kind of aware that it's going to make this mistake.

37:33.000 --> 37:45.000
So there's actually previous work in computer vision showing that this sort of self monitoring am I about to make a mistake actually can improve model training and so there's something else for trying to incorporate into language models.

37:45.000 --> 37:53.000
If you have sort of like this self monitoring loop that's not trying to do the prediction just trying to tell if the predictions going to be right can we do.

37:53.000 --> 37:57.000
Can we improve model model generation.

37:57.000 --> 38:11.000
Nice nice and then more broadly what are some of the directions that you see kind of taking your your research you know across the board given some of the projects that we've talked about.

38:11.000 --> 38:22.000
Yeah so I'm really I am really interested in studying language development so I have some amazing collaborators who study infant language representation infants.

38:22.000 --> 38:39.000
So I would like to be able to study how language representations change as you acquire more words so I talked about learning a new language for adults that's different than like learning your first language and understanding meaning in the world as you incorporate new words.

38:39.000 --> 38:55.000
I think it would be really interesting to study that and study how the semantic space shifts and changes over the developmental sort of lifespan so as an infant you have us and understanding the world and then sort of as you learn to walk and talk you that that understanding changes.

38:55.000 --> 39:14.000
As you know as you learn to read the understanding changes again can we sort of see those change points in the developmental representation says they develop and also then you know bigger picture can we use sort of how the brain does that updating process to improve computer models of language.

39:14.000 --> 39:30.000
As you know like children learn language at a rate that is way faster than our current neural network models and so maybe there's something there maybe you can figure out how it's doing that space reorganization in a more effective way so that computer models can do it.

39:30.000 --> 39:34.000
That's big that's big picture blue sky that something I'm excited about.

39:34.000 --> 39:42.000
Very cool very cool well a lot of thanks so much for taking a time to share with us a bit about your research and what you're up to.

39:42.000 --> 39:44.000
Thanks much for having me.

39:44.000 --> 40:12.000
Thank you.

