Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Alright Twimble listeners, this is your last chance to register for my upcoming AI Summit.
The event takes place next week, April 30th and May 1st in Las Vegas, Nevada, in conjunction
with the Interop ITX Conference.
I'll be presenting an introductory session on ML and AI Fundamentals and we'll have
deep dive sessions on AI for NLP and conversational applications, computer vision and Internet
of Things.
Then we'll be discussing more strategic topics including data collection and annotation,
operationalizing and managing AI and building out your organization's AI culture and strategy.
Attendees will also have a chance to discuss their organization's unique AI opportunities
and challenges with our panel of distinguished experts.
To learn more about the event, head on over to Twimbleai.com slash AI Summit for Details
and a Discount Code.
In this episode, I'm joined by Marco Couturi, professor of Statistics at University Paris
Saclay.
Marco and I spent some time discussing his work on Optimal Transport Theory at NIPS last
year.
In our discussion, Marco explains Optimal Transport which provides a way for us to compare
probability measures.
We look at ways Optimal Transport can be used across machine learning applications including
graphical NLP and image examples.
We also touch on GANs or Generative Adversarial Networks and some of the challenges they
represent to the research community.
And now on to the show.
All right, everyone, I am here in Long Beach, California at the NIPS conference and I am
with Marco Couturi.
Marco is a professor of Statistics at University Paris Saclay.
Ah.
University Paris Saclay is how I should say it.
You're perfectly fine.
Marco, welcome to the podcast.
Thanks.
Thanks for having me.
So why don't we get started by having you tell us a little bit about your background.
Are you coming at things from a stats perspective, but applying them to the world of machine learning
in AI?
How did you combine these interests?
Yeah, exactly.
So I'm here at NIPS, so I might maybe start with that.
I've been coming at NIPS for about, I think, 12 years maybe since I was a PhD student.
And you haven't missed any?
Oh, no, I have missed a few right in the middle, but let's say I've probably been here more
often than not during those years.
Okay.
So probably maybe seven times, something like that.
And my background is, so as you mentioned, I have more of a mathematical training in statistics,
maybe a little bit of CS, to blend of everything because I come from a French chronicle, so
those are universities that try to give you a pretty general training.
And when I was a PhD student, I was interested in machine learning already, always doing
bioinformatics.
I mean, trying to do bioinformatics with some mathematical tools.
And then I kept from there, and I did a bit of finance.
I worked also in the financial industry for a short while and worked as a lecturer in
the US for a while as well.
You said an actor?
I'm sorry.
As a lecturer?
Lecturer.
I'm sorry.
No.
That is exotic.
No, no.
Definitely more.
And I was in Princeton for a while, and then I worked in Japan as well.
For a long time, I was a professor there for six years, and then just one year ago, actually,
I came back to France.
Okay.
So I looked back to France.
And yes, as you said, my training is more math, so I'm more interested in how can you
say, well-behaved, well-posed problems, mathematical problems that we can think of, and try to
reply them, and make them work, and actually the messy setting of the real data, et cetera.
Okay.
And so one of those mathematical tools that you're working on now is called Optimal Transport.
And in fact, you're doing a, or you did, a tutorial on that here at Nibb's.
What is Optimal Transport?
Yeah.
So it's been a bit my, I couldn't say my main subject of research for me, for about maybe
three, four years now, before that I was, so maybe if you see Nibb's through years, there's
waves of technologies and ideas and methods that come by, and so one of them was current
methods, and that was when I was a PhD student.
Okay.
And then it started receiving when deep learning exploded, and then I, well, I had this,
an interest for this theory for a while, the theory dates back to the 18th century.
And it was developed by a French mathematician.
It's very intuitive concepts, but for a while, this was not, there was no real progress.
And then this was really discovered in the 20th century by a, by a Russian mathematician
called Kantorovich, and he got a Nobel Prize, actually, 70s in economics, for, for work
on this, in this theory.
And then more recently, there's a also very famous French mathematician.
You might know him, he's now, he was recently elected, like in the Congress, in the French
Congress, and he also got the Fiat Medal, which, you know, is like the, the biggest
in the prize you could get in mathematics is Nibb's, like, Vylanie.
And so this is a really beautiful theory that's, that's, that the many mathematicians
have been investigating for, for the last decades.
And yeah, I think recently we made some progress on the numerical side, computational
side, and this can become very relevant in machine learning.
So across those three, four years, we, we've seen a wave of papers that have, I think,
brought forward, proved a bit that this was useful, and now the tutorial was a way to,
to showcase a bit, all, all that progress.
Okay.
So for someone who's not familiar with optimal transport, what is it, and what is it trying
to achieve?
Yeah, so let's just break it down and, you know, two words, right?
Okay.
It's optimal and transposed.
So why do we need, like, transport?
Transport is actually, it really dates back to very simple idea of, of transporting goods
from where we produce them to where we need them.
Okay.
And typically, of course, if you only have, let's say, one mine and one factory, then there's
no, not many questions you should ask.
I mean, you just bring, whatever the mine produces, and you bring it to the factory, to,
to transform it into some other goods.
But now suppose that you have a map, and you have a lot of mines everywhere in the, on
the map, and they are, they all produce this, you know, a raw resource in different quantities,
and then you have factories all over the map as well, and they all need that resource
to, to, to, for them to keep on producing final goods.
And the question is, how you move, you know, those, those resources around in an optimal
way, in a cheap way.
So it's a, it's really related to a deep problem, you know, in operations research, or just
industrial problem, which is, uh, optimally move things.
And this was, the reason why this really boomed as a theory in World War II, is that this
is probably when really, we had a lot of stuff to move, and really, we really wanted
to be optimal.
But not just a matter of profits, or trying to squeeze more money, or anything, this was
just vital.
You'd have to, there were some, you know, front lines, you would need 10,000 soldiers there,
20,000 soldiers there, 20,000 soldiers there, etc., and then you had them in, in, another
place, another location, and you really had to find out very quickly, where was the
best way to do that.
And so, unfortunately, I mean, during World War II, I guess, none of those techniques
actually were used, but they, that situation triggered a lot of thinking, and that's why
you control it, and actually other people here in the US, as well, Hitchcock, Kupmans,
there's lots of names that are associated with this problem.
And then, this was really a topic that was mostly investigated by people in operations
we search.
But then gradually, this idea of transporting things, in an optimal way, also found applications
and, for instance, in physics, like you would try to guess how a distribution of gas would
slowly evolve towards another distribution of gas.
I mean, there's many, many different settings where you're basically trying to compare those
two things, like the probability distribution of what, what you have, and the probability
distribution of the, what you would like to get, and how to transform it optimally.
And so, this is why mathematicians got interested in that.
And from our point of view, basically, it boils down to the following thing.
This idea of optimal transport provides a fresh way to look at what we call probability
distributions.
Probability distributions basically quantifying how things are likely to happen.
And they also quantify, basically, where we observe data.
What kind of images do we observe in the real world, right?
We don't observe just any kind of image.
We observe images with a certain structure.
And so those observations form a probability distribution, and we try to come up with
probability distributions of the world, which are, we, as modellers, is what we can do.
And we try to compare them, and this is where the theory is actually useful.
And so, I'll stick with this background question.
Is the implication then in the context of World War II?
Like they were probably, probabilistically modeling troop movements, as opposed to determining,
I would imagine that to be more deterministic.
So, I'm going to move these troops from A to B as opposed to-
Yeah, very good point.
So, here, let me just tell you how you could see, imagine probabilistic troop movements.
Imagine you have front-line A and front-line B, and you need 10,000 soldiers at A and NB,
okay?
Because somewhat the enemy is coming, so we really need 10,000 guys there, and 10,000
guys there.
So, suppose you have two barracks, each with 10,000 soldiers.
So, a very naive way would be to, let's say, send all the 10,000 soldiers of the back
one to front-line A, 10,000 soldiers from back two to front-line B, okay?
So there's no real probabilistic thing, right?
It's just everyone goes there, and then everyone goes there.
It sounds great, but imagine that, for some reason, you're a bit suspicious that maybe
something might happen to a 10,000 soldiers that are sent through just one road.
So one thing you might try to do is split those 10,000s into five and five, and the other
barrack also is split into five and five, and so you send simultaneously 5,000 soldiers
to each of the front-line B.
You see what I mean?
Yeah, so now you've got four routes.
Exactly.
Four people, four groups of people, five thousand people moving around.
So it's not clear you gain a lot by doing that, right?
Because somewhat maybe if barrack one was very close to front-line A, and barrack two
was very close to front-line B, then you're actually asking people to cross basically over
and have this much longer trip, right?
But what you've gained is that you have something, maybe this is where maybe the probabilistic
aspect can be seen like, each soldier basically from back one had chanced one over two to
go to A or B, or on the other hand, if you want to just stick to the optimal plan, you
will just send them directly to the closest to front-line, right?
Is what you're trying to model probabilistically the chance that they'd actually get there,
like the danger of their route or some kind of...
Well, this is something I might have in mind to actually choose the one where they split
into two instead of just going directly to the best possible, I mean, the shortest basically
front-line point.
But what I'm saying is that when you have to distribute resources in a network like this
from a starting point to an ending point, well, sometimes you can find that the best possible
way, the cheapest possible way is to do this a bit deterministically as you imply, there's
no probability you just send everyone to the best possible route.
But you could add some little fuzziness, so you know what, maybe in case there's a problem
with that route, maybe this breaks down, this bridge breaks down, if this bridge breaks
down, maybe no one will get front-line B and then I'm really in a mess, right?
So to hedge up it yourself, you might want to choose something that's a bit more fuzzy,
a bit more probabilistic.
And on the other hand, if you really go towards the fuzziest possible way to do this, where
you split everyone equally, then you see that there's a problem because you're paying
very high cost usually, in terms of travel, because maybe front-line B is like 100 miles
away, from like A's, maybe 10 miles away from the back one, and you're actually asking
for the half of your troops to walk those hour and a half.
So see, there's this, there's a trade-off somewhat, in terms of robustness, on the one hand,
you would like to do what's cheap, on the other hand, I didn't need to be a fuzziness,
makes it a bit more robust against any problem that might occur.
So here, I'm using this, you know, vocabulary of maybe there's a risk or there's something
that I beg down, etc, but in, from mathematical perspective, what this really boils down to
is more of something, which is, you know, the stability of the solution is, you know,
having a little bit of fuzzier things makes it a bit, makes it a bit more stable, having
something that's really routing everyone to a, just one in one location makes things a
bit unstable.
And so why-
Make you think of, if you're familiar with Nassim Taleb's anti-fragility?
Yeah, exactly, you could think of this, so we're going a bit away from basically my exact
technical contribution, but actually the idea is exactly that.
And my, all my, part of my talk, so first I explain this problem of moving around optimally
things.
This moving around optimally things helps you figure out whether two things are close,
whether two distributions are close.
If there's a way, if there's a cheap way to move all the troops to all the frontlines,
then it means that the two distributions are very close.
But if you write down the equations, and this is why Kantorovich called the Nobel Prize,
this yields something called a linear program.
It's one major, very important, family of optimization problems.
And the way this was formalized 50 years ago was just linear programming, so let's go
for the cheapest possible distribution.
Now what I was arguing a bit during the tutorial is that you might trade off a little bit of
that optimality in terms of the cost, and accept a little bit of fuzziness.
And this makes things much better behave computationally when you run all those algorithms.
So yeah, actually this is a meaning, meaning that introducing the randomness not only has
these properties, these beneficial properties, and kind of the physical world we're talking
about troop movements, but also introducing probability and randomness allows you to give
you a more computationally efficient solution in solving the linear program?
Yes, like one million times faster, like in some relevant dimensionalities, let's
say you have, let's say 10,000 barracks, 10,000 frontlines, and you want to compute this
optimal transport, where it turns out that if you use, if you're willing to take a little
bit of fuzziness, not that many, but just a little bit, then it can actually, you can
compute this transport, and maybe let's say, let's say one second, okay, for the sake
of the argument, but if you were to solve it with a linear program, that would be like
one million times slower, that would really be much longer.
And the other thing that's also very relevant I think in this NIPPS conference is that computing
transport with that little fuzziness, not so much, but just a little bit, allows you to
compute everything with GPUs, so everything becomes a parallel, very embarrassingly parallel
algorithm, and this is one of the also big sources of speed up, whereas doing it the linear
programming way, the way that was proposed in the 50s or 60s by such great names as
George Tansik, people that invented all those algorithms in 50s or 60s, those algorithms
don't parallelize well with GPUs, and this is the main reason why we're trying to get
a bit away from them, and a bit more towards those algorithms, but because they include
some fuzziness, a little bit of fuzziness, for some reason, you go down in the math, you
observe that you can cast them as matrix products, and so everything works well on GPUs.
So if I want to push a bit the analogy, this field of optimal transport has been around
for many years, I propose this little, to add this little fuzziness, and everything all
of a sudden can be executed on GPUs, so it looks a bit like someone that comes at a machine
learning conference maybe 10 years ago, and says, oh, I have deep learning algorithms,
not only are they great, actually they run on GPUs, and because they run on GPUs they
are going to get much better performance, because we know this was one of the main drivers
of innovation for deep learning was the fact that they could really well exploit GPUs.
So then are you also applying optimal transport to machine learning problems, or are you,
you know, is this a method for doing better work with traditional types of problems that
we think of as optimal transport problems?
So yeah, it's a very good question, so my point of view right now is that,
so maybe I didn't stress this enough, but what really optimal transport allows you to
do is to compute a distance between probability distributions, and by this I mean, let me
just give you a very concrete example, and this was actually presented at NIP last year
by another team, so you have, suppose you want to compare two text documents, so very
standard approach is to see those documents as bags of words, right, just a long list
of words and another long list of words, right, so I think it's a very relevant question
to say how can we compute the distance between those two bags of words, right, so there
are a few approaches that exist, and one of them is using optimal transport, so basically
what this optimal transport approach requires is that you are able to define a distance
between pairs of words, so basically if you can define a distance between pairs of words,
optimal transport allows you to define a distance between histograms of words, it's
like a meta distance, so you have a distance on just, I don't know, the word cat is that
meters away or miles away from the word, let's say dog, and then very close to TT, for
instance, if you can actually quantify that distance for every pair of words, I will be
able to come up with a distance that can quantify the distance between histograms of words,
cloud, point clouds, you know, those point clouds that we see very often on, yeah, word
clouds, so optimal transport allows you to compute that, that very accurate distance between
word clouds, and now what I'm trying to use is, suppose that my goal is not only to compute
the distance between two things, but also to use that distance as what is called a loss
function, something that allows you to quantify whether your prediction kind of matches what
you should observe, so imagine that, imagine, so this is another example I was taking
for a nips paper two years ago, imagine that your task is to predict from an image cloud
of tags, okay, you just see an image, there's a tree, sun, bridge, etcetera, imagine now
that you have a predictor that takes that image and says that, on that image actually
it's a forest, there's sunlight, and there is a river, so as you can see I've managed
to find a few tags that are somewhat overlapping the original tags that are the ground truth,
but it would be really useful if I could accurately quantify how different those two histograms
of words are, and that's where optimal transport can be used, and so what I've shown in the
tutorial yesterday is that you can not only use optimal transport as a distance just
to put a geometry on the space of histograms, but you can use that as a loss in the learning
algorithm, so whenever, whenever I will, so very often for instance an algorithm just
produces one guess, one guess, well just think about a meta algorithm that would not produce
one guess, but the histogram of guesses, the probability distribution, some confidence
on basically several possible guesses, well then it's useful to use optimal transport
theory to compare those histograms of guesses, instead of maybe you've heard about this,
a coolback library divergence, or other, those are, that there is a very important block
in many machine learning algorithms, including deep learning algorithms, which is this cross-centropy
loss, or coolback library loss, well this is another loss that has been around for centuries
to compare to probability distributions, basically my main agenda is to maybe remove that loss
and replace it by this optimal transport loss, okay, and is the primary, what are the advantages
in doing, so you mentioned the computational advantage with our ability to do these in parallel,
is that something that you can't do with cross-entropy loss, actually it's the other
way around, cross-entropy is very popular because it's very, very simple, it's very, very
easy to compute, but it uses very little information, the sense that, imagine that you have two
histograms of words, and I compute the cross-entropy between them, all the coolback library
divergence, what that won't be able to detect is whether words are synonyms or not, in
the sense that if, for instance, one of my histograms, the one that I want to predict
as a word cut, and for some reason my machine predicted the word kitty, in the cross-entropy
world, those will be two completely different coordinates, and there's no, there will,
three relative, there's no, basically the cross-entropy formula will just say, okay cut appeared
here in the, in the ground truth, my guess was kitty, so I have put a zero on cut, then
you pay a price, and then on the other hand, the ground truth didn't have the word kitty,
but I predicted it, so you also pay the price, you pay the price twice if you want, whereas
this optimal transport loss allows you to say, oh, come on, actually, the ground truth
is cut, and my prediction is kitty, but I know that cutting kitty are very close, I mean,
they're almost the same, they're almost synonyms, so I'm paying the very small price, because
I'm able to somewhat transport the word cut onto the word kitty, and that's the main
idea where this gives a loss, which is more flexible if you want.
Now, it sounds like, in this case, it's calling them on, like, word embeddings, and word
spaces, and so do you need to do all that in order to use it?
Exactly, so the thing is, is that an alternative, or are they complementary?
No, so exactly, so basically, so in those papers, those people are called the word movers
distance, because the idea of a tumultranspactory has many names, it's called Vassarstein distance,
earth movers distance, and so it was recently discovered by this word, word movers distance,
where you're actually computing the distance between two point clouds of word embeddings.
Okay.
So you can imagine that one text is just a bunch of points in dimension 100, those are the
embedding, these embedding dimensions of the words, and another point cloud is another
text in dimension 100, and then you're trying to transport them optimally, and what you
need to do to get there is a notion of distance between every pair of words, right?
So that's what the word embeddings give us.
So what embeddings are just the fuel for this, I mean, just the main ingredient that allows
us to put a bit of tumultransport.
Okay, interesting.
So there's an application in NLP, what are some of our machine learning applications?
So we have, so let me just list you what the application where things really worked very,
very well.
Okay.
So the ones where it works really nicely is graphics.
Okay.
So it's a bit away from machine learning, because graphics people are mostly concerned with
low dimensional shapes.
So we've had a few successes, you know, doing interpolations of shapes, and I mean, we have
a few C-graph papers, and what are the specific examples?
So when there's the C-V, well, imagine that you have a probability distribution, which
is a shape, and another probability distribution that's a shape, what optimal transport allows
you to do is to do interpolations between them, like more things, without very, very little
assumptions on the data.
So what we've seen is that this can be used.
So what can you give me a more kind of concrete example of where you'd have these probabilistic
shapes?
Yeah.
So it's one of those cases where a picture actually is worth a thousand words, but we have
those pictures in the C-graph papers where you would see, for instance, a duck, like
a duck, like a kid having a bath, that would gradually morph into some cow or any other
shape.
So you can actually compute interpolations between tens of shapes, one another, the application
of the brain imaging.
We can try to morph one brain into another without making any assumption on actually the
parametric assumption of the shape of the brain, for instance.
And so optimal transport is very natural for that.
Right, right.
So you've got your beginning distribution, your end distribution, and now this allows
you to compute intermediate states at any point in time or point in, which makes sense
geometrically.
So it's very easy to compute intermediate probability distribution.
You just add one to the other, divide by two, then you will probably get those weird
effects.
Yeah.
So optimal transport actually allows you to morph continuously from one to the other.
Okay.
And this was discovered in the late 90s.
For many years, we had no idea how to compute it, because it was actually very, very heavy.
And those ideas that I told you about about adding a little bit of fuzziness, exploiting
GPUs, basically make it possible now.
And so you can average not only two, but maybe let's say 10, 20 brains and things like
that.
So that can help you from templates.
So this is what the few, those are the examples where there are actually visually a bit
more striking.
Okay.
So dimension, we can, we as humans can get a few of what's going on.
Then the NLP applications, I think, are pretty exciting.
And then there has been the water application, the NLP, the NLP, the NLP, the dimension.
And then we had a, there's a lot of imaging also applications with a distribution of color
experiences, like image palettes, more thing one image with a given palette, changing the
palette of an image, for instance, you want to, yeah, the palette is, has colors reminiscent
of autumn.
And you want them to be all of a certain kind of blueish, etc.
So there's lots of applications like this.
But what we've seen in machine learning recently, and which is pretty exciting, is more
applications to a generative models, those models that are able to create images.
And so there's been a few papers in the last year, actually, in just one year from now.
And one of them is called the Vassarstein Gan.
It has attracted a lot of attention.
We also tried to write a few papers in that line.
So here in that case, the, the story is as follows.
You can see the distribution of natural images in the world, let's say, as a probability
distribution of the space of all possible images, right?
We think of an image as a three channels, and a hundred times a hundred pixels, it's
basically a vector of dimension, 30,000.
But there's very few actually in that space of dimension, 30,000.
There's not that many images that are natural in there, right?
You were to basically write a map of every image that can be natural.
Right.
It would be very thin there, right?
So the problem of generative model is to create a probabilistic model, something that can
generate images that fall not too far from that manifold of natural images.
And this is a statistically very, very messy problem.
And so what you want to do is imagine that I have a model that generates images.
It will have its own manifold in that space of dimension, 30,000.
And then I collect data that's another manifold in dimension, 30,000.
And so the point is I want my manifold to look like the manifold of natural images, right?
And I need to be able to say how far I am, right?
That's the whole point of doing gradient descent.
It's quantifying how far my manifold is from the real manifold, and trying to make that
distance smaller and smaller.
And what people found out just about one year ago, is that many usual distances like
cross-entropy, you know, kubak library, et cetera, were not well suited for that problem.
Because we are really talking about very degenerate manifolds.
And what we use kubak library for is typically to compare gaussians, for instance, so to compare
things that are very smooth.
And so when you have this problem of trying to quantify the distance between one manifold
and another, optimal transport can be useful.
And so there's been a lot of interest for that.
There's Vassarstein Gaan has burned a lot of papers, and there's many, many tricks that
people are trying to use to make this work, because it's computationally challenging.
But then again, in the world of Gaan's, it's very difficult to measure performance.
Right.
Right now, everybody's a bit frustrated, but I think there's still a bit of progress there.
I mean, still progress that's being made now.
I haven't picked up on the frustration around Gaan, as much as the enthusiasm.
Where did you talk to frustration specifically around the ability to measure performance?
Yeah, exactly.
I mean, if you look at the papers, most of them end up being a collection of the images
that are being generated, and that's not a very scientifically correct way of, yeah.
You want some more objective measure of performance, but that in itself is already a big
scientific problem, right?
Trying to find a good measure of performance there.
Okay.
Interesting.
I really enjoyed learning about optimal transport.
Any final words or...
Oh, yes.
So we've written a small, I'm just doing it to put some little advertisement for a survey
that we've written recently.
So if you go to Optimal Transport and just one word, Optimal Transport.github.io, you
will find the slides of the tutorial and you will find also a survey, a 200 pages survey
that I've written with my colleague Gaby Alperin.
Okay.
Interesting. And is there...
In terms of...
You mentioned Github and I'm thinking implementation.
Yes, it is.
It could be available.
Yes, of course.
Okay.
The good thing is that, as I was telling you earlier, all of this eventually is actually
very simple to code.
I mean, I wish I could tell you we have really a great toolbox that, but actually, if
you really look a bit at the papers, it's just...
The code is just a few lines.
It's just...
Okay.
Put that loss...
This magic Optimal Transport loss in your learning problem just requires you writing
maybe a wild loop and three lines in the wild loop.
Okay.
So it's more...
What I mean, this right up your alley, it's more about the math that gave you those three
lines and the three lines themselves.
Exactly.
There's not much in there involved, yes.
Okay.
Awesome.
Well, Marco, thanks so much for taking the time to chat with me.
Thank you so much.
It was really a pleasure.
All right, everyone, that's our show for today.
For more information on Marco or any of the topics covered in this episode, head on over
to twimmaleye.com slash talks slash 131.
And of course, thanks so much for listening and catch you next time.
