All right, everyone. I am here with Jeff Galhar. Jeff is VP of technology and ahead of AI software platforms at Qualcomm, as well as a great friend of the show. Jeff, welcome back to the Twoma AI podcast.
Thank you very much for having me. It's great to be back. I think for a third visit, give you guys an update on what we've been up to Qualcomm AI software.
Yeah, I think we'll have a great conversation. I've heard a little bit about some of the updates that we will be talking about and lots of good stuff.
It's been coming up on nine months or so, I guess, since the last time we spoke. And since then, we've covered a lot of interesting conversations with folks on the research side.
But before we jump in, I'd love to have you share a little bit about your background, as well as your current role at Qualcomm, which congratulations been expanded a bit.
Yeah, thank you very much. So yeah, to refresh the audience a little bit, rejoining the podcast, Jeff Galhar VP technology.
Look, my role right now is really about expanding and enhancing Qualcomm's investment in software for AI across our whole portfolio.
The expanded role is really to include the cloud AIC 100 product line as part of that. And so we can talk maybe a bit about how we're looking to harmonize our AI snack across our whole portfolio.
A bit of a background. I spent a long period of time in what's now Qualcomm at research, you spoke to one of my colleagues on that side recently, but happy to be partying really closely with Qualcomm at research to take their ideas and put them in the software and working of course with our hardware teams around our complete AI software solutions.
You mentioned the cloud AI 100 and that portfolio of products. We've talked about it a little bit on the show previously, but kind of give us a refresher and share a little bit about the current updates there.
Yeah, great. Thank you very much. So the AIC 100 is a product designed for cloud edge applications. So data center applications, smart cities, you think about videos, surveillance kinds of applications, autonomous robots.
We're using that everywhere from from our automated driving program all the way up into, you know, cloud inference infrastructure kinds of applications to power very high end, you know, hundreds of tops kinds of applications and when put in a rack.
And I think the admitted joke about, you know, petta ops kind of class applications. So we've got quite a range of compute available in that device. And then we're using that in conjunction with our, let's say, more traditional snapdragon processors.
To power a whole range of smart city robotics and autonomous devices include in addition to our cloud edge kinds of use cases.
Awesome. So one of the topics that I wanted us to dig into a bit is on ML compilers is an increasingly kind of interesting and hot topic with projects, the likes of glow out of Facebook and TVM.
Let's start by having you kind of introduce us to the concept broadly and what's the role of ML compilers compared to some of the other types of technologies that, you know, we've seen like on X and others.
So we think when we think of compilers at Qualcomm, we think of it in the broadest set of the words. Let me define that a little bit. We were thinking about the whole problem of compilation. And typically when we think about a compiler, C++ compiler, we think we have a high level language and somebody writes an algorithm in it and we want to reduce that to, you know, machine code.
And of course, we have really good compilers like LVM to do that. When we think about a neural network analogy holes. I've got a neural network. I want to, you know, bring it in wherever I bring it in from.
We'll talk about PyTorch, TensorFlow, we'll talk about Onyx in a second. And I want to compile it, you know, in a sense. Now, in some cases, the hardest part of that problem is figuring out all of the parallelism that's going on inside of that neural network.
Sort of standard to feed forward kind of neural network. There's a lot of things you could do in parallel. And so the area where we're making, we're making a big investment is in that I'll call it scheduling and tiling part of the problem. How do you deploy this compiler technology in a way that your neural network is mapped to hardware in a highly efficient way.
The original code generation piece back end of it is also a challenging piece in its own right generating highly performant code to produce, you know, what we'll call neural network ops.
So fragments of code that represent the actual work being done on a neural network. In addition to getting those tiled and deployed correctly onto the hardware is a pretty tricky operation. And so in the case of the AIC product, we're largely leveraging work that Facebook started with the glow infrastructure.
We've done a lot of customization and optimization, what I'll call the back end of the compiler, the part of the compiler that deals with that scheduling, tiling and code generation piece.
And also more towards our Snapdragon portfolio, working with the TVM community to do a similar kind of thing more on the sort of single core, you know, mobile, IoT kind of devices around whole graph compilation of taking in a graph, solving that tiling and scheduling problem and then deploying it, you know, onto our hardware.
Can you maybe give a little bit more concrete detail that illustrates the difference between the tiling, you know, what kind of operations would be covered by tiling versus the code generation part.
So when I say tiling and there's a lot of different sort of ways to slice this, this problem, I'm really thinking about the problem of how do I do things like take my tensors, break them up into pieces so that I can paralyze parts of the operation, how do I make those tensors fit into blocks of memory that I have in ways where those blocks are packed into memory efficiently.
I'm stored in local memory, you know, a correct amount of time, so I'm using my local memory really efficiently, I'm being efficient about when I go to DDR, when I don't go to DDR, how do I bring in data for the neural network, how do I send the results, you know, to the next stage of processing.
Parallelism, these are highly parallel machines, especially when we talk about a I see 100 it's a highly parallel machine, just solving that problem of parallelism is a big part of the ML compiler job.
The code generation is really about once I've solved that problem like if taking my data and I've kind of chopped it into pieces and I figured out how to paralyze it and I figured out what has to happen first.
Just generating the code to do that is kind of what I'll call the code generation piece so whether it's the code for that, you know, that pipeline I've just kind of figured out.
Or it's the actual code of the kernels I've actually got to make it run on the hardware right so at the end I've got to take that schedule I got to link it with code and I got to deploy that onto.
You know I set a course to execute the workload and that's really in the broadest sense that we think of when we say a qualcomm when we think of when we say compilers got it got it so the.
The thinking about it from the kind of the graph that the developers created down the.
The code generation kind of looks at the graph and turns that into code and the tiling is more the low level data structure and it's close to hardware.
Another way around so so it you think of the tiling as the part where I'm taking the neural network and in the most naive way I would just sort of executed as specified by the you know by the data scientist but to get maximum performance I want to be really.
I want to be really efficient about how we're using memory and how big those memory blocks are and I want to break operations that I can make parallel into smaller parallel operations and do them all at the same time on a parallel machine.
The code generation is within the very last stage like once I've decided that whole recipe if you will for execution what are the actual instructions I have to give my hardware to do that.
Really bad figuring out the puzzle I have a giant neural network how do I efficiently you know order the instructions so that it produces the correct semantic results.
But so that it says optimal on the hardware as possible and every one of the custom accelerators in the market has picked a different architecture so they have a slightly different optimization problem the salt.
Yeah is there on that note is there yeah how do you think about the a c 100 architecture from a you know principles perspective relative to other approaches that are out on the market what's different about it.
Of course I can't share a ton of details but I'll say it's a it's a leverages our traditional focus on high performance and low power.
I think he had shared a lot of metrics at a high level about that when he was on the show with you and your audience and.
The way I think about it is that other devices in the market I think if picked very large arrays to solve their problem and I would say about this is we picked a more kind of finer grain highly parallel architecture right.
Therefore we can take advantage of the parallelism that exists sort of naturally in some of these core and some of these neural networks i'm sorry and deploy them onto the hardware in a really efficient way and so one of the sort of evidences of that we did quite well in the recent ml commons.
And so the audience can go see those results and see that we you know best in class results and a number of categories deploying this exact stack bring in a neural network you know figure out how to tile and deploy to hardware and then get very good power performance out of the solution.
And from a software perspective are there differences in the architecture or approach of glow and TVM that led to glow being the preferred route for the cloud inference solution and TVM being the preferred route for the devices or is that more market based.
I'd say it's a bit of market and a bit of technology to be fair and the glow I think was designed from from the beginning with these large cloud workloads in mind and we had in mind to build as large you know cloud oriented array and I think maybe a little bit later we discovered wow this is a really cool device we can actually apply it to all kinds of things besides.
You know cloud and edge and so we had started on that road and we started at a time when compilers were I think still a little nascent for applications of actually compiling ml kernels a lot of a lot of companies have written a lot of handwritten code.
And so a little bit of the market forces is that there's a you know maybe these are the two big contenders for ml compilers today apart from maybe for some proprietary offerings and so it is a bit of a horse race it's still early I think as we've discussed before in the ml compiler space although it's sort of crystallizing a bit more as people understand the problems better.
And that's a little bit why also Qualcomm kind of is looking at both because because we feel like there's elements of both that are very strong and we want to take advantage of that.
But in the past we've talked about some of your support for tiny ml and work that you've done with Google is that to what degree does that relate to this compiler conversation we've been having.
So it's in a space that that we are invested in we've got a tiny ml work with Google and there is a tiny tv m effort and we're looking at we're looking at that space as well for the super deeply embedded always on kind of use cases.
And it's you know it's a smaller part of our overall portfolio but we're definitely you know looking at that as well.
So the the is the way to think about it is you've got the kind of cloud class large scale infrastructure devices that's a I see 100 and using glow as the compiler there you've got the handheld devices.
There are still you know relatively full feature and powerful and TVM is the approach there and then tiny ml is more for these you know smaller footprint more constrained always on devices and tiny TVM is an offshoot that is going to provide that same set of compiling capability for those devices.
Yeah it's a great way to summarize it absolutely.
And then to close the loop I mentioned Onyx earlier Onyx isn't exactly a compiler is it no no so so we were involved very early in the Onyx you know standards establishment if you will sort of de facto standard.
We chaired part of the onyx edge working group for a good period of time and so onyx is is kind of gone in kind of maybe two ways we still use it very extensively as an interchange format.
So our tools can read onyx files are exported let's say from PyTorch and frequently our customers bring us those kinds of models and we work with them to deploy them onto our silicon.
There's also an onyx runtime right so onyx runtime has been started to be integrated into places like win ml and the Microsoft stack and in some of the data center applications as well.
And so we've got work going on to bring Onyx is a runtime on top of our underlying API is into our portfolio as well so we think of onyx as like you said not a compiler but as the data interchange format.
And it's a sort of execution framework that that can sit sort of on top of our silicon.
Yeah I kind of think of it as like a CSV file for spreadsheets you know yeah TensorFlow might be Excel and PyTorch might be pages and you can still interchange the interchange data using the CSV format similar to that.
So it wasn't familiar with the runtime effort that sounds pretty interesting.
So think about it there's you know when we think about it and talk about it a little bit but we're providing a set of APIs across a whole platform with this you know as we sort of harmonize our offering.
So what we then find is that there are a number different what I'll call sort of execution frameworks that various markets or various customers want to use so in some markets they want to use TF light we think of that like an execution framework TF light can accelerate on our hardware for very good experiences and.
So in some markets that like in the windows market Microsoft is established win ML Android Google is established Android neural networks these are all execution frameworks if you will they can read neural networks can orchestrate their execution on the hardware and do so with our underlying drivers and we can talk a little bit about that that's part of why we did AI engine direct.
So in order to provide a best in class acceleration in a package that can then be exposed to the ecosystem along a number of different routes and on its runtime is one of those kinds of routes.
So on its runtime orchestrates the deployment of the network and we provide libraries that accelerate that on our hardware is the way to think about it got it got it.
So you mentioned the platform what's new in the on the platform front at Qualcomm.
Oh my gosh so much stuff so with a couple different things going on so with the announcement of the Snapdragon 888 we announced the AI engine direct which is a evolution on the hexagon and end direct we hit announced in the previous generation product.
The basic idea there is is we realize that there was this diversity of routes that our customers wanted to use to get on the platform we just talked about TF light and Android neural networks and sell on.
And we realized that the kind of biggest innovation we can provide is linking that compiler discussion we had earlier this tiling and deployment.
The English is very hard to do and and it's unique to our hardware and provide a bridge to these execution frameworks so this AI engine direct is sort of a mid level API that's consistent across it will be consistent across all of our product offerings as we move forward.
But starting with the Snapdragon portfolio of products and that API can then be used above that API you can write an orchestration layer like a TF light or like an on extra on time and below that API we provide best in class hardware acceleration across our IP blocks.
The advantage of the developer is a common API so they can talk to our hardware whether they want to talk to a GPU or they want to talk to HTTP they can talk to our hardware with a common API it's on us to provide best in class scheduling and tiling and acceleration.
And then they can build their application kind of however they want.
So we're going to take that and we're going to extend that concept over to the AIC product so that you will have a kind of horizontal experience regardless of where you want to use a Qualcomm piece of silicon in your product.
You will have a common way of accessing our APIs and if you want to use you know common orchestration like an on X runtime you can then build on top of that and of course we'll partner with you know industry partners to make that you know smooth and seamless so we're really trying to open up our portfolio.
So we're going to provide best in class sort of hardware and software acceleration at the low level and then really provide kind of choice is you go up the stack depending on what your application needs are.
And as you bring more capability to the platform are you seeing a shift in the types of developers that are accessing your API's meaning is it shifting from kind of the device manufacturers and OEMs to the end developers at all.
Yeah it depends a little on market segment right so in in like her traditional smart phone segment we still work very very closely with the big names you'd be familiar with and in those areas you know we you know you and I often talk about what are the use cases you know we work with them I think in the last generation you know well over a hundred different models or we work with them to deploy and optimize obviously different OEMs have different combinations.
But it's not unusual for you know our flagship devices with our partners to have 50 60 70 neural networks maybe in a single device.
And these are models that are provided you know via the operating system for things like facial recognition and you know photography for example lots of things.
Yeah so fingerprint anywhere from unlocking your device so face unlock fingerprint right these are biometric secure secure payments you know camera nighttime photography some of the cinematographic effects you know face detect face tracking subject tracking.
Super resolution right to this idea that you can take like a lower res sensor and create the you know the appearance the effect of a high res sensor and these are all little secrets that your smartphone does for you and they're cool and maybe you're taking from granted but behind there you know one frame might be processed by 10 or 15 different neural networks that are each providing.
You know one layer in that one is going to your move noise another one is going to improve the sharpness and other one is going to you know deal with nighttime lighting and so on and the composite effect of these incredible you know photos that we see coming out of our devices right they say the best camera you have as someone's in your pocket.
And in a lot of cases while I'm a photographer and I like my you know cameras the one I always have is my smartphone right right right and so we but we see that in audio processing you know mic noise suppression and so on right and when you look at segments like compute you and I are using some kind of you know laptop or computer.
You know making like the video conferencing experience better in this you know new work world these are things that we're doing already with our partners using AI on our silicon.
And you know these kind of platform environments you often see the you know the platforms provide higher and higher level of abstraction capability.
First of you can comment on this at all just initially you know the large vendors whose name we all know like they had a distinctive advantage and you know their phones because they they had the ML chops to make their cameras do amazing things I'm imagining a world in which you know as opposed to them just relying on you know great APIs and hardware to make that happen in proprietary models.
This can start to offer you know a night vision model built into the platform and I'm curious you know and that's just an example but I'm curious the extent to which that's happening today the extent to which you you foresee that in the future.
Anything you can share on that.
So it's a complicated question because in the following sense it used to be that the things that made these cameras great were people who were really good at cameras and of course we still have those people but it was a algorithmically solved you know traditionally solve problem and now these neural networks are really the IP that these
camor vendors. You know whatever the application is whether your automotive auto autonomous driving and it's important to do segmentation and you know pedestrian detection and sign detection whatever that
So the model is now the sort of IP that's so valuable, right, to the end, to the end, OEM, whatever you're building, whatever your product is.
And so let me turn that around.
I will say that we use the same tools and the same innovation inside of, let's say, Qualcomm's camera pipeline, Qualcomm's audio pipeline.
So I can't speculate with the audience on, you know, whether we would introduce a, you know, face beautification model or a, you know, relighting model as part of our end and camera offering, except to say that we use the same kind of techniques to improve our camera pipeline.
And some of that stuff is baked in and then some of it, the OEMs come to us and say, no, no, we have a proprietary solution, we think is a differentiator, and we want your help to get it on and get it into our products.
And so we respect the fact that our OEMs, whether we're talking about a hand set OEM or a IoT or an automotive developer that that's part of the value that they're offering to their, to their customer on top of our silicon.
So we want to be mindful of that, respectful of that. And so sure, we'll bring certain innovations to the market through our channels and with partners, but we want to be respectful of, you know, the fact that our customers want to build their own IP and differentiate on it.
So we want to be an accessible platform for that innovation.
Yeah, we talked a little bit about benchmarking in the context of the AIC 100, are there also benchmarking results that you're doing in the context of the platform.
Yeah, so more broadly, and maybe in a more established sort of way, there are a number of AI benchmarks. There's been some early benchmarks and then more and more interest from the traditional benchmark developers.
I'm sure your audience is familiar with the geek benches of the world. And when they last bought a PC, they probably looked at, you know, a spec marker geek bench and looked at specs. And so by similar, some of those traditional benchmark vendors are doing now AI benchmarks.
And there's some new entrance. And so we very actively participate in that on the mobile side, our Snapdragon portfolio.
So whether it's it's ludashi, very popular sort of AI benchmark in China, whether it's benchmarks oriented oriented around like Android neural networks is a few Android neural network benchmarks.
And getting back to ML Perf, we also participate in the ML Perf benchmarks that are mobile oriented. So ML Perf has a whole categorization of benchmarks.
In fact, of course, that a benchmark for a mobile device is not going to be competing with a benchmark in the cloud market to their segments of benchmarks in ML Perf.
And we're very active participants there. Now one thing that I wanted to maybe highlight, get back to when you're asked about the platform.
This was announced at Google IO and it ties in with the kind of idea around benchmarks is we're working with Google have announced that we will be working with them on these updatable drivers for Android.
And so the way that ties with benchmarks is that you know, we release a device and it has hardware in it.
We talked early in the conversation about compilers and about tiling and about optimization. These are not solve problems. I mean, we're doing some really interesting innovative work in the compile or tiling algorithm development area.
So as our customers bring use cases as we continue to innovate, we find that we can make marked improvements in the performance of these platforms through software, you know, we can find better ways to schedule these graphs onto the hardware.
We do that. And in many of our products like our Snapdragon neural processing SDK, I'll call it our oldest most established SDK. We make monthly releases to to the marketplace and to our customers.
But this often requires them to make a product refresh like update the wrong, you know, issue a new app, this kind of thing. The work with Google will actually allow us to make updates working with Google directly to the end device to have sort of continuous improvement on Snapdragon with these updates.
So the tie back to benchmarks is people like to say, Oh, I had this snapshot in time. I did this benchmark and I got this score. I got like 800 and it was amazing. And it's great. And then, you know, three months later, four months later, six months later, that scores 900 and people are like, well, second, you didn't change the hardware. And it's like, yeah, but we're continually updating these algorithms that tile and schedule and make improvements.
And being able to update the marketplace as fast as AI is changing. So you don't need to go get a new phone. You can actually get an update that's going to make your phone work better. That's really where we want to push also, right.
The product cycles kind of match as best we can, of course, the rate of innovation that we see happening in the AI space in a general sense. And so these, you know, improvements that we're doing, lead to improvements and benchmarks improvements and end user experience.
And also, you know, give us a chance to work with partners like Google to, you know, find ways to make that happen. Yeah, yeah, interesting, interesting. Related to the topic of performance and, you know, somewhat related to the topic of compilers.
When we spoke last, we talked about quite a bit about quantization and the aim at toolkit and some of the work that you're doing there. It's one of the areas.
Now often talk to Qualcomm researcher is about compression and and quantization. And it's one of the areas there seems to be a free flowing pipeline of innovation from research into product. Can you give us an update on on that.
Sure, absolutely. So again, we talked a little bit had a short update in our tech summit at the end of the year around Snapdragon 888. As you mentioned, there's a sort of continuous innovation happening. And this is really a good example of one of many, but a good example where
we're mostly partnering with our colleagues in Qualcomm research and trying to, you know, make those ideas that you hear from Amir and from others, maybe on the podcast over into into the product.
The update there is that's part of my expanded role will be taking on the, you know, full commercialization of these tools will be integrating them as part of this. If you will harmonization of our full stack, that will include being able to bring in, you know, quantization results from tools like Amit, it won't be the only tool.
Except, of course, PyTorch and TensorFlow, but to complement that and we think to advance sort of state of the art and quantization, the techniques that are in Amit that are published by the research group.
Many of them are made available in open source. Some of them aren't, we are going to have a sort of pro version of Amit that'll go out to our customers that will be, you know, scaled up and it'll be sort of impedance matched to the road map of the hardware and the software that we're developing.
In the overall product portfolio, so that we can continue to assist our customers in getting best in class performance, power performance on our hardware in a way that that allows them to take advantage of our optimized hardware at these lower bit rates, right, getting to lower bit rates as you've probably heard is not always a straightforward thing and it's an area where we partner with our customers to get right.
But by making sure that we've got tools that can do compression that can do things like eight around, right, this is statistical rounding technique to improve accuracy for these bit rates.
As we do that is we do other things in the future that improve on our tooling.
You'll see more and more of these things, you know, hopefully fit together. And in that area, we are, we're really, you know, we're really trying to harmonize that to again reduce the friction they asked about developers and whether it's just the OAMs reduce the friction so that more and more
less and less, you know, data scientists and more and more developers can do this for themselves and can get the advantage of it. So that's really where we're going with that. So keep stay tuned at the audience to stay tuned as we do that.
We're already engaged with our partners. Many of our partners have already, you know, basically aim at pro and they're using it and they're giving us feedback on it.
So that will become more and more a mainstay of our products as we move forward.
And can you speak to the, you know, maybe the most promising or most recent techniques that have been incorporated into aim at pro for quantization and, you know, what they, what they offer from a relative performance perspective or, you know, how you think about that landscape.
So, so we, so the little secret here is that tricks that are in aim at an aim at pro have been shipping in our products for a while.
So the some of our best quantization techniques we kind of snuck into the product like 18 months ago before we had sort of a toolkit sort of fully identified.
Some of the what you see in the marketplace already takes advantage of those techniques, but techniques the network that's in the product and you've used these quantization techniques in that network.
The tool is to allow other folks are building their own networks to achieve the same things.
Yes, both so both networks we've developed that have used it, but also before it really had a product name, we were already taking these techniques, putting them into tools like our Snapdragon neural processing SDK and they were shipping out to customers.
So your if I can say you are, you know, galaxy device already leverages some of those techniques and it has for a period of time we're now creating a more say well defined product around it and we're adding more techniques.
So exciting techniques are kind of two things in the area of things like quantization this eight around thing we keep coming back to is a very interesting idea of sort of stochastically rounding up or rounding down and playing with that decision turns out to have a really interesting effect on quantization positive effect on quantization.
The other thing I think is important and maybe a little subtle to the audience is when you're doing, for example, quantization of our training right so this is a training technique where you're you're training the network in the face of quantization or quantization noise so that it's becoming if you will familiarized with the effects of quantization so when you actually move to quantize the network.
It's already kind of aware what the consequences are it's been trained in the face of that environment.
The better you can do that in a way that's aware of how the network will actually be deployed on the hardware what we like to call hardware aware quantization aware training that's a mouthful.
Better name but the idea being if I know not just that the network will be quantized and I train it in the face of that but I if I can make those techniques aware of what it'll actually look like on the hardware so the more I can move the training part of the problem closer to the hardware the better my results are going to be.
We're just starting to see some of that and we've got some ideas in the pipeline about how we can do that in a better way so the sort of longer term and maybe in another nine months or whatever I can come back and maybe we'll be ready to share some more is closing that loop as a hardware manufacturer.
As a developer of these you know softer toolkits and as a research innovator I feel like we really have the three main elements here where we can close that loop and the more that our tools are aware of how it goes on to hardware and therefore by consequence longer term how we can change the hardware in ways that it makes it easier to close that loop.
We really have a word to a cycle and you're we're going to start to see some of those loops close and that to me is one of the most exciting things about about that toolkit right is being able to fully close that loop.
Yeah when you talk about closing that loop what what exactly does that mean i'm imagining you know trying to you know taking decades to train a transformer on a device.
Let's go close in that loop is more yet it's a good question closing that loop in my mind is more about for example making sure that the simulated quantization that's happening during the training during let's say quantization that we're training past that that is.
Or call it hardware where it's aware of some of the eccentricities that a particular piece of hardware might you know impact.
Right although we can talk about that too but in this case i'm really talking about that that there's fidelity and that they're complimentary they're aware of the eccentricities if you will.
Of both environments in a way that we can kind of leverage that eccentricity for benefit.
Got it got it and then you mentioned we can talk about training on device that is that in the context of like a federated type of environment we've talked briefly about that in the past and talked about it from a research perspective any any updates there in particular yeah so the you know it's a research group has been busy at it since we talk last and.
I can't really talk about specific applications that we're not ready to unveil that done a lot of research got labs up and running by labs I mean you know to do this you have to simulate real and virtual devices at some scale to understand you know how to federate learning and what the trade offs are.
And of course we're first and foremost a wireless communications company so this becomes really natural we often have to simulate large numbers of users scale that have unreliable radio links and so on so this is perfect we're just now doing it with federated learning in addition to the communication link.
And I think you heard Ziad in a previous podcast and those who didn't listen please go back and listen to my colleague Ziad talk about you know 5G and AI be complimentary and this is a great example where you know we're know a lot about the link we know a lot about the fidelity and in fidelity as you know robustness of the link.
And when you're federated learning of course you have to factor in that you might have somebody in your federation that goes away because their battery guys or the call goes away or they drive into a parking lot or who knows why.
And but statistically you need a certain number of samples and so on and the applications here are really about things that are amenable to crowdsourcing in a general sense so data sets whether it's I think I think Ziad talked about you know Google keyboard but.
Generalizing that data sets where the experience of a large number of users improves the whole for everybody is kind of we have in mind and we don't have in mind start with brass tax and like you said train a transformer you know large language scale transformer on devices that would take an eternity perhaps but.
Can we start with though if you will a lab train cloud train model deploy it to a number of devices right it's in every shipping galaxy device maybe or something right and can the experiences of the users collectively improve because their environment the experiences that they and their device go through are all different or largely different right.
And this can be for sound detection this could be for maybe wake word improvement this could be for you know censor modalities right and so when we talk about the life cycle of a device.
Each of our devices goes to different life cycles but we can you know group those experiences and have a better you know collective outcome and so we're looking at the infrastructure frameworks for that and then also some use cases kind of in that direction where we think that this technology is really
complementary to improve user experiences by if you will fine tuning models around those experiences got it got it awesome awesome.
Well Jeff it is always a pleasure to chat with you and appreciate the updates as always wonderful to have you on the show.
Well thank you very much really enjoy it. Thank you for having me and and a great conversation about all these exciting things we're doing and come back the next time and give you and get another update.
Awesome and I should mention that we have mentioned several other conversations with Ziat and some of the researchers and we'll have related links on the show notes page so folks can check them all out.
Thank you.
Thank you.
You have a good afternoon.
Thank you.
