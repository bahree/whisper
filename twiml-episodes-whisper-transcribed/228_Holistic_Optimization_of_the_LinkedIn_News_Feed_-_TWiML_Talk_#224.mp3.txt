Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Tim Yurka, head of Feed AI at LinkedIn.
As you can imagine, Feed AI is responsible for curating all the content you see daily
on the LinkedIn site.
What's less apparent though to those that don't work on this type of product is the wide
variety of opposing factors that need to be considered in organizing and optimizing the
feed.
As you learn in our conversation, this challenge is what Tim calls the holistic optimization
of the feed and we discuss some of the really interesting technical and business issues
associated with trying to do this.
In particular, we talk through some of the specific techniques used at LinkedIn like multi-arm
bandits and content embeddings and we also jump into a really interesting discussion
about how to organize for machine learning at scale.
Before we get going, I'd like to send a huge thanks to LinkedIn for sponsoring today's
show.
LinkedIn Engineering solves complex challenges at scale to create economic opportunity for
every member of the global workforce.
AI and ML are integral aspects of almost every product the company builds for its members
and customers.
LinkedIn's highly structured data set gives their data scientists and researchers the ability
to conduct applied research to improve member experiences.
To learn more about the work of LinkedIn Engineering, visit engineering.linkden.com slash blog.
And now on to the show.
All right, everyone.
I am here with Tim Yurka, head of feed AI at LinkedIn.
Tim, welcome to this week in machine learning and AI.
Thanks for having me.
I've listened to a lot of your episodes, so it's great to be here on the show.
Fantastic.
I'm really looking forward to diving into some of the ways that you use AI to optimize
the LinkedIn feed.
But before we do that, how did you get started working in AI?
Via a pretty circuitous route, I think.
So that's less surprising than you may think, maybe.
So my undergraduate, actually, I started in political science and I could really not
decide between political science and computer science.
And I was very fortunate to stumble upon a research assistantship halfway through undergrad
with a professor at Davis, Amber Boydston, who was doing a lot of computational social
science.
And to that credit, political scientists have basically been aggregating news for the
last several decades and annotating it manually with research assistants.
So they had these massive code books, massive quantities of data that were all manually annotated.
You know, this is about the Warner Rack.
This is about like the economy, on and on and on.
And it seems like a perfect solution for machine learning, right?
This is like you have the data there.
And so we started working together.
We wrote some tools in R to start doing text classification and basically classifying millions
of news articles over the course of the last 60 years into all these categories.
But the goal being, can we start looking at how the American electorates issue priorities
have changed over time?
So this is kind of like a dynamic trend over time, you know, what do people care about?
And how does the media influence what they care about in American politics?
So it's a really interesting intersection of, I think, computational social science or political
science and computer science that really turned me on to machine learning and AI.
And I actually continued, I started doing my PhD in political science also with Amber at
UC Davis.
And that kind of continued for a few years until some personal family circumstances caused
me to leave the program.
And I had to find a job to pay the bills.
And I stumbled across a startup called Pulse, which was an RSS aggregator.
And it was, it was really just a perfect fit.
When Aakshai, the CEO of Pulse, talked to me and asked me to join the, as their first
machine learning engineer, he was like, we're an RSS aggregator.
We're aggregating millions of articles, like every single day.
And we want you to basically classify these into categories and start personalizing people's
news reading experience.
And so it was like the perfect fit of what I had been doing already in graduate school
within the industry.
So while at Pulse, we built up the first recommendation system.
And I did not spend much time there.
Within four months, we had been acquired by LinkedIn.
And that's how I entered LinkedIn.
So that's quite secure to sort out to actually make my way into LinkedIn and into AI.
Once here, we started redesigning the Pulse app.
And the really cool thing I think about LinkedIn data was the fact that we could use the
news content that we were ingesting and the economic graph data.
And we could start generating insights.
And so these insights, you actually see them today in the LinkedIn feed because they
eventually made their way out of the Pulse app into the main LinkedIn feed.
But you can say, hey, here's stuff that's trending in your industry.
These are, these are news insights you will not get on other social networks because we
have that kind of standardized data about where are people working? What are their skills?
So we introduced a bunch of these kind of what we call intelligent insights into the feed,
trending in your industry, trending at your company.
Like what are people reading in aggregate and what might you be missing?
So that's how I started working on the LinkedIn feed.
Yeah, I think that most folks are probably familiar with the feed from a user perspective.
One of the things that you and I were chatting about before we got started was this kind
of broader challenge of what you called holistic optimization of the feed, which seems
like an interesting place to kind of jump in talking about some of the challenges that
you're working with.
When you say holistic optimization of the feed, what does that mean?
Yeah, so I use the term holistic because whenever I go to a conference or like I'm presenting
our work, a lot of the people that are in this space, like building recommender systems,
are still thinking about the like nation stages of their problem.
And so they're saying like, hey, let's like sort by CTR or some like a simple metric,
right?
Like let's just show the most likely stuff that somebody's got to click on at the top
of whatever they're building at the feed, like an ad, whatever it might be.
And I think the feed definitely started there, but we started identifying a lot of different
actors.
There's so many different reasons that people come to the feed, right?
You may be coming to look for a job.
You may be coming to learn a LinkedIn learning skill.
You may be coming to read content or build an audience.
And all these all these actors in the feed have a different different intent.
And when we talk about holistic optimization, it's really understanding those individual
intents and making them work together.
So in addition to, for example, sorting the feed for engagement and showing the most
engaging content at the top, we may also want to say, you know, what incentivizes a creator
on LinkedIn?
Why are they visiting?
And they're likely visiting to connect to an active community, to connect to an audience
and build their brand.
And if they post, like if you came to LinkedIn right now and you shared a link and explained
the latest twimmel podcast episode topic and nobody engaged with that, that would not feel
great.
And it probably wouldn't be a really good incentive for you to come back to LinkedIn.
And so we want to incentivize, we don't necessarily want to just incentivize the hyperviral
actors on LinkedIn that get, you know, a lot, they have a lot of influence already.
We also want to incentivize new creators on the platform and encourage them to come back
again and again.
When I say holistic optimization, these things are sometimes in contradiction with each
other, optimizing for engagement and optimizing for like creator side value, sometimes work
in contrast to each other.
Because sometimes you will sacrifice some engagement to give a creator a bit more attention.
With the intent that longer term, they will want to come back and connect with that audience
that they've discovered on LinkedIn.
And so these kinds of problems, there's several of them within the LinkedIn feed.
The creator kind of consumer side is one form of holistic optimization.
I think the other one is what happens after you have engaged with your feed or created
a conversation in the feed.
So somebody created something you may have left a comment.
How did that impact your downstream network?
Were you able to impact your second and third degree network to participate in that conversation?
That is also like part of this holistic optimization equation.
And then there's some more traditional kind of trade offs when you consider holistic optimization
when it comes to things like revenue and engagement.
So this is a pretty well known problem as ads are not as engaging as organic content.
And if you completely optimize for revenue in the short term, you might actually be sacrificing
like folks who want to come back long term if they just saw more organic engaging content.
And that has much more value over the lifetime of a member using LinkedIn.
And so you really can't be short-sighted when doing kind of a holistic optimization.
You can't just focus on short-term revenue or short-term engagement.
You really need to create these retention follows that people have built an active community.
They want to come back and get insights from that community on LinkedIn and not just exploit
kind of a short-term optimization.
When I hear what you're saying, I think of some optimization function with a bunch of
different components.
But then I think of a whole different set of challenges around really understanding
what those components should be and thinking about how to weight them more.
Things that are closer to the business side and the vision of what you're trying to create
for the service.
How do you apportion or think about the relative challenge of those different aspects?
It's exactly the kinds of problems that we've had to tackle over the last three years.
And I think there's two solutions.
There is like a technical solution, there is also kind of a product or business-based
solution, as you mentioned, which is if you have strong opinions about how your business
should operate, you can actually give the individual components of optimizing these different
parts of the ecosystem and allow somebody who has a really good product intuition understands
the business really well to weight them.
That's like the simplest non-technical way to handle this.
But when you actually get into the technical solutions, there are a lot of interesting
techniques that can be used to balance between these different objectives.
We've started with things like multi-arm bandits to start tuning between these different
objectives and finding kind of a sweet spot by exploring a bunch of different arms and
seeing like, you know, which one is most promising.
And we still have to have some human intervention in terms of which one we finally choose.
But it helps us understand the trade-off space.
And that starts getting a bit more into the technical space.
I think if you go even farther and it's somewhere linked and has not gotten yet, but you
can start using neural nets to understand all the different players in the ecosystem and
understand how to balance between those different incentives.
And that's something that we're starting to look at in terms of reinforcement learning
right now to solve this kind of problem of how do you balance between all these different
objectives.
And the multi-arm bandits side, when you're applying that kind of approach, how are you kind
of structuring that problem?
So it's really like, we take the traditional example of like revenue engagement trade-off,
which we talked about earlier.
I think there's a strong opinion that engagement is the best way to long-term bring a member
back to LinkedIn, right?
And so in our case, in the InfiDiS case, we're really focused on can we continue to grow
the organic and kind of engagement ecosystem while keeping revenue healthy?
And so you set some constraints to your multi-arm bandit to explore a space by which it's
either keeping revenue flat or slightly positive while trying to maximize engagement, both
short-term and long-term engagement.
And so it's actually kind of a constrained problem.
So you're not fully letting it go off and make a decision for you, but we're specifically
applying these techniques in an individual kind of trade-off problem within the ecosystem.
So we're not yet trading off between like three or four objectives with one system, right?
We're making trade-offs between these pairwise objectives.
So then how do you then map that to the three or four different?
Is it a hierarchical type of approach where you're using the multi-arm bandit to kind of
define the trade-off between engagement and revenue and then, you know, for a particular
kind of value of engagement, you then have other systems or, you know, kind of hierarchically
apply multi-arm bandit or, you know, some multiple components to an objective function,
like determine how you trade off the underlying things that contribute to engagement or...
So we're thinking about the...
You are always facing the right way.
You're thinking about it exactly in the right way.
And it comes down to, you know, we have to set constraints that are informed by this
kind of business and product opinion to the problem that's being solved by the multi-arm
bandit solution, right?
So we may say, let's keep revenue like flat and let's like focus a bit more on engagement.
And then there's the creator side of the equation.
And we'll say, we actually want to, up to some limit, we want to incentivize creators.
So we give the system a kind of budget to explore solutions by which it can incentivize creators
and still move engagement forward.
And similarly, when we look at downstream, there's like the same kind of constraint is put
in place, which is like, can you move downstream engagement without affecting a viewer's engagement
within a given session?
So can you still keep them engaged with their feed, but like lengthen their impact down
the chain of like second and third degree connections?
So it's not a purely kind of unsupervised problem here.
We are definitely introducing some product thinking and opinions as to like what the constraints
should be and letting the system explore solutions there, which is, that's, the exploration
part is really what I think humans are not good at.
Like reasoning about the various trade-offs once you've put some constraints in place.
And so that's where I think these kind of techniques are super helpful in finding that balance
to how do we holistically optimize across all these different objectives with some opinions
on what the constraint should be in that ecosystem.
And so there's a big part of making this actually usable relate to the kind of interface
between the systems and the folks that are responsible for kind of managing this trade-off
and the objectives and kind of what does that look like?
Yeah, so I mean there's a very kind of tight coupling between the AI team, the infrastructure
teams, the product teams that build all this, our entire feed product team is very familiar
with AI techniques and how they think about the role AI plays in the products and how
to trade off between different objectives.
I think we don't have to change the balance very frequently, right?
We set forth a strategy in terms of what we want the ecosystem to look like and we really
design our system around like can you optimize given the strategy and given the constraints
that, you know, our product partners have laid out and that does not change very frequently.
Of course the system has to constantly adapt because the models change and so it has to
constantly recalibrate kind of how much weight you're giving to each model.
But the actual underlying strategy will mainly remain in place, you know, over time.
So you don't have to adapt that.
The approach that we're talking about where you're using multi-arm bandit to balance
between a couple of metrics and then within that space kind of optimize, even as I'm talking
about is still fuzzy.
Like how do we get to the next level of detail here and make it a little bit more concrete?
Yeah, so I think it's good to think about it in terms of we have a lot of different objective
functions with trade-off parameters in the broader optimization, right?
So the broader optimization function can be, for example, we have your viewer-side engagement,
we have the creator-side value and we have some trade-off between those two.
We have the revenue component, we have a trade-off for that component and downstream impact.
All of these have different tunable parameters in terms of how much weight we give each of
these objectives.
And so what's really difficult is finding the right balance across, say, those four objectives
while meeting the constraints that we've laid out in terms of, you know, we don't want
to drop revenue.
We want to continue to provide creator value within some realm.
And so what the multi-arm bandit really helps is it can explore all the different kind
of trade-off variations for these four different objectives.
And then propose a few that actually will meet the constraints and the criteria that
we've laid out in accordance with the strategy.
So really what it comes down to is can we explore space and identify a set of, let's say,
four trade-off parameters that meet all the constraints that we've identified?
And so that requires essentially setting up a system that for different parts of the
population on LinkedIn is trying different variations of the parameter and seeing what
the outcome is, like, how is this impacting the revenue metric?
How is this impacting the engagement metric or downstream metric?
And then getting a readout and a gradient across like a bunch of different parameters.
You may have just answered my next question, which is, is this, are you applying the multi-arm
bandit as a way to explore on kind of live interactions or are you doing it in simulation
and using that to determine model parameters that you deploy out to prod?
There's two ways we do this, right?
So for individual objectives, some of them are actually quite predictable offline, right?
We can predict whether our models are doing a better job of, say, starting a conversation,
purely via offline simulation because when we train our models, we basically will compare
it to a randomized list of updates that a member has seen via unbiased data collection.
And we'll basically see, does this new model actually lift items that they've clicked
on like higher up?
And so via offline simulation, we can basically tell, replaying on old data, hey, this
model is actually more likely to start conversations.
When it comes to things like revenue, it's much more difficult to simulate offline because
you have so many moving parts around like seasonality around like supply and demand of
the ads ecosystem.
And so those are really things that you have to adapt live online once you've kind of
shipped the model to production.
And so it's a dual answer, right, offline for the objectives where we can actually replay
on historical data where there isn't a lot of these confounding factors.
We will ship those models online.
And then for those that we can't, we will tune via the multi-arm bandit system between
the different objectives because our primary goal is like, can we get people to form active
communities and start conversations?
That is the primary objective that we can simulate offline.
And it's like what we base a lot of our like ship decisions on like, have we enabled
people to start more conversations, does a small do a better job at that?
If so, bring it online.
And then let's find how to tune for the other constraints that we've imposed on the ecosystem
rather than trying to solve all the problems all at once offline.
So engagement is really like starting conversations and incentivizing creators.
That's really the seed of where we start kind of the optimization.
You mentioned reinforcement learning as a potential feature direction in this area.
What's, how would you see it applied here?
So I think it's still super early days, so I don't know how much detail I would go in
here.
But if you can define kind of the action space and the different actors in the ecosystem,
you could potentially set up a problem that can be more holistically optimized in terms
of rather than manually training each objective individually.
You can actually optimize for all the objectives simultaneously.
So again, it's super early days.
I probably would not go into a lot of detail here, but you are kind of optimizing this kind
of holistic optimization of the elements that you're showing in the feed.
To what degree are we also applying AI to understand individual elements that pop up in the feed?
And what are some of the pieces that come into that like I'm envisioning, like applying
models to images to understand which images are more engaging, applying NLP to headings
and to try to understand which of those are more engaging, are you, do you think about
it at all like that?
Like Macro and Micro type of, yeah, I mean, to kind of like, we've spent most of our time
thus far like talking about like how we frame the optimization of like the overarching feed
ecosystem, which is like a really tricky problem because of all these actors, then you have
all the kind of like understanding that goes into the different features of the content.
These are actually like features that are then put into that model to like help us do
a better job actually predicting whatever target we're trying to predict.
And so there's a lot of things we do on this front for like text and image-based content.
We have, you know, we generate content embeddings to understand let's say like a text update
and like how text updates relate to themselves relate to each other and mapping that to like
members interests.
Like what have you read in the past?
What is your, what is on your profile?
Where have you worked?
How does this content relate to your interests?
So we definitely use like neural net-based techniques to generate embeddings there.
Can we drill into that for a second?
So we've talked quite a bit on the podcast in general about embeddings and the idea that
you can kind of map the set of, you know, content into some embedding space and then use
that to determine, you know, where a new piece of content, what it's like and kind of related
to the attributes of other content that you've already seen.
But then when you're, you are, you mentioned relating that to the user.
Like how do you create the connection between this content embedding space and different
users and user contexts?
Yeah.
So let's talk mostly about just like the text-based problem because I think it's a good example
to start like any text-based update.
So there's a lot of articles and posts that go into the link to infeed that have text.
Every member profile is also heavily text-based.
We have like the past work history, we have a job summary, we have a list of skills that
a member has.
You can actually essentially create a joint embedding space for both members and articles
where you are understanding a particular token and it's been trained on top of both
of these data sets, right, like my profiles and articles.
So abstracting away from what the individual token represents, whether it's a piece of
content or user or some entity in the economic graph to more, you know, it's some, something
that has a relationship with text in this broader context in this data set.
And so that's like at the token level, right, the document then becomes the document that
shows up in the feed, whether it be the article or whatever, or the member profile.
And so you end up essentially creating like vector representations of say like a member
profile or vector representations of whatever update type in the feed.
And you can actually look at the similarity of these and say like, you know, how much
does this member care about this particular update?
Is it relevant to their work history at all and like will it actually advance their
professional career via providing this information or is it very tangential to their interests?
So that's kind of, that's like the deep learned like white and deep nets used to like,
you know, understand content.
And we also go a much more like human in the loop route.
So we have a content understanding team in Dublin.
And that team is composed of linguists and engineers and the linguists actually created
what we call like the interest graph, which is an ontology of interests, professional interests.
And there you actually have hierarchical relationships that are curated by linguists and
taxonomists in terms of like, how do these concepts relate to each other?
And this is particularly helpful when you have applications where you actually want to
expose to the user like why you're seeing this, you know, like you're seeing this because
you are interested in this particular topic.
And we want to display that it's much more difficult to like reverse engineer an embedding
and explain like, you know, this is why you're seeing this.
But with that ontology, it not only lets you kind of explain to the user why we're showing
them, but it also because of the hierarchical nature, it introduces a lot of interesting
applications, right?
Like you can, you can back off to like more generality if you're getting like too specific,
like if you're tagging it with, you know, reinforcement learning, but you actually want
to back off to just let's say like machine learning or artificial intelligence, you can
do that up to the ontology.
And also when you go, when you go into problems like candidate selection and like what kinds
of content you want to show members, if you do not have enough liquidity under a very
granular node in this interest graph, you can back off and, you know, go to broader topics
that are still related, but may have more inventory, meaning people are writing more about
that topic on LinkedIn.
And so both those techniques in tandem are actually super helpful in understanding the
content and then explaining to members, you know, why are we showing this to you in your
feed?
So when you say both of them in tandem are there use cases where you use the ontology and
use cases where you use the embeddings or are there ways that you confuse them for specific,
you know, category of problems where that makes sense?
So we started with actually treating these individualists like separate features, right?
So and they both showed that they provide unique value in terms of predicting whether
members will participate in conversations, but as our kind of thinking evolved, these
interest-based features can actually directly be incorporated into the embedding.
And you can introduce even more like non-linearity that maybe the humans have created a very strict
structure to this ontology and actually putting it directly in the embedding can actually uncover
even more relationships between these different interest nodes that we were not able to uncover
just, you know, through curation.
So it's an evolution, right?
You start kind of proving the value of both independently as like individual feature
sets within your broader model.
And then you find ways to actually like have one help the other, for example, like putting
the interest graph-based features into the embedding.
And then they actually start complementing each other and start fusing into one broader
content understanding feature.
So I guess what I'm hearing you say is, you know, we're just talking about text, you take
a piece of text and you map that to the interest graph and then you stick that in the embedding
as opposed to, I guess what I was thinking through was it's natural for me to take a text
piece of content and map it to like a topic that it's related to.
It's less natural for me to map a person to a topic.
But if the person is just kind of a bag of text, then you can do that.
You can also look at like what has the member engaged within the past, right?
And you have the label data from the interest graph.
And so you actually create this like a vector representation, but not learned via like
a neural net of like what interests in the interest graph does the member care about.
And that's super helpful because it's actually much more understandable to the member like
they understand like, okay, this is what you think my interests are.
That's why you're recommending this piece of content to me.
So it's less about taking like their profile and mapping it to the interest graph and
more about like looking at their historical behavior, what have they read on LinkedIn and
using the interest graph.
So I think that's where like the more the ontology based technique, the manual curation differs
slightly from the the deep learning embeddings where we can actually take the members profile
directly and project it into the embedding space.
So what are some of the other big challenges that you focus on from a feed perspective?
So yeah, we've kind of covered like, you know, optimization.
I think there's like a lot of different feature engineering that goes into understanding
the content that's in the ecosystem.
And then I think the third probably big one is foundations in terms of how you empower
your team to move fast and run experiments.
I know you had Beach Young Chen on like a previous podcast and you were talking about
ProMail.
So feed obviously like, you know, it's a really large vertical at LinkedIn and we want to
experiment with a lot of different ideas.
And so running, you know, hundreds of AB tests every single quarter to figure out kind
of how are we going to activate members, professional communities and spark professional conversations.
To do that, it's you're kind of starting to go into the realm of using AI to make like
the AI team more productive, right?
So it's less about AI to optimize the business end of the equation and more AI to actually
optimize engineering.
So what's the specific example of that?
So I think starting from automation, automation automation, like just getting your models
automatically deployed in production, automatically identifying which variants you want to
deploy to production.
And identifying, so something like feature engineering, which was like the bulk of work
that I think we did in the industry, you know, 10, 15 years ago.
Now a lot of that feature engineering can actually be just automatically, like that can
be automatically explored via like an AI, right?
And when I say AI, I mean, like you can probably like generate an embedding from a bunch
of features that are available to you in a feature index and see which of those actually
like make sense in an advancing your objective.
That no longer requires kind of a human in the loop for a lot of those problems.
And so if you can automate some of the stuff like how you deploy models, how you engineer
like features or simpler features, you actually free up, you may like start automating 60%
or 70% of what say an entry level engineer on the team would do.
And their time is freed up to think about these harder problems around optimization of
the ecosystem.
And like how do they think about tradeoffs and how do they automate those tradeoff decisions
that we were talking about earlier when we're thinking about holistic optimization?
So automation is definitely one of one of the key aspects here.
And then the other aspect is how do you make sure that you know, you're constantly monitoring
your model performance once stuff is online and that you are making sure your models
don't rot kind of in the way we think of code rot.
So you have systems that are like automatically retraining models like on sliding windows
like every day on newer data and saying like, okay, this period of data actually is like
not as good because you know, something was wrong with the tracking data that was passed
back to us.
And then it might identify a new period and say, hey, this is actually a much better training
data period.
Let's actually, let's actually ramp this in production.
And so the system can now automatically retrain the models on newer data and also exploit
that and push out production in an automated way.
These are all things that like historically engineers would all do manually.
And actually like the kind of competitive advantage to being a machine learning engineer
was understanding this process and to end.
And now like I think a lot of what AI empowers and machine learning empowers is these kind
of discovery mechanisms of like which feature should I use in the model which period of data
should I retrain on like all that is abstracted away from the engineer and they focus on like
the harder AI and machine learning challenges.
And imagining that feed is a big customer of ProML, can you speak at all to the relationship
between kind of the customer of a platform and a platform.
So we recently, you may have seen the interview with Beachong was in the context of a broader
AI platforms series of podcasts that we did.
And that was largely with kind of the supply side, if you will, the folks that are providing
those platforms.
But as a customer of those platforms, you are on the other side of a whole set of decisions
that they're making around the degree to which the platform is opinionated versus not
the degree to which a lot of the decisions kind of blow down to that.
That's your take on it from the other side.
Yeah.
So I mean, it's a very, it's a very symbiotic relationship in the sense that, you know,
we are giving a lot of requests to like what we want as a capability to be enabled for ProML.
I think if you think about ProML as like the foundation of LinkedIn AI, the first thing
that ProML has to do is handle the average case like the average path of how an engineer
is getting something to production really well.
And so for some verticals that are starting to face unique challenges at scale, there's
this relationship where we might identify a problem before another team has even identified
it.
And we pass that knowledge back to ProML and say, hey, you know, we're now realizing that,
you know, we're having trouble, let's say, there's too many models being shipped to production
and we need some way to like actually stage these in terms of how they get out there.
And we need to like rebase them automatically on top of each other.
So like this is something that ProML may have not seen from other verticals just yet.
And then we will identify that problem, give that request back to them.
And they then build that out in terms of a platform capability that everybody can use.
So it's really about, I mean, ProML is essentially enabling a lot of the new directions that
we want to go through building the financial infrastructure.
If you want to build something like a GLM mix model and you want to start introducing
like decision trees into that, maybe that's not supported today and maybe nobody has
had a use case yet for that.
That's where the verticals, I think, really push the kind of horizontal ProML initiative
into thinking about how they shape their roadmap.
So I mean, that's kind of it from a customer side.
To what extent is there a tension there or is at least in the case of the way things
are set up at LinkedIn, like do you end up having to make decisions when you're kind
of pushing the edge and you're one of the bigger customers, whether you kind of wait
for the platform to deliver something or you kind of build it yourself and do you end
up with, how do you manage, like I'm imagining code that kind of situations where you invest
in some kind of the ability to do things that you need, eventually the platform catches
up and do you have kind of formalized thinking around this or mature thinking around this
already kind of have to figure out at each time?
I mean, I think we're still, I mean, like I said, a lot of things at LinkedIn are still
evolving.
Sure.
Yeah, we covered a lot of things where we're still learning.
Like I said, even like the holistic optimization started touching on reinforcement learning
and I was like, you know, we haven't really matured our thinking too far there.
So you have to try something to like actually know whether it's warranted to invest in it,
like horizontally across all of ProML because that's going to be like building it in a scalable
horizontally leverageable like abstracted way, that's a significant investment, right?
You have to tailor not just to feed, but like a lot of different verticals, a lot of different
use cases.
And so if we're proposing, if we're like, if we're pushing the boundary of techniques
at LinkedIn or trying something new, you have to try it.
And so often times we won't wait for the platform, right?
We will, we will test it out and maybe it's not built in a generalizable way.
But we actually have to prove that there is business value to what we are doing before
we ask like a horizontal team to actually make this available across the board because
there are many like foundational things that we could be building that will, we know will
apply, you know, across verticals and maybe the particular technology we're building actually
is only useful for feed.
And so or maybe it's only useful for feed for now like for the, for the next like, you
know, year or two, maybe like other teams just don't need it at this stage in their lifecycle
because they're evolving down a different, different path.
And so we do actually within the feed AI team, we have a, a foundations team that is separate
from ProML and that team is completely geared towards kind of empowering this fast iteration
of like innovation, figuring out like what's working, what isn't.
So we can go back to the ProML team and say like, Hey, you know, we were, we were trying
to solve this problem.
We found this technique that works.
We built out like a prototype of the infrastructure that works for feed and this will scale.
Like we think you should consider this as part of your roadmap to like rule out across
all verticals.
And that's what I meant when it's like a symbiotic relationship like it's not like we can,
as an organization, wait for everything to be built perfectly horizontally.
You have to push the boundaries and kind of like an agile and a quick way.
And you're right that that sometimes will generate some code that you'll have to, you
know, pay it down at some point in the future.
I like I'm of the opinion that like, you know, technical debt is not tear.
I think some people, some people avoid technical debt at all costs.
Technical debt is there so you can take on some debt to move faster.
Obviously, do not want your debt to like spiral out of control.
But if you're able to manage that in a, in a sane way, then like you're actually going
to be pushing for both the boundaries of your vertical, but also how fast ProML moves
and discovers new opportunities that they should be making available horizontally.
Are there things that come, they come to mind as a customer of a platform, you know, independent
of the specifics of the relationship between the ProML team and your team that you just
as a customer, I want a platform team to be, you know, doing thing XYZ or thinking thing,
you know, in way XYZ, like what's your wish list for kind of that relationship, I guess?
One of the unique things I think about how the AI org is organized outlinked in is that
it's completely centralized, right?
Like, you know, Deepak Agarwal has like all the engineers working with AI report to him.
There's like, you know, 400 whatever engineers.
All the requests are very centralized and all the teams work together, like in a really
collaborative way, it's, it never feels like we are a client asking like some remote team
or like, you know, horizontal like platform provider to support us, not definitely not
in the way like you might, if you're bootstrapping a star today, you'd go to AWS, right, and
might use their machine learning platform.
You don't have like a direct path to ask them for like what you need.
This is all within the same team within the same company outlinked in where we all kind
of have a joint understanding of what our requests are.
So as a consumer of ProML, like I think that there is, like I said, there's that loop
of innovation where like verticals kind of push the boundaries in areas where like maybe
there's a unique problem in their domain, that feeds back into the ProML platform.
I don't think there's ever kind of attention in terms of how what we're asking for from
the platform, which I mean, it's kind of rare, and I think we're fortunate to be in
that situation.
So kind of thing one on that list is just make sure to keep that collaborative loop tight
and don't kind of minimize the barriers between kind of platform consumers and platform
providers.
And I think, I mean, you can take examples, I mean, just from my past, like, so AI at
LinkedIn was not always like this centralized, so when Pulse was acquired, the Pulse relevance
team was actually a separate team and a separate work, not in the relevance work.
And the advantage there is that those AI engineers were embedded with like the product domain.
And so like they completely understood the strategy, they were really close with all
their other engineering partners had really close relationships there.
The downside is that you can't tap into this like giant wealth of knowledge and momentum
like a centralized AI org in terms of all the kind of research that's going on foundationally,
new techniques, like you're only a team of maybe like three or four embedded engineers
in a product vertical when you have the distributed model.
And that makes it much more difficult to tap into like that knowledge base that we have
in today's model.
So it is like really important how you set up your organization to make sure you're maximizing
the value and the feedback that's going to the horizontal platform.
And along those lines is the AI org exclusively centralized, or do you have kind of a, you
know, I'll talk to folks that embrace more of like a hub and spoke model where they'll,
you know, there's a centralized org that is, you know, defining, you know, best practices
if you will, building tooling and platform, you know, but also like embedding data scientists,
machine learning engineers within product teams, you know, for periods of time to help
them deliver features like how does that manifest itself at LinkedIn?
It's definitely not exclusively centralized like I work very closely with our product feed
product team work very closely with like our feed consumer engineering partners that
are building like the mobile clients.
I mean, you have to be and that's kind of like a virtual team that works on that particular
product area.
And that's usually how I guess hub and spoke model might be, might be kind of the right
analogy here.
But it's really, it depends on like a verticals and needs and so it differs from vertical
to vertical.
At the end of the day though, like, let's say you don't even have a well fleshed out
like product team or product counterpart.
But this kind of centralization within the AI work offers is you can actually bootstrap
something using the wealth of knowledge and techniques that are like already at your
fingertips from like pro mal and all these other verticals without starting from scratch
embedding with like a brand new product like you would maybe a startup.
And so it's not exclusively centralized in the sense that we obviously collaborate
with our partners, but there is a great degree of centralization that enables collaboration
across the world.
Because we're kind of following this thread around, you know, organizational themes and
kind of platform providers consumers like any other thoughts for folks that are maybe
earlier on in this path and LinkedIn, things that have helped you be successful.
There's so much kind of fragmentation in the like cloud machine learning offerings in
the industry now.
So like, I'm not really going to go into that in terms of how you might build like a machine
learning shop from scratch.
But I do think that actually like the important lesson going back to what we were talking
about earlier is really understanding how you formulate your problem early on.
And I think this is actually something now looking back at polls that like we may have
been really short-sighted by like focusing on like, you know, this day's revenue or like
this day's engagement and not focusing on the like more holistic picture of like what
is your ecosystem to find your problem space and then understand both the short term and
long term effects and how they trade off with each other.
And I think, you know, coming from startup oftentimes you're very focused on that like
short term milestone and you actually might be sacrificing a lot of long term growth and
engagement by going by not taking time and saying like, let's actually think about how
we're using machine learning to solve this problem.
It's very easy to be a machine learning expert these days and kind of like find a tutorial
to do like solve the exact problem that you're trying to solve for your startup.
And I think the real like value of machine learning is when you actually give it a harder
problem than those that you see like the simple problem that you've thought of like we need
to solve this particular like ranking problem for like a feed.
Thinking more about all the actors in your ecosystem, all the different content types, all
the different intents and pushing the boundary of like what machine learning can help you
do.
I think that's probably the most useful insight going from a small startup to a big company
where we actually have had much more time to think through those different components.
And you know, in some ways it's like jumping to the end game, can you do that without like
going through the steps like what's the is there an approach to getting you there or
a way of thinking about it that allows you to know which of the steps you need to skip
versus what you have to pass through.
Yeah, your point is like spot on which is like it's easy to identify like this as something
you should do when you've already like evolved to this point in terms of technical capability.
But actually, I think it's always useful to evaluate kind of like how far AI has come
and how easy it is now to like do a lot of really fundamental basic like things, solve
a lot of like the simpler problems in AI when I was saying like, you know, automating
all these kind of basic capabilities within our team.
A lot of that is offered on like a lot of different like cloud platforms, et cetera.
And so folks that want to use machine are actually kind of freed up now from some of those
lower level logistics and you can kind of skip a few steps ahead.
You know, if you go to the very beginning of LinkedIn, we were still like trying to get
like logistic regression working for like a CTR based model.
We had to start with like a recency based model.
You don't actually have to do that anymore, right?
Out the gate, you can actually start with kind of what is pretty state of the art in
the industry.
And I think that's why it's so important to spend that extra time thinking through your
ecosystem, like what is it you're building?
What are the levers you have at your disposal in your product to actually to shape the product
experience you're trying to build?
And so I do think that like the the advancements and ML and AI kind of over the last decade,
like it's gotten easy enough to do that you're actually maybe not burdened by some of the
stuff that you know, LinkedIn had to deal with or pulse had to deal with in the really
early days when like the tooling was not nearly the way it is today.
So the kind of technologies that were available out of the box in terms of how you use neural
lots, I mean, even TensorFlow, like that's something that I think in the last really five
to seven years has really blown up as like, you know, standard tooling.
And that is all now available to anybody who's shopping a product out of the box.
So you can skip that.
You can skip that and LinkedIn did spend a lot of time in that area and you can start
moving into like these problem definition type scenarios.
So invest the time and energy saved and building out kind of core infrastructure and being
more thoughtful about the way you approach the value you're trying to deliver to whoever
the customer.
I think you summarized it much more, particularly than I did, but yeah, that's a great point.
It's a great point.
Well, Tim, thanks so much for taking the time to shout out to me.
It's a great conversation and I've enjoyed it.
Thank you.
Awesome.
Thanks.
All right, everyone.
That's our show for today for more information on Tim or any of the topics covered in this
episode, visit twimmelai.com slash talk slash 224.
Thanks again to LinkedIn for their support of this show.
Be sure to check out what their engineering team is up to at engineering.linkedin.com slash
blog.
As always, thanks so much for listening and catch you next time.
