WEBVTT

00:00.000 --> 00:16.880
All right, everyone. I am here with Suci Saria. Suci is the founder and CEO of Bayesian Health,

00:16.880 --> 00:22.560
the John C. Malone Associate Professor of Computer Science, Statistics and Health Policy,

00:22.560 --> 00:28.480
and the Director of the Machine Learning and Healthcare Lab at the Johns Hopkins University. Suci,

00:28.480 --> 00:34.560
welcome to the Tuomo AI Podcast. Thanks, Sam. The long affiliation that made me very nervous.

00:36.320 --> 00:42.320
It was quite a mouthful, but I am super excited to have you here on the show. This is an

00:42.320 --> 00:47.520
interview that I've been looking forward to for a very long time. I think I remember seeing one of

00:47.520 --> 00:54.080
your very early presentations on machine learning for sepsis, and this was how long ago was that work?

00:54.080 --> 01:00.640
I've been working on it for over six years now, so I don't know when which presentation you saw,

01:00.640 --> 01:05.920
but yeah, it's been a while. That's awesome. The podcast has been going strong for five,

01:05.920 --> 01:12.720
so it was probably early days for for both of us. So nonetheless, I'm excited to have you here

01:12.720 --> 01:18.320
on the show and would love to start out by having you introduce yourself to our audience,

01:18.320 --> 01:23.200
share a bit about your story and kind of give us a sense for how you came to work at this convergence

01:23.200 --> 01:27.520
of machine learning, AI, healthcare, medicine, all these great things.

01:28.640 --> 01:35.120
So I grew up in India, in like a tiny little town in India, and it just so happens, you know,

01:35.120 --> 01:39.280
India is a very nerdy place. It's people are totally encouraged to be engineers and computer science

01:39.280 --> 01:46.560
nerds at a young age, and I got into computer science very early and actually got fascinated by AI

01:46.560 --> 01:54.240
as a field, and just really got lucky and trained at a very young age with people who are luminaries

01:54.240 --> 01:59.520
in the field, which means got tons of opportunities that were uncharacteristic for someone my age and

01:59.520 --> 02:09.840
background. And in terms of for me, actually around 12 years ago, 2006, 2007, eight around then,

02:09.840 --> 02:14.080
I was kind of going through an early midlife crisis where I realized a lot of the kinds of ideas

02:14.080 --> 02:20.240
we were exploring in AI and machine learning, the applications at the time were advertising,

02:20.800 --> 02:27.920
or like, you know, personalization on a phone or personalization on a desktop, you know,

02:27.920 --> 02:36.160
email foldering. And what that made me think about was, you know, like, is that I wanted to sort of

02:36.160 --> 02:44.320
do something with more social immediate social impact. And that meant I considered everything,

02:44.320 --> 02:52.640
and around that time also got introduced to colleagues at Stanford who were physicians. So these were,

02:52.640 --> 02:57.680
like physicians who took care of premature babies, these babies at risk of major complications.

02:58.800 --> 03:03.520
And if you can turn out, because they're very, very tiny premature,

03:03.520 --> 03:08.000
treating them in a timely way is very important for being able to impact the health outcomes.

03:08.000 --> 03:12.160
Like, they're much more at risk for declining deteriorating, had poor having poor neurodevelopmental

03:12.160 --> 03:18.640
outcomes, and not surviving if you don't catch them in a timely way. So that was sort of my first

03:18.640 --> 03:26.240
introduction, actually, to moving from just sort of understanding and studying methodological problems

03:26.240 --> 03:33.360
in machine learning and AI, broadly applied to, like, hard messy time series data sets to thinking

03:33.360 --> 03:39.840
harder about real world applications, but he really could make an impact. And, you know, that

03:41.120 --> 03:45.600
and healthcare is just really hard. I didn't realize how hard it is and what I was getting into.

03:46.240 --> 03:50.160
But I really didn't. It's got me so many sleepless nights.

03:50.160 --> 03:55.040
But yeah, but, you know, it sort of also made me realize, like,

03:55.760 --> 04:02.400
wholly, majorly, like, there's so much opportunity. But, you know, it's going to require the

04:02.400 --> 04:10.320
right types of efforts to make progress. And that's how I got started and kept going down that path

04:10.320 --> 04:15.920
and actually have considered the whole nine gamut from, like, advising companies to

04:15.920 --> 04:20.560
the previous seeding particle research companies to now spinning out this company out of Hopkins.

04:21.280 --> 04:29.520
But to obviously being a professor, faculty, you know, innovating on research. And all throughout

04:29.520 --> 04:37.360
sort of with one singular focus, which is healthcare is moving from, you know, in 2009, there was the

04:38.560 --> 04:44.720
particularly big event that happened, which was the passage of the high tech act. That made it so

04:44.720 --> 04:50.560
that systems were going from no data to data, data were now going to be stored electronically at

04:50.560 --> 04:58.160
scale across hospitals and clinics around the country, which means it was entering this era,

04:58.160 --> 05:05.040
like almost like a 1999 where the web came. So in medicine, electronic data is coming,

05:05.040 --> 05:12.720
electronic infrastructure is coming. And since 2009, in the last 10 years, there's been widespread

05:12.720 --> 05:18.000
adoption because of policy changes off this electronic infrastructure. But the use of this data

05:18.000 --> 05:22.320
is still extremely limited in healthcare delivery. Like today, the way physicians practice is still the

05:22.320 --> 05:29.040
way, you know, practice occurred 50 years ago, 100 years ago, 200 years ago. And so the singular focus

05:29.040 --> 05:35.280
being there are so many opportunities for improving the quality of care if we could use data in a

05:35.280 --> 05:40.560
more intelligent way correctly using the right type of AI. And so how do we make that happen?

05:40.560 --> 05:49.760
Nice. And you kind of jumped directly to your recent history. But I noted that you earlier in

05:49.760 --> 05:55.680
your career, you interned with Eric Corvitz, who was a recent guest on the show and you did your

05:55.680 --> 06:01.040
PhD with Daphne Kohler, who's been on the show. You've had some amazing opportunities.

06:02.080 --> 06:06.640
Yeah, it's true. Actually, I have a funny story about that. So I didn't actually want to be

06:06.640 --> 06:13.520
faculty at all. My thought was, I'm going to go into industry and, you know, like I love the pace

06:13.520 --> 06:20.000
at which industry moves. And I was doing a lot of work. You know, the work we did in new units,

06:20.000 --> 06:25.040
we were able to show by using machine, like approaching this data from a new lens, you really

06:25.040 --> 06:29.920
could actually predict outcomes and these little babies, which babies at risk for complications,

06:29.920 --> 06:37.520
much earlier than physicians were recognizing them. So at the time, I remember there was opportunity

06:37.520 --> 06:43.520
to start a company to build improved new natal care. And Eric's been a mentor of mine for a while

06:43.520 --> 06:49.600
and Eric met me at Nurebs, where he's like, why aren't you becoming faculty sushi? And I sat

06:49.600 --> 06:55.520
then I was like, I don't know, things that academia move at just a pace that feels a tad bit too

06:55.520 --> 07:00.640
slow. And we had this sort of soul-searching conversation for like half an hour where

07:01.680 --> 07:06.000
that got me to like reconsider where I was like, you know, what we're doing is very foundational.

07:06.000 --> 07:11.440
I think this was back in 2011. Very, very foundational, the kind of work I was doing back then.

07:12.000 --> 07:18.160
And realizing like, we're very early in our use of, we need novel methodological developments

07:18.160 --> 07:23.040
that was really going to unleash this kind of data. Our technology wasn't ready yet at the time,

07:23.040 --> 07:28.880
which meant it really needed to be in a very deep research environment, being at a place like

07:28.880 --> 07:36.480
Hopkins, right, which is in a way the mech-off healthcare. Like, you get to sit next to people who are

07:36.480 --> 07:42.800
some of the leading policymakers who study, you know, guideline policy change and got treatments.

07:42.800 --> 07:48.400
And like, so in some sense, it felt like in order to be able to bring about any kind of change,

07:48.400 --> 07:53.600
coming to the center of the activity and trying to bring change from within could be really

07:53.600 --> 07:59.360
productive. And so yeah, so that was really exciting and really enjoyed, you know,

07:59.840 --> 08:04.240
Daffney and Triedar have had a number of really amazing people who influenced me from a very,

08:04.240 --> 08:11.120
very young age. That's awesome. You're, often when I'm talking to folks that are applying

08:11.120 --> 08:19.040
ML in the medicine and healthcare field, there's like, I guess the point I'm getting at is like,

08:19.040 --> 08:23.120
there's a distinction. You know, there's a set of folks that kind of think of it from a policy

08:23.120 --> 08:26.560
perspective and healthcare. And there's a set of folks that think about it from a medicine

08:26.560 --> 08:34.240
perspective. Your work seems to span the two. Is that true? That's right. So I think the way

08:34.240 --> 08:39.440
to think about this is almost everything starts with a discovery, right? You want to first figure

08:39.440 --> 08:45.680
out, you know, what is something? Where is there opportunity for change? And what would you do

08:45.680 --> 08:52.320
differently? So in other words, are you inventing a new software based? So often in machine learning,

08:52.320 --> 08:57.200
the kinds of interventions we'd be looking at is like new software based tools for being able to

08:57.200 --> 09:02.400
do diagnosis more correctly. New software based tools for the moving, doing early detection of

09:02.400 --> 09:09.680
adverse events, new software based tools for targeting drugs more precisely. Software based tools

09:09.680 --> 09:16.560
to avoid adverse drug effects. So these are all examples of totally new opportunities that machine

09:16.560 --> 09:22.960
learning and AI have opened up. And in order to scale any of them, they always start from a discovery

09:22.960 --> 09:28.320
phase. So you're learning about you're using data to identify what's possible. Then you construct,

09:28.320 --> 09:33.120
you know, just like you would construct a new drug, here you would construct a new piece of

09:33.120 --> 09:39.120
software just like a drug happens to be bits and bytes that's using the data to do something

09:39.120 --> 09:45.680
differently. You should ideally go through the same exact process of creation, validation,

09:45.680 --> 09:52.800
evaluation, showing it works in a prospective setting upon, you know, when used. And once you've

09:52.800 --> 09:58.000
done all that, then you move into policy. So when we think about policy, there are two levels of

09:58.000 --> 10:06.080
policy. They're sort of at the level of like clinical guidelines, which means societies have to go,

10:06.080 --> 10:14.480
there's a whole process in medicine dissemination of new ideas are much more rigorous, methodical.

10:14.480 --> 10:19.840
I would even say somewhat slow in the sense that you're trying to convince, you know, and you

10:19.840 --> 10:25.440
saw this with the vaccines, right? We had to think very hard about how is evidence communicated,

10:25.440 --> 10:31.120
because you can have all the right stats and data supporting something. It still matters how

10:31.120 --> 10:37.760
it's disseminated. And through what channels is it disseminated in order to build trust at scale.

10:37.760 --> 10:42.320
And so that's sort of where you move into the policy realm, where you are thinking hard about

10:43.200 --> 10:48.640
one, like clinical guidelines for a specific new invention. And then at the policy level,

10:48.640 --> 10:55.600
also what is the general mechanism of framework by which these kinds of ideas get absorbed over and

10:55.600 --> 11:02.080
over again at scale. And then of course, the implications of that on everyday practice, whether it's

11:02.080 --> 11:10.160
cutting costs, improving outcomes, and everything that's needed to accelerate disciplined adoption.

11:10.160 --> 11:22.320
And a big part of that is dealing with the whole pay system here, the payers and the insurance

11:22.320 --> 11:28.240
companies. And I was having a conversation with a friend who comes at things from that perspective.

11:28.240 --> 11:35.920
He's a pharmacist by training. And he noted that, you know, we're just now getting to the point

11:35.920 --> 11:42.800
where, you know, the first algorithms are getting coded by the insurance companies.

11:42.800 --> 11:48.400
I don't know that world very well. So I'm sure I'm butchering it. But it's taken a long time to get

11:48.400 --> 11:56.400
to a level of progress where we're, where we're seeing the kind of impact. Yeah. So actually,

11:56.400 --> 12:00.160
some, let me unpack that question, because I think there are like a couple of different things

12:00.160 --> 12:06.560
you touched on. So the first thing is, why has it taken this long? What is hard about it?

12:06.560 --> 12:10.560
What's taking long? And then the second question is, well, where do we stand now?

12:11.200 --> 12:16.400
Yeah. So let's start with like, is it taking long? What is taking so long? Why?

12:17.120 --> 12:22.560
So, you know, 10 years ago, electronic health records came to be, right? And the digital

12:22.560 --> 12:28.320
infrastructure started to be. Now, turns out, the data collected within these and through variables,

12:28.320 --> 12:32.720
which means there's been, and COVID's only accelerated this, right? There are, you know,

12:32.720 --> 12:37.760
now data being collected in social format through devices, through measurements in the clinic,

12:37.760 --> 12:43.600
in a hospital, also like any kind of billing data, reimbursement data, like lots of different

12:43.600 --> 12:49.600
sources for really building what is kind of like a digital longitudinal picture of a person.

12:50.240 --> 12:54.800
And how they've, you know, how they've evolved, but also how like different treatments have

12:54.800 --> 13:01.280
impacted them, which means now in the last five years, what's been possible is, you know,

13:01.280 --> 13:07.040
researchers like myself, you know, I was sort of relatively early in this movement, but like,

13:07.040 --> 13:12.560
really going neck deep to understand, head deep, to understand like, what's hard? Like this data,

13:12.560 --> 13:17.200
there are hundreds of different data streams. It's not like imaging data via one type of data,

13:17.200 --> 13:22.320
you have hundreds of different data coming in, different kinds of bias, different kinds of

13:22.320 --> 13:28.960
missingness, and you're integrating all these data to draw real-time clinical signals,

13:29.440 --> 13:36.000
but these signals have to be much more trustworthy, safe and precise compared to safe, you were just,

13:36.640 --> 13:43.040
you know, choosing whether to show somebody the ad for a shoe, right? It's like a whole different

13:43.040 --> 13:50.880
ballgame. And so a big part of this was being able to build the kinds of methodology and

13:50.880 --> 13:58.800
technology to be able to really draw safe reliable trustworthy inferences that could then power

13:58.800 --> 14:04.480
specific applications. That's the first part. Second is then tying it to real concrete use cases

14:04.480 --> 14:12.080
that are well supported by clinical users where there's naturally need for it, like hospitals

14:12.080 --> 14:17.600
are struggling or providers are struggling, they actually need solutions to help them. As opposed

14:17.600 --> 14:22.240
to, you know, often the way technologists start by solving problems is they start with the

14:22.880 --> 14:27.120
problems that are technically hard, that they find technically interesting and not all of them

14:27.120 --> 14:32.400
are technically useful. So in this case, we want to marry the heart, you know, we had to build

14:32.400 --> 14:37.360
the heart technology, but we also had to deeply understand medicine. The practice of medicine to

14:37.360 --> 14:43.920
understand where are their use cases where today people are struggling, where there is need,

14:43.920 --> 14:49.600
and we can marry the two. And then the third part, that is actually really hard. And so I thought

14:49.600 --> 14:54.640
we were like almost there back in 2016 and I was like, oh, this is so beautiful. We've written

14:54.640 --> 15:01.280
all these papers in a number of different use cases and we've started to show how it's feasible

15:01.280 --> 15:06.320
to take the technology and to even get it to a place where it's possible to show how it would be

15:06.320 --> 15:12.240
used by providers. But then I realized like, you know, there are all these other barriers like how

15:12.240 --> 15:19.040
the technology is delivered within a provider's workflow, like it's user experience. How do they,

15:19.040 --> 15:24.400
you know, how will they use it? Is it easy to use? And how does it communicate? Like going back to

15:24.400 --> 15:30.160
machine learning and AI, one of the super cool things in the field in the last couple of years

15:30.160 --> 15:35.680
that, you know, our lab, my team at Bayesian and then others in the field have been thinking about

15:35.680 --> 15:43.040
is how do we make machine learning amenable to collaboration with experts, right? So in my example,

15:43.040 --> 15:49.760
if physicians and nurses and care team members are going to use the software, these are high stakes

15:49.760 --> 15:54.720
decisions. We need them to be able to collaborate with these software outputs. It's not like a black

15:54.720 --> 16:02.880
box system where the system can just, you know, say something and overrule what the provider is going

16:02.880 --> 16:09.360
to do. It's actually, it requires teaming. It requires the ability for the software to identify

16:09.360 --> 16:16.720
patients at risk for the providers to come in and agree, disagree, reason with it. And that means

16:16.720 --> 16:20.800
it's a joint decision making process. And how do you facilitate that with machine learning in

16:20.800 --> 16:25.520
high stakes environment? So that's been, those are the kinds of areas where we need to keep pushing

16:25.520 --> 16:30.560
the field. And we've been able to make a lot of progress in terms of the quality of the underlying

16:30.560 --> 16:36.400
methodological stack to be able to get to really high quality inferences that are trustworthy.

16:36.400 --> 16:40.240
Of course, there's always room to improve. And then, you know, in terms of use, like we built

16:40.240 --> 16:45.520
and deployed, for example, in sepsis, which is one of the leading causes of inpatient death.

16:45.520 --> 16:50.000
I lost my nephew to sepsis. So it's sort of like a personal area that I've been working in for

16:50.000 --> 16:57.600
almost now seven years. And in sepsis, for instance, you know, timely treatment is one of the most

16:57.600 --> 17:03.600
effective ways to improve outcomes. Basically, the earlier you can catch a patient, evaluate and

17:03.600 --> 17:07.920
give them the right treatment, the more you're likely to completely alter the clinical trajectory.

17:10.000 --> 17:16.000
But and our very early work back in 2015 showed you could identify sepsis early using machine

17:16.000 --> 17:21.920
learning, but getting it to a place where you could identify it, surface it, get providers to use it,

17:21.920 --> 17:29.360
adopt it, act off of it to actually improve outcomes was like a whole five year, six year journey.

17:29.360 --> 17:36.880
And like, and basically now we're we're recently going to release a study that shows, you know,

17:36.880 --> 17:43.360
our experience deploying this with thousands of physicians and nurses using it over the course of a

17:44.240 --> 17:49.520
two and a half year period, where we've been able to see, you know, very meaningful adoption.

17:49.520 --> 17:55.440
So like 90% of cases that the software flags, the providers actually go in, look at what the

17:55.440 --> 18:01.040
tool has to say, and they provide an evaluation. And then they treat patients if they agree. And

18:01.040 --> 18:07.280
and that's lead to very meaningful shifts in in terms of in our cohort, what we found is

18:08.480 --> 18:14.960
in sepsis, every hour is daily is associated with increased risk of associated with significant

18:14.960 --> 18:19.920
increase in mortality and we've been able to meet very significantly impact how earlier these

18:19.920 --> 18:26.880
patients are getting treatment. And so that's an area that you've worked very closely and

18:26.880 --> 18:34.320
are you able to give us a sense for more broadly, you know, where are their pockets of success?

18:34.320 --> 18:41.520
I mean, you know, we over the past few years or or many years at this point, you know, we've

18:41.520 --> 18:48.880
gone through the waves of, oh, hey, we, you know, I can read, can read x-rays better than radiologists.

18:48.880 --> 18:55.520
So, you know, we're done there, you know, x-rays can identify, you know, cancer and biopsies,

18:55.520 --> 18:58.640
you know, we're done there. But, you know, when you talk to folks in the industry,

19:00.080 --> 19:05.760
you know, we're far far away from done. How is ours? Excellent, excellent, excellent point.

19:05.760 --> 19:10.480
So this is so hard as a researcher in the field, because I get to watch those headlines all the time.

19:10.480 --> 19:17.520
One of the big, big, so it's sort of the innovation has gone through phases. The first phase was

19:18.640 --> 19:24.000
hubris. We went in, there were people who went in and were like, we can do everything, because they

19:24.000 --> 19:28.720
like can do everything and anything, so it can do everything. They underestimated how hard medicine

19:28.720 --> 19:36.640
healthcare is. So that was hubris. Then phase two, a renewed pack of researchers who came in

19:36.640 --> 19:44.160
wiser and went in and really rolled up their sleeves dug in deep and came up with methods that

19:44.160 --> 19:49.600
actually work for this kind of data, integrating domain knowledge, causal reasoning,

19:50.240 --> 19:55.680
thinking about safety, reliability, actionability, that sort of thing. So then the next phase was

19:55.680 --> 20:01.520
a sequence of methods that were better, higher quality, which resulted in these kinds of headlines

20:01.520 --> 20:08.000
you've seen where they're doing evaluation studies in the lab where they say, okay, let's compare

20:08.000 --> 20:13.520
how the software does to how a human expert would do either by looking at historically

20:14.400 --> 20:20.000
on a population, what the human did in comparing the software in the background or putting them in

20:20.000 --> 20:25.200
front of the software and see if they would change their mind. What's new now is phase three,

20:25.200 --> 20:30.880
where basically we've gone from experiments in the lab to the kind of example I'm talking about in

20:30.880 --> 20:37.120
the last three years where we've now deployed in real life settings where providers are actually

20:37.120 --> 20:43.520
using these software and actually making decisions with it. That's been very hard to come to and

20:43.520 --> 20:47.520
it's been a long you know a long time coming and that's why this is so exciting to be able to get

20:47.520 --> 20:55.600
to a place where VC providers adopting VC providers in engaging, interacting and actually a changing

20:55.600 --> 20:59.760
practice in a meaningful way. So I think this phase three is going to be extremely exciting because

20:59.760 --> 21:07.600
we're going to see more and more. So through Bayesian for example we've applied sort of a platform

21:07.600 --> 21:13.200
that we've built which provides basically these kinds of real-time signals to empower providers

21:13.200 --> 21:18.160
to catch life threatening complications early to save lives and we've done this in a number of

21:18.160 --> 21:24.640
different clinical areas and my sense is like just like we've done it there are a couple of other

21:24.640 --> 21:29.600
you know there are other groups around the country now there are also sort of like in imaging some

21:29.600 --> 21:33.840
of these early results you saw where people are like I have software can do better in some of these

21:33.840 --> 21:38.640
areas people have already started operationalizing it. So there's this in diabetic orthinopathy

21:38.640 --> 21:46.000
which is an area where you know often diagnosis is missed because patients go to the prime and the

21:46.000 --> 21:52.640
question was can the automate the diagnosis or screening of diabetic orthinopathy with primary care

21:52.640 --> 21:58.480
providers and so there are groups now that have built software that gets deployed at primary

21:59.280 --> 22:03.280
physician's office that can be used for screening automated screening and then if they're at

22:03.280 --> 22:08.080
high risk they're sent to a specialist. So that's sort of an example there's already now in white

22:08.080 --> 22:14.080
spread use and and in terms of billing and coding and reimbursement which is sort of what you alluded

22:14.080 --> 22:19.200
to earlier that's now starting to happen for some of the treatments. So there are like a couple

22:19.200 --> 22:25.840
different treatments are already where these AI type screening diagnostic workflow is tools

22:26.480 --> 22:32.720
where you know they're getting reimbursed today by either from the health system paying for it

22:32.720 --> 22:38.800
itself or the insurance company is paying for it for the use of it. Yeah yeah and that ends up

22:38.800 --> 22:45.120
being a big accelerator for innovation in the space is the impression I'm under. You always have

22:45.120 --> 22:51.760
to understand at the end of the day if you you can do things to make you know this was for me one

22:51.760 --> 22:59.840
of the most rude of evenings and you know like back in 2011 I sort of thought well doesn't it make

22:59.840 --> 23:05.520
so much sense we could save lives would not be enough but the reality is that's not enough that's

23:06.400 --> 23:10.480
the way a provider would see it is or a physician would see it is there are so many opportunities

23:10.480 --> 23:15.600
for saving lives you need to solve more than one problem for me need to help me save lives but you

23:15.600 --> 23:20.320
also need me to help me do other things you need to save me time you need to make my job easier a

23:20.320 --> 23:26.960
health system administrator will say you need to help us cut costs you need to help us improve our

23:27.760 --> 23:33.760
you know reduce penalties that we get so this is where deep marriage of the domain and the

23:33.760 --> 23:39.280
financial system and how it works and then marrying that to where the use cases are where there's

23:39.280 --> 23:44.560
real opportunity for adoption near term and then obviously long term this is where policy plays

23:44.560 --> 23:50.240
a role again right as we see more examples like this it's not my policies frozen there's always

23:50.240 --> 23:56.320
opportunity for new policies to get adopted that incentivize the use of these kinds of technology

23:56.320 --> 24:02.320
so for instance healthcare historically has been pretty reactive which means you know when a

24:02.320 --> 24:06.960
problem happens you show up I look at what's happening and I fix it I think where AI can make a

24:06.960 --> 24:12.800
big difference done right is moving it from being reactive to proactive we can fork we can look at

24:12.800 --> 24:17.840
your data in a granular way we can forecast we can make it possible for you to anticipate these

24:17.840 --> 24:24.240
complications and act on a timely way that is pretty exciting but today in some scenarios

24:25.040 --> 24:31.440
moving to proactive care might actually reduce the amount systems are getting paid which means

24:31.440 --> 24:37.680
there's a natural financial barrier to the adoption of these kinds of technologies that is also changing

24:37.680 --> 24:42.080
this is you know awareness that's coming people are becoming aware you know there are new

24:43.440 --> 24:49.200
financial models that are coming you know new sets of financial models like

24:49.200 --> 24:55.440
band of payments and value-based care where there's incentive for systems to be more

24:55.440 --> 25:02.480
more focused on preventative care proactive care and that all of that will also mean more

25:02.480 --> 25:12.160
opportunities for AI to impact lives got it got it you in kind of describing this phase two you

25:12.160 --> 25:18.960
rattled off a handful of methodological changes improvements that have happened over the

25:18.960 --> 25:26.480
past few years it seems like an interesting area to maybe dig into a little bit deeper so that folks

25:26.480 --> 25:31.040
you know that are thinking about entering the space have some ideas for the the way that they

25:31.040 --> 25:38.320
need to approach problems in the space can you elaborate on what some of the big you know differences

25:38.320 --> 25:43.360
that you've seen and you know the way folks need to approach machine learning problems in

25:43.360 --> 25:52.480
healthcare nowadays yeah so one of the very very big differences is sort of in some of the other

25:52.480 --> 25:58.720
areas there's this notion of like you have really good gold standards and really clear evaluation

25:58.720 --> 26:04.720
metrics so for instance you could go into face recognition say and maybe there's a very nice

26:04.720 --> 26:10.800
data set where everybody sat down and everybody can agree this person is this person and that person

26:10.800 --> 26:16.560
that person doesn't so much debate about it and you can and if your goal is to do face recognition

26:16.560 --> 26:24.640
or face detection the error metric is pretty clear so you can go in get a data set it's well annotated

26:24.640 --> 26:30.480
there's in a whole lot of disagreement and you have a clear metric to optimize and then people can

26:30.480 --> 26:37.600
go to town with all the creative ideas for optimizing that metric there are so many ways in which

26:37.600 --> 26:45.520
health data sets are not bad so for example in most clinical areas the notion of like what is

26:45.520 --> 26:51.360
the goal standard and what is the metric you're optimizing for is very unclear so as so when we first

26:51.360 --> 26:57.600
frame the successio a success early detection problem the question was well how early do you want

26:57.600 --> 27:04.000
to detect it because if it's too early maybe providers won't recognize it but if it's too late but

27:04.000 --> 27:10.160
that's not very productive so that's one example the second example okay what is sepsis that's

27:10.160 --> 27:16.320
an existential question people will sit down and debate like this person was treated for sepsis

27:16.320 --> 27:22.640
because they likely were septic but somebody else might say yeah but this person was being a bit

27:22.640 --> 27:28.400
conservative and treating them so what do you do do you treat that person as septic or not septic

27:28.400 --> 27:35.680
or do you treat it so how do you think about that third you could take data set from one hospital

27:35.680 --> 27:41.920
or one health system and you could learn a model that is very good at predicting there but as soon

27:41.920 --> 27:47.440
but you know we've written numerous papers on the topic like when you move it to a different hospital

27:47.440 --> 27:52.800
if you have these big rich deep models that are very flexible can learn anything they can easily

27:52.800 --> 27:57.920
pick up patterns that are very specific to how people practice in that hospital when you go to a

27:57.920 --> 28:03.120
different hospital that method may not generalize at all in fact there are papers showing you know

28:03.120 --> 28:11.760
certain methods are very brittle easily break as you shift the underlying data and and you want

28:11.760 --> 28:19.440
methods that are robust and to these kinds of shifts so can we so we need almost sort of like new

28:19.440 --> 28:26.160
class of reliable learning methods or you know ship stable methods like they have a number of

28:26.160 --> 28:31.440
different names in the field but basically methods that like where if there are new sense things

28:31.440 --> 28:36.400
that change in the data they're not going to actually impact the quality of the learning system

28:37.040 --> 28:43.760
or differently put up front you can get guarantees that if certain types of new sense changes

28:43.760 --> 28:48.880
happen they're not actually going to hurt the software's performance in an unpredictable way

28:48.880 --> 28:54.160
which is something that's very important in applications like you know social impact applications

28:54.160 --> 29:01.760
right where it's often a question of life as opposed to say like advertising breaks by contrast

29:03.120 --> 29:07.680
I could talk about a number of other methodological issues but you know all of these like

29:08.640 --> 29:15.680
how do you like health data are like so messy so messy and there's a lot of messiness how do you take

29:15.680 --> 29:24.640
into account the like by tackling the messiness and the messiness and measurement models in an

29:24.640 --> 29:32.800
intelligent way you really can show 200 300 percent improvements in precision or sensitivity so

29:33.680 --> 29:39.760
so yeah I think yeah I was actually going to ask about that because in the in the setups all

29:39.760 --> 29:46.960
this you described high tech and the introduction of electronic medical records as kind of opening up

29:46.960 --> 29:55.360
this you know huge opportunity but I still hear from folks that you know as much as you know we

29:55.360 --> 30:04.320
digitized the data is still very very messy very dirty and that remains a huge constraint in this

30:04.320 --> 30:10.720
particular set of applications I'm just curious so there if you can elaborate on that and what

30:10.720 --> 30:18.160
kinds of examples you can give us to help us understand the state of healthcare data yeah so I

30:18.160 --> 30:24.160
think healthcare data is definitely far more messy than any other domain I've ever worked with

30:26.240 --> 30:30.640
the the way I think about it is there are certain things you can do with it and there are other

30:30.640 --> 30:35.440
things you can't do with it which is why it's even more important to have deep expertise in understanding

30:35.440 --> 30:39.920
the data the complexity of the data but also the problems you're looking to solve with it to

30:39.920 --> 30:45.120
understand the risk profile right just like in a drug there's a notion of risk benefit analysis

30:45.760 --> 30:51.280
right almost everything comes with like a some kind of side effect so how is it getting used on

30:51.280 --> 30:55.440
home what's the side effect what's the benefit and there's a risk benefit trade off so in the same

30:55.440 --> 31:02.320
vein in terms of how data sets there are some applications where today's technology is just

31:02.320 --> 31:10.960
not there there are other applications where today's technology isn't there but you could build

31:10.960 --> 31:16.640
the right technology to improve it and yet other applications where no amount of amazing

31:16.640 --> 31:22.160
technology can help you because the information doesn't exist so I think there's a there's

31:22.160 --> 31:26.880
um certainly a number of problems where the information doesn't exist and we just need new

31:26.880 --> 31:32.880
modalities but the vast majority of problems are of the kinds of problems where the data exists

31:33.520 --> 31:39.440
because today human experts like physicians nurses care team members are looking at this data

31:39.440 --> 31:45.040
and making decisions right so the data exists and there's an opportunity to leverage the data in a

31:45.040 --> 31:51.440
much more intelligent way to be able to so it's all to be able to improve the quality of decision

31:51.440 --> 31:56.720
making and outcomes right so it's like a human expert is looking at it yeah and they're making

31:56.720 --> 32:02.160
decisions so whether you like it or not it's happening and so now the question is how amenable

32:02.160 --> 32:09.280
are these and the way and I think that's where we need the right kind of um machine learning

32:09.280 --> 32:14.560
AI technologies that are you know humble where like the researchers building these tools are humble

32:14.560 --> 32:20.240
they understand the difficulty of it and they're intelligently approaching you know which problems

32:20.240 --> 32:26.960
too tackle and then doing very careful evaluation so I guess another team here is evaluation so

32:26.960 --> 32:33.760
because this area is hard and correctness is so crucial I almost feel like unlike other fields where

32:34.880 --> 32:42.320
people spend you know 10 units of time like 90% of the time developing a model and then 10 minutes

32:42.320 --> 32:48.160
writing it up here it's the flip it's like whatever time you might spend the model you need to

32:48.160 --> 32:53.280
spend 9x that much more time triangulating in many many ways to get to a place where you know it works

32:54.080 --> 33:02.160
yeah and and that makes this really hard so um yeah so I think I think it's very promising I think

33:02.160 --> 33:07.360
the data are exciting I think the loads of opportunities is just uh it requires more patience

33:08.000 --> 33:15.440
more thoughtfulness more carefulness uh maybe back to methodology your you've named your company

33:15.440 --> 33:23.840
Bayesian health uh you mentioned causality uh in that list of um tools yeah talk about the role of

33:23.840 --> 33:32.480
causality and and um maybe by extension the approach that Bayesian is taking yeah so um when they hear

33:32.480 --> 33:40.400
the name Bayesian they often think oh is it only using Bayesian methodologies um so um so let me just

33:40.400 --> 33:47.120
sort of first quickly explain that which is um just like any smart human when we have a lot of

33:47.120 --> 33:54.800
different data coming at us we integrate it over time to uh to update our view of what we think

33:54.800 --> 33:59.920
is happening that's how the best physicians practice right they're continuously doing new tests

33:59.920 --> 34:04.640
they're integrating the new piece of information coming in they're doing uncertainty quantification

34:04.640 --> 34:08.000
they're thinking about how certain and uncertain they are about the different pieces of information

34:08.000 --> 34:13.280
coming in and putting it all together to come up with the forecast which updates as new information

34:13.280 --> 34:18.640
arrives that's a very Bayesian way of thinking so it's that's basically so the Bayes the Bayesian

34:18.640 --> 34:23.760
health the name comes from the idea of building intelligence software that gives you the ability

34:23.760 --> 34:30.880
to do that with largely with large scale health data in terms of the kinds of techniques it's it's

34:30.880 --> 34:38.800
really um a comment you know so you ask me causal inference so when we have today for these models

34:38.800 --> 34:47.040
to be able to be um intelligible and actionable it's so important for it to not capture

34:47.040 --> 34:55.200
spurious correlations or spurious dependencies that are almost like hurt the user trust

34:55.200 --> 35:02.160
so in some sense that's where knowledge of the domain the data generating process and as a result

35:02.160 --> 35:08.640
uh techniques from causal inference are really helpful more more than in in being able to build

35:08.640 --> 35:14.800
models that are going to be more intelligible and actionable turns out these models are also more

35:14.800 --> 35:22.800
transportable or you know more likely to generalize as you go across sites or across uh you know even

35:22.800 --> 35:27.760
within the same site across time where you know data collection methodologies might change or practice

35:27.760 --> 35:33.600
patterns might change and so on and so forth by making models that aren't just learning memorizing

35:33.600 --> 35:40.640
what's in the data but it's reasoning about what if like if this were to happen then what if that

35:40.640 --> 35:48.640
were to happen then what i'll give you a simple example there was a really nice um paper a few years

35:48.640 --> 35:55.600
ago where it's a very well cited paper now where um this team of researchers were trying to learn

35:55.600 --> 36:01.040
a model for predicting patients who came in the pneumonia into a hospital emergency department

36:01.600 --> 36:06.640
their risk profile with the idea that if they were high risk they would place them in the intensive

36:06.640 --> 36:11.520
care unit which is the sicker unit or like the high acuity unit right like where the sickest patients

36:11.520 --> 36:17.200
go and then if they were lower risk maybe they would go to the floor and uh they took historical

36:17.200 --> 36:22.400
data sets and they learned using like you know classical supervised learning techniques a model

36:22.400 --> 36:27.520
and what that model did is looked at retrospective data and said okay great if the patient came in

36:27.520 --> 36:34.400
and they died then that's my training data for high risk if the patient survived that's my training

36:34.400 --> 36:39.440
data for not high risk and then based on that they learned a model and then what the model learned

36:39.440 --> 36:46.800
was turns out patients who had pneumonia with asthma were actually lower risk than patients with

36:46.800 --> 36:53.760
just pneumonia which is uh for totally counterintuitive because you know asthma complicates the case

36:53.760 --> 36:59.920
quite a bit and actually you tend to have poor outcomes now when they went and looked in the data

36:59.920 --> 37:04.320
they realize actually the reason that was happening is because the people with pneumonia and asthma

37:05.280 --> 37:10.080
were getting escalated to the intensive care unit versus the people with just pneumonia

37:10.080 --> 37:14.720
were on the floor but in the intensive care unit they were just getting constant monitoring,

37:14.720 --> 37:21.680
constant supervision, constant care which meant when you looked at the resulting models all it was

37:21.680 --> 37:28.000
really like it was ignoring the fact that this person had been so you know like what if what the

37:28.000 --> 37:32.560
model should have done is what would it would this person have been high risk if I didn't send

37:32.560 --> 37:37.680
them to the ICU would this person be high risk if I went exactly you want to do counterfactual

37:37.680 --> 37:43.600
reasoning so that's sort of an area where we you know very early on he's you know started realizing

37:43.600 --> 37:49.680
and with health data sets where you really are trying to reason about a patient's risk profile

37:49.680 --> 37:52.800
you really want to I mean this isn't just pertinent to health this is just pertinent across the

37:52.800 --> 37:58.560
board when you're looking at any kind of temporal or sequential decision making problem or decision

37:58.560 --> 38:02.720
making problem you want to ask you know under different interventions what would the trajectory

38:02.720 --> 38:08.320
have looked like so you can basically figure out what's the right thing to do and the question

38:08.320 --> 38:13.680
therefore one should be asking when developing these models is what would this person's risk profile

38:13.680 --> 38:19.680
be had they not been to the ICU as opposed to what's the risk profile ignoring what was done to

38:19.680 --> 38:26.800
them right right so that's sort of a very simple example of a place where you know how machine like

38:26.800 --> 38:33.440
the next trend these the types of machine learning approaches we're using embrace causality

38:33.440 --> 38:39.440
in order to be able to get more sensical models that are both more accurate more actionable but also

38:39.440 --> 38:49.680
more transportable and safe so that by the time this show is out in the wild and folks are listening

38:49.680 --> 38:57.360
to it Bayesian health will be announced and released and as part of that you're releasing a study

38:57.360 --> 39:01.200
can you tell us a little bit about the study that you're that will be published by the time folks

39:01.200 --> 39:11.440
yeah this yeah yeah so we're releasing a copy of a manuscript where basically starting in April

39:11.440 --> 39:18.960
2018 across five different hospital sites we built and deployed the software for early detection

39:18.960 --> 39:27.040
of sepsis and making it possible for care teams specifically providers and nursing staff to be able

39:27.040 --> 39:32.640
to get access to these real-time machine learning inferences entirely within the workflow

39:32.640 --> 39:38.080
to flag patients who are high risk and then make it possible and and so you know it we provided

39:38.080 --> 39:43.360
these dynamic workflows within the EMR so EMR is the electronic medical record it's the infrastructure

39:43.360 --> 39:49.840
the system they use for documenting for recording you know it's sort of the platform that they

39:49.840 --> 39:55.680
you know care team like providers spend the vast majority of the time in and so what we did is

39:55.680 --> 40:02.480
provided this real-time software that like pulls in in the background in real-time crunches it

40:03.360 --> 40:08.880
puts information back into the EMR into the provider's workflow flags a patient when they're at

40:08.880 --> 40:14.240
risk makes it very easy for the provider to come in and see why were they flagged more context

40:14.240 --> 40:20.240
around the patient in terms of you know what was the risk for certain comorbidities certain

40:20.240 --> 40:27.280
um certain adverse events and then also like what are the indicators that they could quickly look at

40:27.840 --> 40:33.520
and um now they look at it they evaluate and then if they agree it septic they treat it

40:33.520 --> 40:38.960
and what the study does is analyzes um and it's sort of a first study of a guy in the sense that

40:38.960 --> 40:44.800
it's large it's analyzing physician adoption factors impacting physician adoption which is

40:44.800 --> 40:51.120
really crucial for the purpose of building future CDS tools that have chance of getting increased

40:51.120 --> 40:58.400
adoption and also like yes sorry clinical decision support so any kind of like AI driven decision

40:58.400 --> 41:04.320
support decision augmentation tools and it's impact on outcomes and so what we find is one

41:05.360 --> 41:10.880
89% of the so it was a you know upwards of nearly 500,000 patients was screened through the

41:10.880 --> 41:18.960
software 10,000 cases of sepsis were analyzed and then providers when the software flagged a patient

41:20.480 --> 41:27.840
90% of the times or 89% to be precise providers came in you know it was a passive tool but they

41:27.840 --> 41:36.160
sorted out interacted with it put in an evaluation and then we found that basically when providers did

41:36.160 --> 41:43.200
that in patients on whom providers came in and entered in evaluation and treated them a 1.9

41:43.200 --> 41:49.200
hour median difference like so for the in the meeting case 9 1.9 hour difference in

41:50.080 --> 41:56.160
movement and treatment timing so and then this obviously has subsequent impact in terms of

41:57.040 --> 42:04.640
reductions in mortality, morbidity, length of stay, this manuscript only talks about

42:04.640 --> 42:08.480
impact on clinical treatment practice and the key clinical metrics people care about like time to

42:08.480 --> 42:14.160
antibiotics the next set of manuscripts will release will be more on clinical also discussing

42:14.160 --> 42:18.720
then verifying like the mortality impact and morbidity impact and then one of the interesting

42:18.720 --> 42:25.280
things was in sepsis being able to recognize it precisely is just really really hard and like

42:26.000 --> 42:31.040
what we were able to see in our study is like with very high sensitivity we were able to detect

42:31.040 --> 42:40.960
cases but also you know the like you know like the 10x higher precision than typically is seen

42:40.960 --> 42:45.920
with widely available software we were able to like help them you know like reduce false

42:45.920 --> 42:50.720
alerting quite a bit by basically improving the precision right so like in our study one in three

42:50.720 --> 42:56.000
cases were confirmed as septic and you know it's a needle in a haystack problem only a small

42:56.000 --> 43:00.560
number of cases in a given day gets sepsis so it's really a problem of like how do we go from

43:00.560 --> 43:08.000
a hundred cases to the three or four we need to worry about yeah yeah I'm curious about the

43:08.000 --> 43:13.840
adoption side of things what type or level of engagement did you have with the physicians who

43:13.840 --> 43:19.040
were you know ultimately gained access to did they gain access to the tool passively where they

43:19.040 --> 43:24.320
actively onboarded what goes into getting a physician on board and what have you learned about

43:24.320 --> 43:32.240
that process yeah so actually I think so we very actively onboarded them when we initially launched

43:32.240 --> 43:37.600
it in 2018 it was super funny we built it we were like all the papers here it shows it works

43:38.240 --> 43:45.360
the data's there we built it we integrated it we launched it and then literally two physicians

43:45.360 --> 43:51.520
used it at this particular site and it was so disheartening because I think that was like end of

43:51.520 --> 43:57.040
2017 early 2018 it was sometime around then I can't remember maybe sometime in 2017 it feels like

43:57.040 --> 44:01.680
forever ago but like I just remember distinctly like we launched it we thought it was going to be

44:01.680 --> 44:10.240
such a big deal and it was so sad when we were monitoring the data coming in to see literally like

44:10.960 --> 44:16.400
nobody was using it or like a small number of people who were deeply involved in the development

44:16.400 --> 44:22.320
were using it so we had to do a lot of work to get it from like machine learning researchers and

44:22.320 --> 44:29.040
engineers and launching a piece of software to people who understand human machine teaming and

44:29.040 --> 44:35.680
collaboration and workflow integration and design and you know like that gap needed to be closed

44:36.240 --> 44:40.640
and then part of that is also how you launched the software and in medicine this is really

44:40.640 --> 44:46.720
important because it's part of the trust building process so we basically partnered with champions

44:46.720 --> 44:53.920
in local sites to basically create materials that was really easy for them to read and you know

44:53.920 --> 45:00.000
gave very and the tool was designed to be very intuitive easy to use and then and it's very simple

45:00.000 --> 45:06.560
but basically making it so that we could basically have a little video where they could see

45:06.560 --> 45:12.000
understand how to use it and then we also had a whole infrastructure for monitoring engagement

45:12.000 --> 45:18.320
adoption which was super crucial because if you can monitor then we could understand like you know

45:18.320 --> 45:22.160
both in terms of the health of the models itself and how they were working as we scaled across

45:22.160 --> 45:27.600
sites but also like what was used looking like where there were barriers and how we could mitigate

45:27.600 --> 45:33.600
those barriers and so we partnered with our champions to be able to then close barriers for adoption

45:33.600 --> 45:37.600
right and in some and these were very barrier barriers some around software some around the

45:37.600 --> 45:43.440
perception some around the understanding and we took different approaches to closing these barriers

45:44.160 --> 45:50.720
based on you know what we learned awesome awesome um maybe to wrap things up you can share

45:50.720 --> 45:57.760
a little bit about where you think the field is headed yeah um I mean I'm super it's I'm super

45:57.760 --> 46:02.240
excited about where I think the next five years will be in this field I think we're really at a

46:02.240 --> 46:07.520
place now where you know the data exists the infrastructure exists the ability to deliver these

46:07.520 --> 46:12.560
inferences within workflow exists and the ability and our experiments and papers show the ability

46:12.560 --> 46:18.160
for you know and providers are willing to engage and they're engaging and we can have meaningful

46:18.160 --> 46:24.240
impact on practice so to me the next five years is about leveraging this whole stack to apply

46:24.240 --> 46:29.520
this thoughtfully in a number of other clinical areas and start doing really thoughtful evaluations

46:29.520 --> 46:35.520
like one of the very big things that's been missing is because there are no evaluations people don't

46:35.520 --> 46:40.160
know if something is working and if they don't know something is working they they are not going to

46:40.160 --> 46:46.800
adopt it and trust it and medicine this is such a key currency for anything so today a lot of

46:46.800 --> 46:52.720
software and medicine software based tools people are just people deployed but they haven't had the

46:52.720 --> 46:59.120
infrastructure to really evaluate measure efficacy um but in you know as we've sort of worked in this

46:59.120 --> 47:04.080
area and I think um you know we've we've developed in front of monitoring so to me the next five

47:04.080 --> 47:10.160
years is we others in the field release more and more studies showing efficacy that now accelerates

47:10.160 --> 47:15.040
adoption learning around what increases adoption and then obviously if you have a good quality

47:15.040 --> 47:18.000
intervention that is precise and gets adoption you're going to get outcomes.

47:19.520 --> 47:24.960
Awesome awesome. Well Suci thanks so much for joining us and sharing a bit about what you're

47:24.960 --> 47:33.680
up to and congrats on the launch of the company. It's been great to chat with you. Yeah like

47:33.680 --> 47:38.240
why Sam I'm so glad we finally got to catch up and hopefully the next time we speak a few years

47:38.240 --> 47:43.200
from now I'll have much more good news for you in terms of the number of different clinical areas

47:43.200 --> 47:57.360
that are already being impacted by AI in that real time. Fantastic thank you.

