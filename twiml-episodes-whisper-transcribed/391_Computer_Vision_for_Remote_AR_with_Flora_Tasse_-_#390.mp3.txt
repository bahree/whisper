Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Before we get to today's episode, I'd like to send a huge thank you to our friends
at Qualcomm for their support of the podcast and their sponsorship of this series.
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,
reasoning, and action ubiquitous across devices.
Their work makes it possible for billions of users around the world to have AI enhanced
experiences on Qualcomm technology's powered devices.
To learn more about what Qualcomm is up to on the research front, visit twimalai.com-qualcomm-qalcomm-al-c-o-m-m.
And now onto the show.
Alright everyone, I am here with Flora Tase.
Flora is head of computer vision and AI research at Stream, a company that she joined
through the acquisition of another company which she co-founded, Solario.
Flora, welcome to the Twimal AI Podcast.
Thank you Sam, it's a pleasure to be here.
Thanks for the invite.
It's a pleasure to have you on the show and I'm really looking forward to digging into
your recent CVPR presentation.
You did a keynote at the ARVR workshop at CVPR.
I was excited to see that there's enough happening at the kind of intersection of ARVR and
machine learning to, you know, that there's a workshop at CVPR on that topic.
These tons happening.
It was awesome.
These tons happening.
That's face.
So it was definitely a good workshop.
Yeah.
Well, you can tell us all about that, but before you do, please share a little bit about
your background and how you got started in computer vision, how you came to found Solario,
how you ended up at Stream.
I'd love to hear all of it.
That's like a, that would be a long story, but I would try to be the shot.
So I was born and raised in Cameroon.
So Cameroon is in Central Africa and more precisely in the city of Duala.
And so I was raised in the French speaking part of the country, so you might notice that
for my accent.
Okay.
There's some French in there and yeah, so from a very early age, I was very much into
special effects and movies and more precisely Jurassic Park.
So I was a big fan of the, of the movies because of the dinosaurs.
I would just like, I would sit like this close to the TV.
I would introduce dinosaurs and wondering, how can it be like, why is this so real?
And then I asked my dad, like, how can they make these extinct creatures look so realistic?
And then he said, graphics, it's like, okay, yeah, that's what I'm going to do.
That seems like my perfect dream, you know, making the impossible become possible.
So fast forward a few years, I did my bachelor in maths in the English speaking part.
So I had to learn English and then move and did maths because I couldn't, they had no
course in computer science.
So then move out of the country to South Africa, where I did a master in, in Cape Town, beautiful
city in South Africa.
Yeah, it's amazing, I recommend it's a great vacation spot.
So I did my, my master's there then came to Cambridge for my PhD.
So 2012, I arrived in the UK already to make my dreams come true.
And so that was definitely like a good experience.
So at the Cambridge, I was looking at how do you take real things in images and turn them
into 3D content.
So I was doing some 3D shape retrieval, shape analysis from images.
I mean, you have a 2D image and you want to turn that into a 3D shape.
Exactly.
Yeah, that's exactly what I was doing.
So for years or three and a half years, I stumbled upon like a great discovery that if you
actually incorporate NLP, so if you're in corporate language information, you can basically
get really, really accurate results going from 2D to 3D.
And so the addition of that, yeah, so the concept was that rather than just looking at images
as just, you know, pixels that you can then turn into some features and then use classification
on them, it was a concept that those features can actually be semantic features.
So there's some semantic meaning attached to those descriptors that you are, that you
are generating.
Okay.
And so that means that of the text that would associate or would be associated with an image.
So yeah, the very early version was just, so if I look at the chair, I know it's a chair.
So I have like my data, my data set has labels like this is an image of a chair and this
is a chair.
And what we are trying to do is create a representation that capture both of those information.
So it captures both the visual information and the text information, which in this case
is just the semantic meaning of what the word share is.
So in language processing, they have a lot of language models.
So that's what they use for translation and other NLP tasks and they do this by representing
those words as multi-dimensional vectors.
And so what we were doing was creating a representation that would capture both the visual
data and that language based representation.
In that, it would capture semantic data inside the representation.
And since that is actually, you have the same concept in 3D shapes, in hand drawn sketches,
you can use that as a way of bridging the garbage in all of these domains.
That was my PhD, basically.
And so we had really good results, really increased the bench, the baseline by like 30% in some
of the use cases.
And so that was great.
And the question is what happens next, what do we do with this?
So there were some offers on the table, like big companies could go and work for, but
I was really passionate about that specific discovery and how you could scale that to not
just one object, but multiple scenes.
Can I then take a picture of my living room and then turn each object into a hidden shape
and build a scene graph of my surrounding?
And so that became Celerio, that's what Celerio was doing.
So it's fun, now that work out of the university created a startup called Celerio.
That was looking at how do you take the real world and turn that into a digital format?
Yeah, the story of that for how long before stream came along.
Two years and a half.
Okay.
Yes.
So it was a great journey.
Like journey, we got to like build something I was running on the phone, had it tested by
a bunch of like AR developers, it was a really cool and interesting journey.
And during that testing phase, stream was one of the people who were testing our platform,
looking to integrate that into their product.
And we had you had met stream like a year ago because we have a common investor.
And so we really like what they were doing.
So stream is all about remote collaboration and how do you add value to that using open
technology?
So you take you take a video collaboration platform and then you incorporate it at a
very core of it.
And so they were doing that for like really amazing use cases like repairing, customer support,
home repairs and so on.
And so they came with us with an offer because they saw what we're doing as very close to
like the vision of them of their product.
And we were very interested in like bringing this tech to the real world.
So we had been hard at work building the technology and we're really happy about the results,
but we couldn't wait to actually see that being used by real customers and stream hello
one of the best use cases for AR.
And so that's how the acquisition happened.
That's awesome.
Yeah, go.
Thank you.
Thank you.
So I'll say before we get very far, we will link in the show notes to your slides from
the AR VR workshop.
And folks who are listening should definitely check them out because there are a lot of
cool videos and demos and things like that in the slides.
So you know, maybe tell us a little bit about your talk and kind of what some of the main
points that you're presenting.
So the talk was entitled computer vision for remote AR.
And so a lot of time people think of AR VR, they think of a mobile phone, they think
of a headset, they think of, you know, like, yes, basically.
And oh, like they would think of like an experience where you are looking at something and then
you have something augmented on top of that.
So remote AI is very different because someone else across the ocean is kind of experiencing
what you are experiencing, but they are not in that same physical location.
And so then I was talking about like, yes, what remote AR is and how computer vision can
then help solve some of the issues that take place in that kind of scenario.
And more precisely, I was looking at the stream use case, what we do and how we are using
computer vision to solve some of the issues.
One of them is something like measuring the environment.
So a quick example, if I'm here in my living room and I have an issue with my fridge and
I call one of these companies, I call their customer support and say I have this issue.
Now they can use stream and the augmented reality platform that we build so that the expert
we call it a pro on the other side can have visibility into my environment and what I'm
looking at.
And so sometimes one of the main use case, main question that they have is can we measure
things?
We want to be able to measure something so that if we send, if something is broken and
we have to send something over, you know, you know, we know what size it should be.
How can you make measure and more accurate?
And it's really hard to do that on an image.
So one thing that we do is being able to build a 3D mesh of the environment.
And so once you have a 3D mesh, then the pro can just say from point A to point B, what
is the distance?
And so things that are important is measuring like the geometric data, but also the texture
data because once you have texture, it gives you context.
And so I was, yeah.
The meshes that you build or are working with are these based solely on 2D data or like
the iPad now has a lidar sensor in it, it's got to make the job easier.
Yeah, definitely.
That has been, yeah, that's, that's, I look forward to more devices have interesting.
So we definitely have a framework that we build in house for doing meshing from only to
the images.
Okay.
But that wouldn't be needed as accurate as using depth sensing, definitely.
So we have different solutions, depending on the capability of the device that the customer
is using.
Okay.
Yeah.
And it's building the 3D mesh from the 2D image.
Talk a little bit about, you know, is that a largely solved problem?
Is it still like really hard?
Like where is it on the spectrum of, you know, computer vision, AR, types of challenges?
Yeah, it's still, it's still really hard, so it's a solved problem.
As solved as it can be, what people are not doing is adding AI for predicting depth, but
I see some challenges around using predicted depth for meshing because you need temporal
coherence.
So as you go from A to 3D, you need things to be kind of correct.
You need a depth to be current across the different frames.
So that's still a, that's still some of the people are looking at.
So for the vast majority, multiple are using motion stereo.
So you look at consecutive frames and you are trying to find the values that are consistent
across multiple frames.
And so that is going to be harder if you have frames that don't have much texture, depending
on the change of lighting, that still would be tricky.
So people are now looking towards like stereo systems or, you know, devices that have
their same thing.
So there's not much work going into, you know, meshing from 2D images.
There's not that much work going in there because there will always be some limitations
because of just the nature of the, of the problem.
And the stereo use case that you're describing is that assuming multiple cameras?
Yeah.
So they, one of the, some of the techniques that people are using from 2D images is like
using motion stereo, so you kind of look at two images from the same camera and if they
are close enough, you can simulate stereo.
And obviously you also have like phones that have left and right images.
In that sense, you can just like make the problem easier because you don't have to try to
rectify, you know, images that are happening at different time frames, time stamps.
You, I'm sorry, I interrupted you, you're going through these use cases that you see and
one of them is measuring things.
Yeah, measuring things.
Yeah, I'm just making the point that for measuring things, meshing and texture are very important.
So part of my talk was, how do you attach texture to a mesh that is changing all the time
and how do you do that in a way that's real time so that it doesn't interrupt the customer
experience.
So that was one of the big points I talk about in the presentation.
Okay.
Yeah.
It was drilling to that.
Is that something that your research is focusing on at a stream?
Yeah.
So that's one thing that you are actively working on is.
So a lot of prior work, if you look at some of the mesh reconstruction frameworks out
there, they will give you a color per vertex.
So they don't really have, so the colors kind of look really washed out and blurry.
And so it's kind of hard to like make out the edges of, you know, a countertop in the
mesh.
And so we see texture as a big part of, you know, making that easier, making is easier
for people to be able to like make out the context and the edges.
And so it becomes really tricky to like most of the time this, this is happening.
The testing process happens after the meshing.
We are trying to do that in real time as the mesh gets updated all the time.
So that's, that is what makes it very challenging.
And that's the research problem that you are working on.
And so the textures that we're talking about also like at some kind of texture estimate
or something per vertex.
Yeah.
So you have, you have like a mesh that you obtain from, it doesn't matter where you got
it from.
And you have some video and you're trying to kind of, and you know where your camera is.
So I mean, time you can say, I'm at this point looking into this direction, how can I use
that information and apply that to my mesh.
And the camera is moving all the time, the video has blur.
So how do you then like kind of try to make a mesh that looks good, at least like with
no artifacts.
So that's, yeah, that's a problem that you look at.
And is the end application in trying to apply the colors and textures to the mesh?
Are you essentially trying to create a, like a 3D model of whatever the camera is seeing
someplace else?
Yes, because yeah, not only you are doing that, you are trying to, so it, this is remote
AR, right?
So this could be shared with one person, multiple people who are looking at the same scene and
they actually can like, they can insert 3D elements and things like that.
So not only you are trying to do this, meshing this texturing, you are also trying to sync
it across devices.
So there's a lot of networking challenges that come into place when you start doing these
things.
So I'm wondering why I need the mesh to be very accurate if I've got the video stream.
So because it's really hard to do measurements in a video stream because the camera is always
moving.
So if you're a professional on the other end, you pretty much have to say, oh, like, can
you just be still for like the next 20 seconds?
Well, I take this measurement and the measurement would be accurate because you might be clicking
at some place, but actually, it's not, it's the pixel or the vertex data at that location
is not that correct.
You actually look at something behind, let's say behind the fridge instead of like on the
edge of the fridge.
And so when you have 3D information, it gives you just more context.
You can rotate the mesh and then make sure that you're actually clicking the right end
point.
And so then having the color and the texture information associated with the mesh allows
the person who's remote to make sure that they're clicking on the right things, exactly,
exactly.
So they're the experts so that they know how to take this measurement.
Usually they have to come into the home to do this.
And in the home, they are in 3D because they're in the location.
So how can we kind of replicate that experience in a remote setting where they're not physically
there, so that's where like 3D information is so, so important.
And if you fast forward like in a setting where you have headsets, then obviously you can
put on your headset and you can actually walk around the environment and feel like you
are in the physical location.
And again, with the color and texture, you want, certainly with the headset, you want
to have a situation where you're kind of walking around navigating the world and the
video might not be there anymore or the video might be going in in a different direction
or exactly.
Exactly.
Yeah.
Okay.
Cool.
So texture as a big element of measurement was one of the points that you went into.
What were some of the other cases you covered?
So one thing that we don't see more shared computer vision conference, like even ARV are,
you know, applications, it's just been able to know is extract metadata from the environment.
So more precisely, if you are looking at see a washing machine and a place looking to
fix this washing machine, it's very important for them to know what is the serial number,
what model are we looking at.
So that's that's like the bread and bread of customer support.
And how do we, and then the customer really don't know this information or had you kind
of like they are focused on the fixing part of it, not necessarily on the data collection
part of the process.
So one of the computer vision problems that you have is how do you look at a device and
be able to extract information about model number, serial number, especially from labels.
So if some of these devices, they make it easy for you to have, they have labels.
And then can you look at text and know what text is relevant and what text is associated
to a serial number or to a model number.
So that's the metadata extraction part of what we do.
Having repaired a bunch of home appliances or at least attempted to, you know, and
seeing a lot of these labels, they have all kinds of numbers on them and trying to figure
out which ones are the right ones.
It sounds similar to looking at some other kind of structured or semi structured text,
like a document, like an invoice and trying to figure out what's the invoice number
and what are the other items and that kind of thing.
Yeah, yeah.
And it can be so hard, right?
The orientation is probably going to be weird and a perspective is probably going to
be weird.
Yeah.
The lighting is probably going to be bad.
Definitely.
Like we do a lot with like skewed angles, like depending on where you are looking from,
it can be very distorted.
Adding to the fact that like manufacturers have very different standards.
So you can't have like a rule-based system that says, oh, like we'll always be looking
at, you know, this area of the label.
So AI, this is where like AI really comes, comes handy because so yeah, we just basically
look at it as a two-part problem.
First is an OCR problem.
So how can I just extract text only data from this image?
And then it's, you know, which part are relevant to what we care about.
And so that becomes a classification problem based on the textual data, based on ways
located, train over a lot of labels and a lot of a lot of data.
We can now kind of look at the label and tell the custom, the pro that this is a serial
number.
This is the model number of these appliance.
What are the OCR part of that?
Are you kind of taking the image as is and pulling the letters out?
You know, however they are, are you trying to reorient that kind of as a, you know, ortho-rectification
process so that it's straight?
So we, so we leverage a lot of like the existing frameworks for OCR because OCR is just about
taking some image, getting text out of it.
So we can do, you can do a lot of, like a lot of data augmentation that happens in this,
so these type of models have done a lot of data pre-processing to get ready.
So they're very good at, you know, at, at extracts and text, it's not straight.
Yeah.
Yeah.
So we don't have to worry about that part that part.
So the biggest, so in this case, the biggest effort is like building a model that is
going to do the, the classification right because obviously like OCR just gives you
random text and so, and in the case of Cian, these numbers, they might occur in different
places, might be like some of the tops, some of the buttons.
And then the OCR might give you groups, but those groups could be like very disjointed.
So how do you combine this information into something that makes sense?
That's where our focus is on, less on the OCR, more like making sense of the OCR output.
And are you using as a feature in that part of the problem, the actual text that might
say, you know, model no or serial no, or are you kind of ignoring that and just looking
at trying to do it based on the number itself?
So we do it based on the whole text.
So from the whole text, we would basically classify, we say, okay, we kept by the Cian
number, what bits of this text represent the Cian number.
And we do that by training a model that does that.
So implicitly, the model is using, is using all of this like, you know, implicit information
around like where this is a like Cian number on top, behind before and so on.
We don't explicitly do it, but I think it's implicitly different.
It's a fun problem.
There were some other use cases that you walked through?
Oh, yeah.
Well, my favorite is always, so when you talk, so going back to what I was seeing around
like blending, what's real, what's virtual and having like this immersive environment,
one of the key things that I'm very excited about is walk through like tutorials in AR.
So you get a new appliance, you want to know how to get started, we can have an AR tutorial
experience around that machine to tell you how to like use it, like it's broken and then
you can have a troubleshooting walk through around that appliance.
And so for me, that's like one of the most fun use case because it gives the power back
to the customer.
And and one of the key problem in being able to do that is being able to identify the
six-duff pose of whatever appliance you are looking at or whatever object you're looking
at, we need to be able to know what is translation, rotation, so we can then impose data like
virtual data around it.
So I talk a bit about how we do six-duff pose estimation for objects like that.
And elaborate on on what that means because I saw that in a presentation, you know, you
get right as 60 and it's like 60, what it was, the six.
Six-duff, well, six-duff, a six degrees of freedom.
So three for translation, three for rotation, so that makes six variables that you have
to estimate in order to like get a post-transform for where your object is respect to your camera.
Oh, got it, got it, got it, got it.
So you've got your you're located in an object in 3D space and it's got some rotation and
that engine is next position.
Yeah.
Okay.
So at any point in time, we are trying to figure out what are the six values so that you can
build a transform from that and then you can use it to impose your digital content around
it.
Okay.
Yeah.
So maybe what are the challenging parts of that problem?
And it's one of, even today, I think for the past, as far as I can remember, people have
always been working on post-destimation.
Even DCI-CDPR, there are so many papers around that because it's a very hard problem to solve.
For us, it's even harder because you are actually not looking at the image, you are looking at
a video.
So your camera is changing and you expect that post to be correct across multiple frames.
So for us, we use a lot of, we use a combination of techniques.
So at DCI-CDPR, you see a lot of work around pure MLB techniques where you can just give
an image to a model and it will tell you what are those six values or it will tell you
what are the key points.
So if you have the key points of your object, you can recover the post-information.
So what we saw with those technologies that are not nearly accurate enough, like typically
you have like a 10 degrees margin of error, which is for us not good enough because we are
going to be superimposing content on top of this in AR so that we need like super accurate
post.
So what we end up doing is we have a combination of both like getting some MLB techniques to
identify key points or even just asking users to select those key points.
And then we focus on like from that initial post, how can we refine it further to really
fit the edges of what is currently in the image.
And so these are called region based approaches where it's looking at what is a background, what
is the foreground, and how can we find a post that best differentiate between the two.
So those are very accurate for this type of use case.
Okay.
It sounds like in this case a big part of the way you improve your results is by kind of
manipulating the user interface so that you can get more information with that.
Exactly.
Overly burdensome on the part of the person is using it.
Yeah.
So that's so we try because accuracy is so important for us.
And if we need some user input, we need to leverage some user input to get to that level
of accuracy.
And that's something that we are happy to do.
And in a sense, it also helps us collect more data so that we can also improve the accuracy
of our models.
Have you seen many products out on the market that are kind of putting this kind of AR in
the hands of the consumer?
Like you mentioned this training use case for washing machine or something.
Is anyone doing that?
I haven't seen anyone doing that.
So I think like streaming is in a very good position because we have like an existing
customer base who are actually asking for these kind of features because they see that
as a big need.
And so that's kind of driving the problems you are working on.
So I think I haven't seen it out there yet.
So maybe people are doing working on that like behind the scenes.
I was just going to say, I've seen a lot of demos of it in a kind of industrial type
of use case like at one of the Microsoft conferences a year ago, the CIO, Sachin
Adela demonstrated a kind of remote repair, you know, a worker goes into a telecom closet,
you know, out in the middle of nowhere and they're trying to figure out how to, you know,
get the 5G working again or something.
There's a, you know, they're supported by someone who's pushing screens, overlay screens
on top of what they're seeing telling you a lot of fixed stuff.
The closest I've seen to it in something, consumery is I don't know how it's kind of
prosumer slash low end enterprise.
And it's actual, I think it's real as opposed to a demo.
But there's a networking company ubiquity networks that makes these again, kind of prosumer
networking gear and their latest line of switches in some of their promotional materials.
They're showing you kind of holding your phone over the, these switches and they're
showing you like what's connected to what in the different ports.
Yes.
But that's probably the only example I've seen of that actually in the wild.
Yeah.
It's a pretty cool.
Yeah.
Definitely.
Yeah.
And I've seen obviously like AI Kit has some demos around like being able to, if you
have like a Lego set and then you, you're able to kind of recognize that and then in
people some 3D content around that.
So there's definitely been some use some demos around this type of use cases.
So when we mentioned Microsoft, I was just thinking, oh, it has a depth sensor.
That would make our life easier.
Yes.
Yes.
Yes.
Yes.
That's why I can't wait for more their sensing to come to phones, but assuming just a phone.
And so you're only able to use things that are commonly available on phones.
Yeah.
Yeah.
So definitely the world where what is going to happen is that like on phones that don't
have the sensing, we have these techniques on phones that do have extra capabilities, then
we can offer.
We can use that data that's sensor data to even push further the accuracy of our methods.
So the more devices of those like the better.
What are some of the other big challenges that you see in the space?
In our space.
Okay.
So one of the big.
So at the end of representation towards the end, I mentioned spatial AI and the yeah, and
what it meant for us.
And for us, it just means that going beyond trying to understand these tasks at the granular
level, but like look at a big picture.
So at the end of the day, what you want to know is in a video, in a collaboration between
two people, what is the problem?
What is the solution?
We're talking about, and how do you capture this information in a way that you can then
reproduce it somewhere else?
And so yeah, you have this idea of both chat boards.
You have this everywhere, like in the banking systems where you have an issue, and then they
can just kind of based on your question, they will suggest you something.
And if they can't fix that, then it will escalate to a human being.
And so I take that AI and I add special AI on top of that, the sense that now this AI
both actually they understand, they can look at what you're looking at, and they can make
sense of it.
So ultimately, can we create agents that are able to solve, like to help you solve these
issues that you're needing to actually talk to a human being?
That's kind of like the big moonshot problem that you are looking at, beyond this individual
computer vision test.
Yeah.
Something as an example there, the, you know, the cliched scene we see in the movies where
the bomb squad comes in and defuse the bomb, you know, someone's holding their phone
over it in the system, as opposed to some remote expert saying, oh, it's definitely the
yellow cable.
Clip the yellow cable.
Exactly.
Exactly.
Yeah.
The bit I'd be right.
Yeah.
The bit I'd be right.
Yeah.
But yeah, that's what some of the things that we are thinking of now is how do we start
getting there by maybe like starting by making sense of the video data, like can we start
clustering videos by, you know, the functionality.
So not just similarities in a sense, yeah, they have similar scenes, but they're trying
to solve similar problems.
So how can you capture that out of a video and be able to start creating these clusters,
which can be very useful for just like training for a company to understand, like what are
the most common issues and capture, you know, what are the solutions to these issues so
that they can replicate that in other environments.
Mm-hmm.
Sounds a little bit like you're describing kind of going back to expert systems types of
approaches for, you know, but coupling that with information that we've pulled using
modern AI techniques, you know, neural networks and the like, am I hearing that correctly?
Yeah.
Special expert systems.
Special expert systems.
The our mission at stream is making the world expertise more accessible.
Okay.
And so how do you open access to like, you know, to everyone.
So that's that's part of how we get to that vision.
Okay.
Yeah.
Do you follow the computer vision space more broadly or is it what do you find interesting
at CVPR this time around?
Oh, man.
Like obviously I'm always kind of looking at more like problems that we are already looking
at.
So one of the work that I read recently was normal assisted death estimation.
And so in the sense, you're not looking at one image to create to predict death, you're
actually looking at a couple of images, which for me is really interesting because then
you just get as far information and you make sure that whatever your outputting is consistent
across multiple views.
And so for me, that's like the stereo problem that you were just using AI, which is good
because you can now scale across like different type of scenes and you can be robustness
to like letting issues and so on, which you can't do with like traditional methods.
So that's that's very interesting.
I'm still like reading through, there's a lot of content at CVPR so I'm still working
my way through everything that was presented, but definitely that's one of them.
And just in a general sense, I'm very excited about machine learning with no without needing
global training data, I think that's a big thing people are still looking at that.
So I'm less interested in being able to like generate photo realistic images, there's
a lot of focus on that, but I really think of like machine learning and computer vision.
And you look at all of these models, they need tons and tons of data to train them.
So there's definitely a new line, some line of work around how do you basically do self-training
systems, how do you do that without needing like a bunch of data, data, so that's something
that I'm very interested about.
Nice.
The workshop that you presented at, is there something that you've been participating
in for a long time or is it new at CVPR?
So this is the second year, the workshop is taking place.
So last year I was attending and we were presenting this scene generation from images work that
I mentioned earlier.
We presented that at the workshop and I was invited for this year keynote, for one of
the keynote this year.
So I was definitely like, I was glad to have that from the talk about our work.
Yeah.
It's a great workshop.
I kind of encourage everyone to go and check the other speakers because there was a lot
of work around just like the vision of AR VR, what's stopping us from, you know, from
everyone doing conferences, you know, through VR, like, what are we not doing that today?
And things like that.
So that was the thing you also.
Interesting.
Yeah.
I did organize the conference a few years ago and we had a presenter on AR VR and kind
of some of the implications for machine learning, but it was super early.
Like, there was nothing really happening.
And so it's exciting for me to hear that, you know, the field is advancing and, you
know, there's a particularly, and is it, is it fair to say that AR in particular because
the scenes need to be understood more.
There's more implications for machine learning than VR, which tends to be more kind of
generative and scripted.
Yeah.
Yeah.
Definitely.
So in a lot of these tasks in augmented reality, we just, we need machine learning because
no one is going to, there's no time to sit down and try and do like, like, mind reality,
take a real scene and then create a digital.
So if you're going to do like physics interact, if you're going to interact with the environment,
then you need to understand that environment and that's computer vision.
That's what computer is all about, taking reality and making sense of it.
So it's very crucial to augmenting your reality.
Well, Flora, thanks so much for taking the time to share what you're up to.
Very cool.
Thank you.
Always happy to talk about these things.
Yeah.
Thanks.
Nice.
Thank you.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
