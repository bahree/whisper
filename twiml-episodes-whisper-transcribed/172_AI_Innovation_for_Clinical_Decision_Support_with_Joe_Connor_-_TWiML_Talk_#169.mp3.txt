Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode I speak with Joseph Conner, founder of Experto Crete.
Joseph has been listening to the podcast for a while and he and I connected after he reached
out to discuss an article I wrote on AI in the healthcare space.
In this conversation we explore his experiences bringing AI-powered healthcare projects
to market in collaboration with the UK National Health Service and its clinicians.
We take a look at some of the various challenges he's run into when applying ML and AI in
healthcare as well as some of his successes such as tackling effective triage of mental
health patients using emotion recognition within a chatbot environment.
We also discuss data protections, especially GDPR and the challenges that come along
with building systems dependent on using patient data under these restrictions.
Finally, we take a look at potential ways to include clinicians in the building of these
applications and the importance of doing so.
And now on to the show.
Alright everyone, I am on the line with Joe Conner.
Joe is an innovation associate in machine learning and AI with the UK's National Health
Service and founder at EXPERTOCREDI.
Joe, welcome to this weekend machine learning and AI.
Hi, it's pleasure to be here.
Well, it's great to have you on the show.
You reached out.
I believe following maybe an article or something I wrote up in my newsletter on the healthcare
space and my interest in further exploring it and offered to help out.
So I'm really glad to have you on the show.
And why don't you start by telling us a little bit about what you're doing at NHS and your
background and kind of how you got involved in applying ML and AI in healthcare?
Well, you know, one of the things about coming on your show is it makes it actually look
back and realise where you've come from.
I'm a self-aminent engineer by first discipline and I joined the time energy authority doing
probabilistic risk assessment back in the 85.
So our 28 years in this particular field and then moved to the Advanced Robotics Research
Lab at the University of Software doing VR and Telepresence.
But then moved out to that in two commercial due diligence in the tech space, I think
it's supply chains and modelling for banks and VCs, but got back into machine learning
making things back in 2006 looking at computer vision based classifiers for emotion recognition
effective computing type things.
And for the past two and a half years I've been embedded with an organisation called NHS
Digital, which is part of the UK National Health Service and been looking at technologies
that it can adopt, but more of late part of an organisation for code for health.
And its primary role is to work with clinicians, find out what they need, and make some open
source software and tools that they can use, and it's obviously the best thing I've ever
done in my life.
I'm really enjoying it, the clinicians are fabulous people and it's nice to be able to help
in that space.
Oh, that sounds great.
You mentioned you've been working on emotion recognition.
Is that something that you're applying in the health care context?
Well, yes, it's important to actually consider, well one of the things I've been looking
at for the past two years is the subjects of mental health and how that can be assisted
by technologies, cell chatbots, etc.
And you can overlay effective computing type technologies in that domain.
So for example, a clinical issue that exists is how do you actually triage people effectively
when the demand for services is so high?
And the clinician approached me and he wanted to know is it possible to do what's
called PHQ9, GAD7, which are sort of scoring mechanisms for looking at mental health in
a chatbot environment.
So we built a tool which you could consider, which looks at scoring those triage processes,
but all the laying on top of that effective computing.
So analyzing emotion in text and all paid to look at suicide ideation words and expanding
on the current models that exist, which are usually just numerical question and answer
type things where you get a rating, but going into dialogue because I think the future
does probably lie in augmenting services using these types of technologies.
Has that project gone into production uses at live and are patients interacting with
it?
It's currently out with clinicians and the supply chain that could potentially use it.
Obviously, it's an open source piece of software, so anybody can use it.
But there are existing providers for mental health services and triage in Britain and we're
saying to them, here it is, play with it, use it, test it.
And if you find a value, here's the code, fill you books, crack on.
But obviously it would need to go through some rigorous testing, not least because it
would be a change of clinical practice.
And so I think it's the early days, I think before to say it's very early days.
You say it's open source, are the models that you're using open source as well or just
the chatbot platform itself?
Everything, the code, the manual, everything is open source.
Okay.
But the role code for health is to actually produce things that people can use quickly.
A clinician comes to me and says, can I do this?
And what's the art of what's possible and I can show them?
So it's great to build things in the NHS.
Just prior you mentioned or alluded to the process of bringing new products, projects
to market in the healthcare sphere, maybe it's worth taking a step back and kind of starting
there and having you talk a little bit about your experiences with machine learning
and AI in general at NHS and in healthcare and getting projects out to clients.
Okay.
Well, I think like most engineers, you think you can solve problems and you go ahead
and solve problems.
I think one of the things that I, big slaining point I have come across is asking the
question, how wrong can you afford to be?
Because this is an environment that's a bit like my early days in the 80s when we're
talking about nuclear power stations, you cannot afford to be wrong.
Right.
And the degree to which you could be wrong needs to be understood by both the person designing
the system and also the person requesting the system.
And I think most people don't, if I were set to be a broad generalisation, most clinicians
do not actually ask that question of themselves before they consider using machine learning
in AI.
So that's the first thing I've learned of one of the things I'm very keen that people
start to do is to recognise that look, how wrong can you afford to be?
So let me give you an example.
We have a system via which we dispatch ambulances and medication and services in Britain.
You could look at that and you could optimise the questions because that's an existing system
that is an expert system.
You could put a Bayesian ranking system in there.
You could optimise that, ask fewer questions, but chances are not because you're asking
fewer questions, you might get it wrong.
So 80% of the time you might get it right, 20% of the time you might get it wrong.
The risk associated with that is significant because in the 20% of the time you don't get
it right, that might be an ambulance.
So there are certain situations where there are highly critical situations for health
that I think machine learning isn't appropriate for.
So early stage triage for low risk areas, I think it isn't appropriate for.
But when you're building these things, the challenge always is how do you get hold of data
to be able to build them?
And I think there's a misconception that because the NHS has 1.7 million people in it and
it's fourth largest organisation in the world, that it is one discreet organisation and
one discreet set of data sets.
And in some cases that's true with public health data, etc.
Most of the time they sit within fairly siloed groups of NHS practices across Britain.
So it's very difficult for me as a machine learning practitioner to say, okay, you, as
in your trusts, will you please show your data with me, personally identifiable data?
It just doesn't going to happen.
So I think the Chitkey skill for myself and people who will follow on from me will
be to actually get some skill base at a local level embedded at a local level, so that people
can build classifiers and prediction mechanisms that can be used locally, built locally within
the boundaries and the firewall of a particular trust.
The only other option is to try and agglomerate all that data together and well, there's been
further successful with that with a group of organisations providing details to Swansea
University.
Not on a national Welsh skill, but I think one of the challenges I, I think I'm going
to look at probably from May onwards is the subject of federated machine learning because
there's a better chance I think of that being used and by federated, it means different
things to different people, but by by federated machine learning, I mean, let's build models,
let's build models external to the NHS and apply them locally and just get the gradient
descent or just get the error back, let's see what these things work, so we're not actually
looking at personal data, we're not actually looking at anything relating to anything
that could define a person in the real world, but just looking for the success or not of
the models, I think there's a better chance of being able to deploy models in that sort
of environment, but it's early days.
So do you have a taxonomy of sorts or mental model for the different application areas
within healthcare broadly or the NHS of ML and AI?
I think within like an organisation this size, you wouldn't be surprised to find out
that it sits within text, it sits within images, and to a lesser degree, it sits within
video, but it's primarily text and images, so for example if you wanted to do some
analysis of drug use and drug use effect, it's very difficult for you to do anything other
than natural language assessment on text within a health record system, and the taxonomy
is associated with doing that sort of analysis that you might expect to see like ICD-10,
UMILs, SNOMED, are not the ways in which people classify a diagnosis, just don't exist
in these systems, they are just free text systems.
So people are busying themselves, developing, um, annotation mechanisms to understand for
a particular medication, these are the likely ways in which a clinician will express those,
and running those algorithms to understand the effect of drugs and the effect of treatment,
particularly in mental health and in mind about in London they're doing some success in
that area using products like Gates and to a lesser degree glove, the natural language
tools, but within the realms of images, it's very difficult because most images have also
got embedded in them, tools, techniques for maintaining personally identifiable data,
so an x-ray will have identifiable data in it, so we've been busing ourselves over the
past three months looking at ways in which we can, um, allow people to label data without
actually, um, so say we had a physiotherapist who wanted to create some label datasets for
shoulder impingement, it'd be very difficult for them to create that dataset, um, if you
don't give them say web-based or mobile-based tools by which they can take pictures label
the data and give that to in a controlled environment to the people who could produce the algorithms.
So yeah, labeling of data and producing data that you can use for machine learning is
one of the biggest challenges I think that we have, and it's not going to go away probably
because we are a very disparate organization.
So it sounds like you're, you kind of think of things in terms of the media type if you
will, text versus images versus video, uh, I'm curious even at a higher level, there are,
we could probably rattle off the different types of applications, but there you've, you've
talked about diagnostic applications, you've talked about with the ambulances, like the
efficiency types of applications, I was just curious what other broad categories of
applications there might exist.
I imagine, you know, I say sometimes look at an org chart and you can figure out where
you can apply it, the same thing applies at the NHS in terms of HR and lots of other types
of applications.
I think we need to be realistic, and the doing machine learning and AI on clinical subject,
I would break it up into two areas, what is clinical and what's process?
If you look at it just as in process and people were not different from any other organization
in the world, somewhat bigger, so there are ways in which you can do process improvement,
whether that's booking people onto of treatment plans, booking people into hospitals, um,
waiting this management, that type of, these are sort of the sort of bread and butter you
would expect machine learning solutions to be, uh, well made for.
And there's not little or no clinical risk associated with some of those, uh, right,
but when you start to look at clinical applications, you come across one thing that is probably
unique to healthcare, which is the subject of a healthcare pathway.
So for every form of treatment, there will be a healthcare pathway, which we'll say you
do this by this, do this, this, this, this, this, and this will be, um, under what's called
the nice guidelines, so it's simply equivalent to the HIPAA compliance, et cetera, you would
have in the stands.
And some of those processes, if they're in a pathway, you could augment.
So if we take the subject of mental health, you could augment the, the process of, um,
triaging by getting better data to people more quickly.
And that's important because, uh, like, not unlike other, um, conditions, the sooner you
treat somebody, the better it's going to be.
So gathering data and analyzing data and scoring that data more efficiently to help a clinician
understand who they should see isn't much better than what currently exists in some areas
of mental health, which is you get a questionnaire.
So if you're, uh, paper questionnaire, in some cases.
So if you've got a condition, they will, uh, they will meet you and they'll ask you
to rate how you feel on certain subjects.
If you work with paper questionnaire, you will see, and then maybe too excited, you have
to fill it in again.
The data gets lost, and that's just not the way to work.
So you can imagine in a chatbot environment, you can capture that data, but person doesn't
have to be there to actually, um, gather that data, and they can control that data and
share that data as and when they see fit.
So it doesn't, it could be a system that sits on a mobile phone, which they control their
user, and it may not necessarily go to the NHS unless the person specifically wants it
to do.
So these sort of solutions, the triage in making better decisions about what to do next.
At the early start stages of a, uh, a diagnosis is worthwhile.
It's very difficult to suggest, um, alternatives, uh, one of the key and the most successful
bits about being involved with code for health is the clinicians involved from the onset.
And I think that's critical for anybody wishing to bring systems into, into any health
care system, anything that's going to affect the current clinical practice should be designed
with clinicians, not presented to them as a solution for increasing efficiency, which
I've seen done, um, and also it's going to use some, some machine learning terms, you
know, tailor it back a bit and explain exactly what these things do, because people need
to understand specifically, because a GDPR, how these things work, and it's clinicians
at the end of the day that will want to either use it or not or refer it or not.
Right.
So it's, it's how you, how you design, how you decode design a product is the most important
thing.
And being close to clinicians is, and clinical practice wherever you were in the world
is, I think, is the, is a key thing.
If you want to do useful things in machine learning, that's why it's such a good thing
to be, we embed it in the NHS, although it's two and a half days a week and the rest
of time I mean, I spread a creed.
Just working with people, working with clinicians is a, is a wonderful thing, because you know
you can remove some of the pain of doing practice, uh, by optimizing it and augmenting it.
Um, and that, maybe that's another subject that's worth discussing is the subject of what
is AI to most people.
The conversations around what is AI and is it going to remove jobs, et cetera, is just
prevalent in the press.
If I see another discussion with a look of a humanoid robot in it, I think I'm going
to scream.
It drives me a party.
It's just, it doesn't help people to understand that really, what are we looking at here?
We're looking at maths stats and computers.
And they are programmed to do things or not by humans.
And they can be used to augment practice in a clever way by doing predictions and classifications
faster than some people can do.
It's not going to remove jobs.
I know, I don't believe it was going to remove jobs in, in healthcare, and it neither
is should it, because you know, unless we get, um, systems that are good at socratic
dialogue, um, listening better, responding better, very quickly.
And I don't see that happening at the moment.
Uh, I think we're, say, safe to say that clinicians are safe.
I can certainly see it changing a lot of jobs.
Yeah.
Right.
Hopefully for the better one would help both from the patient perspective as well as the
worker perspective.
Yeah.
I mean, the National Health Service is about making people better.
It isn't about keeping people well, although I think increasingly people are thinking like
that.
So public health England does exist and there are various initiatives to keep people
well, but I think in economies like ours that are very densely populated regions, um,
we need to think more about how can we build AI systems that keep people well.
That's become a pretty popular topic here.
Uh, the whole population health management idea.
Hmm.
Um, I don't know that we're necessarily any further ahead than the NHS is.
Um, I'm certainly far from an expert in that.
Uh, it's a topic that comes up quite a bit when talking about kind of health care
directions.
Yeah.
I think if we look at what we have in our phones in terms of technology, it's wholly
believable.
And in my experience, when we built emotion classifiers that can sit in a wholly powered
bike, but it's not within the NHS, but within expert degree, we build emotion classifiers
that can sit in a phone and passively understand your emotional state and put it in a, in a diary
that you can subsequently review using computer vision technologies.
So that sort of thing could exist and could that could enable me to reflect about what
makes me happy and what works you sad.
And therefore I could with the right tools that might be chatbot might give me some
ideas to the way in which I can improve my behavior to make me more resilient.
These sort of things can be encompassed within our day to day lives if they, if somebody
wants to do that.
And I know, I mean, there's quite a lot of resilience apps coming out, resilience technologies,
monitoring, sensing technologies that exist, but behavior change, but a population level
isn't just about the technology, it's about a country's willingness and often society's
willingness to be assisted.
And we've got to trust the thing that's assisting us.
So if you've got a phone that's saying, look, Joe, you've not been happy, you've been
grumpy for a couple of days now.
What do you think's causing that and what do you think you could improve that?
Now, if I, if my phone did that, which it could do, I built a bot that actually does that.
Now, I built it, so I know it works.
But if I had, I've got to have that given to me and I didn't involve, wasn't involved
in the process, didn't know it would work, didn't know what its purpose would be.
I might feel like, you know, what is this thing?
It's an issue of trust.
You trust AI, but you can't trust it if you're involved in the process of its design.
And I think most people are designing tech with including people in the process a lot
of the times, because it just doesn't get used.
So a population level behavior change does require, I think, societies and I think it's
incumbent on health services to say, look, we want to, we want to move our country in
this direction, that might be mental health, etc.
Let's do this.
And these are the tools and techniques we're going to use on that.
And that requires governments to want to do that as well.
But also requires people to feel comfortable with the tech.
And I think that's my concern at the moment is that there's too much rubbish being talked
about machine learning and making it frightening when in fact, it isn't.
You mentioned previously and now just the notion of centering this process around the
clinician.
What has to happen to fully enable that, right?
You know, you said machine learning is maths, stats and computers.
Clinicians are typically focused elsewhere, right?
Do they need to, do we need to turn our clinicians into mathematicians, statisticians and computer
geeks?
Or have you come across some interesting ways to pull clinicians into the process?
And do you have any hints at ways to do that at scale?
So let's say a group of people in the NHS want to, a group of clinicians are wedded to
the idea of doing changing customer practice within, after a clinical risk assessment of
system, let's call it a chatbot, just because we've been talking a little bit about that.
If they wanted to deploy that, the next challenge for, not for me, because we're in code for
how we just give it away, but the next challenge is if you're not doing what I'm doing, which
is giving away things free to the NHS, is who's going to buy these products?
Because it won't be the clinician.
The clinician may specify that they should be used and therein lies the other challenging
issues within health boards or within NHS trusts.
There are purchasing processes, typically three to five year contractual process that
you need to go through to get that through the door.
So getting it specified is one thing.
Getting it passed in compliance with nice guidelines and in compliance with clinical
guidelines is another thing, so that might take you two years.
You then have to wait or you have to create a process for which that process, that product
is subsequently specified and purchased, that might take you another three years, and
I know very few SMEs, very few SMEs, and not many large companies.
For particular technologies who have both the cash or the pre-deliction to want to wait
to run for those types of processes to go through.
It's not easy, and quite rightly it's not easy to implement technology in health care.
But once you're in, the people who do it, stay there, but I have a slight problem with
the fact that once they're in, they don't often innovate.
They get there, they don't make any changes, so customer practice, clinical need changes,
the systems don't change with it, and that's why, and that actually causes the thing that
we have in the NHS, which clinicians will want innovation, and they will subsequently
make it happen because they're not happy with what they've currently gone.
So that's the strange cycle, I don't think it can be that unusual for other large organizations,
particularly public sector organizations, it must be common, I would have thought.
It actually calls to mind a lot of what you see on the traditional enterprise side as
well.
Take, for example, the evolution of cloud computing.
A lot of that came out of pent-up frustration in being able to get infrastructure through
the traditional procurement process and IT processes, so you'd have what they call this
whole shadow IT, and it makes me wonder if there are a shadow health care type of thing
arising where individual clinicians or groups are looking outside of these traditional
structures, or as the bureaucracy, if we can call it that, is it so strong so that that's
less possible in healthcare?
You've got to bear in mind once I've been a beneficiary of the NHS for 55 years, I've
only been within its walls for two and a half, but I would have to say that an outsider
looking in what you've said is a fur representation of customer practice.
So we've talked about some of the clinical challenges and along the way you've mentioned
GDPR.
Do you have any particular insights into how that lays out either generally or within the
healthcare confines?
Very much so, I've been pondering on the subject of GDPR since I came in through the
door of the NHS, not least because it directly impacts on black box algorithms, which my
particularly fond, so it's very difficult, it's very difficult for me and I would say
difficult, I'd say impossible for me to recommend the use of a black box algorithm, and I don't
have a problem doing that in some cases because we both know that majority of problems
can be solved with, you know, logistical regression, experts, you're going to get very good
responses on your dataset by using things that people can understand.
But you seem to have accepted the labeling of black box to describe deep learning where
I've come to refer to it as opaque as opposed to black box, it's hard to see, but you
know, you can see some things.
Of course.
I mean, you can apply models on the outputs of a deep learning model and create an approximation
of how it's created, like a decision tree, you can throw lots of data at a deep learning
model and look at the outputs and explain them.
But the worst case scenario from my perspective, and maybe becomes, you know, because of the
background I come from, looking at probabilistic risk assessment type stuff, is this.
I have an algorithm and it dispatches an ambulance to you or it doesn't, let's say it doesn't,
you die, your family have a right to understand what happened.
So let's say, the process by which that happened involved in some cases, a black box algorithm
on opaque algorithm of some form, and you say to me, okay, well, you obviously made
that decision based on this algorithm, how did it work?
Now under GDPR, because of the issue associated with algorithmic responsibility, I have a legal
requirement.
There is a legal requirement on me, if I was the designer of that system, or indeed the
user of that system, to explain that to you.
It has not yet been tested in law as to the degree to which that explanation will be
acceptable.
So, the current process is that all such algorithms will have a person in the loop.
But let's face it, you can put a person in the loop, the degree to which that person
made that decision, or do we switch that algorithm in the decision, isn't always clear.
Knit a couple of those together, and you might have an excellent explanation for the way
in which a call was handled, for example.
You might have some gap in speech type technology, which would understand that that call was
handled very well, and then you might have some technologies which will take you through
a questionnaire that will dispatch an ambulance.
One might, the first one, might be a black box algorithm, I don't know, opaque algorithm,
because the one might be an expert system.
What actually was, played the greatest influence in that decision to dispatch the ambulance,
was it that the system said I handled the call effectively and asked all the questions
correctly, or was it the decision tree that I went through?
It would be very hard to say the two things are linked, and if the former doesn't have
a very good explanation that you will understand, it's quite a large financial hit, I mean,
20 million euros, potentially, put instance of not being able to explain an algorithm.
I don't know any SMEs in the world that prefer to take that degree of risk, and in practicality
is termed, that wouldn't happen, I don't see that happening, but it's not been tested,
so it's very difficult for me to suggest the widespread use of opaque algorithms because
of this issue.
Even though I know, in some cases, some specific ones that the algorithm may be more accurate
and have less risk associated with it, but if I can't explain it, then what do I go
with that?
So it's been, I completely understand GDPR, I think we have to comply with it, we have
no choice, if we want trust in machine learning, we need to be able to explain things well,
so I've been showing a lot of those people running, well, since the days of line, when
line was originally, how many years has that been around there, two, it's been a couple,
so if somebody were able to solve the problem of explaining how that works, in a manner
that would be acceptable to a member of the public who was affected by it, I'd be a very
happy man, because I think these algorithms have a very important role for just speeding
up the process for which we learn about things, not necessarily just doing things, how we
learn about things.
Do you have any views as to, well, I mean, I mean, obviously, this typically is something
that were you ask very useful questions of the person that you're interviewing, and
they hopefully come up with decent responses, but I'm sitting here, left scratching my
head as to the best way to go with deep-lying algorithms in terms of the explanation
of them.
Given this podcast, maybe listen to by people in healthcare, what techniques do you think
we should be considering in healthcare, because let's face it, you have one of the few
people in the world that goes out and speaks to leading people in the area of machine
learning in AI, so I guess from the nature in which you ask questions, you also have
a reader of papers, and you understand how things are coded.
Do you have any idea as to what sort of things we should be considering?
I think the way I think of the explaining deep-learning landscape is that there are a
couple of broad camps.
There's one broad camp that is basically try to fit some explainable model to the output
of your opaque model, and you've mentioned and acknowledged that camp, and the other
is try to gain some insight into what individual neurons are doing by introspecting weights and
things that are happening in the network.
I think that's a step towards explaining, but it's not quite.
I don't think it produces anything yet that necessarily meets the bar of explainability.
I think that's more providing insight.
There have been a couple of conversations with folks that are offering software tools
that aim to one way or another provide the user who's deploying these tools with a dial
that lets them dial in explainability when needed, presumably at the cost of model performance.
But in a case where at the NHS, that dial will be pinned all the way to one end, right?
Then I think you're back to these two types of approaches, and the various methods of
training a decision tree or something like that against a moral-pake model is what I've
heard the most of happening in real life.
I'd love it to happen, but I think the other challenge we've got is when you string
a number of them together, who takes responsibility for the decisions each one of them have made?
That's an interesting thing in itself.
If you've got a dial, how many dials can you afford to be be bothered to press whilst
you're waiting for an ambulance?
Yeah, it's a big issue.
I think time will tell us to whether GDPR had a positive or negative effect on health
care AI.
At the current time, I think we need to spend more time explaining what we're doing and
making less of a bogeyman, so maybe slowing things down as it is doing is a good thing,
so maybe I'm being slightly pragmatic and thinking on the positive, but I think that
wouldn't be a bad thing, and there are plenty of ways in which organisations can make money
with deep learning algorithms and it doesn't have to be in health care.
I think the challenge, however, when it comes when you don't apply, I'll pick algorithms
to problems that are just intractable with any other way, so I'm talking about computer
vision type problems.
So looking at a label data set for say glaucoma images, which is what I'm going to be doing
next month, and building an algorithm that could identify, say, features of those particular
images that might help somebody understand whether glaucoma is early on say glaucoma is
happening, I think the people who would like me to do it need to understand you can't
do it any other way than with some form of a vague algorithm, and therefore the ramifications
are going to be, it needs to be one where it's a very low risk, how wrong can we fall
to be on this particular subject?
I think that's anybody applying AI in this sector needs to think of that first, and I
don't know enough about the subject as I'm expecting to be in May when I get briefed
by the clinician, but that's the first question I'm going to ask is how wrong can we fall
to be?
Yeah, there's a podcast I did recently with Ryan Publin, who's a research scientist or
research engineer at Google who worked on looking at retinal fundus images and trying
to predict, I think it was diabetes in his case, and his research ended up focusing on identifying
potential risk factors, in other words, inferring demographic information from the retinal
scans, like age and...
Yes, right, right, so they looked at a whole bunch of data sets and came up with inferences
about the nature of the person that didn't relate to the condition that they were looking
for.
That's right, and so in that interview, there was an interesting tidbit about how his grappling
with this explainability issue and the context of being able to go back and validate these
results with the clinicians, and what he suggested was that you've got a deep neural network
model, you've got this layer at the end that says yes or no or makes a prediction of some
sort, but the roughest level it sounds like what they were able to do is kind of push back
a layer and maybe train a layer on identifying features that are kind of known contributors
to a particular condition, and it does strike me that maybe that might help us still use
these opaque algorithms, but pass the explainability test, right?
If the algorithm's not making the decision, it's just providing, it's augmenting the intelligence
of the clinician.
I think that I think therein lies an interesting and probably important factor that we need
to consider is that it's different doing healthcare research than implementing systems
that augment practice.
So I'm particularly focused on implementing systems that can augment practice.
The use of deep learning systems or any opaque algorithms that can improve people's understanding
of causation of illness is wholly appropriate.
The difficulty comes as to when somebody needs to understand why that came to that discussion,
but I think it opens up identifying feature sets that people clinicians need to consider
in more detail, as that one did, is highly appropriate for healthcare research and should be
considered, because it wouldn't be implemented in this particular algorithm that's going
to be used for diabetic myopathy.
I believe so.
I'm not sure I got that term right.
Maybe to be more concrete about it to make up some terminology and apply it to this
example, but intuitively there's a big difference between using an opaque algorithm to determine
whether I have the diabetic condition based on an eye image and using an ensemble, let's
say, of opaque algorithms to identify or maybe a single opaque algorithm to identify features
that may be difficult for me as a clinician to identify or time consuming, but to raise
the flags and say, well, there's this kind of vein structure, there are these kind of
spots, there's this kind of coloration.
So this is a candidate that should be explored more fully by the clinician.
So I think the point I'm trying to make is maybe don't throw the baby out with the bathwater
kind of thing here, right?
There's still potential value in these opaque algorithms, but it's really where potentially
where the decision is being made.
Awesome.
Awesome.
Well, Joe, thank you so much for taking the time to chat with me.
It's been great.
It's been awesome.
Well, thank you very much.
I've really enjoyed it and keep up the good work and you're a very good communicator
in the area of machine learning, and if I have my way, lots of people in the NHS will
be listening to this podcast, not to me, but to what you do on a regular basis.
Well, I really appreciate that.
That works.
Enjoy your weekend.
You too.
Have a good one.
All right, everyone.
That's our show for today.
For more information on Joe or any of the topics covered in this episode, head over to
twimmaleye.com slash talk slash 169.
If you're a fan of the pod, we'd like to encourage you to head over to your Apple or
Google podcast app or wherever you listen to podcasts and leave us a five star rating
and a review.
They're super helpful as we push to grow this show and community.
As always, thanks so much for listening and catch you next time.
