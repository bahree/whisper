Hello and welcome to another episode of Twimble Talk, the podcast.
We interrupt our program to bring you this important message.
Happy birthday, Twimble.
Today is a very special day and this is a very special show because we are celebrating the third.
That's right, the third birthday of your favorite machine learning and AI podcast.
I'll be honest, if you told me three years ago when I was struggling to get that very first episode of this week in machine learning and AI,
recorded and posted, that three years later, I'd still be at it, that we'd be publishing two, three, four or sometimes five shows a week,
that I'd have interviewed well over 300 incredible ML and AI researchers and practitioners.
That we'd have an amazing community of fans that engage and encourage us daily.
That we'd help hundreds of people learn machine learning and deep learning through our volunteer led study groups.
That we'd hit five million downloads on our third birthday?
Yes, you heard that right, that just happened?
Well, let's just say that if you told me any of that, I'd have definitely thought you were crazy.
It's been a truly amazing and humbling experience bringing you the podcast every week
and I am more appreciative than ever of you, our listeners and the amazing community that we've built around this show.
From the entire Twimble team, thank you so much and Happy Birthday.
With that said, we'd love to hear your Twimble story.
In our conversations with listeners over time, we've heard quite a few stories detailing what you've learned from the podcast
and how you've applied it to your own work, research, philosophy and lives.
And now we want to hear from all of you. Please take a moment to hop over to TwimbleAI.com slash 3B day
or leave a voicemail at 1636-735-3658, letting us know your favorite tidbit that you've been able to take from the show
and how you've applied it to what you do.
Everyone who joins in will be sent a limited edition third birthday Twimble sticker
and the best submissions have a chance to be featured in an upcoming episode of the show.
Again, that's TwimbleAI.com slash 3B day for your written comment or 1636-735-3658 for your voicemail message.
And whatever you do, stay tuned.
We've got tons of exciting things in the works that we can't wait to bring you.
For starters, next week we'll be releasing volume 2 of our AI platform series,
a follow-up to one of your all-time favorites based on the feedback we've received.
We've got much, much more in the pipeline for you including a huge update that we are super excited about
but those are still top secret for now.
Just be sure whatever you do to listen in next week.
And now on to the show.
Alright everyone, I am on the line with Dave Farouche.
Dave is the founder, CEO and Chief Scientist at Elemental Cognition.
Dave, welcome to this week of Machine Learning and AI.
Hi, how are you? It's a pleasure to be here.
Absolutely, absolutely great to have you on the show.
So in some circles, you are perhaps best known as having built and led the IBM Watson team.
And I'm curious, how did you arrive at that point in your career?
Well, I was always interested in artificial intelligence.
I mean, I have been since, actually since high school early college.
It's been my fascination. I started programming, I guess, in high school.
And right from the very beginning, I mean, after I think I wrote my first program was like in basic.
I think it was on a PDP 11, I can't remember exactly, but it was really cheaper.
And I think what blew me out of the water right away was, wow, if I can just describe my thought process,
I can get a computer to do work for me.
And that was just very exhilarating.
And my mind directly went to this notion, I don't even think I personally had a word for it at the time.
I learned later, it was called artificial intelligence, but this notion that if I can describe how I solve problems,
if I can describe how I think, and I could put it in this language that the computer executes for me,
I can get the computer to do all the hard work and allow me to kind of be creative.
And I hated doing repetitive tasks, so this was just mind blowing to me.
And I was on the path to become a medical doctor.
My parents wanted me to be a medical doctor.
That was like the thing to do in my neighborhood.
If you were smart enough, I guess.
But I just got this bug.
And so by the time I was in college, I was doing more and more and more programming, programming everything I could possibly get my hands on,
and then eventually switched to wanting to go to grad and set up going to medical school,
wanting to go to graduate school and computer science.
I eventually got my PhD and got a chance to work at IBM Research on an AI project.
And interestingly enough, this was back in the, I think it was the late 80s.
And artificial intelligence at the time became a bad word.
And there was an AI winter.
And it was all about the customers worrying that AI was going to take jobs.
Can you imagine?
And here we are in the same spot when we used later.
It's fascinating.
And they went around canceling AI projects.
And my manager at the time said, you know, Dave, you've got to learn how to do.
You could become a specialist in like a domain area.
And you don't want to be a technologist, right?
And we're not going to be doing AI anymore.
And I quit.
It's like because this is not that's clearly what I want to do.
This is actually between my masters, my PhD.
And so I went back and I finished my PhD.
I actually ended up returning to IBM.
And, you know, working on a number of different projects related to AI,
to tech, analytics, speech, image analytics, building, software and infrastructure.
I eventually got into open domain question answering had built a team.
We competed in a lot of government-sponsored competitions in the open domain question answering track.
So I got to a point where I was doing all the things I love to do.
Software, software, architecture, software engineering, all around solving AI-type problems.
And so I had built a team.
So by about 2006, I had built a team of about 25 people,
all, you know, specialists and a combination of software engineering and different aspects of AI,
including natural language processing, machine learning, knowledge representation and reasoning.
And this idea of doing this Jeopardy challenge came up.
And actually, it came up two years before that.
But the executive who wanted to do it kept on being turned down by the vast majority of scientists and researchers he would go to
and say, hey, can we get this done?
And they said, no, it's impossible. You can't do it. It's too hard.
And I was interested in it when it came around in 2004 and 2005,
but very busy with another project.
And then at the end of 2006, I was coming off of the project.
And I said, you know, I think this is possible.
And I think we not only should we do it, we sort of have an obligation to do it.
We've been working in open domain factoid question answering for some time.
My team had participated in the track question answering track for a number of years.
It was, I was doing a number of different things.
It was maybe the team was about seven or so people dedicated to that.
And I think we have to take this on.
And if we had a chance of making the appropriate investment, we should do it.
And even if it looks hard, we have to understand why it's hard.
I mean, even if I fail, I'll be able to kind of tell the community, hey, look,
we made a concerted effort at this. Here's how we did it.
Here's what was hard about it. Here's what worked. Here's what didn't work.
So it was a great opportunity to do real research.
And to get that, you know, funded, you know, by IBM at the right levels,
because they were excited about, you know, getting the the jeopardy challenge
to think on television. And so I did, I did a feasibility study.
We did a little bit. We played around some ideas.
And I proposed it that we can do it.
And they bought in and said, okay, you know, you're the guy.
We're going to invest in this. And you're going to take on this challenge.
And so I built up the team over the, that was the end of 2006.
So 2007, 2008, 2009, 2010, completely focused on that building up,
building up and rounding out the team.
And basically going from scratch and building, building Watson,
that ultimately won on jeopardy. And I guess we played it in 2011.
So that's kind of the story from, from, from my passion all the way through,
right through, right through the Watson stuff.
And after Watson, did you jump right into elemental cognition or?
So what's stopping points along the way?
Yeah, so Watson was interesting because it was a huge,
it was a tremendous success for us.
It was a tremendous, the technical team was just so,
had worked so hard and was so proud.
We went into the final contest with about a 70, 75,
between 70 and 75, there was 73% chance of winning based on all the stats that we did,
all the simulations that we did.
We hadn't worked our way up from when we started.
There was about, you know, we were getting,
we had zero percent chance of winning.
You know, the, the system out of the box when we started was getting like 13%.
All right.
So it was a huge accomplishment.
And it was in the middle of both sort of a combination of science,
obviously, and we had the scientific results,
business, you know, the business and the marketing at IBM.
It was a big project for them and entertainment,
because we had to work with the jeopardy of production folks and the television folks
and pull the salt together.
So having accomplished that, no one knew where it was going.
It was such a focus on those things,
and then no one knew where it was going.
And interestingly, and I think this relates to kind of the perceptions
of artificial intelligence.
I think interestingly, and if you've saw, if you watched the games,
but we presented this thing we called the answer panel,
where it showed the top three answers and the chances we thought,
the probability of that, those answers being right.
And I think that the, putting that up on the television screen,
got people to imagine that the machine was doing more than looking this stuff
at a table, which of course it was.
But I think when people just watch a computer give an answer,
you know, the general thing is, well, computers just know everything.
They just look it up and they know everything.
But when they see this notion of wait a second,
I must be doing more than that, because it's coming with a probability.
So it must be kind of calculating why an answer might be right
or why an answer might be wrong.
And in fact, we were doing something certainly along those lines,
including a confidence model for that based on hundreds of features
and our underlying machine learning algorithms.
And so this really captured the imagination, I think,
of the consumer, of the customer.
And before you knew it, IBM had a lot of people coming and saying,
hey, we want this thing.
Like, how do we get this thing?
And at that point, the project took on a very different,
you know, tone, very different nature was all about,
how do we commercialize this?
How do we bring AI and this technology to customers?
And I personally wanted to kind of take a step back.
And looking at how Watson worked and what my dreams were
for artificial intelligence in general.
So, you know, we're not really there yet.
Well, there's a lot of technology here that could be exploited
and leveraged in a variety of different applications.
We're not at this point where the machine really understands language.
And that's where I wanted to be.
I wanted to be where I can fluently converse with the computer
just like you can on Star Trek.
And I can get it to understand what I'm saying.
I can get it to deliver and summarize and read stuff,
summarize information for me.
I can get it to help me problem solve, you know,
become like a thought partner through the fluency of language.
Think the way I do only better.
But in a way that I can understand it,
it can be explained to me.
I can tell it what I know.
It can tell me what it knows.
And this was the dream.
And we were far from that.
And I really wanted to step back and do that kind of thing.
And IBM was on a very different road at that point
because of the consumer interest, the customer interest.
To be precise, not necessarily the consumer, but business interest
and stuff like that.
So I really wanted to take that step back.
I got a chance.
I was, you know, for a variety of reasons won't go into it.
I, you know, some of my choices at the time were for limited.
And I got really interested in this company not far from where I led,
called Bridgewater.
And I started working for Bridgewater.
And part of the reason I was interested in Bridgewater
also relates to my philosophy around AI,
which is that Bridgewater is a hedge fund that was approaching markets
in what they call the fundamental and systematic way.
Meaning that any kind of prediction they were going to make,
they were going to make sure they had an explicable model for that prediction.
So this was the kind of AI I was interested in.
Of course, you know, if you're in the market,
you're a washing data.
So it is about data and it's about data science.
But it's not about sort of blind past predicts the future.
It's really about building theoretical models and being able to explain yourself.
So that was very aligned with my interest of completely different domain.
But nonetheless, not expressly about language,
but the same sort of approach toward AI.
And I got involved with them and did some work for them,
still doing some work for them.
But during the course of that,
got them really interested in sort of my vision for language understanding.
And that ultimately the future of AI has to land in a place.
And this is where we both really agreed that the future of AI has to land in a place
that we build machines that are understandable, they're explicable.
Their logic can be probed and can be challenged and can be explained.
So I got, they gave me the opportunity to start my own company called Elemental Cognition.
And that was in 2015 that I started that I started that.
And that's been our mission is to focus on language understanding
with the, to build a learning machine that can learn in a way that ultimately interprets language,
builds on causal models based on its interpretation,
can speak to people, acquire knowledge, reason about that knowledge,
answer questions and provide explanations.
I think this is the Holy Grail for language AI.
And that's the mission for Elemental Cognition.
Awesome. I hadn't realized the Bridgewater connection,
Bridgewater for being perhaps popularized more recently with Ray Dalio's book.
Yes, Ray wrote the book on principles
and is popularizing that.
Of course, it's the biggest hedge fund in the world.
And it's sort of known for this approach toward markets,
which is again fundamental and systematic where they build,
they of course leverage the computer, but they build explicable systems.
And that's kind of the link between Bridgewater and me
is we have a very similar philosophy about AI.
And one of the things that occurred to me as you were talking about the Watson experience
and the way that you presented the results via this panel
was that it was in some ways kind of an early view at AI explainability, right?
We're not just going to show this one result and say it's the answer.
We're going to show these results and explicitly acknowledge that, you know,
there are probabilities involved.
That's right, and there's a great story about that.
I thought that was so important to do,
because we had, you know, we had taped many games,
we made practice games, and we would tape them and show them to different, you know, audiences.
And at the time, I think my younger story was seven or eight.
And we showed her one of these tapes of Watson playing against former Jeffrey players.
And Watson, you know, didn't know an answer and decided not to answer.
We didn't have that answer panel up that showed its top three choices.
And when the computer didn't answer, she turned around and said, you know, did a crash.
And I thought that was fascinating that, you know, at that young age, you know,
she already had this notion that a computer should answer if it didn't answer, it's down.
And I thought, no, I said, you know, Watson decided not to answer because it wasn't sure.
And that's so completely different.
And so, so then I just became convinced.
And I, of course, was watching the game too.
And I thought, you know, this is just not nearly as interesting and as exciting.
Because, you know, when you're watching a human, you think the humans like you,
when you're projecting what you would be doing answering a question,
you'd be like wrestling with whether you do the answer or not,
and whether or not you wanted to buzz in.
But, you know, you don't have that same model for what a computer does, interestingly.
And so, it was so obvious just not as interesting.
So, I said, we really have to get that answer panel up on the screen.
And at first, Jeffrey didn't want to do it.
And eventually, long story short, we convinced Alex Trebek by showing him
a couple of games with their answer panel up.
And then, and he got so fascinated.
But he said, you know, this is really jeopardy because your, your attention is distracted.
That answer panel is so fascinating.
It completely takes you away from the normal experience of what a jeopardy game is about.
And so, we, um, so he then showed him a few, uh,
a show him a few games with without the answer panel.
And he was like, wait a second.
Like this is more. Right. So he sort of, and it was so clearly the worst.
And he was like, we got to get that up there.
And I think that really changed the perception of AI.
And you're absolutely right.
It is the beginnings of why.
And it's not enough because I think when machines are, are, are not doing
factoid question answering, but they're making predictions, um, whether they're in,
uh, law, um, you know, um, policymaking or law or, or, or, um, you know,
health care or, or even finance anywhere where, you know, wow,
this is going to affect your life in a serious way.
Um, you, it's really a, a, um, it's a collaboration.
I mean, I think the best way, it's not just an answer.
It's really a back and forth collaboration.
It's a dialogue that you have to go through.
Because as you hear the answers, you want to know, why does that answer make sense?
Am I missing something? Um, do I, now that I hear you tell me that answer?
Maybe there are risks that I just didn't imagine or values that I weren't
considering or weighing properly.
Now that I see the answer and the reasons why that answer may be right or wrong.
Um, so it, and, and, and you, and you can experience that just through the thought
experiment.
Just think of how you, um, interact with any human expert, whether you're consulting
with the, um, with you're consulting with the lawyer or you're working with the
teachers trying to help you understand something.
Yeah.
Um, I was going to say that sounds like my conversations with my wife about what we're
going to have for dinner.
There's that much back and forth.
There you go. Right.
Even when it's not that important, right?
You know, you think you have the answer, but, um, or, you know, our doctor or a
lawyer, you know, it really is, you know, you think it's simple, but it's not so
simple when it's, when it's important enough.
And I think at that point, you're really expecting the expert to start to bring
you into the decision making process to give you the explanations that you need
to kind of have that back and forth.
Can you imagine, you know, making an important decision and, and asking an
advisor and the advisor says, well, I think you should take this treatment or you,
or you should invest your money here, whatever it is.
And then you said, well, why do you think that?
And the person just says, well, trust me.
Right.
It's my intuition.
Well, this idea of it, uh, being not so simple is, is one that, uh,
reminds me of something you said when we were speaking before, uh, the
interview started.
And that is that there's this big question for you around AI.
And that is, are we really being honest about how difficult the problem is?
Can you elaborate a little bit on that and what that, you know, what that statement,
what that question means for you?
Yeah.
So, um, I think there's, I think what's going on in, you know, in the industry
right now is we have a set of techniques that are good at a particular approach.
Um, so we have this deep learning stuff, which by the way, I, I love,
and I use any chance I get, but I'm also honest about what it's capable of and
not capable of so far.
And we have this technique and we build data sets and challenge problems that
are, that are, you know, subject to that tech subject in one form or another
are susceptible, vulnerable to that technique working.
And, and I don't think they're ambitious enough.
I think that understanding is actually a very hard problem.
And I think if we step back and really think about how hard it is,
independently of how a computer would work, and we say,
what does it mean to understand stuff?
To understand language, we just have to think about how much,
how much energy and thought humans put into understanding each other.
This is not a simple thing.
We, we're, we're sort of language of machines.
We're great at it and terrible at it at the same time.
Right? We're great.
We're, we're great at generating all kinds of language.
We can write prolifically.
We can talk prolifically.
We can fill up, you know, news 24 or seven new shows.
Um, do we reach and understand it?
Like if you sit down, everybody's sort of nods,
but what's your level of understanding?
Even reading, um, whether, you know, whether it's an article or a book or whatever,
people can debate endlessly for what's really meant,
what you're really getting out of it.
What, what, what is, what, what, what information is actually communicating
and how precisely and how confidently,
and why are we having different opinions about what's going on?
In science, of course, we invented formal languages for this kind of stuff.
Um, so in engineering, we don't rely on that.
We rely on engineering specification diagrams and formal semantics and things like that.
Um, and even there, uh, there are challenges in mathematics.
We do mathematical proofs.
And yet humans don't want to communicate that way clearly,
but there has to be some in between.
There has to be a recognition that understanding is actually pretty hard.
Uh, humans invest an enormous amount of energy.
Whether it be in teaching or journalism or, uh, writing or film,
or that's an enormous amount of energy communicating and trying to understand each other.
We struggle mildly with it.
So, so do we expect, you know, computers to take a large corpora,
digest them statistically and come out and do this?
Right.
There's just a lot more going on here.
And I think we have to, we have to kind of open our eyes to what else is going on.
Um, for you and I to get comfortable understanding each other,
we probably have to spend a lot of time sort of synchronizing what our background knowledge is.
Um, what are, what, how we, how we communicate about different things,
how we use particular words, phrases, metaphors.
If I know you're an expert at something, um,
and if I know that too, I can use that as a foundation for doing metaphors.
Um, so there's just a lot to do.
And I think when we imagine elemental cognition,
we imagine the computer kind of engaging with the human continuously.
You know, it was becoming this thought partner that evolves within a community of humans,
talking about a thing and reading about a thing and learning how to align its internal models
and, and how to acquire the right background knowledge to speak and build understanding.
And we start like with the kids, you know, the kid in first grade,
reading to try to understand stuff and imagine collaborating with the teacher.
Look, I don't know what this means. I don't know what that means.
What, um, you know, why would, you know, why would you do that?
And if you're talking about learning about plants and how they grow with light and water
as the simplest sort of stuff, um, what do you have to know to understand that
and put that in the right framework in your head so you can make me a useful predictions about what you've learned?
It's a complex process that involves learning the language,
learning how to map it on to models, learning how to reason over those models,
learning what background knowledge applies and what doesn't apply,
learning what the metaphors, the analogies, the phrases, the words,
the word sense is mean in that context.
And it always involves the kind of the going, the back and forth,
doing the back and forth to kind of get your bearings in your context.
So it's just a challenging, it's just a challenging problem.
So one of the points you made is that we're not setting our aspirations high enough.
And earlier in this conversation, you talked about the AI winter.
Like, is there a relationship between, you know, where we set our aspirations
and the kind of expectations that were set last time with the AI winter
and what we need to do to manage them?
Like, or rather, is there a risk in setting our aspirations too broadly
that we're painting a picture beyond what the technology is capable of
and we kind of set ourselves up for another deflation of those expectations?
Yeah, so it's a good question.
I mean, my perspective on that is a little bit too fold.
I mean, I think we're not in the danger that we once were
in terms of suffering another sort of full-blown AI winter.
And the reason is because I think that there's so many,
there's so much low-hanging fruit for deep learning to continue to provide value.
And I think what deep learning has done for voice recognition,
image recognition, how is it actually helped advance
at least superficial and at least superficial NLP?
I think these are for control systems as well.
I think that this is enormous and will continue to deliver for some time.
And I think we'll continue to deliver value and transformation
actually for some time. The other prong of my answer though is I think
that in some circles there are these expectations for this understanding thing.
I think that's a harder problem.
And I don't think we're yet making the right advancement.
I think that problem is, I think that problem is tractable.
But I don't think yet we're making the right investments.
I mean, the elemental cognition, one of my goals is to demonstrate what I think
the right type of investment is to tackle that problem.
But I think we need more of that type of investment.
And hopefully I can convince people that that's the case as we progress to get to that.
But I think that's a longer road.
I think that the other promising thing where these two paths come together
is that as the AI continues to be impactful and transformative,
it's engaging humans more and more.
So what that means is that you're using, you know, you have your phone,
your computer, you have all these different ways in which.
You're interacting with applications today that engage humans in various forms of interactions.
I think that's essential for the vision that if you ultimately want machines to understand
you're going to need a deep levels of engagement from humans.
And I think that's happening.
That's transforming that's happening.
We're seeing that.
So you have huge opportunity to engage them.
The chicken and the egg problem is that you kind of have to,
you have to engage them well enough that they want to have the back and forth with you.
I'm thinking of IVR systems and immediately pressing the zero button.
Exactly.
Or, you know, giving up on your favorite chatbot, you know,
for anything, anything that's actually complicated or involved,
and you just kind of give up.
Or, let me talk to a human right away and you keep hitting the pound sign until you get a human, whatever it is.
I mean, I think that, you know, so there's a little bit of a chicken and egg where they have to be good enough
to engage you so they can learn from you.
And so that's kind of like what you need that initial investment.
So I'm optimistic that we're not going to suffer an AI winter.
I worry a little bit of people go out there and expect these things to be extraordinary,
at least from the understanding of the site right away.
We have to kind of have a little bit of a cooperation, a collaboration,
and you have to inspire that.
So it sounds like you, you know, while you're respectful of deep learning and what it offers,
you don't think it's the entire solution.
How do you articulate what you think that solution looks like?
So that's also an interesting question because I think the answer is a little bit more nuanced
and my philosophy about that is a little bit more nuanced.
So I'm sort of, with regard to the power of deep learning as a general approach to intelligence,
I'm a little bit, I'm probably lean toward, you know, agnostic may be positive,
a little bit passive as a believer that maybe it's enough to achieve general intelligence
in some theoretical way.
Meaning there exists a neural network of some arbitrary depth and breadth
that can get us to AGI?
Yes.
Given some unspecified data and training method.
Given some unspecified features, your features, right?
Exactly.
So, probably right, exactly, you got it, you got it.
And just to put a fine point on that, you know, the brain is generating features.
It's generating an enormous amount of internal features, particularly when it comes to
socio-economic stuff, emotional stuff, things that relate a lot to how we understand
and build internal models that we could then apply language to.
It's generating a lot of its own stuff.
It's not external data and some effect it's internal data.
You can argue that with exposure to enough of exactly the same stimulus
that, you know, some specified, you know, yet to be specified in the neural net
would generate a lot of those internal features.
I don't know, not necessarily.
And that's the interesting thing about the technology is not necessarily.
It may build a completely different conceptualization of the world around it
in order to survive or do similar things that would be completely incompatible with yours.
So, but anyway, like I said, I mean, sure, maybe, right?
And now the other approach is, if I said my goal was to take all this content,
all these symbols that is our language, and map it to representations
that represent our understanding of that language,
which is now something that more directly models the full internal representation
that we may have or something that's isomorphic to it.
Not necessarily the way we represented it in our brain,
but represented in a way that we would end up with the same language for it.
And I had enough of that data.
And I trained in a deep learning system.
Would it be able to now read and produce an understanding of a blind language?
Maybe, maybe, but you need a hell of a lot of that kind of data.
And we don't generate that type of data readily.
We can't even agree often on what that common understanding of that thing is.
As I said, this goes back to the back and forth.
We sort of assemble and refine and align that understanding through our interaction and collaborations.
So that's an interesting question.
So where I end up with is, we need a hybrid system.
And that hybrid system puts some stakes in the ground.
It says, this is what I think an understanding is.
And I demonstrate that that understanding is ambitious, but not in the lab.
It's ambitious, but good and not so ambitious that it becomes impossible.
For example, you can read a text and you can go on and on about all the depth of understanding
and the layers and layers of meaning and the metaphorical implications and so forth and so on.
Or you can get that text and you could say, I know all the agents.
I know what they did. I know when they did it.
I know the relative geospatial relationships.
And I know how it lays out in time.
And I can tell you what all the individual motivations were.
And their incentives were to take the actions they took.
And I can tell you what events cause what other events.
That would be impressive.
I would be very happy with that.
If that was my fifth grader, I would say, good job.
That's an impressive level of understanding.
If I give you an arbitrary text and you can do that.
But that's also, that's not everything, but it's damn impressive.
So now to do that, what would I need to do?
And the system would need to dialogue effectively.
It would need to systematically be able to acquire knowledge.
To do the full NLP stack and then some that we're also familiar with.
It would need to be able to, if it's going to converse with you at all,
it can't be completely absent of any background knowledge at all.
So it needs to do kind of corpus analysis and knowledge graph building.
It needs to be able to build an internal representation.
And then reason over so it can make logical predictions and entailments.
So you can imagine as I'm going through all this stuff,
the system that we're building at EC has dialogue components,
has deductive and inductive reasoning mechanisms.
It does uses deep learning to do corpus analysis and knowledge graph building.
It uses deep learning to do basic NLP.
And of course, it has an architecture through which all these components are integrated.
It's been, it's enormously challenging.
We have a ways to go, but those are the ingredients that we're playing with in this system.
How do you characterize?
Can I wear you are with it relative to?
Well, relative, relative to benchmarks that we need to define.
The one one is like you, which is just outlined with regard to understanding.
Well, that's exactly right. I mean, I think that look,
we're about six months to a year away from I think defining a good,
the way I like to phrase is a good, ambitious challenge problem.
So in other words, one that has a data set, a clear metric.
As I said, a very ambitious metric, a button on the less a clear metric.
And in an evaluation process, that just sets the bar a lot higher than what we're seeing today.
It's not limited by what deep learning stuff can do today.
It sort of takes that barrier and says, hey, let's forget about what's possible and isn't possible today.
What do we think on what we should be able to do?
Would be a good definition of this understanding problem that is ambitious enough and challenging enough.
But not impossible, or at least our perspective is not impossible.
And also to demonstrate an approach that is viable against that challenge problem.
And I'm super excited about that because I think this is what AI needs.
I mean, if we really want to tackle understanding, we need that.
When you think about this definition of understanding and the challenge problem,
to what extent does it incorporate elements they get at nuance,
like I'm thinking of things like Winagrad Schema challenge and things like that.
Yeah, I mean, I think the Winagrad Schema challenge is interesting.
And we should be able to tackle that type of stuff.
So I think it absolutely has to get at the nuance that you suggest there.
And again, because it is building an internal representation,
it will get confused. It should know it's confused.
It should be able to say, I don't understand this.
I can't fit this into my prior models of how the world works.
And here's how you can help me.
I mean, that's how it should behave.
And so you characterize this as a hybrid system.
And I'm envisioning hybrid not just being a connection of two different things,
but lots of different things.
And it strikes me that a big part of how you glue all this stuff together
is kind of formalizing the formal knowledge piece and representation.
To what extent is that an important element of the overall functioning of a system like this?
Yeah, so the formal representation is an important part.
And so you could imagine, I mean, it's not, I don't think it's hard to imagine if you thought at all about,
you know, architect these kinds of systems.
You know, you have to, you know, language comes in.
You have to go a little parsed.
You have to be able to do an initial syntactic and semantic,
at least shallow semantic interpretation.
You're going to get all kinds of possibilities.
So you're going to get different parses.
You're going to get different word senses.
You know, you're going to, you're going to get different semantic interpretations at different levels.
And you have to start to make sense of it.
And so when it gets to the making sense part,
you have to be able to reason about it.
So, you know, there's a formal representation that ultimately we map to.
There are multiple reasoning engines that pour over this
and start to evaluate different interpretations for the level of sense that they make.
It's as smart as it's prior knowledge.
So there's a Bayesian aspect to this as well as it uses prior knowledge to actually try to determine confidence
in what is the right interpretation and how do I move forward?
It has to be open ended as more information lands and gets acquired.
It has to be able to kind of go back and say, okay, which direction do I go to continue to make sense out of this?
It has to know what it's doing so that it can dialogue and ask questions
and ask for help about its interpretations.
And it has to do that in a way that isn't incredibly stupid
because otherwise humans aren't going to engage in it.
So it has to do everything it possibly can do in extracting back our knowledge from large corpora.
Even if it doesn't completely understand the large corporate yet
because it doesn't have the foundations to do that.
It at least has to be able to use that to prune its search and shape its interaction with humans.
So it doesn't get so stupid that nobody wants to talk to it.
So these are all enormous challenges that bring in sort of every aspect of AI,
save robotics type stuff, hardware type stuff,
but in terms of knowledge representation, reasoning,
natural language processing, learning, deep learning, reinforcement learning,
dialogue management, you name it.
You certainly make a good argument for the level of difficulty of the problem.
The other part of the argument you're making is the type of investment
and need to invest in solving it.
How do you characterize that for folks?
Yeah, so I mean, you know, the team, so first of it's interesting
because I'm all, you know, EC while we're doing or approaching a hard research problem,
you know, we're not strictly an academic research institution.
We are engineering a system and we're continually refining it, building it, and evaluating it.
So we have a mix of engineers and researchers,
and you can imagine that there's quite a bit of diversity.
So if people who specialize in machine learning, deep learning for folks,
we have NLP folks, dialogue folks, we have knowledge representation,
we have reasoning folks, we have linguists, so, you know,
and having a underlying architecture that is laid out well enough
that these individuals can work together.
You know, and I did this on, even though this is,
and towards a magnitude more complicated than the Watson architecture,
the basic approach toward managing a team is like this is very similar,
and how to conduct and assemble that team is very similar.
So the first set of interests, very committed to the mission,
you know, is incredibly important.
And at the same time, coming at it from very different perspectives
they all see themselves as contributing to a larger, more complex architecture
that isn't just an architecture in theory.
It's an engineered system and sort of you know where to plug in
and how to contribute and sort of move the ball forward.
So anyway, these are how I think about managing this type of project.
In terms of the scale of the investment, also good question,
always thinking about that myself, I'm always careful to invest incrementally.
So as I see, as I see the pieces coming together and, you know,
you start with a few people, you grow out, as you see pieces coming together,
you look for where the bottlenecks are, you look where for the opportunities are
and where the bottlenecks are, and you grow accordingly.
And so I tend to do that very carefully and pick the right people to fit into
the right places as we go. And so we've been doing that.
We're up to about 24 people now or, and is that enough?
No, I think that ultimately investment requires more than that.
And it'll probably continue to grow. But as I said, we'll grow incrementally
as we, as it's clear that that's where we're going to get the,
you know, the incremental value.
Earlier you mentioned that there's plenty of low hanging fruit
in deep learning. What's the, the argument for investing in clearly a difficult
problem relative to picking off some of that low hanging fruit?
It's tough. It's a tough call.
You know, it's difficult and it's difficult in terms of,
I'm lucky, you know, that I have the investor that I have who's interested
in the bigger picture and the longer term role that AI has to play.
And how it, how it, how the human machine interaction gets shaped over time.
And also has an appetite for a longer,
longer term type investment and sort of bigger bang for the buck in terms of the impact.
So that's good, hard to find than the 18 months or 24 month lower hanging fruit stuff.
So that's hard to find, but I've got that.
But it also hits you in the recruiting side because you get a lot of people
who are like, you know, I want the quick win.
And there are like quite a number of exciting applications.
I think you could approach with, with just, you know,
straight applications of deep learning.
And they're cool and they're fun.
So you end up really looking and finding people who are just really want,
they want that machine that fluently talks to them.
They want that, they want that, you know, that Star Trek computer that becomes their,
you know, thought partner.
And, you know, just some people really have that dream.
And they understand that, that, that this is,
this is one of the best shots for getting there.
Maybe to kind of start to wrap things up.
Can you talk a little bit about how you deliver something like that in phases?
Or, or, is it, you know, definitionally kind of a big bang thing?
Like we just get there because we have to have all these pieces just right
in order to execute that ultimate vision.
Yeah, so another, I, another really good question.
And, and, and we, we work, we, you know, we struggle through that question.
I mean, I think that there are phases.
And I think that are both phases from a business perspective,
from an investment perspective, and there are also phases from a,
more of a scientific or research perspective.
And we do spend time thinking about, you know, incremental applications
that we can build with the technology that we're creating.
It's part of the investment.
It's not the main thrust right now.
The main thrust is really staging the, the scientific work.
And as I mentioned, you know, defining the problem really well,
making sure that we can scientifically evaluate progress against it.
We continually actually apply deep learning systems and see how they fail
and do the air analysis.
We continually apply our evolving system, look at where it's failing,
and continue to try to improve it.
And we, we create incremental milestones for ourselves.
And that's important because to manage the project,
you can't have this thing that's like five years away.
You have to create those incremental milestones
and even creating those themselves are challenged,
particularly in the very beginning stages.
We're getting to a point where, where we can,
we can create those intermediate scientific milestones,
I think more right, really, and more effectively,
because we have enough of the architecture built out that we can do that.
We can run this on content.
We can do the air analysis. We can identify where the problems are.
We can start to iterate on making the system smarter.
So we're getting, we're at that point now, which is great.
But that's a very important thing.
And learning how to evolve those milestones well is important.
On the, on the kind of incremental application side,
that's also kind of getting some attention for us.
And we're thinking about ways that we can use the,
even though we don't have that deep general understanding yet,
we've built a lot of impressive kind of NLP and,
and representation and reasoning stuff that we think can help out
in a number of different areas and replaying with that.
We don't, we don't have to spin off though,
because of our investment structure,
we don't have to spin off those applications.
But we want to keep ourselves honest and make sure that we can demonstrate
to ourselves, our investors, that this technology will have impact
and that it can, and it can do that incrementally.
So that certainly is a part of what we, we think about and do.
But it's, it's, it's good.
You're asking like, you're, you're asking key questions
to figuring out how to, how to manage a business like this.
You're absolutely right.
Well, Dave, thanks so much for taking the time to chat with me
about what you're working on.
It sounds like really interesting stuff.
You bet. It's been a pleasure.
Awesome, my pleasure. Thanks so much.
Thank you.
All right, everyone.
That's our show for today.
Make sure you leave us your birthday message
over at twimmelai.com slash 3Bday
or via voicemail at 1636-735-3658.
We cannot wait to hear from you.
We want to know your favorite gem from the podcast,
what you've learned and how you've applied it
to what you do.
For information about today's guest, visit twimmelai.com.
As always, and especially today,
thanks so much for listening and catch you next time.
