Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
Over the next few weeks, we'll be exploring just a bit of the great research that was
showcased during last week's CVPR conference.
Before we get to today's episode, though, I'd like to send a huge thank you to our friends
at Qualcomm for their support of the podcast and their sponsorship of this series.
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,
reasoning, and action ubiquitous across devices.
The work makes it possible for billions of users around the world to have AI enhanced
experiences on Qualcomm technology's powered devices.
To learn more about what Qualcomm is up to on the research front, visit twimmelai.com-qualcomm
QAL-COM.
Next up, a quick community update.
If you're interested in the topic of causal modeling and machine learning, but missed
out on the initial cohort of the course we hosted with Robert Osa-Zooness, I'm happy
to announce the second cohort of this course and study group will be starting soon.
For more information, join us this Thursday, June 25th for a live webinar that Robert
and I will be hosting to introduce causality and review all of the details of the course,
including the many enhancements he's made this time around.
For more information, visit twimmelai.com-causal, and now on to the show.
Alright everyone, I am on the line with Bobak at Tashami Bishnordi.
Bobak is a research scientist at Qualcomm AI Research.
Bobak, welcome to the Twimmelai.com podcast.
And thank you for having me.
It's great to have an opportunity to chat with you and I'm looking forward to learning
a bit more about your research and what you're up to there at Qualcomm.
But to get us started, why don't you share a little bit about your background and how
you came to work in AI?
Sure.
So I did my master's in electrical engineering at Chonmer's University of Technology in
Sweden.
And in that program we had a bunch of courses like machine learning and patent recognition
as well as some image analysis courses, which kind of drew me to this field and I decided
to do my master thesis on cervical cancer diagnosis using ML.
So kind of in the medical imaging domain and I absolutely enjoyed it and decided to do
my PhD in the similar field.
So I came to the Netherlands in Rodwald University and for my PhD I was developing machine learning
models for breast cancer diagnosis in histopathological images.
histopathological images are microscopic images of tissue.
And kind of in the early mid part of my PhD this deep learning revolution happened.
And me and a lot of my colleagues quickly switched to use deep learnings and get familiar
with retraining it ourselves and kind of moved to that to using that in my entire project.
And during my PhD I also organized the chameleon challenge.
It was a challenge on finding cancer metastases on breast to more patients.
And it turned out to be a very successful challenge.
And was one of the first examples in which using AI the top leading algorithms were actually
outperforming human experts.
We compared the top two algorithms in the challenge with a panel of 11 pathologies and all they
were actually beating all the 11 pathologies without exception.
I also did a visiting research at Harvard at bedclap and also towards the end of my PhD
I decided to join Qualcomm where I'm here for a bit more than two years now and I'm mainly
working on conditional computation.
What is conditional computation tell us a little bit more about that?
So conditional computation in the context of neural networks refers to a class of algorithms
that can selectively activate its units conditioned on the input it receives.
Such that on average we have lower computation costs.
And when I'm speaking of units it could be layers or individual filters for example.
And the thing is in our feed forward neural networks we usually have this prior that no
matter what input is is receiving we always run all the layers, all the filters no matter
what.
And reality some examples might be simple and some harder and maybe for simple examples
we could exit earlier from that from the network and finish the classification or sometimes
we were for a classification task classifying the image of a cat and sometimes classifying
the image of a vehicle for example.
But now even in the middle of a network that we are kind of certain that we are dealing
with a picture of a cat we are still applying all those vehicle detection features or filters
in our to our feature map which is superfluous and also from a generalization perspective
that is bad for our network.
So conditional computation aims to selectively activate parts of a network.
It could be different layers, it could be channels, it could be actually a whole subnetwork
in your main network which you could completely deactivate.
In fact if we want to look back the first examples were maybe by Hinton, Jaffa Hinton
and Robert Jacobs from 1991 they had a paper adaptive mixture of local experts and the
idea there was they trained a giant network with many kind of small subnetworks which were
experts for a specific subset of a data and then there was a gating network which would
get the input as well and based on the input it decides which of these subnetworks should
be selected.
And in that way they were kind of encouraging the network to be expert, the subnetworks
to be expert on their own specific data.
And also in recent years in the era of deep learning we see more of the more approaches
are using conditional computation.
Maybe one of the early examples was branching it which adds auxiliary classifiers in your
network, let's say some early in the network, some in the middle of your network.
And the aim is that easy examples should exit early years.
Another work which was an inspiring work was convolutional networks with adaptive inference
graphs which basically decides to gate basically activate or deactivate individual layers
in a red net block, conditional input.
They hypothesize that maybe some layers are important for specific classes but not for
the others and they could learn how to turn on and off individual basically layers in
a network.
And they were actually inspired by their own previous works which showed that if you
actually delete individual layers in a resonant model at test time it's accuracy will not
drop.
That was very interesting.
Of course on the layers which you have a stride too so a down sampling you may get
a little bit of performance drop but on the rest of the layers you didn't get so they decided
to learn then to drop.
And this is also in short contracts to other types of complex like like VGG network you cannot
actually do that because if you drop a layer that their whole sequential part will break
but in resonant it was perfectly doable.
And that actually constituted the basic idea for what we want to do as well which was starting
to gate channels instead of layers.
Before we get into the details of your specific research are the conditional computation techniques
that you're applying exclusively focused on inference or conceptually it seems like
you could apply it to both.
All right.
Yes, exactly.
You could definitely apply it at training time as well.
So there's like an if and if else argument inside your training for example is saying
the activate you could choose not to send the gradients to that part of a network and
you could basically gain speed or an inference training time and also inference time as well.
And the specific techniques that you focus on are those more concerned with inference
time?
Yes, well, our main focus has been on inference time we try to, so it is definitely possible
to do that for training time as well but we mainly focus on inference time as well.
With the over-arching implication that you're trying to reduce the power and cost of doing
inference on a device.
Right, exactly.
So maybe that is actually not the only motivation.
Okay.
Interestingly, if you train such networks these networks tend to perform much better than
counterpart like a fixed neural network with the same computation cost.
And I will describe that in a bit that the reason is that these models tend to you can
start training a very giant model but then at inference time select only very limited
subsets of the resources available, condition on the input, you say, okay, this network is
huge but you should just pick the relevant ones.
It induces the network to have expert subnetworks inside the network as well because by gating
it understands, okay, these subset of filters should be activated for cat detection and the
rest maybe for another for vehicle detection.
So it actually interestingly improves the performance as well which is a good thing.
Well, why don't I take a step back and have you kind of describe at a high level the
different types of approaches you're taking in these papers and then we can dig into more
specifics.
Sure, sure.
So in the first paper which was batch shaping for learning conditional channel gated and
networks we were basically thinking instead of gating individual layers in the network,
let's make it more fine-grained and gate individual filters.
That would give us more, basically more flexibility and more power representation power.
It would allow us actually to gain more interpretability as well to understand which filters
are firing for what classes.
So that was the main focus of the first paper.
The second paper which was on continual learning actually before starting the second paper,
we already knew that this idea of channel gating would be very useful for multitask learning
and continual learning because for example in multitask learning one problem they have.
So we know that if you want to learn a couple of tasks together, it generally tasks might
help each other so that the performance gets a little bit improved in general because
they can share the features among them.
But as a number of these tasks, in principle, suddenly you see the performance actually
starts dropping and that is because of feature interference.
By forcing a specific task to use a feature which is not relevant for the task, it actually
starts degrading performance and we thought that if we could use our gated networks we
could decide when to activate or deactivate features and not allow feature sharing if
a feature is not relevant.
And in continual learning these gates could actually serve as memory which would allow
us that if a feature is particularly important for a specific task, maybe we have to preserve
it and not allow other tasks to update it too much.
And in the last paper, if we mainly focused on video long range activity detection videos
for classification and we thought we might get as input a video which is like 10 minutes
but we know that there is huge amount of redundancy and relevant things happening which are not
really important for the actual classification task and we did dynamic gating of individual
snippets in the video to only focus on the important parts and actually saved a huge amount
of computation based on that.
Okay.
Well let's jump into the first paper because I have some questions there.
You mentioned that this paper is kind of shifting from thinking about gating layers to
gating individual filters.
What are the specific types of filters you're gating here?
So the design is something like this, assume we have a resonant block, we are augmenting
this while we are adding to this resonant block a gating module, this gating module gets
the same representation that goes to the resonant as input.
And its output is a bunch of gates and the number of these gates is equal to the number
of filters let's say in a first convolutional layer of the resonant block.
So one gate couple to each filter and the filter wants to activate or deactivate the use
of that filter and we had a bunch of criteria for example we wanted the gating module to be
very light because we didn't want to have a lot of additional overhead.
We wanted the gates to be input dependent so that they don't make just arbitrary decisions
but they see the input and condition and the input make a decision.
And last we wanted it to make a binary decision.
We didn't want it to be like an attention because attention would not keep saving you
any compute.
We wanted to be zero or one which means it would have been a discrete optimization problem
and we used gumball softmax for training this network.
But it was very interesting problem because we started training this model and from the
very beginning if you have let's say it's a classification problem let's say and you
have the cross entropy loss.
The gate automatically want to learn the most trivial solution which is just be on all
the time.
And by being on all the time it satisfy the objective of the cross entropy loss and it's
as if you're training a boring regular network it's just a standard regular network.
While we wanted the gates to behave conditionally to be sometimes off for certain types of
input and sometimes on.
And for that.
So training this network it kind of all at once you're not separately training the gate
layer and the network itself right we're training into and all at once exactly.
So then we started saying okay we want to specify these gates we don't want it to be always
on.
And by doing that we found out actually if you for example apply L0 loss a lot of gates
turn out to be permanently off which is actually not interesting either because permanently
off would be equivalent to these model compression techniques like pruning techniques.
We want the gate to be at the same time sometimes on and sometimes off.
So and interestingly when we decided to encourage entropy to to have a higher entropy let's
say sometimes on sometimes off these gates could easily cheat they could generate a probability
distribution center that 0.5 so assume at the output of the gate you pass it to a sigmoid
and then take the arc max or or a threshold 0.5.
It could put the distribution center 0.5 that when you add the gumball noise and and take
the sample it is randomly on and off it's not really like yes just randomly so it it
had learned to be a random dropout which was really not not something we wanted.
But we wanted in contrast was a U shaped by model distribution which means for certain
the type of data the output is 1 and for certain data it is 0 and we said okay we have
this strong prior why don't we actually help the gate to have a distribution like this
and that we that came the our other contribution which was the batch shaping loss which was
inspired by the Kramer fanmeasus criterion what it does it basically takes our prior distribution
which is a better like a U distribution we can see the F and then we also compute the
empirical CDF of our sampled distribution from the output of a gate for batch of data
and then we minimize the distance between these CDFs and the interestingly well CDF is
perfectly differentiable the derivative of that of a CDF is PDF so we could get that
freely from the forward pass and then the distributions perfectly matched each other
which was very interesting to see and we think this batch shaping loss could be very interesting
for a lot of other applications when you want to match to distributions before we go into
the some of the applications which distributions were matching one another was a better distribution
so we we forced the gate to have a better like distribution that's a 50% on 50% off
and that helped a lot the the gate to behave more conditionally because he knew that it can only
be 50% on so let's turn off turn it off for whatever irrelevant type of data and this helped the
network to behave really conditionally and we could also visualize when it is on when it is off
and it was really making sense and interestingly we saw very good behavior of this network we
we saw that we could actually take a resident 18 model let's make this resident 18 model
incredibly wide let's make it 20x wider but at an inference time before it to make it so much
as small that this is the site of the size of a standard resident 18 with widths one so it has
a lot of filters but you are only allowed to choose among them such that the size doesn't
doesn't change that much but on average so for more complex examples you could choose more filters
and easier less filters and this resident 18 at an inference time size could perform as with as
a resident 50 for example it performs much better than it's it's kind of an equivalent size
network which is with a fixed architecture and how would you compare the complexity of the
very wide resident 18 with the resident 50 so the complexity we we measured that in terms of
MAC operations so multiplication and accumulation operations so it's much much more cheaper than
resident 50 of course and we of course forced it to have with disparity to not how to not use
all the filters so that in front time it is actually still the size of a resident 18 when you were
applying the the bad shaping is the idea that one of your parameters is how many of the gates are
active is that how you how you were able to shrink down this wide resident 18 to a much more
narrow one yeah so so we had a prior for these for this better distribution the prior was actually
being 60% on and 40% off and we started with with a very strong coefficient at the start of
training so all the gates are forced to take that shape but we as we go on we slowly reduced that
that coefficient so that the model has a little bit more flexibility and then we see that some of
the gates might actually just get rid of that conditionality and go and be completely on those are
maybe the filters which are very important and it's like too fundamental they they need to be
always executed but there's some gates all the majority of the gates keep that shape but we also
added the L0 loss and L0 loss has the ability to actually throw away if a filter is completely
useless so for example if due to poor initialization some of the filters turned out not to be that
useful for the classification task it could actually push it down to completely off so we have
a combination of completely off filters and completely on filters but the majority conditional
filters and they take the conditional ones take different shapes some of them might be 80% off
20% on and some of them 80% on 20% off and we using the coefficient of these two losses we
controlled how much Mac we want to save basically now it sounds like you if I'm understanding this
right you kind of set off with this idea of building these conditional channel gate and networks
and when you saw kind of the gates you know all on or all all for kind of flipping randomly
back and forth were these presumably these were your initial experiments and they were surprising
like did you I'm curious as you were seeing these kind of results you know how did you
note it to go to the to the batch shaping you know as opposed to I'm thinking about you know maybe
this just doesn't want to work right yeah exactly so so we had several minutes
meetings with Maxwell and we had a lot of brainstorming sessions very interesting one one day he came
so I was plotting the histogram of the gate output a lot of them were centered in the middle
and he was so sharp like as soon as he saw he said oh these are like random dropout you want
these U shape right and I said oh I want that U shape but it said let's think about how how to
make that U shape and we thought of a lot of different experiments for example pushing the values
to be away from 0.5 so the values tended to be in the center we decided to make it pushing towards
the two sides but that didn't work we we tried the entropy as a loss that didn't work because the
gating much it could easily cheat and after everything in the middle and a lot of losses actually
led to not conditionality so complete pruning which is like static compression methods which
were we were not interested we wanted the conditional way of doing that and over time we thought
that actually you cannot so a single gate you cannot cause a specific distribution without
regarding the entire batch so that the whole point was that this distribution is only defined
well when you take the entire batch into account so you know in this batch there is a collection
of data points and you want to these data points to get to behave differently for some of them
we wanted to be on and for some of them to be off and we didn't know which ones are going to be
on off isn't it the network has to learn and we actually didn't even know of the so at least
I didn't know of the Kramer-Famizus criterion we came up with this loss and then later stage
we found out oh did this Kramer-Famizus criteria did exist and it was like proposed like 40-50
years ago but it was never used in the deep learning and we made it like differentiable and
and there is one key point inside that and that when you get the output of a gate you have to sort it
and and then generate CDF of the sorted sorting values and then during the backprop you have to
remember to un sort the operation so that the gradient goes through the right input and the
right direction so there were a bunch of tricks but then we tried to basically narrow down the
problem make it as simple as possible to find out what makes it work and this turned out to work
nice nice and you you mentioned that this batch shaping technique has your starting to
mention that this back shaping technique has other applications sure sure I'm thinking that it could
potentially be used for whatever application in which you want so you want to match the distribution
of a parameterized feature in your network let's say a feature map to any distribution for example
in batch norm we may want it to be normally distributed we could actually enforce that with the
batch shaping loss and basically in whatever application that you want to impose a prior on the
distribution of your feature maps you could do that in this with this technique. So the next paper
where you're looking at task where continual learning is that building on this batch norm
or sorry batch shaping technique or is it a separate research in the direction of the
conditional gated networks. So it is a follow up on the channel gated networks we use the same
the exact same type of network but instead of applying it to ResNet because we knew it would work
for regular networks as well like VGG type we we applied it both for regular types of models
and and ResNet models so maybe I could briefly tell of talk about the continual learning problem
well in continual learning we we have basically this setting that you want to learn a specific task
let's say task a using a neural network and you train this model using data from task a and
get a very good accuracy let's say then you go to task b but you assume you should consider that
you will never have access to data from task a anymore and now you want to train this previously
trained model again such that it performs very good on task b but doesn't forget what it has
learned on task a in reality the fundamental problem with continual learning is that we have the
catastrophic forgetting problem because as soon as you update the weights they completely forget
what they previously learned and that is actually a fundamental problem we have in training neural
networks that's why we when training we train these models using many epochs or we have to shuffle
the data all the all the time we cannot just take a batch and perfectly learn it and go to the
next batch to perfectly learn it because we keep forgetting and this problem also arises in
continual learning so our idea was that we could use these channel gated networks
to bring several benefits but but let me let me before that tell about two very fundamental
approaches which people use one of them is is basically maybe a work by deep mind on elastic
weight consolidation what they do they measure at the end of training on task one they measure
the importance of the individual filters using the diagonals of the Fisher matrix that if a
feature if a particular weight is very important let's not update it too much let's slow down the
learning on those weights and the features that were less important we could actually
learn them better or make them more available for the future task well obviously the problem with
this method is that as you as you go on and on and learn more tasks you tend to have this trade
off between learning new things and not being able to change things previously learned and this
approach works very well on simple data sets like MNES for for now for let's say two or three tasks
but as soon as you increase the number of tasks it fails there is another vein of approaches
which are basically progressive neural networks which they say okay let's train our model on task A
and then when we go to the next task list add neurons and learn task B and we have the ability to
learn previously to use previously learned features but are not able to change them the problem
with this approach is that as you add neurons it will become really not very scalable because your
model is going to get huge and also you're forcing to share previously learned features which
might be irrelevant for the current task that is coming so so our idea was was like this so we
thought we let's use our channel gated networks so you have task one you you train it using our
channel gated network and each of the layers have a specific gating module which is specific for
that the specific task so each task has its own gating modules these gating modules are not only
task dependent but also input dependent so they basically during training we basically enforce
these gates to not use all the filters because we want to make a lot of filters available for future
tasks so we impose a lot of sparsity let's say a specific layer has 100 filters using channel gated
methods we and sparsity we enforce that it for example uses 20 of out of the 100 filters available
and because filters or these gates are input dependent as well not all not all of these 20 filters
will be actually used for all the examples some may use only five filters some maybe the entire
filters this makes these these these channel gated models extremely efficient and very with very
low computational cost you're essentially adding the task as a another input to the channel
gate that you already had before yes that would then put dependent yes but then we go to the
to a new task now the new task is going to have its own gating module well these gating modules
are actually very important things because they if we study them how they are firing they could
easily tell us how important the feature is if a filter is if a gate is firing too often it means
the filter is going to be used all the time we should never change this filter so what we do we look
at the the most important features and in practice we actually even because of a heavy sparsification
if a if a filter is selected even only once we chose to keep it and freeze it so all the features
or the filters that were important for task one we freeze all of them and all the rest of the
filters which we are not used we re-initialize them and make them available for the new task
and the new task can obviously learn the new filter is based on based on basically its own objective
but it also has the option using its own gating modules to to choose to use previously learned
features as well it is not allowed to update them but it can choose to use or not use them
which would make these networks to have positive transfer of information but not forcing it
so if a feature is not irrelevant it will not be shared and you can keep growing and increasing
more tasks the great thing is that these gates are always input dependent and inference time
they are extremely light so if you make this network extremely wide still it would be super light
in at the inference time because only a very small subset of filters are going to be selected
at each layer so how exactly are you allowing the the tasks to choose the filters from the previous
tasks is this based on initialization or something something else so so just imagine we have a
resonant block and in and in the let's say in the first convolutional layer we have 100 filters
10 of them are frozen so they are not he's the gate is not or they're not updateable
and 90 of them are are learnable so the gating module gets the same representation that goes to
these resonant block as input and chooses okay should I use this filter number one which is
not updateable or not it makes a binary decision pretty much like before but it cannot update the
filter it just has to say choose to use or not but for the rest of a 90 it can update them
and so we trained this model and very interestingly we saw that there were actually a lot of
SOTA and very good methods already in the literature and on four data sets we were able to
to achieve as good or better performance than all competing methods but then we wanted to make
this a little bit more challenging there is a specific setting in which you actually don't even
know which tasks you are trained you are operating on so up to now we were assuming that you know
which task you are going to work on at test time and you would only activate that that branch
of the network and the gating modules of that is specific task but in some cases you might not even
know which task you are working on and this is a very complicated setting and there is not much
working the literature and what we did was that so we we assumed from the start of at the inference
time so you could train as usual but at inference time you would when you get an input you would
gate them with different hypothesis this is task one this is task two this is task three the same
input but at the end of the network we we added the task classifier which looks at the gating
patterns of this feature map how how this feature map has been altered based on different
hypothesis and based on that makes a makes a decision that which task you are you should be classifying
this network but but of course this classifier at the end of the network is going to suffer from
forgetting as well because you have as you increase the number of tasks you have to retrain this
classifier and we chose to use a generative model to remember the data that was used for previous
tasks so when we are training on task one we are also using a generative model to learn to generate
data from task one and these generated are some generated samples are used for training the task
classifier at the end they're not that many methods but we there was one method from cvpr 19 that
we compared against and we were outperforming it on the on all the data sets but actually a very
large margin but it's still a very complex task it's like it sounds like your model is starting to
get very complex when you start to add a generative model on top of everything you've already done
yeah definitely if this is kind of a problem which is very complex already and adding the complexity
of the generative model and being able to generate very sharp and it nice examples just
adds more complexity to the continual learning problem and do you think that there's something
in particular that you've done with the application of conditional networks that allows the
generative model to work or could you have that could you then now that you've seen some good
results with the generative model take that back and apply that to other aspects of the network
without the conditional gating for the unknown multitask problem that that's a very good question
so first of all these gating models sorry these generative models are not going to be used at
the inference time we don't need them it's just used for training the basically the task classifier
and after training you can throw them away but we also actually thought about training this
generative model using a unified giant generative model which can be gated as well in the same
way that we are we are training this this inference model we didn't try that but I think it would be
an interesting idea to use gating for for generating examples as well so creating with task one
generate examples of task one things like that we've got a third paper that we wanted to make
sure to cover as well and that's the time gate paper refresh us on on the setting that that
paper is looking into sure so in that paper we primarily focused on recognizing long range
activities in in long range videos so the problem with long range activity what what first of
what do we call when do we call a video a long range activity so assume we have a 10-minute
video and all that is happening inside that video is a guy skiing and the label of that video
is skiing that's going to be so easy to classify because even if you look at a single frame you
would be able to classify the entire video this we don't call a long range activity a long range
activity could be for example making a pancake because it has so many atomic small action items
which you have to recognize getting getting the egg from the from the freeze for example getting
powder scrambling or mixing everything together and I'm putting on the path so it has a lot of
atomic activities and in order to recognize that this action is making a pancake you have to
identify all of this but all of these could happen at different time intervals inside a 20-minute
video and if you want to go and classify frame by frame all these 20-minute video is gonna explode
it's gonna cost you need to recognize all that or can you just recognize the round thing you know
kind of with bubbles and then getting flipped at the end and then you figure out you you're making
a pancake it could be that there are a lot of classes which are similar for example scrambled eggs
might be making it scrambled eggs could be very similar as well and maybe there are items like
picking up the flower could also be a key item here that would help you identify that it's
actually making a pancake and picking out the flower by itself might not be enough because
the person could be making a very puffed cake as well so you need really multiple things to be
able to to recognize the entire video so for the setting you've got this long video with multiple
tasks being illustrated tasks in the human behavior sense yeah and you're trying to classify
all of the entire video as one thing that's happening exactly exactly so the so one of the very
common way of doing that is if you divide this entire video into chunks of snippets which let's
say each snippet is eight frames and then you could give it to a very heavy model like i3d or
or a resident 3d model that would make a representation out of these eight frames and then you do that
for like a sliding window all throughout your network then you have a classifier that looks at
these features and does the classification this is going to be extremely expensive so what we saw
we saw that we could maybe actually do a gating using a gating module dynamic that dynamically
looks at these frames and decides which parts of the video are interesting to look at so our
approach basically has made up of two steps it has a very very light classifier or basically
network in the start which gets a very compact representation of the of an input frame then we have
a gating module and this gating module is conditioned both on the on the segment and also looks at
the context so not only the current frame but also at the other frames and using this gating
module we are able to to say whether this particular frame is relevant for classifying this video
or not and only the ones which are the most relevant and we could also similar to previous works
impose a sparsity so that the network the light network does not pick so many frames only
a limited numbers and only the most relevant ones would go to a heavy network and do the actual
heavy operation which is which is common we use the same models as well and the good thing is
that you could couple these lights network with any model you could actually use very super efficient
models which are also already implemented and complete with this gating module it would still
give you a lot of benefit and no matter what model we use as the main feature representation
extractor we roughly saved half of the mac operations and even at that point we slightly got
improved performance maybe because we we are able to help the network focus only on the
relevant parts of the video and get over from the distractions and actually get slightly better
performance even even at half the computation cost and this approach is definitely orthogonal to
a lot of compression techniques because some some approaches focus on making the models compact
and let's say you could you could basically do our channel gating on these models as well or
you could do static compression methods and adding these gating modules to only focus on the
relevant parts of the video could really bring down the computation cost overall.
And so is the use of channel gating or conditional gating in this context you know how similar is
it you know an application is conditional gating in the other two papers it seems like it's
somewhat different. It is somewhat different actually so we use the same gumball softmax trick
trick for for training these gating modules but it is basically for training this we we sought
of first extracting a bunch of concept kernels concept kernels could be extracted using
let's say a network which is pre-trained on the data set you want to work on and getting some
representations let's say I want 100 kernels at the end and you you basically compress this data
and say these are the core concepts of my data set and then this gate as inputs instead of
getting or training or during training this is before the start of the training you could this
is an independent concept so you could extract that anyway okay and then for these gating is so
previously we had for example as input to our gates the representation coming from the
resident block here is very different here the gating module would get as input the representation
coming from the last layer of the light network in the start of the model so the light
network first give us some representation and using that and and also we do a dot product of that
with the concept kernels to see if there is anything interesting in this frame given the concept
kernels that I know about this data set and this dot product would go to our gating module and
this gating module well of course it uses the gumball softmax trick which which we've been using
previously as well I think I'm struggling with what is actually gated in this are you gating the
kind of the input flowing into the kind of the the overall network or you you're still doing
something similar where you're you're gating you know resident modules or filters or something
like that the former so so the light net and a heavy net and if the light net says the current
frame is irrelevant the entire frame and and actually this snippet around it so we we analyze the
center of the snippet let's say we have eight frames that goes to the heavy net but the light net
only analyzes that the middle frame of these eight among these eight frames and if you know it's
not interesting then we just completely get not analyze the entire snippet okay and so you've got
these eight frames and you're analyzing the center frame are you striding or is it eight and then
the the next day uniformly sampled and so uniform sample a lot of these segments let's say eight
frame eight frame eight frame until the end to cover the entire video and the only the two net only
analyzes the center frame among these let's say the frame number four and based on that makes
a decision is these the entire chunk of frames useful for the current classification task or not
are there other applications to this then video or why did why did you even go after this
particular problem so we thought gating in general would benefit a lot in let's say classification
problems or or we even tried that on segmentation problem as well but a dataset with the biggest
amount of redundancy we think is video video analyzing video there is so much correlation in
in different frames that you're basically observing the same thing over time that it makes it that
the most low hanging fruit for for gating operations we are actually following up a lot of tasks
using a conditional compute for video analysis as well including video segmentation classification
etc apologies if you mentioned this already but the specific results of this in terms of the
videos and your the computational complexity all that what did you end up finding so you found
that for example no matter what heavy type architecture we we use whether it is the most complex
resident 3d model let's say resident 3d 101 or it's a very efficient model we can reduce the cost
by half while still getting slightly better performance than the original model which so we both
have and perform better and one very interesting property of this this approach is that it makes
interprecipibility very easy because you could immediately look at the chunks of video in the entire
10 minute video that are selected and are they actually correlated so you could very easily analyze
if the gates are working properly or not and did you see any interpretability results in other
couple of papers or problems that you were looking at as a result of this yeah absolutely so in
the first work for the channel gating we we targeted some of the some of the gates that would
uh that the most interesting gates are the ones which are very selective in firing for example
only firing five percent of the times if you look at them the examples look very similar for
example only tiny insects in the middle of grasses are detected so by by this particular gate
or for example all the giant creatures in in the water are detected by a particular gate
and then in the continual learning it was we thought it would be harder to recognize these these
gates uh to to see differences but it was uh very very interesting we we looked at a gate which was
running in all different tasks let's say the task was one versus two classification for task one
three versus four for task two four versus five task three et cetera and there was a gate
that would activate whenever the wall in in the first like blanks i didn't recognize it but then
i focused on it said okay this this gate is actually firing for all the bolt phones and all the
ones which are like the tiny phones uh which is the the defter is not very bold it would not fire
which which found that feature interesting but but it's in general it's a little bit more difficult
uh when the number of classes is not huge to find interpretability but for internet for example
it was very easy to to see uh interesting uh to get interesting insights of what these
gate is doing yeah yeah this this work in conditional gated networks yeah how does this um
ultimately kind of get expressed in you know products and and things like that what do you do with
you know this stuff that you're you know learning and discovering uh so this is currently mostly
at our at a research level of course um we we have been um so the the entire vision is that
we could probably at some point have a very very giant network that could be very sparsely activated
it could even work for multiple tasks completing independent tasks and then condition on the input
it would find out which path in the network uh it would it would actually should should need to be
activated so at the inference time it's gonna be very light but uh but of course um the first step
that we took we talked to the software team and uh to see what are the requirements for
for uh making use of such models and usually um our discussions were were so we we need to convince
them that we need to implement a specific layer is that that would allow uh implementing this
on our our particular hardware uh we are in the middle of a discussion and uh trying to use this
channel gating in a in a number of works um but in general a lot of times this when the when the
trick that we are using or researching is straightforward that could have very fast impact and go
directly into the product for example one of my uh colleagues uh Marcus Nagel and and Martin
Taiman uh have been working on a quantization paper uh which was from iccv 2019 i think uh and this
paper was working very well they they talked to the software team it was immediately implemented
and uh it was suddenly like in a in a matter of a couple of months event to our Snapdragon
platform and people could could actually use it and then later on we got in our uh private
what what's that group i got a message that this uh this particular quantization method is now
employed by this very famous app that i'm not going to name it but but but but not suddenly maybe
hundreds of millions of people are going to are going to make use of the uh the the technique that
you developed it was completed research at some point but now it is actually in your product
wow wow but something like uh like this work is it um you know to what degree is it you know
broadly applicable uh to a wide variety of of applications and in particular we've talked about
you know different applications that it it can be applied to but uh in each of those cases
you're kind of tuning the application of um the conditional gating uh i think quite a bit
you know do you do you see a uh a point in which this is um a kind of off the shelf you know
thing that's automatically applied across different use cases you know what's the the path towards
making it generally applicable and that's a good question i think uh so i'm really advocating
the use of conditional compute uh in general i think it is biologically also very uh very
uh relevant because uh a lot of uh uh the models of the brain say say that the neurons are activated
in a very sparse fashion and it's not like a pruning method that some of the neurons are never
activated it's it's really dynamic sometimes some elements are activated and sometimes not so i
think this should definitely be um a direction in the future uh and uh i think it's kind of
uh it would be weird for me if the future of AI would be that we have let's say for 100
applications we have 100 independent models uh which are only trained with that data and we
run them all one at uh one at the time this this is not going to be the future i think i think it
would be a centralized model which would be very good in operating uh uh on multiple tasks at the same
time and it could sparsely get activated um so initially even i when i talked to some hardware
forks they they thought this might be a little bit uh and not uh in maybe by next year as it may not
happen but they definitely see uh that this might happen at some point because they they see the
vision that this uh this could be part of the future uh of AI models and this immediately says that
for example to get this implemented you need an if else statement so if this happened if get
gate says this operation uh you need to do this and else do that so we need uh first of to see
what can be done by the software forks uh and then if if necessary we could even reach out to
to the hardware forks to influence the next generation of even hardware i think well ballback
thanks so much for taking the time to share with us uh what you're up to there it sounds like uh
really interesting stuff that um you know i hope to to see more of in the future
it was my pleasure thank you very much for having me thank you
all right everyone that's our show for today for more information on today's show
visit twomolai.com slash shows as always thanks so much for listening and catch you next time
