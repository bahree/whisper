WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:24.040
I'm your host Sam Charrington.

00:24.040 --> 00:29.760
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring

00:29.760 --> 00:36.360
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other

00:36.360 --> 00:37.760
smart devices.

00:37.760 --> 00:38.840
You name it?

00:38.840 --> 00:40.160
It was there.

00:40.160 --> 00:44.920
Of course, I was also able to sit down with some really interesting folks working on some

00:44.920 --> 00:48.360
pretty cool AI-enabled products.

00:48.360 --> 00:52.400
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

00:52.400 --> 00:55.680
interviews and other quick takes from the show.

00:55.680 --> 01:01.960
And beyond the look up for our AI and consumer electronics series right here on the podcast,

01:01.960 --> 01:03.200
coming soon.

01:03.200 --> 01:08.280
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning

01:08.280 --> 01:11.760
Summit in Montreal back in October.

01:11.760 --> 01:17.760
This was a great event and in fact their next event, the Deep Learning Summit San Francisco,

01:17.760 --> 01:23.200
is right around the corner on January 25th and 26th and will feature more leading researchers

01:23.200 --> 01:28.320
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow

01:28.320 --> 01:33.920
of Google Brain and Daphne Kohler of Calico Labs and more.

01:33.920 --> 01:40.440
Definitely check out the event and use the code TwimmelAI for 20% off of registration.

01:40.440 --> 01:45.920
In today's show, I sit down with Eric Humphrey, research scientist in the Music Understanding

01:45.920 --> 01:48.000
Group at Spotify.

01:48.000 --> 01:52.400
Eric was at the Deep Learning Summit to give a talk on advances in deep architectures

01:52.400 --> 01:56.520
and methods for separating vocals and recorded music.

01:56.520 --> 02:02.320
We discuss this talk, including how Spotify's large music catalog enables such an experiment

02:02.320 --> 02:07.720
to even take place, the methods they use to train algorithms to isolate and remove vocals

02:07.720 --> 02:13.400
from music, and how neural network architectures like UNET and Pix2Pix come in the play.

02:13.400 --> 02:18.280
We also hit on the idea of creative AI in general, Spotify's attempts at understanding

02:18.280 --> 02:23.840
music content at scale, optical music recognition, and much more.

02:23.840 --> 02:26.400
And now on to the show.

02:26.400 --> 02:36.600
All right, Eric, so why don't you tell us a little bit about your background and how

02:36.600 --> 02:38.840
you got interested in machine learning and AI?

02:38.840 --> 02:39.840
Sure.

02:39.840 --> 02:42.120
I guess it all started in high school.

02:42.120 --> 02:44.720
I always played music.

02:44.720 --> 02:46.680
My dad was an engineer.

02:46.680 --> 02:50.280
And the compromise of that when I was going into college was electroengineering.

02:50.280 --> 02:52.000
I really wanted to make guitar pedals.

02:52.000 --> 02:56.600
I wanted to learn about how amplifiers worked, and then that kind of got into more signal

02:56.600 --> 03:02.040
processing and algorithms, which led to a lot of parameter tuning by hand.

03:02.040 --> 03:06.000
If you want a delay pedal, you know, you have all these different knobs and whatnot.

03:06.000 --> 03:10.040
And I started doing some more stuff around beat tracking and tempo tracking for running

03:10.040 --> 03:11.040
to music.

03:11.040 --> 03:14.880
And it was like, I spent so much time designing these algorithms by hand.

03:14.880 --> 03:19.040
It's like a lot of the tradition for a lot of digital signal processing things.

03:19.040 --> 03:21.720
And that was right around the time that machine learning really started to pick up.

03:21.720 --> 03:25.600
And it's like collecting data for these things and then training an algorithm.

03:25.600 --> 03:27.920
It just simplified the process so much.

03:27.920 --> 03:30.200
And like, well, this is a no-brainer.

03:30.200 --> 03:34.240
I can have the system that I want to do all these cool things, but leveraging these kind

03:34.240 --> 03:39.120
of like elegant data-driven solutions that freed me up to go running and play music and

03:39.120 --> 03:42.160
do all these other things and just, you know, have these systems work.

03:42.160 --> 03:43.720
And this is all why you're in school?

03:43.720 --> 03:49.120
Yeah, so I did a master's down at University Miami in the really great music tech program

03:49.120 --> 03:52.800
down there and got to do a little bit more with running music and worked with the music

03:52.800 --> 03:58.640
therapy department and then parlayed that into a PhD at New York University, where I got

03:58.640 --> 04:04.440
to work with Juan Pablo Beo and Jan Lecun a little bit, which really kind of pivoted it

04:04.440 --> 04:05.440
even further.

04:05.440 --> 04:06.440
Okay.

04:06.440 --> 04:10.080
So that was really when it was, I took a pretty formational machine learning class with

04:10.080 --> 04:14.360
Jan, my first semester at NYU, and kind of put me off on that.

04:14.360 --> 04:15.360
That's the way to get started.

04:15.360 --> 04:16.360
Yeah.

04:16.360 --> 04:17.360
Yeah.

04:17.360 --> 04:18.360
It was pretty, pretty certain dipitus.

04:18.360 --> 04:21.760
I didn't fully appreciate what I was getting myself into at the time.

04:21.760 --> 04:24.120
It was right around 2009-2010.

04:24.120 --> 04:28.800
But for me, it was always much more about computer visions, doing a lot of things with images

04:28.800 --> 04:29.800
and audio.

04:29.800 --> 04:35.400
It had some stuff around speech recognition, but music was always kind of, you know, lagging

04:35.400 --> 04:36.400
behind.

04:36.400 --> 04:42.760
So for me, I always wanted to say, you know, to be similar to computer vision or ASR, recognize

04:42.760 --> 04:43.760
chords and music.

04:43.760 --> 04:44.760
I'm like a tourist.

04:44.760 --> 04:46.520
Can you show me how to play this song automatically?

04:46.520 --> 04:49.720
Can you show me where the beats, the bars, the chords are?

04:49.720 --> 04:54.160
Can I have a playlist of just choruses, these kinds of things?

04:54.160 --> 04:59.440
So when you really start to think about the opportunities around music, leveraging things

04:59.440 --> 05:05.000
like deep learning and machine learning, you can, you know, the imagination can run wild,

05:05.000 --> 05:06.840
the kinds of things you can do with that.

05:06.840 --> 05:07.840
Awesome.

05:07.840 --> 05:08.840
Yeah.

05:08.840 --> 05:11.120
Tell us a little bit about what you're up to nowadays at Spotify.

05:11.120 --> 05:12.840
Did you go to Spotify right after your grad school?

05:12.840 --> 05:19.160
No, I spent some time at a small music ed tech startup that was trying to do things around

05:19.160 --> 05:20.160
optical music recognition.

05:20.160 --> 05:24.880
So in the same way that you could do document scanning, one of the core pieces of technology

05:24.880 --> 05:29.680
that we were working on was if you took a picture of sheet music, could you turn it into

05:29.680 --> 05:35.800
MIDI so that kids could learn how to play any piece of music at their disposal and then

05:35.800 --> 05:39.720
be able to, you know, on the longer arc gives them feedback about how they're doing and

05:39.720 --> 05:40.840
kind of taking it from there.

05:40.840 --> 05:43.360
It seems like that should be fairly straightforward.

05:43.360 --> 05:48.960
So you'd think that one of the really, really interesting things about music in all of its

05:48.960 --> 05:53.800
forms, and I'll mention this tomorrow is that it's so fundamentally intelligent that,

05:53.800 --> 05:59.600
you know, when you have even just a piece of music for a single voice, monophonic instrument,

05:59.600 --> 06:01.480
you don't have polyphony or these other things.

06:01.480 --> 06:06.440
You actually have, instead of OCR for being generally one-dimensional, it's linear and

06:06.440 --> 06:08.800
you'll span a vertical axis.

06:08.800 --> 06:16.280
But music, you actually have a two-dimensional grid that moves linearly with these really

06:16.280 --> 06:19.640
complicated links backward and forward.

06:19.640 --> 06:24.440
So if you have DSL signals or quotas or repetitions or multiple endings, it turns into death

06:24.440 --> 06:29.000
by a million paper cuts because one of the things you'll find is triplets will be

06:29.000 --> 06:33.480
notated in the first measure and then known after because it's cheaper that way to not

06:33.480 --> 06:36.680
print these additional threes on all these things.

06:36.680 --> 06:39.120
But there, any musician would kind of know that it's there.

06:39.120 --> 06:41.880
Any intelligent musician would know that I was there, got it.

06:41.880 --> 06:46.560
Or you'll run into these other really interesting common cases, there aren't even educations

06:46.560 --> 06:51.040
where for children's music, they won't notate rests because they're trying to simplify

06:51.040 --> 06:52.200
the musical surface.

06:52.200 --> 06:53.200
Okay.

06:53.200 --> 06:57.320
So it's in four, but you'll have these two notes here and then those two notes and the

06:57.320 --> 07:01.080
upper staff and a machine is like, I guess they're the same.

07:01.080 --> 07:07.920
So having all of these and then it doesn't have a ton of data for supervised training and

07:07.920 --> 07:12.480
it creates this really, really interesting, challenging, but fundamentally intelligent

07:12.480 --> 07:13.480
problem.

07:13.480 --> 07:14.480
Interesting.

07:14.480 --> 07:15.480
Yeah.

07:15.480 --> 07:20.120
It's one of those things that, you know, to crack from a very like human interest level,

07:20.120 --> 07:25.000
there's a ton of music that just hasn't been brought into the 21st century yet, you know.

07:25.000 --> 07:30.360
Something that was notated from Gregorian Chants all the way up to kind of now where we've

07:30.360 --> 07:36.560
started to shift away from notated music to more recorded music or digital audio workstations,

07:36.560 --> 07:41.800
like Ableton style project files where there isn't really an artifact of the music except

07:41.800 --> 07:42.800
the recording.

07:42.800 --> 07:48.280
There's all this older stuff that could be brought into the future for creative purposes,

07:48.280 --> 07:52.480
musicology and kind of more anthropological consideration.

07:52.480 --> 07:53.480
Okay.

07:53.480 --> 07:54.480
Interesting.

07:54.480 --> 07:58.840
So I did an interview with the dog at Google Brain Project Magenta.

07:58.840 --> 08:04.360
He had an interesting presentation at another conference a few months ago that talked about

08:04.360 --> 08:09.160
even the step beyond what you're just describing, like once you have this sheet music or you

08:09.160 --> 08:15.200
can describe the music to the computer accurately, how do you then get it to play expressively?

08:15.200 --> 08:16.800
And they've been doing some interesting things there.

08:16.800 --> 08:19.760
I don't know if you're familiar with it, but it's interesting stuff.

08:19.760 --> 08:27.560
Yeah, I mean, expressivity and music creation and composition, it's so interesting to really

08:27.560 --> 08:31.000
dig into because I think it cuts to the core of humanity.

08:31.000 --> 08:37.520
There's been so much amazing work around game playing and AI recently, AlphaGo, Atari,

08:37.520 --> 08:40.400
but you have these well-defined objective functions, right?

08:40.400 --> 08:44.720
Make the score high, win the game, that so you have these extrinsic motivators that fit

08:44.720 --> 08:47.800
pretty well into a reinforcement learning formulation.

08:47.800 --> 08:51.800
And music, the most interesting things are internal, they're intrinsic.

08:51.800 --> 08:55.280
It's the novelty and the surprise of, I didn't see that chord coming.

08:55.280 --> 08:59.480
You know, when you're sitting in your room when Hendrix was really just digging in and just

08:59.480 --> 09:03.920
sinking his teeth in deep to a solo is for him, this sort of is hard.

09:03.920 --> 09:08.640
And you could think about having these feedback loops for listens on Spotify or revenue

09:08.640 --> 09:14.520
generated, but these aren't really what cuts to the heart of creativity and expression.

09:14.520 --> 09:16.440
Like, what are you, what are you getting after?

09:16.440 --> 09:20.640
And I think that's going to be one of the really interesting challenges as we start to move

09:20.640 --> 09:28.320
into that next stage of kind of like really autonomous or, you know, things that are self-directed

09:28.320 --> 09:29.880
in the AI space.

09:29.880 --> 09:30.880
Why is it doing it?

09:30.880 --> 09:31.880
What motivates it?

09:31.880 --> 09:33.160
Does it have this notion of self?

09:33.160 --> 09:39.200
So when you think about elements of, I think music, humor, sarcasm, these kinds of things

09:39.200 --> 09:44.760
kind of start to encroach on that in a way that a lot of the recent history of machine

09:44.760 --> 09:46.600
learning hasn't gotten to yet.

09:46.600 --> 09:47.600
Interesting.

09:47.600 --> 09:51.800
Interesting that you put sarcasm in that bucket as a New Yorker living in the Midwest.

09:51.800 --> 09:56.920
I find that sarcasm is underappreciated in a lot of places and I would love to dig deep

09:56.920 --> 09:59.440
into AI and sarcasm.

09:59.440 --> 10:05.520
I think it's an, I think it's a East Coast Northeast kind of thing and I joke off and back,

10:05.520 --> 10:11.360
nothing to back this up that it's probably related to just, you know, local climates and

10:11.360 --> 10:15.680
it's a way to kind of deal with it, a great deal, great weather we're having, right?

10:15.680 --> 10:20.360
And it's just gray snow and there's sludge and all the street corners are backed up and

10:20.360 --> 10:23.440
whatnot, but you don't have that same thing on the West Coast where it's beautiful every

10:23.440 --> 10:24.440
day.

10:24.440 --> 10:29.320
sarcasm doesn't land in the same way with native Californians.

10:29.320 --> 10:30.320
So true.

10:30.320 --> 10:34.040
So at Spotify, you worked, what aspect of this problem are you working on there?

10:34.040 --> 10:35.040
Sure.

10:35.040 --> 10:39.640
So I think at a higher level to the extent that I can kind of delve into, I work on a team

10:39.640 --> 10:47.040
of researchers where we are building algorithms that can understand music content at scale.

10:47.040 --> 10:52.280
So some of the obvious applications would be to fit into, can we better understand users?

10:52.280 --> 10:54.480
Can we help provide better recommendations?

10:54.480 --> 11:01.280
A lot of recommender systems right now have gotten very far by looking at how users,

11:01.280 --> 11:07.360
consumers, listeners interact with content, whether it's purchased at the same time or

11:07.360 --> 11:11.880
in the same catalog, or they've been grouped into the same basket or playlist or things

11:11.880 --> 11:12.880
like that.

11:12.880 --> 11:15.680
You can get really far without having to look inside the box.

11:15.680 --> 11:22.000
So the metaphor I like to make is algorithms for content work as well as they do for say

11:22.000 --> 11:26.440
Amazon when you can kind of just, you don't have to look inside the box, but when you really

11:26.440 --> 11:31.280
want to say more deeply understand a user and what they're after, it's like what color

11:31.280 --> 11:32.280
is the shoe?

11:32.280 --> 11:33.280
Is it felt?

11:33.280 --> 11:34.280
Is it vinyl?

11:34.280 --> 11:40.720
Is it interact with music, do they really gravitate toward an artist or is it lyrical content?

11:40.720 --> 11:42.760
Is it about the harmonic content?

11:42.760 --> 11:47.880
Because one of the things that's really interesting about music in particular is your ability

11:47.880 --> 11:53.520
to enjoy it is tightly coupled to how well you can understand it and to what extent you're

11:53.520 --> 11:54.960
surprised by it.

11:54.960 --> 12:02.080
So for example, there's a one artist called Marsbo and for the untrained listener, it's

12:02.080 --> 12:06.400
going to sound a little bit like noise art, but if you go to a Marsbo concert, there will

12:06.400 --> 12:10.000
be people that are just totally rocking out and they're in it and they get it because

12:10.000 --> 12:13.840
they have a model and an understanding for what's going on.

12:13.840 --> 12:18.880
So you can use how people interact with content as a proxy for what they understand and what

12:18.880 --> 12:20.320
they can relate to.

12:20.320 --> 12:22.680
How explicit is that understanding for them though?

12:22.680 --> 12:27.480
A lot of people can't, well, I can't speak for Marsbo fans.

12:27.480 --> 12:32.480
But one of the really interesting things about music is that a lot of ways that we describe

12:32.480 --> 12:37.400
it are so personal and occasionally they're cultural or they're niche.

12:37.400 --> 12:42.560
So you'll find that in certain subgenres, certain words are used certain ways and you'll

12:42.560 --> 12:43.560
see things pop up.

12:43.560 --> 12:49.120
Oh, this playlist is on fleek and it's like you'll find that some of those language and

12:49.120 --> 12:53.560
the semantics, there are course intermediaries to describe the thing you actually mean.

12:53.560 --> 12:57.960
You're like, I love the part of this song where it really just makes my heart pump.

12:57.960 --> 12:59.280
You're like, well, what is it?

12:59.280 --> 13:00.280
I have no idea.

13:00.280 --> 13:01.840
I have no language to describe these things.

13:01.840 --> 13:06.680
Which makes education and visualization and interaction with the content, a really

13:06.680 --> 13:11.080
interesting area for some of this content understanding at scale.

13:11.080 --> 13:12.080
Botify.

13:12.080 --> 13:13.080
Why do I like this song?

13:13.080 --> 13:16.840
Are there other songs out there that you don't know about that would make you feel

13:16.840 --> 13:19.960
have such a strong physiological reaction in a similar way?

13:19.960 --> 13:21.920
Am I going to be able to ask botify that?

13:21.920 --> 13:22.920
Why do I like this song?

13:22.920 --> 13:23.920
Wouldn't that be a great question?

13:23.920 --> 13:24.920
That would be an answer.

13:24.920 --> 13:25.920
That would be awesome.

13:25.920 --> 13:31.360
I mean, especially because as I hear you describe this, I am not a music kind of sore by

13:31.360 --> 13:38.160
any stretch of the imagination, but it resonates for me that a lot of that is not really having.

13:38.160 --> 13:42.680
I guess when we talked about understanding right there's, you know, the impact that music

13:42.680 --> 13:46.320
has on you and kind of its ability to move you.

13:46.320 --> 13:53.400
And then your ability to understand it moving you and how and why and when.

13:53.400 --> 13:56.800
And then the next level is like your ability to articulate all that.

13:56.800 --> 14:01.600
And I feel like out of for me, if I was able to maybe come at it from the back and be

14:01.600 --> 14:05.480
able to articulate and understand kind of these things at a conceptual level, that might

14:05.480 --> 14:08.760
help me to connect to other types of music.

14:08.760 --> 14:13.040
And it would be really cool if I could ask if Spotify could basically teach me this.

14:13.040 --> 14:17.920
And I would actually take to the logical conclusion because beyond that, then you could say all

14:17.920 --> 14:20.640
this music was composed by another person.

14:20.640 --> 14:25.880
So generally when you're composing as a composer, you're pulling upon all of your experience

14:25.880 --> 14:29.760
and your own surprise and novelty, but that's going to relate and land with an audience

14:29.760 --> 14:31.120
in a very particular way.

14:31.120 --> 14:36.360
So when you think about an artist composing for their fan base or in a genre or a style

14:36.360 --> 14:41.240
or trying to achieve a certain outcome, you know, you might say, you know, I really want

14:41.240 --> 14:46.120
to drop, I want to a verse that's minor so we can step into a major chorus because people

14:46.120 --> 14:49.560
will feel that as this release intention.

14:49.560 --> 14:53.880
But the only way that that can be conveyed appropriately is if everyone has that similar

14:53.880 --> 14:55.280
expectation.

14:55.280 --> 14:59.760
So you're playing off of certain kind of musical behaviors that are in culture in certain

14:59.760 --> 15:06.160
ways, which also makes music at the level of like a global culture really interesting.

15:06.160 --> 15:10.280
Or micro culture as you start to be able to connect the dots across really neat genres

15:10.280 --> 15:15.000
that, you know, didn't have any bandwidth in more of a mainstream music era.

15:15.000 --> 15:17.200
And are you a musician personally?

15:17.200 --> 15:18.720
Do you play what do you play?

15:18.720 --> 15:20.640
I play everything I can get my hands on.

15:20.640 --> 15:26.080
I grew up playing saxophone for about a decade, switched over to guitar and then guitar

15:26.080 --> 15:27.080
and voice.

15:27.080 --> 15:29.320
And I've been learning drums for the last couple of years.

15:29.320 --> 15:33.440
And anyway, that I can kind of express myself with sound is a good time.

15:33.440 --> 15:38.200
And when you're expressing yourself with sound, do you think about it in the way that you

15:38.200 --> 15:43.000
previously described, like, I'm going to try and hit this chorus and I don't even have

15:43.000 --> 15:46.440
my, I can't even, my ability with the words is so poor.

15:46.440 --> 15:49.160
I can't even repeat what you just said.

15:49.160 --> 15:53.400
But I get at least the impression that I have from, you know, pop to immediate TV, whatever

15:53.400 --> 15:57.720
is it, you know, they just go into a room and then music comes out and not like, oh,

15:57.720 --> 16:00.600
I'm going to nail them with this crescendo right here.

16:00.600 --> 16:02.840
I think everyone's process is different.

16:02.840 --> 16:07.800
And I mean, as my PhD was in a music program, so I had to take some graduate level theory

16:07.800 --> 16:13.840
courses, which actually gave me not a perfect vocabulary for it, but a better understanding

16:13.840 --> 16:16.240
of the things that I had developed an intuitive feel for.

16:16.240 --> 16:20.000
And I certainly, I don't think about it that way in the moment.

16:20.000 --> 16:22.200
When I'm playing music, I'm very much in it.

16:22.200 --> 16:26.200
You may have heard of like the idea of like creative flow, where it's like time flies

16:26.200 --> 16:27.200
and whatnot.

16:27.200 --> 16:32.800
So I think for myself kind of coming back at it as an editor, I can think about it like,

16:32.800 --> 16:35.240
oh, you know, like, this is a really good raw idea.

16:35.240 --> 16:38.560
And I can massage this in a way, you know, if I piece these things together, here's a

16:38.560 --> 16:40.320
really interesting musical pun.

16:40.320 --> 16:42.480
I don't think I'm more in the moment.

16:42.480 --> 16:44.560
It's a little bit more like I surprised myself.

16:44.560 --> 16:45.560
You know, that was neat.

16:45.560 --> 16:49.440
I have to record everything and then go back through with a little bit more of a higher

16:49.440 --> 16:50.440
planning process.

16:50.440 --> 16:54.720
So I may be getting away here and turning this into this week in music and, uh, they're

16:54.720 --> 16:55.720
related.

16:55.720 --> 16:56.720
And the arts.

16:56.720 --> 16:57.720
I contend they're related.

16:57.720 --> 16:59.600
But you're speaking here at the conference.

16:59.600 --> 17:00.600
What's your talk on?

17:00.600 --> 17:06.280
I'm talking about one project that we've recently published out of our newly minted music

17:06.280 --> 17:13.840
understanding group at Spotify around primarily singer vocal separation from recorded music.

17:13.840 --> 17:19.120
I lovingly refer to it, hat tip to a colleague from Miami, but it's unbaking the cake in

17:19.120 --> 17:20.120
a way.

17:20.120 --> 17:25.080
So in recorded music, you have these, this is basically acapella from any song, any track

17:25.080 --> 17:27.200
or instrumental from any song.

17:27.200 --> 17:30.560
So you can isolate the vocalist, you can remove the vocalist.

17:30.560 --> 17:34.880
It's a little bit of like the audio processing wizardry that, you know, if you look to computer

17:34.880 --> 17:40.400
vision, there have been some amazing, really interesting things with style transfer or texture

17:40.400 --> 17:41.560
mapping.

17:41.560 --> 17:49.200
So we took some recent advances with the picks to picks and the unit architecture and have

17:49.200 --> 17:51.880
adapted that to music processing.

17:51.880 --> 17:55.640
But the thing that really kind of made it work for us is that, you know, we have this really

17:55.640 --> 18:01.800
the large music catalog and one of the big bottlenecks for source separation for a long

18:01.800 --> 18:03.040
time has been data.

18:03.040 --> 18:07.280
And we were brainstorming one day, it's like, you know, deep warning is great when you

18:07.280 --> 18:10.760
have data for training and you have two options when it comes to data.

18:10.760 --> 18:14.000
You can curate it or you can get clever and try to harvest it.

18:14.000 --> 18:19.000
And a lot of computer vision stuff has gotten really far by using text around images and

18:19.000 --> 18:22.720
leveraging these other kind of serendipitous signals that occurs a byproduct of other

18:22.720 --> 18:23.720
kinds of things.

18:23.720 --> 18:26.960
It's bought a similar thing with play listing.

18:26.960 --> 18:31.000
So one of those signals we were able to harvest is that instrumental versions actually occur

18:31.000 --> 18:33.600
with a non negligible frequency.

18:33.600 --> 18:38.480
So if you have, say, a web scale music collection, you can actually end up with about a month

18:38.480 --> 18:42.240
or two's worth of straight audio for training these kinds of algorithms.

18:42.240 --> 18:47.520
So we were able to do some work out to like a percentage basis, like what percentage of

18:47.520 --> 18:52.600
your catalog has instrumental versions, a very small portion that I probably couldn't

18:52.600 --> 18:56.440
give you a number, either from memory or other reasons.

18:56.440 --> 19:00.920
But it's small, but when you have a large enough catalog, it becomes sufficient.

19:00.920 --> 19:04.200
So we end up with a couple of months of audio and we're able to train these algorithms

19:04.200 --> 19:08.520
to both isolate and remove vocals from the mix.

19:08.520 --> 19:12.320
So we nudge the state of the art a little bit versus some other models that have been

19:12.320 --> 19:13.320
published.

19:13.320 --> 19:14.320
Well, let's jump into that.

19:14.320 --> 19:17.520
So you mentioned, you mentioned picks to picks and another one unit.

19:17.520 --> 19:18.520
Yeah.

19:18.520 --> 19:20.920
So the unit architecture preceded the picks to picks work.

19:20.920 --> 19:21.920
Okay.

19:21.920 --> 19:22.920
What about those two?

19:22.920 --> 19:23.920
Sure.

19:23.920 --> 19:29.160
So the unit architecture, actually taking a step back, one of the ways that a lot of, you

19:29.160 --> 19:33.240
can generally call it signal processing in machine learning has worked for a while as

19:33.240 --> 19:37.120
the idea that if you could have some kind of compressing auto encoder, reducing the

19:37.120 --> 19:40.480
dimensionality, you'll preserve the attributes that are most important.

19:40.480 --> 19:45.240
You can back it out and then by minimizing some loss over the reconstruction, then you

19:45.240 --> 19:48.760
can start doing some interesting things, fiddling with your intermediary representations.

19:48.760 --> 19:55.280
But what ends up happening is especially for audio, a lot of the high frequency detail

19:55.280 --> 19:57.600
in the outputs is just the loss.

19:57.600 --> 20:01.840
So it works pretty well for general shapes, but sharp edges, these kind of things fall

20:01.840 --> 20:06.480
away in these auto encoder and butterfly style architectures.

20:06.480 --> 20:12.040
The unit takes this butterfly style architecture, folds it over into a V or U, however you want.

20:12.040 --> 20:15.640
And butterfly, like as the visual, you got this wide input, you're kind of compressing

20:15.640 --> 20:18.320
down the dimensionality and you're fanning it back out.

20:18.320 --> 20:19.320
Exactly.

20:19.320 --> 20:20.320
They're like a bow tie.

20:20.320 --> 20:23.560
But if you're going to take the butterfly, the bow tie, and fold the wings on itself,

20:23.560 --> 20:27.040
what you can do is you can take the, this is a convolutional architecture, you can take

20:27.040 --> 20:31.520
the feature maps from the forward path and concatenate them with the feature maps from

20:31.520 --> 20:33.320
the reverse path or the inverse path.

20:33.320 --> 20:39.000
And what that ends up doing is providing a lot more detail and just like more fine-grained

20:39.000 --> 20:43.840
granularity so that when you, in source separation, what you generally try to do is produce

20:43.840 --> 20:48.480
a mask and then you apply that mask over say an input, time frequency representation,

20:48.480 --> 20:53.600
like a spectrogram, spectrograms are kind of like an equalizer curve drawn out over time.

20:53.600 --> 20:59.080
So you can wait the relative contribution of each frequency been in time, just as a zero

20:59.080 --> 21:00.080
to one.

21:00.080 --> 21:05.600
So we use this unit architecture to produce a mask over the input representation.

21:05.600 --> 21:06.600
Let's back up a second.

21:06.600 --> 21:07.600
Sure.

21:07.600 --> 21:13.160
Because I'm losing, what does it mean to map the, what does it mean to take the feature

21:13.160 --> 21:15.000
map and circle it back on itself?

21:15.000 --> 21:16.360
I think that's the way you said it.

21:16.360 --> 21:20.400
So generally in a convolutional architecture, you'll have a bank of kernels, you'll take

21:20.400 --> 21:25.480
each kernel and you'll confolve it with an input representation and you'll get out generally

21:25.480 --> 21:29.040
a three-dimensional tensor of feature maps.

21:29.040 --> 21:34.560
If your input is 2D, you'll have k kernels by x by y.

21:34.560 --> 21:39.240
And on the inverse path, you can, you're also producing these feature map tensors.

21:39.240 --> 21:44.400
So with the corresponding layer in the inverse path, you can concatenate the input feature

21:44.400 --> 21:51.160
maps with the reverse feature maps along that case dimension, which becomes that in input,

21:51.160 --> 21:55.680
you can convolve another kernel matrix or kernel tensor, I guess.

21:55.680 --> 22:00.080
So it's kind of like you have your forward path features and your inverse path features

22:00.080 --> 22:02.120
being processed at the same time.

22:02.120 --> 22:08.400
So it allows the kind of like the composition of parts intuition for a deep architecture

22:08.400 --> 22:14.160
that information's propagating all the way to what is effectively this higher level representation

22:14.160 --> 22:19.120
of the network while preserving some of that fine grain detail on the way back.

22:19.120 --> 22:23.960
And what we find is that the masks that are produced by this architecture, do you have

22:23.960 --> 22:27.840
a lot more detail in what they're able to pinpoint in terms of the frequencies?

22:27.840 --> 22:32.560
So it's able to really dial in the components that are contributing to say singing voice.

22:32.560 --> 22:38.000
So the way this works is you can train one of these unit architectures for pinpointing

22:38.000 --> 22:39.000
the voice.

22:39.000 --> 22:41.960
So you can train in a separate one for isolating the voice.

22:41.960 --> 22:46.600
If you had different data, you could imagine doing something similar for say drums or

22:46.600 --> 22:49.440
other source specific architectures.

22:49.440 --> 22:54.800
When you're training to pinpoint the voice, are you using like an acapella version as

22:54.800 --> 23:00.080
you're training data, or are you still using your instrumental and somehow like inverting

23:00.080 --> 23:01.640
it or something like that?

23:01.640 --> 23:02.640
That's a great question.

23:02.640 --> 23:08.080
And for the work that we published and we'll be presenting at the Izmir conference in Suzhou,

23:08.080 --> 23:12.760
China and the near future, what we actually did was there were far more instrumental versions

23:12.760 --> 23:13.960
than acapella versions.

23:13.960 --> 23:19.520
So we kind of estimate what the vocals would be by looking at the positive difference

23:19.520 --> 23:22.560
between a full mix and then the corresponding instrumental.

23:22.560 --> 23:26.240
So it's kind of like a proxy for what the voice would be.

23:26.240 --> 23:30.240
Which is not surprising why we get much better vocal removal results because we're training

23:30.240 --> 23:35.240
on the actual instrumental spectra than the vocal which is estimated.

23:35.240 --> 23:39.400
But it's a really interesting point for future work to say what other kinds of content

23:39.400 --> 23:43.800
do we have at our disposal that we can kind of fold in to this kind of work.

23:43.800 --> 23:46.680
So we've got some really encouraging results, we've got some demos that I'll be sharing

23:46.680 --> 23:47.680
a little bit later.

23:47.680 --> 23:51.160
I imagine there'll be out on the internet in not too long.

23:51.160 --> 23:54.720
And so what else are there any other things that you talked about during your talk or

23:54.720 --> 23:57.560
that you're planning to talk about during your talk you haven't talked yet?

23:57.560 --> 24:01.240
I guess the only other thing I would mention is being that we're in a really pivotal

24:01.240 --> 24:05.760
time for a lot of machine learning and artificial intelligence research as we really start to

24:05.760 --> 24:07.760
frame this conversation.

24:07.760 --> 24:11.520
It's been an interest of mine for a while now that like what we discussed earlier, this

24:11.520 --> 24:14.120
idea of creative AI.

24:14.120 --> 24:19.280
And I do want to take the time tomorrow to just have that shameless plug, music is a really,

24:19.280 --> 24:20.680
really interesting domain.

24:20.680 --> 24:26.000
It doesn't necessarily have the same societal impacts that autonomous vehicles could have

24:26.000 --> 24:31.840
in terms of saving lines, but in a lot of ways music does have that human power in much

24:31.840 --> 24:36.040
more of a emotional and kind of cultural way.

24:36.040 --> 24:41.440
So as we start to think about like what other ways do we want to tackle artificial intelligence?

24:41.440 --> 24:47.720
Like can we really study music without inherently studying humanity and kind of the intelligence

24:47.720 --> 24:50.480
that we've been in doubt with?

24:50.480 --> 24:56.000
And one of the questions that comes up for me in this conversation is a little bit out

24:56.000 --> 25:02.600
of left field, but what in your view is the kind of open source as a concept has like swept

25:02.600 --> 25:08.240
over a bunch of different industries and areas of human activity, right?

25:08.240 --> 25:09.360
Is there an open source?

25:09.360 --> 25:10.960
What is the open source for music?

25:10.960 --> 25:16.440
And the thought that preceded that was it'd be kind of interesting if in some period of

25:16.440 --> 25:22.040
time a musician delivered not this like one final thing, but you know there was like almost

25:22.040 --> 25:25.760
like a project file for your digital workstation it had.

25:25.760 --> 25:28.960
You can pull out the drums if you wanted to, you can pull out, you can isolate all different

25:28.960 --> 25:34.680
kinds of things because it followed some kind of open source, you know, ideology or something

25:34.680 --> 25:35.680
like that.

25:35.680 --> 25:38.440
Is there an analog like that for music or what's the closest we get?

25:38.440 --> 25:41.200
I think that's a really, really fascinating idea.

25:41.200 --> 25:44.280
And you know, I have to smile a little bit when you say what's the analog to that?

25:44.280 --> 25:48.920
In a lot of ways playing music together is that analog, you know, for a while ever since

25:48.920 --> 25:54.040
it was music publishing or sound recording, it's been really interesting what sampling,

25:54.040 --> 26:00.560
remixing, reuse means for music because everything that you compose, create, share is an

26:00.560 --> 26:03.320
amalgamation of all your prior experiences.

26:03.320 --> 26:04.320
So Larry Lessig.

26:04.320 --> 26:05.320
So Larry Lessig.

26:05.320 --> 26:08.320
A little bit of an inherent open source to music from that perspective.

26:08.320 --> 26:09.320
Exactly.

26:09.320 --> 26:12.720
Larry Lessig wrote a really great book called Remix and that touches on a lot of these

26:12.720 --> 26:17.040
interesting things, both from a legal but also a philosophical and kind of cultural perspective.

26:17.040 --> 26:22.920
A focus particularly on music or more generally on touches on, touches on things.

26:22.920 --> 26:27.320
So there have been some famous mashup artists like Greg Gillis and Girl Talk.

26:27.320 --> 26:32.520
I'm testing my memory at this point, but there's a famous composer who decried the rise

26:32.520 --> 26:37.680
of sound recordings and that it was going to like change what reuse and remixing really

26:37.680 --> 26:38.680
meant.

26:38.680 --> 26:44.000
We've seen that get a little bit murky as copyright laws change or whatnot.

26:44.000 --> 26:47.240
But yeah, it's music wants to be open source by nature.

26:47.240 --> 26:51.680
So I think the analog is analog music, it's the acoustic signal.

26:51.680 --> 26:52.680
Interesting.

26:52.680 --> 26:56.560
It'll be interesting to see how music evolves along with machine learning and AI.

26:56.560 --> 26:57.560
Yeah.

26:57.560 --> 26:58.560
I think they have a bright future together.

26:58.560 --> 26:59.560
Absolutely.

26:59.560 --> 27:00.560
Well, thanks so much Eric.

27:00.560 --> 27:02.560
Thank you so much Sam.

27:02.560 --> 27:08.320
All right everyone, that's our show for today.

27:08.320 --> 27:13.320
Thanks so much for listening and for your continued feedback and support.

27:13.320 --> 27:19.280
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple

27:19.280 --> 27:20.800
podcasts.

27:20.800 --> 27:26.880
My producer says that one of his goals this year is to crack the top 10 and to do that,

27:26.880 --> 27:29.800
we will need your help.

27:29.800 --> 27:33.240
Please head on over to the podcast app, rate the show.

27:33.240 --> 27:35.400
Hopefully we've earned your five stars.

27:35.400 --> 27:40.400
Give us a glowing review and share it with your friends, family, co-workers, Starbucks

27:40.400 --> 27:43.880
baristas, Uber drivers, everyone.

27:43.880 --> 27:46.360
Every review and rating goes a long way.

27:46.360 --> 27:48.560
So thanks in advance.

27:48.560 --> 27:53.600
For more information on Eric or any of the topics covered in this episode, head on over

27:53.600 --> 27:58.000
to twomlai.com slash talk slash 98.

27:58.000 --> 28:03.520
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

28:03.520 --> 28:06.680
or via Twitter at at twomlai.

28:06.680 --> 28:34.320
Thanks once again for listening and catch you next time.

