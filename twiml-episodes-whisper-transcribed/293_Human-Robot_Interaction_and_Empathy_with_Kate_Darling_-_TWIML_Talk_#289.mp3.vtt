WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.720
I'm your host, Sam Charrington, hey what's up everyone, before we jump into today's

00:34.720 --> 00:37.320
show, big news on the Twimble Con front.

00:37.320 --> 00:42.440
I am excited to announce that in response to strong attendee and speaker interest, we

00:42.440 --> 00:46.800
have supersized our agenda.

00:46.800 --> 00:47.800
What does that mean?

00:47.800 --> 00:51.640
Well we've added a second track, doubling the number of great sessions and speakers that

00:51.640 --> 00:53.200
we can accommodate.

00:53.200 --> 00:58.680
And we're unveiling part of that expanded agenda today for the first time at twimblecon.com

00:58.680 --> 01:00.840
slash speakers.

01:00.840 --> 01:05.360
Check it out and then hit that big green register today button to join us and learn from

01:05.360 --> 01:09.960
our experts how your organization can eliminate the barriers to building effective machine

01:09.960 --> 01:13.480
learning models and getting them into production.

01:13.480 --> 01:18.240
Hope to see you there.

01:18.240 --> 01:20.840
All right everyone, I am on the line with Kate Darling.

01:20.840 --> 01:24.360
Kate is a research specialist at the MIT Media Lab.

01:24.360 --> 01:27.160
Kate, welcome to this week in machine learning and AI.

01:27.160 --> 01:28.920
Thanks for having me.

01:28.920 --> 01:31.960
I'm really looking forward to this conversation.

01:31.960 --> 01:39.160
We met not too long back at the AWS Remarers Conference or Amazon Remarers Conference where

01:39.160 --> 01:46.880
you did a great presentation on some of your research into human and robot interactions.

01:46.880 --> 01:50.800
And I'm really looking forward to diving into that.

01:50.800 --> 01:54.760
But let's start with the kind of broad brush look at the field.

01:54.760 --> 01:58.640
You're a leading expert in robot ethics.

01:58.640 --> 02:02.200
What exactly is robot ethics and why is it important?

02:02.200 --> 02:04.320
That's a great question.

02:04.320 --> 02:10.760
So robot ethics sounds very science fictiony, very, you know, I robot blade runner-esque.

02:10.760 --> 02:18.280
But when I talk about robot ethics, what I really mean is the ethical use of robotic technologies.

02:18.280 --> 02:24.960
And part of that is, you know, how we integrate technology into the workforce, how we think

02:24.960 --> 02:29.800
about responsibility for harm in all sorts of contexts, whether that's automated weapons

02:29.800 --> 02:34.680
systems or automated vehicles or other types of automated technology.

02:34.680 --> 02:40.360
But the main thing I'm interested in in the sphere of robot ethics is the social aspect

02:40.360 --> 02:44.720
of integrating robots that seem very lifelike to people.

02:44.720 --> 02:49.200
So I'm really interested in the ways that people treat robots like they're alive, even

02:49.200 --> 02:51.240
though they know that they're just machines.

02:51.240 --> 02:55.760
And what sort of ethical issues can arise from that?

02:55.760 --> 03:01.400
And how did you get interested in this field, what's your background and what led you to

03:01.400 --> 03:04.000
this area of focus?

03:04.000 --> 03:12.080
So I originally studied law and social sciences and I did law and economics and intellectual

03:12.080 --> 03:18.600
property, but I think that my interest has always been in how systems shape behavior.

03:18.600 --> 03:24.160
So, you know, if you look at the laws of system or economics is a system, or now I'm really

03:24.160 --> 03:29.000
focused on technology as a system, and how it shapes human behavior.

03:29.000 --> 03:34.880
And I think what really got me interested in robots in particular was this one moment

03:34.880 --> 03:41.000
where I was I was in law school and I bought this baby dinosaur robot called a pleo, they

03:41.000 --> 03:45.640
don't make them anymore, but it was this really cool toy that had this kind of lifelike

03:45.640 --> 03:46.960
behavior.

03:46.960 --> 03:50.360
And one of the things it did was mimic pain very well.

03:50.360 --> 03:54.560
So like if you held it up by the tail, it had a tilt sensor and it knew that it was upside

03:54.560 --> 03:59.840
down, so it would start to cry and squirm around.

03:59.840 --> 04:06.080
It's super cute and it was really cool like for the toy that it was at the time, so I would

04:06.080 --> 04:10.400
show it off to people and I would be like hold it up by the tail, see what it does.

04:10.400 --> 04:15.680
And people would hold it up, and after a while it started to bother me when they held

04:15.680 --> 04:22.680
it up too long, and I would tell them to put it back down, I'd say that's enough now.

04:22.680 --> 04:27.800
And that was really interesting to me because I knew exactly how the toy worked, but I still

04:27.800 --> 04:31.160
felt this empathy for it when it was crying.

04:31.160 --> 04:33.680
And I was like that's weird.

04:33.680 --> 04:38.840
And then I started looking into this more and I discovered the whole field of human robot

04:38.840 --> 04:43.200
interaction that looks at how people interact with robotic technology.

04:43.200 --> 04:51.560
A lot of it is studies that border on psychology and people's tendency to treat these machines

04:51.560 --> 04:55.520
like living things and how to tweak that.

04:55.520 --> 05:00.520
So I got very interested in that, and I started coming at it though from the perspective

05:00.520 --> 05:07.440
of okay clearly we do this, but what does this mean in the broader context of a society

05:07.440 --> 05:11.560
where we're increasingly integrating robots into shared spaces?

05:11.560 --> 05:16.120
You have several of these PLEO robots to this day.

05:16.120 --> 05:21.440
I think you showed some photos of them in your presentation and we'll make sure to either

05:21.440 --> 05:26.880
include or link to some of those photos in the show notes.

05:26.880 --> 05:28.840
Yeah, they're very cute.

05:28.840 --> 05:39.280
So you kind of mentioned the increasing role of robots in our day to day lives as we're

05:39.280 --> 05:45.520
transitioning from world where they, you know, the primary experience that most folks

05:45.520 --> 05:50.280
had with robots was if they work with them like industrial types of robots and now we're

05:50.280 --> 05:58.560
starting to see these robots that are in stores guiding people around or your robot baristas,

05:58.560 --> 06:00.680
things like that.

06:00.680 --> 06:09.360
How is the study of human robot interaction practically applied to, you know, this new

06:09.360 --> 06:13.640
world that we're evolving into?

06:13.640 --> 06:18.320
Well yeah, so like you said, you know, we're very familiar with industrial robots.

06:18.320 --> 06:22.760
We've had those for a long time, but what's happening right now is that robots are coming

06:22.760 --> 06:29.120
into workplaces and households and public spaces, you know, stores.

06:29.120 --> 06:32.440
And right now the technology is still very crude.

06:32.440 --> 06:37.080
I know like my mom recently had an encounter with a robot in stop and shop that she was

06:37.080 --> 06:38.080
not happy with.

06:38.080 --> 06:46.200
She was like, creepy and it's beeping and I don't like it, but I think it's a matter of

06:46.200 --> 06:52.800
time before the design gets better and more compelling and people actually, you know, start

06:52.800 --> 06:54.960
to accept robots in their shared spaces.

06:54.960 --> 06:59.960
I think it's inevitable that this is going to happen. I also think it's inevitable that

06:59.960 --> 07:04.600
we're going to be working more with robotic technology because I know the media likes to

07:04.600 --> 07:07.600
talk about how robots are taking all the jobs.

07:07.600 --> 07:11.400
And that's true in some cases, jobs that are very, very easily automated.

07:11.400 --> 07:15.400
But in most cases, robots aren't good replacements for humans and they have a very different

07:15.400 --> 07:16.400
type of skill set.

07:16.400 --> 07:21.960
So what is actually happening is that we're going to see more technology that people have

07:21.960 --> 07:30.760
to work with and human robot interaction helps to study how people interact with that technology

07:30.760 --> 07:37.600
and how to design it in a way that they might trust it or even enjoy working with it instead

07:37.600 --> 07:44.280
of just saying, oh, this machine is threatening to me or it doesn't work, it made a mistake

07:44.280 --> 07:46.480
and I don't like it.

07:46.480 --> 07:51.560
Human robot interaction is oftentimes about designing the technology in a way that is

07:51.560 --> 07:56.640
more palatable to people and that people might even like to work with.

07:56.640 --> 08:04.960
And it's also about finding use cases for the technology that we might not even have.

08:04.960 --> 08:09.120
So it's not just about workplace integration, but there are also some applications in

08:09.120 --> 08:13.640
health and education that are really interesting where we're starting to see social robots being

08:13.640 --> 08:18.640
used as replacements for animal therapy, for example, in context where we can't use

08:18.640 --> 08:23.120
real animals or robots that are working with autistic children and engaging them in ways

08:23.120 --> 08:24.440
that we haven't seen before.

08:24.440 --> 08:30.720
So a lot of pretty cool things happening in human robot interaction.

08:30.720 --> 08:35.320
So I think it's a very useful field of study for this day and age.

08:35.320 --> 08:44.880
One of the aspects of human robot interaction that you study is, or at least the result of

08:44.880 --> 08:52.440
it is kind of this exploration of human empathy in those kind of scenarios and you've done

08:52.440 --> 08:58.800
a number of experiments to explore that, including I think involving these PLEO robots.

08:58.800 --> 09:01.920
Can you talk a little bit about some of the experiments that you've done?

09:01.920 --> 09:02.920
Yeah, sure.

09:02.920 --> 09:08.360
So the PLEO, I haven't actually done any scientific experiments with the PLEO robots because they're

09:08.360 --> 09:17.720
very expensive, and the experiments I've done usually involve destroying the robot.

09:17.720 --> 09:24.040
But what inspired the experimental work was a workshop that I did with five of these

09:24.040 --> 09:30.160
baby dinosaur PLEOs with my friend, Honest Gossult, where we took the baby dinosaur robots.

09:30.160 --> 09:32.080
We made five groups of people.

09:32.080 --> 09:33.080
These were all adults.

09:33.080 --> 09:34.520
They were at a conference.

09:34.520 --> 09:40.320
We had like five teams of six people each, and each team got a robot, and they named it,

09:40.320 --> 09:43.520
and they had to interact with it and play with it for like 45 minutes.

09:43.520 --> 09:49.520
And then we unveiled a hammer and a hatchet, and we told them to torture and kill the robots.

09:49.520 --> 09:52.640
And it was really interesting.

09:52.640 --> 09:53.640
Like we...

09:53.640 --> 09:57.720
It seems like you're getting uncomfortable just at the thought of torturing and killing these

09:57.720 --> 09:58.720
PLEOs.

09:58.720 --> 10:05.440
Well, it was really, really interesting to see that people were more uncomfortable than

10:05.440 --> 10:08.400
I expected them to be.

10:08.400 --> 10:11.840
We thought that some people would be like, yeah, sure, it's just a robot.

10:11.840 --> 10:16.040
I'll take this hatchet to it, and some people would be like, no, don't do it.

10:16.040 --> 10:21.200
And instead, in this particular group, everyone refused to even hit the robots.

10:21.200 --> 10:25.720
So we actually had to improvise in the workshop, and at some point we were like, okay, you

10:25.720 --> 10:31.800
can save your team's robot if you destroy another team's robot, and they tried to do that,

10:31.800 --> 10:33.280
and they couldn't do that either.

10:33.280 --> 10:37.200
And finally, we threatened to destroy all the robots, unless someone took a hatchet to

10:37.200 --> 10:38.200
one of them.

10:38.200 --> 10:48.600
And it was this very half-joking, half-serious discomfort that people felt when the robot

10:48.600 --> 10:52.120
kind of got destroyed by this hatchet.

10:52.120 --> 10:55.280
And there was actually a moment of silence in the room for the fallen robot.

10:55.280 --> 11:02.880
So it was just this very interesting, very dramatic, very not-scientific experiment day that

11:02.880 --> 11:03.880
we had.

11:03.880 --> 11:10.680
And that inspired some later research that I did at MIT with Palachnondi in Cynthia, Brazil.

11:10.680 --> 11:15.520
And for those experiments, we weren't using cute baby dinosaur robots in part because of

11:15.520 --> 11:18.560
the cost, like I mentioned, but in part also because we wanted to choose something that

11:18.560 --> 11:23.320
people don't immediately bond with and respond to.

11:23.320 --> 11:27.840
So we chose hex bugs, which are this toy.

11:27.840 --> 11:31.920
It's small, it moves around in a really lifelike way, like a bug.

11:31.920 --> 11:36.320
And we had people come into the lab and smash them with mallets, and we wanted to know two

11:36.320 --> 11:37.320
things.

11:37.320 --> 11:40.760
We wanted to know, would people hesitate more if we gave the hex bug a name and kind of

11:40.760 --> 11:41.760
a backstory?

11:41.760 --> 11:46.960
So if we said, this is Frank, and Frank's favorite color is red, and he likes to play.

11:46.960 --> 11:51.600
And the other thing we wanted to know was whether people's hesitation correlated in any way

11:51.600 --> 11:54.000
to their natural tendencies for empathy.

11:54.000 --> 11:59.760
So we did this psychological empathy test with them, and we found that people who scored

11:59.760 --> 12:05.960
low on the test for empathic concern, they would hesitate much less than the other people

12:05.960 --> 12:07.760
they would just hit Frank.

12:07.760 --> 12:11.680
And the people who scored very high on the empathic concern test would hesitate much more

12:11.680 --> 12:14.280
or even refuse to hit the hex bugs.

12:14.280 --> 12:19.720
So it was, you know, it was a little study, but it was kind of interesting because it

12:19.720 --> 12:24.720
indicates that, you know, we might even be able to measure people's empathy using robots,

12:24.720 --> 12:30.440
which is kind of like a weird turn on the VoicConf test from Blade Runner, and I know if you're

12:30.440 --> 12:31.920
familiar with that one.

12:31.920 --> 12:34.520
I don't remember the details of it.

12:34.520 --> 12:40.480
So in Blade Runner, you have robots that look just like humans, and so to tell whether

12:40.480 --> 12:45.880
someone is a robot or a human, they do this empathy test where they tell them these stories

12:45.880 --> 12:47.840
and see how they react to them.

12:47.840 --> 12:52.400
And so our version of that is, we can see how empathic you are as a human by telling

12:52.400 --> 12:55.240
you stories about a robot and seeing how you react to that.

12:55.240 --> 12:58.680
So it's kind of, it was, it's fun.

12:58.680 --> 13:07.680
Was it your presentation that showed a video of kids and a, like a mall security robot

13:07.680 --> 13:12.080
and some of the dynamics that occurred or did I see that separately?

13:12.080 --> 13:14.840
Or do you know the video that I'm referring to?

13:14.840 --> 13:15.840
I know this video.

13:15.840 --> 13:20.960
I did, I have not shown this video, but I have talked about the study.

13:20.960 --> 13:23.680
This is the one in Japan, right?

13:23.680 --> 13:30.000
I don't remember where it takes place, but the basic idea was they were, at least the

13:30.000 --> 13:36.080
thing that I remember is they were, you know, if they were multiple kids or no parents

13:36.080 --> 13:40.640
around, they were identifying the situations in which, you know, kids would come to abuse

13:40.640 --> 13:47.360
the security robot and, you know, things like multiple kids around and no parents were

13:47.360 --> 13:48.880
kind of key indicators.

13:48.880 --> 13:49.880
Yeah.

13:49.880 --> 13:54.600
And they ended up, it was so funny, they, because the paper is essentially about the solution

13:54.600 --> 13:59.880
that they found for preventing the security mall robot from getting beat up by the kids,

13:59.880 --> 14:05.120
which is they made it avoid people below a certain height and move towards taller people.

14:05.120 --> 14:10.520
They figure, okay, if there is an adult nearby, they'll intervene and stop the kids from

14:10.520 --> 14:16.480
like verbally and physically abusing the robot, which is kind of funny.

14:16.480 --> 14:22.680
It's really like, the video is funny, it's like terrifying and funny at the same time.

14:22.680 --> 14:27.040
And do you interpret, when you see that video, do you interpret it through the lens of kind

14:27.040 --> 14:31.000
of the empathy results of some of your experiments?

14:31.000 --> 14:38.560
Well, so my question when I look at that is, if you're a parent, do you intervene to stop

14:38.560 --> 14:44.760
your kid from beating up the robot for reasons other than just respecting property?

14:44.760 --> 14:51.280
Like, is there a reason to worry that your kid might, you know, learn that it's okay

14:51.280 --> 14:56.120
to treat something that responds in a life like way violently?

14:56.120 --> 15:01.680
And could that like translate to their behavior towards other children or animals or things

15:01.680 --> 15:02.880
that are alive?

15:02.880 --> 15:07.040
And so that's, we don't know the answer to that by any means.

15:07.040 --> 15:12.160
It's something that I think needs to be explored, but I also wonder about it in the context

15:12.160 --> 15:17.600
of adults even, you know, how muddled is it in our subconscious to treat something that's

15:17.600 --> 15:21.840
designed to respond in a really life like way violently?

15:21.840 --> 15:26.960
Is that a healthy outlet for violent behavior or is that, as my friend once said, training

15:26.960 --> 15:29.640
your cruelty muscles?

15:29.640 --> 15:30.640
We don't know.

15:30.640 --> 15:36.760
It's the same question as violence in video games, except that for video games, we seem

15:36.760 --> 15:43.720
to have landed on, you know, adults can compartmentalize when it's on a screen.

15:43.720 --> 15:46.600
Children were not so sure about, so we restrict it there.

15:46.600 --> 15:51.720
But robots bring this to a very new, very visceral level because of the physicality.

15:51.720 --> 15:53.200
We're very physical creatures.

15:53.200 --> 15:56.520
There's lots of research that shows that we respond differently to something in our

15:56.520 --> 15:58.400
space than to something on a screen.

15:58.400 --> 16:03.440
And so it seems like we might want to ask that question again.

16:03.440 --> 16:08.080
Well, also, with video games, the question comes up more often than not when the thing that

16:08.080 --> 16:14.920
we're being violent against is another human or a thing that's supposed to be a human.

16:14.920 --> 16:23.080
Whereas we don't often see humanoid robots kind of roaming around in real life.

16:23.080 --> 16:27.480
Very few of us have the opportunity to interact with things that are anywhere close to

16:27.480 --> 16:29.320
humanoid robots.

16:29.320 --> 16:33.680
That is true. It's funny how people always leap to humanoid robots.

16:33.680 --> 16:38.480
I think we have this tendency to constantly compare robots to humans, and I also admit

16:38.480 --> 16:40.920
that a lot of people are trying to build humanoid robots.

16:40.920 --> 16:43.760
That is definitely a fascination that is there.

16:43.760 --> 16:50.680
But when I think about life robots, I think about all sorts of different designs.

16:50.680 --> 16:55.080
I don't know if you've seen the baby seal robot that they use with dementia patients.

16:55.080 --> 16:56.080
No.

16:56.080 --> 17:01.120
It's super cute. It's been around for a long time, at least a decade.

17:01.120 --> 17:04.800
It's used as a therapeutic device in nursing homes.

17:04.800 --> 17:10.320
It's this baby harpsial that is furry and you pet it and it doesn't talk to you or do

17:10.320 --> 17:15.120
anything that might disappoint your expectations like a humanoid robot would because they're

17:15.120 --> 17:16.720
just not good yet.

17:16.720 --> 17:20.880
It just responds to your touch and makes these little sounds.

17:20.880 --> 17:22.920
It's very effective.

17:22.920 --> 17:27.000
I think that even though we're not in a place where we have robots that look like in

17:27.000 --> 17:30.720
Westworld or Blade Runner where we can't tell the difference, we are starting to see

17:30.720 --> 17:39.880
design that we certainly treat like a living thing, even though it's clearly not smart.

17:39.880 --> 17:48.160
And so part of your thesis perhaps is that our interactions with these things speak to

17:48.160 --> 17:54.560
empathy in the same way that our interactions with animals speak to some kind of fundamental

17:54.560 --> 17:57.080
empathy even though they're not human.

17:57.080 --> 17:58.080
Yeah.

17:58.080 --> 18:03.440
I actually think animals are really, really great analogy in this context.

18:03.440 --> 18:09.320
Obviously animals are alive and they experience things and they feel pain which robots absolutely

18:09.320 --> 18:10.320
do not.

18:10.320 --> 18:15.120
But I think one of the commonalities here is that throughout history, we've treated

18:15.120 --> 18:20.560
most animals like tools and products and not really cared about their inner worlds.

18:20.560 --> 18:25.640
And then there are just some animals that we've kind of bonded with and made our companions

18:25.640 --> 18:27.160
and treated with more kindness.

18:27.160 --> 18:31.960
And if you look at the history of the animal rights movement, this doesn't seem to really

18:31.960 --> 18:36.560
have anything to do with inherent biological criteria.

18:36.560 --> 18:40.160
It has more to do with what we relate to.

18:40.160 --> 18:45.760
People didn't care about whales until the moment that someone recorded them singing.

18:45.760 --> 18:49.360
And suddenly you have to save the whales movement.

18:49.360 --> 18:54.240
And this huge movement started back in the 70s because suddenly these were creatures

18:54.240 --> 18:56.200
that we could relate to.

18:56.200 --> 19:02.600
And when I look at how, yeah, I actually just met the guy who discovered whale song.

19:02.600 --> 19:07.320
It was very, I was very, very much fan-girling.

19:07.320 --> 19:10.800
It was amazing.

19:10.800 --> 19:11.800
That's awesome.

19:11.800 --> 19:16.040
But when I look at how we treat robots, I can see this going in a very similar direction

19:16.040 --> 19:20.080
where we treat most of them like tools and products and some of them.

19:20.080 --> 19:26.080
We really want to treat our pets and I think we're going to start to do that.

19:26.080 --> 19:36.960
You mentioned earlier trust in the human robot trust relationship and it reminded me of

19:36.960 --> 19:46.200
my conversation with Ayanna Howard back in February of last year, number 110 podcast.

19:46.200 --> 19:51.640
And we, one of the things that she talked about with some of her research in that area

19:51.640 --> 20:00.520
and how humans would, you know, essentially blindly follow, you know, these robots like

20:00.520 --> 20:06.680
for example, robots that are supposed to guide them out of a burning building, you know,

20:06.680 --> 20:11.280
in spite of the fact that the robots are doing, you know, they're obviously broken in some

20:11.280 --> 20:12.280
way.

20:12.280 --> 20:15.920
Like they're banging into a wall or something like that and the humans waiting around

20:15.920 --> 20:18.680
for them to, you know, give them guidance.

20:18.680 --> 20:24.880
Have you, is that research you're familiar with and how does that relate to any of your

20:24.880 --> 20:25.880
work?

20:25.880 --> 20:33.160
Yeah, so yeah, I love Ayanna's work and it's, it's very interesting.

20:33.160 --> 20:38.600
There is something that we call automation bias that is this trust that people place

20:38.600 --> 20:47.640
in robots or AI systems because in certain cases we place a lot of trust in these systems

20:47.640 --> 20:52.120
because we assume that they have the right answers and that they're not biased and that

20:52.120 --> 20:57.360
they're not going to, you know, make a mistake, the way that we trust a calculator to add

20:57.360 --> 20:58.360
numbers.

20:58.360 --> 21:05.080
So Madeline Ellish is someone who's done work on taking some of that research and looking

21:05.080 --> 21:11.440
at how it applies societally in the world and also on a policy level because she's, for

21:11.440 --> 21:13.720
example, what's that name again?

21:13.720 --> 21:14.720
Madeline Ellish.

21:14.720 --> 21:16.320
Madeline Ellish, okay.

21:16.320 --> 21:20.040
She's at data and society in New York, I believe.

21:20.040 --> 21:24.000
She loved her work, you should totally interview her.

21:24.000 --> 21:30.040
She, so she's looked at, for example, the fact that when there are a machine and a human

21:30.040 --> 21:34.960
working together, so you have a human in the loop and something goes wrong that was totally

21:34.960 --> 21:38.840
the machine's fault, the human usually gets blamed for it.

21:38.840 --> 21:46.320
So we have this over reliance on, on machines that is in some cases really unwarranted and

21:46.320 --> 21:50.760
it's not really clear what to do about that because technology keeps getting more and

21:50.760 --> 21:52.160
more complex.

21:52.160 --> 21:58.840
And so sometimes you can't have that education or that transparency that really lets people

21:58.840 --> 22:02.000
understand the limitations of the technology.

22:02.000 --> 22:07.600
But I also think that it's, some of it is because we currently, maybe thanks to science

22:07.600 --> 22:12.400
fiction and pop culture, kind of overestimate what the technology can do.

22:12.400 --> 22:18.040
I see that a lot that people kind of think that we are much further along in robotics

22:18.040 --> 22:22.680
and artificial intelligence development than we actually are.

22:22.680 --> 22:28.560
And I, I think that's also a problem of science communications and the media in general.

22:28.560 --> 22:33.880
So it's definitely an issue that needs to be addressed.

22:33.880 --> 22:42.360
Yeah, and this comes up in some of my conversations with folks on the business or enterprise side,

22:42.360 --> 22:51.080
just in thinking about how to address kind of statistical literacy and numeracy within

22:51.080 --> 22:58.040
organizations and trying to raise the level of understanding of, you know, as we're adopting

22:58.040 --> 23:01.920
more and more of these ML and AI systems within organizations, you know, what it means

23:01.920 --> 23:07.200
that these systems are, you know, based on statistical models and pattern matching in

23:07.200 --> 23:13.640
our probabilistic, and many organizations are spending a lot of energy trying to come

23:13.640 --> 23:20.040
up with new and innovative ways to educate folks that don't kind of think about systems

23:20.040 --> 23:21.560
in this way.

23:21.560 --> 23:26.480
It kind of sounds like we're going to need to do this on a broader societal scale as these

23:26.480 --> 23:30.680
systems get more and more integrated into the way things work.

23:30.680 --> 23:32.600
Oh, yeah, for sure.

23:32.600 --> 23:37.400
I mean, there's been, so there's this group called the Personal Robots Group at the

23:37.400 --> 23:44.400
Media Lab that I do a lot of work with and one of their students, Blakely Payne, is working

23:44.400 --> 23:48.600
on an AI ethics curriculum for middle schoolers.

23:48.600 --> 23:53.480
And it's really great like she's going to middle schools and she's doing these exercises

23:53.480 --> 23:58.320
with the kids where, for example, like they look at, you know, a YouTube recommendation

23:58.320 --> 24:03.360
algorithm and then they have to, you know, put themselves in the place of the different stakeholders

24:03.360 --> 24:09.280
in the system and not only understand how it works, but who it works for.

24:09.280 --> 24:13.320
And so they really learn to kind of question what's going on and think critically about

24:13.320 --> 24:19.760
it in a really good way, but they, you know, they're getting overwhelmed with requests

24:19.760 --> 24:27.600
from people like all over the place, you know, companies, you know, everyone is like,

24:27.600 --> 24:32.840
how do we adapt this to use in our organization because we need this too?

24:32.840 --> 24:37.720
Like this isn't just for middle schoolers, like everyone needs this education right now.

24:37.720 --> 24:41.400
And so they're trying to scale as fast as they can, but it is, you know, there's a massive

24:41.400 --> 24:45.960
amount of interest in this and I think it's, it's very important and people are starting

24:45.960 --> 24:48.160
to realize that.

24:48.160 --> 24:49.160
Interesting.

24:49.160 --> 24:56.600
So some of your work, maybe going back to the empathy conversation relates to the different

24:56.600 --> 25:03.640
ways that humans anthropomorphize robots and the different implications of that.

25:03.640 --> 25:05.800
And there's some policy implications that you've looked at.

25:05.800 --> 25:08.040
Can you talk a little bit about that sphere?

25:08.040 --> 25:09.040
Yeah.

25:09.040 --> 25:14.200
So my main interest is the anthropomorphism of robots.

25:14.200 --> 25:19.000
So people, you know, projecting life like qualities onto the robots.

25:19.000 --> 25:26.760
And there are some pundits in technology ethics who claim that this is a bad thing and that

25:26.760 --> 25:28.800
we need to discourage it.

25:28.800 --> 25:37.440
And I, I, I see some concerns that we might want to have about the fact that people treat

25:37.440 --> 25:38.440
robots like their lives.

25:38.440 --> 25:46.040
So for example, if, if, you know, the persuasive design folks get their hands on robots and,

25:46.040 --> 25:52.320
you know, start to try to manipulate people for, you know, corporate interest and try to

25:52.320 --> 25:56.640
get them to, you know, buy products and services or reveal more personal data than they would

25:56.640 --> 26:01.280
ever willingly enter into a database through, you know, interacting with, you know, a social

26:01.280 --> 26:02.800
robot in the home, for example.

26:02.800 --> 26:07.720
Then I think that's maybe a consumer protection issue that we might want to think about a little

26:07.720 --> 26:08.720
bit.

26:08.720 --> 26:12.960
You know, there's a lot of privacy concerns, manipulation concerns, but I don't think

26:12.960 --> 26:18.840
that it's inherently a bad thing because there are so many great use cases for this as well.

26:18.840 --> 26:24.440
And so those are, I mean, those are some questions that are popping up, but I'm also interested

26:24.440 --> 26:29.640
in, you know, this question that we touched on earlier of, you know, is it a bad thing

26:29.640 --> 26:34.400
for people to treat life like objects in a violent way?

26:34.400 --> 26:40.920
We don't have the answer to that, but it's already coming up in some policy questions.

26:40.920 --> 26:47.960
So for example, there's some discussion of whether sex robots are something that should

26:47.960 --> 26:52.800
be banned because they encourage certain behaviors in people or whether there's something

26:52.800 --> 26:58.920
that should be encouraged because they, you know, are an outlet for behaviors that we,

26:58.920 --> 27:02.240
you know, don't want to be levied against real people.

27:02.240 --> 27:08.400
So it's, there's, there are the, you know, policy questions being thrown around without

27:08.400 --> 27:12.960
any actual, you know, evidence behind them and a lot of moral panic, especially in the

27:12.960 --> 27:19.520
area of sex technology that, you know, are already becoming very relevant.

27:19.520 --> 27:26.520
And I, I do think that if we found evidence that, you know, it's somehow desensitizing

27:26.520 --> 27:33.160
to people to be violent towards robots that we might need to have some legislation that

27:33.160 --> 27:39.560
says, you know, you can't torture a certain kind of robot in the same way that, you know,

27:39.560 --> 27:43.480
we don't allow torture of animals, even though we still allow people to, you know, kill

27:43.480 --> 27:44.480
animals and do it.

27:44.480 --> 27:49.280
But you can't, you know, set a kitten on fire and throw it, you know, in a bag.

27:49.280 --> 27:53.560
And, you know, there are certain things that would, that just we're not comfortable with.

27:53.560 --> 27:54.560
And this might be a problem.

27:54.560 --> 27:58.000
But there are totally different reasons behind that, right?

27:58.000 --> 28:04.440
But the kitten, it's because we, the kitten is a creature that, you know, experiences

28:04.440 --> 28:09.480
pain and all of that kind of thing with the robot, the rationale would need to be much

28:09.480 --> 28:14.720
more about us than, you know, the target of our violence.

28:14.720 --> 28:15.720
Absolutely.

28:15.720 --> 28:18.560
And that's why I think it should be purely evidence-based.

28:18.560 --> 28:24.080
Like, if we have evidence that is desensitizing to people, then, you know, maybe we need

28:24.080 --> 28:25.800
a solution for it.

28:25.800 --> 28:31.000
But that said, I am also not convinced that when it comes to animals that we truly, truly

28:31.000 --> 28:37.480
care about their pain, because there are certain animals that we're perfectly willing to torture

28:37.480 --> 28:38.680
for our own benefit.

28:38.680 --> 28:43.240
Like it's obvious that, you know, animals like chickens feel pain, but, you know, we keep

28:43.240 --> 28:46.920
them in these, you know, cooped up spaces.

28:46.920 --> 28:50.680
We chop off their beak so that they can't peck each other's eyes out because they're

28:50.680 --> 28:52.320
going crazy in these cooped up spaces.

28:52.320 --> 28:57.400
We torture plenty of animals for our own benefit, even though we know that, you know, there's

28:57.400 --> 29:02.480
no biological difference between a horse and a cow that would justify us eating one of

29:02.480 --> 29:04.000
them in America and not the other.

29:04.000 --> 29:08.400
And yet, people are appalled when we suggest eating horse meat here.

29:08.400 --> 29:13.640
I think there are a lot of reasons that we protect animals that are actually about us.

29:13.640 --> 29:18.040
And as much as we don't like to hear that, I think that the way that we're interacting

29:18.040 --> 29:21.320
with robots is kind of making that very apparent.

29:21.320 --> 29:22.320
Interesting.

29:22.320 --> 29:31.720
And this part of the conversation reminds me of this device that I saw at CES earlier

29:31.720 --> 29:39.000
this year, Bot Boxer, it's like this robotic boxing training thing.

29:39.000 --> 29:46.800
And, you know, the device as presented was basically a punching bag that kind of, you

29:46.800 --> 29:50.720
know, had some behaviors in it to evade you and that kind of thing.

29:50.720 --> 29:56.080
But it makes me wonder, like, if the thing was, you know, one of these humanoid punching

29:56.080 --> 30:04.560
bags and made painful sounds, like what the implications are, it needs to be this discussion.

30:04.560 --> 30:08.880
It creates all kinds of interesting thought experiments.

30:08.880 --> 30:09.880
It does.

30:09.880 --> 30:10.960
Oh, yeah, that is interesting.

30:10.960 --> 30:17.360
But also, you know, if it's not, if we're not subconsciously treating that boxing punching

30:17.360 --> 30:20.880
bag like a human opponent, then it's not a good training device.

30:20.880 --> 30:29.520
So, you know, it, to some extent, that relies on this kind of subconscious treating it like

30:29.520 --> 30:30.920
a person.

30:30.920 --> 30:34.640
And, you know, I think that, you know, there are plenty of situations where, like, I'm,

30:34.640 --> 30:39.560
I would never argue that, you know, because boxing exists as a sport that people who box,

30:39.560 --> 30:41.440
you know, might be more likely to hit other people.

30:41.440 --> 30:46.760
I think we're very good at compartmentalizing in that type of way.

30:46.760 --> 30:51.960
It is, you know, with children, it is, there are some questions.

30:51.960 --> 30:53.600
You know, I have a toddler myself.

30:53.600 --> 30:59.520
And so, I'm starting to have to deal with things like, if he pulls the robot cat's tail,

30:59.520 --> 31:01.600
we have a robot cat at home.

31:01.600 --> 31:05.480
Do I stop him from doing that because I don't want him to learn that it's okay to pull

31:05.480 --> 31:07.120
a real cat's tail?

31:07.120 --> 31:12.160
And there are a lot of stories, even with, like, the very primitive kind of voice assistance

31:12.160 --> 31:17.480
we have today, you know, there are stories of older kids where parents are like, you

31:17.480 --> 31:23.320
know, Amazon's Alexa is turning my child into an asshole because she doesn't require

31:23.320 --> 31:25.960
you to say, please, and thank you, you can just bark commands at her.

31:25.960 --> 31:29.320
And so, my kid now thinks that's okay to do to anyone.

31:29.320 --> 31:33.320
And there were so many complaints to Amazon that they actually released a feature that

31:33.320 --> 31:37.720
you can turn on where Alexa will require, please, and thank you.

31:37.720 --> 31:45.920
So, we don't have scientific evidence that it has an effect on kids' behavior that translates

31:45.920 --> 31:52.840
to, you know, how they interact with the world, but there's some anecdotal concern at

31:52.840 --> 31:57.960
least that we might want to be a little bit careful because interacting with these machines

31:57.960 --> 32:04.800
is subconsciously like interacting with, you know, a social agent, like another person.

32:04.800 --> 32:11.680
Yeah, that reminds me a little bit of, you know, don't let your kids play with fake cell

32:11.680 --> 32:18.480
phones or your animals for that matter because then they'll just destroy your real ones.

32:18.480 --> 32:24.360
But, you know, it seems like that, at least in that case, like there's some age in which

32:24.360 --> 32:29.200
you age range, in which you have to worry about that, but at some point there's a level

32:29.200 --> 32:34.480
of maturity in which they realize the difference between this plastic thing that kind of looks

32:34.480 --> 32:38.960
like a phone and the actual real phone that you get upset about if they throw around.

32:38.960 --> 32:39.960
Oh, sure.

32:39.960 --> 32:40.960
Kids are smart.

32:40.960 --> 32:41.960
Right, right.

32:41.960 --> 32:46.120
And we have a lot of kids, like a lot of the roboticists who work in the meaty lab have kids,

32:46.120 --> 32:51.920
and their kids come into the lab and they totally know how the robots work and that the robots

32:51.920 --> 32:57.960
don't feel anything and yada yada, and yet they're still treating them like friends.

32:57.960 --> 33:06.440
And even the roboticists will do it, like it's really funny, like, you know, um, our

33:06.440 --> 33:12.800
setting of the hex bugs, our participants were mostly MIT undergrad, so people who have

33:12.800 --> 33:16.000
a lot of tech literacy, and we still found an effect.

33:16.000 --> 33:21.760
And I've seen, you know, I've seen effects in myself and others who know perfectly well

33:21.760 --> 33:25.440
how the robots work and will still kind of treat them.

33:25.440 --> 33:30.480
It's very hard to not go with that instinct to treat the robot like an agent because

33:30.480 --> 33:37.880
we're biologically hardwired to perceive the type of movement and interaction that these

33:37.880 --> 33:43.880
robots have as a social interaction, and so we like immediately slip into treating the

33:43.880 --> 33:51.280
robot like, you know, an agent instead of an object, and even adults who are completely

33:51.280 --> 33:54.120
tech literate aren't immune from that effect.

33:54.120 --> 34:01.160
So, you know, kids are smart, like, they know, but, you know, my workshop with the adults

34:01.160 --> 34:05.920
who refuse to harm the robotic baby dinosaur shows that even if you know, it might not

34:05.920 --> 34:06.920
matter.

34:06.920 --> 34:07.920
Right, right, right.

34:07.920 --> 34:14.840
I'm curious what's the example of the most, you know, either the most, you know, human

34:14.840 --> 34:20.320
life or kind of empathy producing or just interesting in general, kind of robots that

34:20.320 --> 34:22.440
you've, you know, seen out there.

34:22.440 --> 34:24.680
I'm imagining it's not Sophia.

34:24.680 --> 34:25.680
Yeah.

34:25.680 --> 34:31.680
I was just going to say, yeah, I think the humanoid ones are actually not as empathy and

34:31.680 --> 34:39.440
gendering because there's this thing in robotic design that some people call the uncanny valley.

34:39.440 --> 34:46.520
I call it expectation management where if you're comparing, like, say you take a humanoid

34:46.520 --> 34:51.720
robot or my shitty cat robot that I have at home, like, these are things that are designed

34:51.720 --> 34:55.480
to try and look as closely to a human or as closely to a cat as possible.

34:55.480 --> 35:00.480
So when they behave, you're expecting them to behave exactly like the thing that they're

35:00.480 --> 35:06.160
trying to mimic and as soon as they don't do that, they make some, like, different movement

35:06.160 --> 35:12.000
or, you know, utterance, it breaks the illusion and you're no longer suspending your disbelief

35:12.000 --> 35:14.080
and it kind of disappoints your expectations.

35:14.080 --> 35:21.280
And so I think that the most successful social robots actually have very different shapes

35:21.280 --> 35:22.280
and forms.

35:22.280 --> 35:25.640
I think that Baby Dinosaur works because no one's ever actually interacted with a Baby

35:25.640 --> 35:26.640
Dinosaur before.

35:26.640 --> 35:31.680
So it's kind of like, you know, I'll believe that a Baby Camerasaurist moves in this way.

35:31.680 --> 35:35.360
You know, I'll totally imagine that.

35:35.360 --> 35:40.440
But even like, have you seen Gibo at all?

35:40.440 --> 35:41.840
Which one is Gibo?

35:41.840 --> 35:42.840
To Gibo.

35:42.840 --> 35:49.440
Gibo, unfortunately, the company no longer exists, but they had this very successful kickstart

35:49.440 --> 35:56.760
or Indiegogo where it's, it's just, it's one of those home assistant robots like Amazon

35:56.760 --> 36:00.520
Alexa, except it had, it looks a little bit like a Pixar lamp.

36:00.520 --> 36:07.720
It has my body and the face and has this animated like circle that just, it's very, very simple

36:07.720 --> 36:12.560
and very, very compelling, like the little movements that it makes, it just makes you feel

36:12.560 --> 36:21.240
like you're interacting with, you know, something that has a character instead of just an object.

36:21.240 --> 36:28.360
And those I think are, those are the most successful robots in kind of engendering people's empathy.

36:28.360 --> 36:36.840
Have you explored anything related to these telepresence robots?

36:36.840 --> 36:44.440
Like the, the kind of iPad on a Segway, things that people can use to be remotely present

36:44.440 --> 36:46.320
in meetings and that kind of thing?

36:46.320 --> 36:52.200
Yeah, so I haven't done any work on that, but I have heard some interesting stories about

36:52.200 --> 36:59.640
how people, you know, people will Skype in or whatever you do to the telepresence robot.

36:59.640 --> 37:05.080
And if someone in that physical space comes up and like picks up the robot and moves it

37:05.080 --> 37:10.080
because of the way, people have reported that they feel like violated, like physically

37:10.080 --> 37:13.080
violated, even though they're only Skyping into it.

37:13.080 --> 37:14.080
Wow.

37:14.080 --> 37:16.080
Or if somebody's standing too close to it or whatever.

37:16.080 --> 37:23.400
So I think there's a lot to unpack there and explore, especially as these get more robot

37:23.400 --> 37:27.000
likes so far they haven't, they've been kind of like just remote controlled, you know,

37:27.000 --> 37:32.640
iPads on wheels, but I think as they develop more autonomous capabilities, it'll be interesting

37:32.640 --> 37:37.960
to see, you know, the intersection of people's feelings of autonomy while they're, you

37:37.960 --> 37:42.560
know, quote unquote, inside the thing and how, you know, the robot is interacting with

37:42.560 --> 37:43.560
the world.

37:43.560 --> 37:47.120
So that's definitely a very interesting area, I think.

37:47.120 --> 37:52.520
Yeah, as you describe that, I can almost, that it becomes almost tangible for me, that

37:52.520 --> 37:59.160
feeling of kind of anxiety of being, you know, someone just kind of, you know, quote unquote

37:59.160 --> 38:07.480
man handling my virtual self also makes me think about like, one of my thing, I guess

38:07.480 --> 38:13.520
the zoom is kind of the example that I'm thinking of if you like, we've got one zoom

38:13.520 --> 38:18.080
account that we share among a few people and like it'll kick people out of a room if

38:18.080 --> 38:21.080
somebody comes in and like takes over it.

38:21.080 --> 38:28.520
And I'm thinking about like the, you know, multiple humans, like struggling to take

38:28.520 --> 38:33.520
control over this virtual robot and like, you know, all kinds of issues related to like

38:33.520 --> 38:36.280
the hierarchy of being able to use this thing.

38:36.280 --> 38:40.680
Yeah, I mean, I'm telling you, the intersection of psychology and how we're using technology

38:40.680 --> 38:43.840
is really, I think it's so fascinating.

38:43.840 --> 38:45.000
Yeah, yeah, it is.

38:45.000 --> 38:52.720
So what are the things that are kind of on top of your list of, you know, exciting things

38:52.720 --> 38:55.840
that are happening, really interesting work, you know, besides the stuff that you're

38:55.840 --> 38:59.360
doing that folks should check out?

38:59.360 --> 39:03.240
I'm personally really excited, this is not my own work.

39:03.240 --> 39:08.320
There have been some advances in the research with autistic children.

39:08.320 --> 39:14.200
So a long time ago, researchers and social robots and social robotics found out that autistic

39:14.200 --> 39:20.040
kids will respond really well to social robots and they'll engage with them more than they

39:20.040 --> 39:25.560
will with, you know, an adult, a teacher, a caregiver.

39:25.560 --> 39:30.720
But not only that, the kids, when you bring them into a room and have them interact with

39:30.720 --> 39:34.960
the robot, they'll also interact more with the person who's in the room with them.

39:34.960 --> 39:39.840
So suddenly, they'll be making more eye contact, answering more questions than they were before.

39:39.840 --> 39:44.980
So it's a really great kind of facilitating device and they're not quite sure how or

39:44.980 --> 39:45.980
why this works.

39:45.980 --> 39:52.760
But what's interesting going on right now is that just last year, they published the first

39:52.760 --> 39:57.840
long-term study where they actually put robots in the homes of children who are on the

39:57.840 --> 40:03.000
spectrum and did a longer-term study where the kids were interacting with the robot for

40:03.000 --> 40:06.240
half an hour every day together with their caregiver.

40:06.240 --> 40:13.560
And then after about a month, they saw like a dramatic increase in the social skills

40:13.560 --> 40:17.200
that they were looking to kind of encourage in the kids.

40:17.200 --> 40:22.480
And so like really thousands and thousands of dollars worth of therapy just from, you

40:22.480 --> 40:24.400
know, interacting with this robot.

40:24.400 --> 40:29.560
So I think there's so much promise in that area and I'm really excited to see what other

40:29.560 --> 40:31.000
work comes out of there.

40:31.000 --> 40:33.040
That's pretty fascinating.

40:33.040 --> 40:38.120
You said they're not sure kind of how or why any hints or indications.

40:38.120 --> 40:45.280
Well, I think so the leading researcher in that space, Brian Scuzzelotti, who's at Yale,

40:45.280 --> 40:51.520
he thinks that it's because the kids view the robot as a social actor.

40:51.520 --> 40:56.360
So something that they engage with socially, but it doesn't come with the baggage of another

40:56.360 --> 41:01.160
child or another adult, you know, because they also understand that it's a robot.

41:01.160 --> 41:07.120
So that's their best guess for why this is having a pretty strong effect on these kids.

41:07.120 --> 41:10.960
But it's pretty cool because it's something it's a tool that we haven't, you know, had previously

41:10.960 --> 41:12.360
and now we do.

41:12.360 --> 41:13.360
Cool.

41:13.360 --> 41:17.440
Anything else that we should check out?

41:17.440 --> 41:18.440
I don't think so.

41:18.440 --> 41:20.640
You should check out Blakely's AI ethics curriculum.

41:20.640 --> 41:21.640
Okay.

41:21.640 --> 41:22.640
Yeah.

41:22.640 --> 41:23.640
Yeah.

41:23.640 --> 41:24.640
We'll definitely link to that.

41:24.640 --> 41:25.640
Very cool work.

41:25.640 --> 41:34.680
And then you mentioned earlier, the possible opportunities to kind of abuse the human robot

41:34.680 --> 41:39.880
interaction and persuasive design, like have you seen anything there that kind of, you

41:39.880 --> 41:46.600
know, put your, put your radar up or that folks should be aware of as kind of a negative

41:46.600 --> 41:52.400
example of how this kind of technology or the interactions are being taken advantage

41:52.400 --> 41:53.400
of?

41:53.400 --> 41:56.560
So I haven't seen anything.

41:56.560 --> 42:01.440
I, you know, I don't think that social robots right now are, you know, pervasive enough

42:01.440 --> 42:05.800
for this to be a problem, but I, I see it on the horizon because, you know, if you think

42:05.800 --> 42:10.680
about it, you know, a sexual robot that has in-app purchases, you know, is that okay or

42:10.680 --> 42:16.040
is that too manipulative or I can think of a million examples or even just so Sony came

42:16.040 --> 42:21.360
out with a new robot dog, the Ibo, which I really want when I'm probably going to get one.

42:21.360 --> 42:26.600
But they're not only are they very expensive, but they require a monthly subscription to

42:26.600 --> 42:27.880
the cloud services.

42:27.880 --> 42:28.880
Oh, wow.

42:28.880 --> 42:32.200
I don't know what functionality you lose if you stop paying for that, but it's kind of

42:32.200 --> 42:37.560
interesting because we know from the older Ibo is that people really treated these robots

42:37.560 --> 42:39.920
like a pet and like they're a part of the family.

42:39.920 --> 42:44.120
And so that does seem like a little bit of maybe manipulating an emotional connection

42:44.120 --> 42:48.360
if you're going to charge a monthly fee to keep this robot going, you know what I mean?

42:48.360 --> 42:55.000
Yeah, you can imagine the, the, uh, uh, loss prevention emails, you know, don't let your

42:55.000 --> 42:59.920
Ibo die kind of, oh my gosh, yes, they should hire you as a consultant.

42:59.920 --> 43:06.080
Oh, yeah, um, yeah, that doesn't, that seems to be on the other side of some line.

43:06.080 --> 43:12.920
Uh, well, Kate, thanks so much for taking the time to chat really, really fascinating conversation

43:12.920 --> 43:18.280
and, uh, I'm looking forward to following along with your work.

43:18.280 --> 43:19.800
Thanks so much.

43:19.800 --> 43:26.320
All right, everyone, that's our show for today.

43:26.320 --> 43:32.320
For more information on today's show, visit twomolai.com slash shows.

43:32.320 --> 43:38.160
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

43:38.160 --> 43:39.640
Conference.

43:39.640 --> 43:52.600
As always, thanks so much for listening and catch you next time.

