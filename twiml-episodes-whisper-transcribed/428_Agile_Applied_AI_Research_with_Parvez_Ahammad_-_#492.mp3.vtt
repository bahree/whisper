WEBVTT

00:00.000 --> 00:21.000
All right, everyone. I'm here with Parvez Ahmed. Parvez is head of data science applied research at LinkedIn. Parvez, welcome to the Twoma AI podcast.

00:21.000 --> 00:46.000
Hi, thanks for having me. It's very nice to be here. I'm really looking forward to digging into our conversation. We had a really interesting pre-call. I love the way you described your kind of organizing principle for the group as well as the analogy that you make for how you decide what you work on and we're going to dig into those areas.

00:46.000 --> 00:52.000
But before we do, I'd love to have you share a little bit about your background and how you came to work in the field.

00:52.000 --> 01:03.000
Sure. It's probably a little bit of a non-linear trajectory. My PhD is from Berkeley. I specialize in computer vision, machine learning.

01:03.000 --> 01:17.000
So my major is electrical engineering computer sciences. After that, I actually wanted to spend some time working on problems more in the biology site. So my postdoc was with Gene Myers working on hythropod, you know, bioinformatics problems.

01:17.000 --> 01:26.000
I worked in a place called Horde Huesment Clean Institute, Genelia Farm Research Center. Now it's called Genelia Research Center.

01:26.000 --> 01:39.000
After some time there, I had a very small lab where we worked on computational models and tried to intersect them with some experimentation models set of experimentation that we were doing.

01:39.000 --> 01:49.000
In 2014, mainly because of some personal constraints, family related aspects, we decided to move to industry.

01:49.000 --> 02:01.000
Since then, I chose to mostly focus on building products end to end. So I've been in both tech and biotech sites of business, but mostly in startup site.

02:01.000 --> 02:19.000
I had a chance to build at least three products end to end. Over time, also transition more on to the management side have been building teams. I came to LinkedIn in 2019. And I took over this team that was actually.

02:19.000 --> 02:32.000
Yeah, the idea is the head of data science at LinkedIn about how to set up a horizontal team that focuses on, you know, more applied research problems. So I've been here about two years.

02:32.000 --> 02:42.000
And your team kind of sits at the intersection between research and products. How do you think about where the team sets.

02:42.000 --> 02:51.000
Another question. So, I mean, it might help you to have a broader sense of how the data science is organized at LinkedIn.

02:51.000 --> 03:04.000
There is a data org, which is a key pillar in engineering organization at LinkedIn and data art has three, you know, three or four key pillar organizations under it.

03:04.000 --> 03:15.000
One of them in data science classically at LinkedIn, data science operated in what we call a vertical fashion where, you know, there is a line of business.

03:15.000 --> 03:26.000
Let's say LinkedIn marketing solutions or LinkedIn talent solutions are even like flagship, which is essentially the central blue app for LinkedIn.

03:26.000 --> 03:35.000
There is a team that's very closely associated with this work, a particular line of business and focusing on problems that surface there.

03:35.000 --> 03:43.000
I try to solve those problems in a very iterative action work very closely with the PM associated with that line of business.

03:43.000 --> 03:50.000
I think one of the things that is relatively new couple of years ago.

03:50.000 --> 03:57.000
We started these horizontal teams that at least couple of them, one of them is called data science applied research, which is my team.

03:57.000 --> 04:02.000
And there is a sister team that we have called data science productivity and experimentation.

04:02.000 --> 04:08.000
So, this team, my sister team is more tooling focus. So, it's a lot of tools.

04:08.000 --> 04:15.000
Our focus is more on methodology, algorithms, platformization.

04:15.000 --> 04:29.000
So, there was a bit of an open space in terms of how we set up this team, what are the right things to focus on, how do we choose where we spend our time sort of idea.

04:29.000 --> 04:39.000
What we did is actually to have this conversation democratically with everybody in the team, we were like 10-ish people when I started.

04:39.000 --> 04:42.000
We are close to like 20 plus people today.

04:42.000 --> 04:49.000
So, we had a series of conversations in team meetings about, you know, what are the considerations we need to take into account.

04:49.000 --> 04:58.000
And in summary, what I see it as as interfacing between really having an impact.

04:58.000 --> 05:01.000
So, applied is a very important part of how we position ourselves.

05:01.000 --> 05:05.000
We actually do care about solving problems. So, we are very problem oriented.

05:05.000 --> 05:12.000
But we want to do so by going deep into hard technical problems and really solving them.

05:12.000 --> 05:17.000
And there are two investment pillars that I mentally use.

05:17.000 --> 05:24.000
One is how do we pick problems that essentially if we solve them very well, help multiple lines of businesses.

05:24.000 --> 05:34.000
Take for example, something like doing a really good job on experimentation helps multiple lines of businesses evaluate how their products are working, help them, you know, iteratively ship the products better.

05:34.000 --> 05:48.000
Something like forecasting, if you do a really good job, it also allows multiple businesses to have an ability to set their goals and actually measure how things are going and recognize when things are not going well.

05:48.000 --> 05:56.000
So, the utility across multiple lines of business is a key pillar for how we think about it.

05:56.000 --> 06:02.000
And the second important pillar is what is the strategic scope or an impact.

06:02.000 --> 06:08.000
So, on this pillar, it doesn't need to be actually a native part of the products today.

06:08.000 --> 06:13.000
But something that we believe is going to be really important for us to invest in.

06:13.000 --> 06:20.000
Some of this insight comes from executive leadership who is thinking ahead about where the company is going in the next two years, five years.

06:20.000 --> 06:26.000
Some of it comes from us thinking really deeply about where, you know, LinkedIn is moving.

06:26.000 --> 06:29.000
What does LinkedIn in three to five years look like?

06:29.000 --> 06:35.000
Examples of this kind would be how do we create products that actually are privacy preserving?

06:35.000 --> 06:40.000
What kinds of technologies do we need to invent or operationalize?

06:40.000 --> 06:50.000
Other aspects could be that, you know, LinkedIn has done a fantastic job of making experimentation a native part of developing products.

06:50.000 --> 06:55.000
But most of the experimentation, you know, traditionally sets up a certain set of metrics.

06:55.000 --> 07:01.000
We watch these metrics and we decide based on these metrics, but what about other things that are moving?

07:01.000 --> 07:04.000
Are there unintended consequences that are happening?

07:04.000 --> 07:05.000
How do we think about them?

07:05.000 --> 07:14.000
So, this notion of, you know, trying to understand unintended consequences of the way we develop products was something that we wanted to invest in.

07:14.000 --> 07:18.000
And that's a, that's also became a focus area in our team.

07:18.000 --> 07:31.000
So, to summarize, I think we pick problems that actually either have utility across multiple lines of businesses and going deep into it helps, you know, across the world.

07:31.000 --> 07:41.000
Or we pick problems that actually have really strong strategic utility where knowing whether to invest further or not has value at a very strategic level.

07:41.000 --> 07:44.000
So we essentially go look into that.

07:44.000 --> 07:54.000
Right now, so each of these have two focus areas or tracks that actually operationalize where we put resources into.

07:54.000 --> 07:57.000
That's where we are.

07:57.000 --> 08:04.000
I think in terms of focus areas, that's the thinking, but there was also a couple of important elements.

08:04.000 --> 08:12.000
We explicitly wanted to be problem oriented, so the applied part of our name is actually a significant modifier.

08:12.000 --> 08:18.000
We do care about solving problems. We want to ground ourselves in really solving hard, large scale problems.

08:18.000 --> 08:21.000
LinkedIn offers a lot of opportunities of this kind.

08:21.000 --> 08:29.000
There are also certain questions that we actually asked ourselves to kind of motivate where we put our time and energy.

08:29.000 --> 08:33.000
I may not be able to go into that, but so there was a lot of thought into it.

08:33.000 --> 08:37.000
One important factor from people point of view is.

08:37.000 --> 08:48.000
The types of problems that we want to go into, sometimes you may need to spend significant amount of work, like maybe multiple quarters of somebody investing their time and going deep into it.

08:48.000 --> 09:01.000
How do we orchestrate ourselves so that we stay relevant are actually solving the problems, but don't send people into rabbit holes where they go find out an answer.

09:01.000 --> 09:10.000
If it doesn't work, we want to make sure that the team is organized in such a way that it doesn't hurt their career trajectory.

09:10.000 --> 09:20.000
If you're building a product and you're a machine learning engineer working for the product, then your fate or the career trajectory is pretty tight to the fate of this product implicitly.

09:20.000 --> 09:31.000
When you operate horizontally, we needed to think about how do we think about doing good to the people in my team while at the same time having an impact across the board.

09:31.000 --> 09:49.000
And what does that mean concretely in terms of how you help folks ensure that the time that they invest on these long term projects is going to be a creative to their careers and how you manage the overall portfolio.

09:49.000 --> 09:56.000
Yeah, I think so you brought up a term that I will actually double click on and go into it.

09:56.000 --> 10:03.000
I see our team as essentially a portfolio of investment PCs.

10:03.000 --> 10:14.000
You think of it like a white combinator of applied research tracks, and that was the analogy that I was alluding to earlier that I thought was pretty interesting when we first discussed it, so please elaborate on that.

10:14.000 --> 10:20.000
Yeah, I mean, I came from startups, so I'm using something that I'm a little familiar with.

10:20.000 --> 10:31.000
So partially, I think it's my own personal bias, but it helps me give a nice framework in terms of thinking where we are and like in terms of trajectory.

10:31.000 --> 10:46.000
In the process of building end to end products in my startup experience, what I see is different phases. You can go from an idea all the way to something that is generally available, shipping, signing contracts, making money.

10:46.000 --> 10:50.000
There are different phases that this idea goes through.

10:50.000 --> 11:02.000
You know, you have to build sometimes a proof of concept, then you have to build an MVP, then MVP needs to get a little bit more shaped to become an alpha beta product, and then you need to scale it, you need to drive adoption.

11:02.000 --> 11:11.000
Different aspects of these in if you translate them to LinkedIn's ecosystem, require different types of partnerships.

11:11.000 --> 11:19.000
It's hard for any single team to do this all alone, because LinkedIn is very functionally organized.

11:19.000 --> 11:28.000
So what we do is essentially, you know, first recognize whether we have interesting idea and it's worth investing in.

11:28.000 --> 11:42.000
And do we have actually an understanding that if done well, this is going to have a significant impact, then we try to actually find people that we feel like have the core competency to take it from A to B.

11:42.000 --> 11:49.000
This is also very important, if you have a really good idea, but you don't have the right set of people and the right mindset, it's not going to go well.

11:49.000 --> 11:54.000
So in that sense, also the Y combination analogy is very, very appropriate.

11:54.000 --> 11:58.000
People do absolutely matter.

11:58.000 --> 12:19.000
So what we have done is to basically say, okay, if it is very researchy idea, let's say, you know, how do you think about every testing for fairness, which we have been working on for multiple years, or how do we think about building actually usable products using differential privacy and roll them out at scale, then you have to start very early.

12:19.000 --> 12:40.000
Try to identify, is it even doable? What kinds of gaps do exist? Is there actually an algorithmic gap? Is there a methods gap? Then once we feel like we have the story together, try to have a partnership with the product and a real problem where we can demonstrate this through a proof of concept and start slowly going through this kind of.

12:40.000 --> 12:54.000
Each day series, series B, as I call it in my head, it's not that necessarily, this is how it's operationalized, but it has given me a mental framework to think about it in a more holistic way.

12:54.000 --> 13:01.000
And we always decide, do we want to go further or stop here? Do we want to go further or stop here?

13:01.000 --> 13:15.000
What that does is that if let's say there is an idea and we have gone to a decent depth and it turns out that either that's enough and actually this is the extent that we need to go or there is nowhere to go and it's a dead end.

13:15.000 --> 13:25.000
We can stop and we can actually stop risking an individual that's actually spending their time because it's important for their career.

13:25.000 --> 13:34.000
But in the cases where we have gone somewhere and turned out that this is a dead end, it saves company costs of other people going into that extent, right?

13:34.000 --> 13:42.000
Because when we come back, we would have a very comprehensive idea about, hey, these are all the kinds of things you can do and it doesn't work.

13:42.000 --> 13:51.000
But we have several positive examples where we invest a little bit and it starts to grow and we see utility and organically starts to grow.

13:51.000 --> 14:07.000
So we have examples of both good and bad kind, but we want to be able to start new things or drop things that aren't working and be much more driven by market fit within the LinkedIn ecosystem.

14:07.000 --> 14:26.000
One thing that I'm finding interesting in this part of the conversation is we often talk about the need to establish your success criteria early on at the very beginning of the project.

14:26.000 --> 14:54.000
So you know when it's good enough, right? So you're not chasing this incremental percent that's not really adding value. And there's, you know, in some ways, attention or contrast at least between, you know, that's upfront establishment of parameters versus what you're saying is maybe more incremental or at least reminiscent of kind of a stage gate type of approach to.

14:54.000 --> 15:03.000
You know, we're going to have some set of milestones or some, you know, frequency with which we reevaluate.

15:03.000 --> 15:13.000
And I think the, maybe the connection between those two is that the first is focused on success criteria and the second is focused on failure criteria.

15:13.000 --> 15:16.000
Is that the right way to think about it?

15:16.000 --> 15:36.000
Let me see if I can speak to this. I think having success criteria or an understanding of what is what does success mean is important, but it's easier to phrase when you can see line of sight where you're going.

15:36.000 --> 15:52.000
And let's say you're working on, let's say you're working on developing deep learning app in a computer, imagine, and you're 20 years ago, how are you going to know that this is going to work in a very particular way you don't know.

15:52.000 --> 16:07.000
Some of the problems that we're investing in, I don't have a line of sight yet. So what I've tried to do is to phase it out in the way we operate and be fairly agile about checking in. I think it keeps us honest.

16:07.000 --> 16:30.000
But it, so what we do is to have a horizon saying that, okay, let's spend, you know, let's say, let's send two really competent people three or four really competent people along this line and give ourselves this much time and say we see where we are and really kind of take a very experimentation type approach in the way we approach these things.

16:30.000 --> 16:44.000
And you do have to have your strong hunches. So if you are operating in these spaces without really understanding how research work, it obviously is hard to make this judgment calls.

16:44.000 --> 16:55.000
But you need to be open to the fact that you might find a very interesting insight that you didn't know before and you need to be able to pivot on that.

16:55.000 --> 17:06.000
For example, you know, there are some examples that I'm thinking about as I tried to illustrate, we started this project to do explainability in my team.

17:06.000 --> 17:14.000
When I came here, it was like one of these fledgling early stage projects, I needed to decide whether to keep it or not.

17:14.000 --> 17:26.000
I have a worked on explainability before in the context of like the product that we built in biotech. So I also saw a way to actually operationalize and we had a really good person. So we start working on it.

17:26.000 --> 17:39.000
Fortunately, we were working on it just in the sense of hey, how do we solve it for go to market and we did something for a particular product and it turned out to be actually go to market, meaning what?

17:39.000 --> 17:46.000
Go to market means, you know, Lincoln has multiple products that is trying to sell. It's a cluster of SaaS offerings.

17:46.000 --> 17:55.000
If go to market implies, you know, if somebody who is the right customer to sell to, how do you take this product to the market?

17:55.000 --> 18:04.000
Because we have our own sales force, our own marketing force, how do we actually help them do their job better empowering them to actually go to the market with our products.

18:04.000 --> 18:09.000
So an internal use case for enabling your sales and marketing teams.

18:09.000 --> 18:14.000
Exactly. It's a well-fraised use case. So we can evaluate it. Okay. Is it working or not?

18:14.000 --> 18:22.000
And also once you sell your product, sometimes there are adjacencies. So you can also always do like cross sales upsells and so on.

18:22.000 --> 18:28.000
So we started there. We tried to see, okay, can we solve this problem for a couple of the problems?

18:28.000 --> 18:40.000
It turns out that hey, we can do it in a generalizable way and we make a call. Do we keep going or we stop here and we decide to keep going on that one and essentially start to expand how it's done.

18:40.000 --> 18:50.000
Sometimes we have to come back and redo. You know, our stack so that it actually scales better. For example, we had written a product stack.

18:50.000 --> 19:01.000
Then we learned that it's actually better to translate it to an internal auto ML like framework called pro ML that's very large inside LinkedIn.

19:01.000 --> 19:09.000
So we actually translated our learnings into a pro ML component and just evolve from there to come back to your original point.

19:09.000 --> 19:23.000
We do have success criteria. We have a sense of what that success mean, but it's faced. And we are open to the fact that between phases, these things can evolve and be open to it.

19:23.000 --> 19:45.000
Yeah, I think one of the things that I was that just kind of peak my interest is that, you know, when you're invested in a project, either, you know, as a team or as an individual, you often get in the scenario where success seems like kind of, you know, just one short win away.

19:45.000 --> 20:04.000
And it's very easy to keep, you know, to continue to double down, double down, double down. And it's, you know, it's hard to know when to, you know, one to pivot, one to cut bait, you know, Seth go didn't wrote a book, the dip, which is like, you know, to set go to book.

20:04.000 --> 20:12.000
It's 60 pages, but it's about this idea applied to careers and the like, like, how do you know when you should change direction.

20:12.000 --> 20:27.000
It's a tough set of problems. Yes, it is. I think it's a problem at multiple levels, but what I really want to solve it for. And I think we have done a good job with LinkedIn is to solve it at the individual level because at an individual level, this problem exists.

20:27.000 --> 20:38.000
And I am really invested in making sure that people that are in my team have a good experience and that look back at the time that they have spent in the team and they think that we did some amazing things.

20:38.000 --> 20:48.000
You know, balanced good time. What is at least in my experience, I've been a researcher, right, I, you know, I've been principal engineer, I have been through this.

20:48.000 --> 20:54.000
So some of it is colored by how I learned to balance these things.

20:54.000 --> 21:04.000
What I have come to realize is that I think as a researcher or as a machine learning practitioner, you should also have a portfolio of projects.

21:04.000 --> 21:10.000
Sometimes, you know, which way one of these or a few of these are going, sometimes you absolutely don't.

21:10.000 --> 21:19.000
But we have a notion of 20% projects, something that we borrowed from an idea that Google popularized sometimes ago.

21:19.000 --> 21:33.000
But what I try to do as a manager is to make sure that people are not indexed on something fully that they have a portfolio at the individual level and the portfolio actually ladders up very nicely to the broader applied research portfolio.

21:33.000 --> 21:38.000
Then what's happening is that sometimes some things we would like for them to be table stakes.

21:38.000 --> 21:48.000
Something that this person with their capability can do and keep going so they can actually justify their job and being funded.

21:48.000 --> 21:56.000
Some things probably is focused on what they truly believe in and maybe they have a really good intuition.

21:56.000 --> 22:04.000
They want to go there and if it doesn't work, that's fine because we are learning something about the way it's not working.

22:04.000 --> 22:23.000
A key component of my team being very, very successful is to create this space where people can go and actually fail and still come out with a reasonably looking average results for their career saying that,

22:23.000 --> 22:33.000
I tried there are two things that was terrible. I tried and nothing worked and there are two or three things that I tried where actually you know things work.

22:33.000 --> 22:51.000
So what it does is maybe it allows the passion projects to have the individual bigger which passion projects need and then also contribute to more team oriented projects where maybe you are doing your part and you're essentially moving the larger project along and.

22:51.000 --> 23:03.000
You know it's probably a little bit more clear what to do and bills have portfolio at your level and it also allows me to have balance portfolio at the air level.

23:03.000 --> 23:14.000
Because I want people to take risks, but I also want people to not get burned out by the fear of you know failing and like have the ability to fail and stop.

23:14.000 --> 23:37.000
Yeah, I thought it may be useful to dig a little bit into a couple of these use cases to both you know because I'm curious about how you approach them, but maybe you know they might also provide interesting examples of the portfolio management aspects that we've talked about and the team and culture aspects that we've talked about.

23:37.000 --> 23:56.000
In particular, LinkedIn's approach to experimentation and then you've got this more strategic aspect of that or extension which is trying to quantify unintended consequences and experimentation.

23:56.000 --> 24:03.000
Can we dig into those areas a little bit and have you share a little bit about what you're doing now.

24:03.000 --> 24:22.000
So let me anchor my comments on what I said earlier, which is that we have these two pillars through which I think about how we are spending our resources on the cross lines of business utility type of an idea.

24:22.000 --> 24:35.000
The right now experimentation is in everything that we do. I mean, most of the features that essentially get rolled out are experimented upon we understand how the impact is happening.

24:35.000 --> 24:45.000
I should maybe interrupt to note that in this context, we're talking about online experimentation.

24:45.000 --> 24:53.000
We're talking about those two kind of experimentation during model development, I kind of think that's right, that's right.

24:53.000 --> 25:05.000
I mean, the scale of experimentation, I think driven by a bunch of people, yeah, I was the one of the architects of this, yeah, has been on your show before is impressive.

25:05.000 --> 25:14.000
It is very impressive just the ability to have an understanding of iterative roll out and understanding how certain ideas of market fit is really amazing.

25:14.000 --> 25:18.000
I haven't seen that before in my experience.

25:18.000 --> 25:31.000
So that it has impact on multiple lines of businesses. So you can at least park it as hey, this is actually something that we have done an IPO like it's well understood has been scaled.

25:31.000 --> 25:37.000
Where do we go from there? It's a different set of challenges that that particular area has.

25:37.000 --> 25:43.000
For something like forecasting, which also has multiple lines of business utility.

25:43.000 --> 25:50.000
We needed to basically start from an idea saying can we just use open source tools to solve this problem.

25:50.000 --> 25:56.000
So we spend some time to essentially do a survey and say, okay, how are things behaving?

25:56.000 --> 26:08.000
The specific problem in that case is essentially trying to deliver something like forecasting as a service that serves multiple organizations within LinkedIn.

26:08.000 --> 26:22.000
That's right. I think the problem is that if different businesses or different groups have their own forecasting done using their own tools, it's hard to roll them up to provide a cohesive view at the business level.

26:22.000 --> 26:32.000
And one of the things platforms offer is that if you have a consistent use and it's trustworthy, then everybody is using the same framework and it allows you to ladder up.

26:32.000 --> 26:37.000
Different businesses, different components and have a like a much more holistic understanding, right?

26:37.000 --> 26:43.000
So it at an exact level, this makes things easier. It also easier to understand.

26:43.000 --> 26:54.000
In the COVID context, obviously, this was very much in demand because there was a lot of volatility. So there's a lot of interest in this kind of work.

26:54.000 --> 27:00.000
The first thing was to essentially look around, look at the tooling that's available.

27:00.000 --> 27:05.000
See if we can just reuse it or maybe build rappers to scale it up.

27:05.000 --> 27:16.000
Then we start running into situations where internal customer, let's say, marketing services has certain forecasting problem and the external tools are not solving it.

27:16.000 --> 27:23.000
Then we try to understand, hey, what is the aspect that we can do better to satisfy the requirements of this customer?

27:23.000 --> 27:35.000
Maybe for example, they need a iterative fast RCA, they probably need much higher customized building and tuning that maybe the external tools are not offering.

27:35.000 --> 27:39.000
So that gives us a very clear picture of what to build, where to innovate.

27:39.000 --> 27:43.000
Then we go a little bit deeper and start building out.

27:43.000 --> 28:09.000
The forecasting work at this point is continues to grow. We have 12 different use cases that are already onboarded and we also had to make design decisions about do we purely build it as a service where everybody has to adopt or actually we also build it as a service and a library where maybe somebody does not want to use their service but wants to incorporate the algorithm library into their service internally.

28:09.000 --> 28:15.000
But engineering decisions we have to contend with and be mindful about how we go.

28:15.000 --> 28:31.000
We open sourced great guide, I think last week, very recently, seems to be well received so far, but with any of these kinds of effort, I feel like we know as much as we do and we do as well as we can.

28:31.000 --> 28:39.000
So if there are cases where it's not working and we have chances to improve, we continually invest in it. It's been a really wonderful journey.

28:39.000 --> 28:49.000
One of the things as a manager that I really enjoy is that when you're building things of this kind, people come together, they organically start to form teams sometimes.

28:49.000 --> 29:03.000
It's like watching Beatles come together where there is a couple of really strong engineers, the different set of people form and they're creating something together and that process for me.

29:03.000 --> 29:11.000
People learn how to work together, people actually understand each other's strengths and weaknesses and produce something that goes out and is very well received.

29:11.000 --> 29:17.000
That's beautiful and I mean probably one of the most fun aspects of me running my team.

29:17.000 --> 29:35.000
On the strategy side, but before we do that, since you mentioned it, I'd love to double click on, I can't believe I said that I hate that as a tech aphorism double clicking on things, but let's maybe dig into, I don't know if that's any better.

29:35.000 --> 29:54.000
And Silverkite a little bit and what you're looking to do there, what you produced and open source and even it was open source, was that a goal originally? How did that come about?

29:54.000 --> 30:05.000
I mean, what we are fundamentally trying to solve is to build something that can solve multiple forecasting use cases across LinkedIn. We want to platformize. That's our primary goal.

30:05.000 --> 30:11.000
LinkedIn has a culture of sharing what we build with the external community, something also I find appealing.

30:11.000 --> 30:15.000
A lot of the people in my team are very much subscribed to this philosophy.

30:15.000 --> 30:25.000
So when we had something that was working and we had an ability to allow other algorithms to be plugged into a unified interface, we thought, hey, we should put it out there.

30:25.000 --> 30:29.000
So if you know somebody wants to use it, they can use it.

30:29.000 --> 30:40.000
So it's also good for folks in my team to have this visibility. I think it's great for their career, but we also genuinely want to encourage people to co-develop.

30:40.000 --> 30:48.000
There have been things that have gotten open source from LinkedIn like Kafka, you know, that have become bigger.

30:48.000 --> 30:55.000
I don't compare ourselves to that. I think there are much more stellar successes, but we build something we are proud of it.

30:55.000 --> 30:59.000
And we think that it's useful. So we just want to share it with the general community.

30:59.000 --> 31:11.000
And the hope is that it will bring like-minded people and it will also help us understand what are the things that we didn't do well, maybe give us a chance to get better.

31:11.000 --> 31:22.000
But our primary focus is to serve our internal customers. We have a lot of them and we actually want to provide them a state of art solution that can solve their forecasting needs.

31:22.000 --> 31:33.000
There, I think we are continuing to prioritize it. We have a wonderful partner in our sister team in the data org that's much more engineering capability type of a team.

31:33.000 --> 31:38.000
So we have a strong partnership to try to prioritize it internally and it's ongoing.

31:38.000 --> 31:49.000
One of the nice side effects of getting the forecasting part right has been that we now are starting to look at a broader spectrum of times here is problems.

31:49.000 --> 31:53.000
So forecasting is a sister problem to anomaly detection.

31:53.000 --> 32:00.000
Most people that come to you and ask for forecasting or anomaly detection also want root cause analysis.

32:00.000 --> 32:03.000
So there are a lot of statistical problems that are very adjacent.

32:03.000 --> 32:08.000
So initially we were very focused on one and then we got it right.

32:08.000 --> 32:14.000
We feel like we have a piece of your answer. We are trying to slowly build something that's more holistic.

32:14.000 --> 32:22.000
What we want to do aspirationally is to try to get to the level that experimentation got at LinkedIn.

32:22.000 --> 32:30.000
To basically inform the performance management and like change the culture of performance management.

32:30.000 --> 32:34.000
There is still a lot of people trying to do week over week year over year.

32:34.000 --> 32:39.000
It's nothing wrong, but it assumes certain steady state to the business that we don't see.

32:39.000 --> 32:53.000
So we are actually trying to provide tools where forecasting and become a maximum natural component of how people do metrics performance management.

32:53.000 --> 32:58.000
You were elaborating on the strategic side.

32:58.000 --> 33:03.000
Yeah, so I can go to that and the strategic side.

33:03.000 --> 33:10.000
It requires thinking a little bit ahead. So the way I think about it is, you know, LinkedIn is where we are today.

33:10.000 --> 33:17.000
So can you imagine what LinkedIn's problems would be and what its place would be in the Microsoft ecosystem.

33:17.000 --> 33:21.000
Let's say three years out, five years out.

33:21.000 --> 33:28.000
The main two investment areas for us on the strategic side has been one is called computational social science.

33:28.000 --> 33:39.000
Predominantly, the work that has come out of there is the every testing aspect of fairness that's actually been open source.

33:39.000 --> 33:40.000
It's.

33:40.000 --> 33:45.000
So there is a blog post from Guillaume and LinkedIn is ranked blog.

33:45.000 --> 33:53.000
And we continue to actually work on how to understand our products, how to understand unintended consequences of our products.

33:53.000 --> 34:01.000
We also look at other elements like, you know, what is the correct way to or maybe better way to build a professional network.

34:01.000 --> 34:08.000
There was a blog post from Indian a few weeks ago on this topic on the separately.

34:08.000 --> 34:19.000
We feel like differential privacy as one of the elements of privacy engineering is something we felt like, you know, very strongly about investing in.

34:19.000 --> 34:27.000
The reasons are the following, you have to remember, I before LinkedIn, I was in farmer or biotech space.

34:27.000 --> 34:40.000
And in biotech space, sometimes even if you're trying to do the right thing, let's say you're trying to develop drugs that actually help people with mental health, which was essentially the mission of the previous company I worked at.

34:40.000 --> 34:53.000
You need data in order to build data products, you need data and certain companies have certain data and it becomes really hard to build good algorithms with very small biased data.

34:53.000 --> 35:03.000
But if you try to collect data across multiple companies, it's extremely hard for some sometimes for good reasons because you're trying to have protections.

35:03.000 --> 35:11.000
But this brings into the question the problem where you are trying to join datasets from different entities.

35:11.000 --> 35:19.000
In the case of LinkedIn, let's say, LinkedIn has some interesting data and let's imagine that Microsoft has certain interesting data.

35:19.000 --> 35:29.000
Can you build a data product that synthesizes data between them, but in a way that does not compromise the commitment we are making to the members.

35:29.000 --> 35:35.000
We have made certain commitments about member privacy, we actually really care about respecting them.

35:35.000 --> 35:39.000
LinkedIn is very much about members first.

35:39.000 --> 35:51.000
Can we build technologies that essentially guarantee that the member privacy is ensured that the re-identification kinds of risks are minimized while still providing utility.

35:51.000 --> 35:58.000
So I find different privacy really interesting from this context.

35:58.000 --> 36:04.000
Now, when we started, this is a new area for me.

36:04.000 --> 36:14.000
So just in a complete spirit of honesty, I had to learn a lot, but I was very fortunate that I started working with a couple of engineers in my team that are fantastic.

36:14.000 --> 36:25.000
And they know the problem really, really well. Ryan has been on your podcast before at the time when I think he's a Newrips paper on TalkK, D.B. Algorithm came out, fabulous guy.

36:25.000 --> 36:36.000
But we started working on it. I started to understand what the technology is capable of, where the gaps could be in the context of our problems.

36:36.000 --> 36:50.000
Then we started to invest today. I feel like my view is that there are two large classes of differential privacy results from my partially educated lens that I see.

36:50.000 --> 37:02.000
Local differential privacy algorithms are focused on enabling privacy at the point of measurement. Let's say you have an iPhone and you are doing an emoji and like how are these emojis being collected at the company level.

37:02.000 --> 37:07.000
You could do a differential privacy. So there is a coin flip at the point of collecting the data.

37:07.000 --> 37:13.000
There are other companies. Let's say LinkedIn could be next door, could be like Facebook that collect data.

37:13.000 --> 37:21.000
They have terms of service that allow you to collect certain types of data from the member and they have a large database of these members.

37:21.000 --> 37:31.000
How do you actually then build analytics products from these large kind of member protected are.

37:31.000 --> 37:44.000
Data set, right? So there I feel like global privacy differential privacy approaches are much more relevant. So we focused a lot on global D.B. work, at least in the last 18 months or so.

37:44.000 --> 37:54.000
We have several examples of this. The one I really enjoyed was Microsoft wanted to do skills initiative announcement with LinkedIn.

37:54.000 --> 38:20.000
So they wanted to actually showcase this result where we wanted to take data from economic graph, which is this giant graph consisting of people, companies, schools and how these things are related and provide insights to policy think tanks, garments about which are the top jobs, which are the top industries that are hiring in different geolocations very relevant.

38:20.000 --> 38:35.000
I think if the time of like, you know, middle of last year because the pandemic had affected the economy and people were interested in, you know, how the labor market is responding where the openings are so on support.

38:35.000 --> 38:49.000
So we wanted to enable this view of economic graph. And we wanted it to be visible, but can we do it while actually making sure that the member data is privatized or actually we can provide guarantees on privacy.

38:49.000 --> 38:59.000
So the technology that we developed on the global D.P. was extremely helpful in this context and it's continued to grow.

38:59.000 --> 39:14.000
One thing that I really enjoy about LinkedIn is that it's such a complicated large ecosystem that when we think that we got it right in one context, something comes up and we realize, oh, we didn't solve for this again.

39:14.000 --> 39:20.000
For example, we thought, oh, we understand how to build labor market insights type products very well.

39:20.000 --> 39:27.000
Then another product came that actually had the constraint that the data was getting updated pretty rapidly.

39:27.000 --> 39:39.000
I mean, think the comparison between sensors, which also is using D.P. maybe gets collected every 10 years to data in the LinkedIn feed that probably updates like every second.

39:39.000 --> 39:45.000
People are doing something, liking something, clicking on something, making new connections.

39:45.000 --> 39:56.000
So if you're building analytics products for the latter, it's almost like streaming data and thinking about how to do differential privacy for streaming data, we realize it was not solved.

39:56.000 --> 40:04.000
Similar to how to think, think about differential privacy for top K before we realize it wasn't solved. So we came up with algorithms.

40:04.000 --> 40:12.000
And not solved from an algorithmic and research perspective or from an engineering perspective or both.

40:12.000 --> 40:18.000
And how does your, where does your team fit on that spectrum?

40:18.000 --> 40:24.000
So in the top K and the streaming difference privacy side, both.

40:24.000 --> 40:38.000
So the strength of the team still is very much in the applied research side. So most of our people, we have very strong engineers, but the core of the team is no strong set of applied researchers.

40:38.000 --> 40:49.000
So when we identify that, hey, certain algorithmic methodological opening exists, we spend some time try to actually see if we can come up with algorithms to solve this problem.

40:49.000 --> 40:59.000
You know that we have that right answer. Typically, we actually put the papers out. If you look at archive, we have a steady stream of papers coming out from our group.

40:59.000 --> 41:07.000
Then we start talking to our partners in the engineering and try to protect us this into a system.

41:07.000 --> 41:19.000
We have really, really good partnerships with engineering teams. And this has been an anchor in thinking really well about, okay, I have this cool algorithm. Can I actually make it work?

41:19.000 --> 41:29.000
What are the constraints that I need to think about? For differential privacy, the partnership with this team called Pino, which is actually an open source all up.

41:29.000 --> 41:37.000
Has been really useful because there are engineers that understand the constraints of high throughput analytics really well.

41:37.000 --> 41:45.000
And they can look at ideas that we come up with and say, okay, it will work well, but maybe you need to think about it differently and so on.

41:45.000 --> 42:11.000
So to come back to your original question, we invest still in the two lens context of like cross business utility or strategy. And for strategy, the fairness and privacy were obvious places to go, at least when I looked at the thesis in the middle of 2019 and it still continues to be pretty relevant.

42:11.000 --> 42:25.000
What other things would become the new players, I don't know, and I'm very open. So this is a conversation we have every six months and we say, hey, what is it that we don't know and should we think about it?

42:25.000 --> 42:45.000
I was actually going to ask about that. We talked about this why commonator analogy from the perspective of an individual project and the stages that it goes through, but there's also the invest don't invest decision and how do you pick the winners identify the bets to make?

42:45.000 --> 42:57.000
Does that analogy apply in this context as well, or are there aspects of, you know, thinking about it as a why commonator type of a process that informs the way you manage the portfolio in that way?

42:57.000 --> 43:09.000
I think it does, but let me just be open and say it's I'm learning to, I don't claim to say that I have this down.

43:09.000 --> 43:30.000
These are judgment calls. What I have really benefited from is by having a point of view and then talking to my boss and talking to their boss and essentially just making sure that the executive see the same thing that I do and this alignment that we have in Lincoln has been a really strong factor.

43:30.000 --> 43:43.000
Because then you're seeing this from multiple points of view and some people see it farther because they have a perspective. Let's say you're talking to a super engineering or CEO, they can see things that we don't.

43:43.000 --> 43:52.000
And just formulating this point of view and saying, hey, this is what we think and like floating it out and having this alignment has been really useful.

43:52.000 --> 44:01.000
Maybe this is similar to going talking to an investor and telling them telling you, I don't believe you kind of an idea.

44:01.000 --> 44:09.000
And I mean, so in that sense, I think it's a sort of a strategic A B test and it has been useful.

44:09.000 --> 44:20.000
Sometimes there are also situations where they may be wrong and you may have to push for it, but it does give you a way to evaluate, are you along the right path or not?

44:20.000 --> 44:42.000
Are there transferable insights that might be useful to someone who's listening, you know, you see everybody doing this wrong and picking projects or you've learned that you need to look for these three things like anything like that that you could share.

44:42.000 --> 44:59.000
Okay, really good question. I would lean on something that, you know, one of my previous bosses that I really love told me success in the industry, he used to say is down to three things and it applies in this context.

44:59.000 --> 45:10.000
So let me repeat at least what he used to say is ability desire and opportunity three things and if they meet, you know magic happens.

45:10.000 --> 45:15.000
Ability and desire is down to finding the right people.

45:15.000 --> 45:22.000
Somebody that's very competent, but does not really care about this particular problem, isn't going to very go very far.

45:22.000 --> 45:29.000
Somebody that's very passionate, but doesn't have the technical ability to carry it out, isn't going to go too far either.

45:29.000 --> 45:37.000
And the opportunity is something where if you solve this problem, does it actually help the company and help our members?

45:37.000 --> 45:54.000
I find personally that grounding what we do in the context of what is the vision and mission of the company extremely useful because it's a way to say, no, we don't really care about investing in certain area because I don't see how it maps.

45:54.000 --> 45:59.000
Or at least it forces a conversation where somebody has a cool idea.

45:59.000 --> 46:07.000
For example, there was a threat about, hey, we should why aren't we thinking about quantum computing? I'm like, okay, how does it help members get better jobs?

46:07.000 --> 46:13.000
If you can tell me the answer, or at least help me think through this, I'm willing to talk.

46:13.000 --> 46:19.000
So it gives a very nice grounding so that we know where we are.

46:19.000 --> 46:28.000
But what is transferable is great technology is made by great people.

46:28.000 --> 46:36.000
If you are building something that can be done by one person and you have the ability and you are passionate about it, then you can identify right opportunity.

46:36.000 --> 46:38.000
Maybe you can do it all.

46:38.000 --> 46:41.000
A lot of the great technology actually requires teams.

46:41.000 --> 46:47.000
So what you should look for is complementarity in finding these three elements, right?

46:47.000 --> 46:57.000
Somebody may be really good at identifying opportunities, somebody might be really good at skills and technical ability and somebody might be extremely passionate.

46:57.000 --> 47:03.000
And if you put people like this together, amazing things happen. Greg Ed is such an example.

47:03.000 --> 47:05.000
Nice.

47:05.000 --> 47:16.000
Well, Parvaz, thanks so much for taking the time to chat with us and share a bit about what you're working on and how you're organizing your work and teams at LinkedIn.

47:16.000 --> 47:20.000
Thank you. I really appreciate the time and it's nice to be here again.

47:20.000 --> 47:22.000
Same here. Thank you.

47:22.000 --> 47:28.000
Thank you.

