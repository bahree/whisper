WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.640
I'm your host Sam Charrington.

00:23.640 --> 00:29.480
We are back with our third show this week and the third in our Autonomous Vehicle Series.

00:29.480 --> 00:35.080
My guest this time is Katie Driggs Campbell, postdoc in the Intelligent Systems Lab at Stanford

00:35.080 --> 00:39.120
University's Department of Aeronautics and Astronautics.

00:39.120 --> 00:44.600
Katie joins us to discuss her research into human behavioral modeling and control systems

00:44.600 --> 00:47.000
for self-driving vehicles.

00:47.000 --> 00:51.540
Katie also gives us some insight into her process for collecting training data, how social

00:51.540 --> 00:55.400
nuances come into play for self-driving cars, and more.

00:55.400 --> 01:00.000
Our Autonomous Vehicle Series is supported by Mighty AI and I'd like to take a brief

01:00.000 --> 01:02.640
moment to thank them for their support.

01:02.640 --> 01:07.800
Mighty AI helps companies working in the Autonomous Vehicle market create training and validation

01:07.800 --> 01:10.360
datasets to support computer vision.

01:10.360 --> 01:16.240
Their platform combines guaranteed accuracy with scale and expertise and includes annotation

01:16.240 --> 01:21.280
software, consulting and managed services, proprietary machine learning, and a global

01:21.280 --> 01:24.680
community of pre-qualified annotators.

01:24.680 --> 01:29.400
If you haven't caught my interview with their CEO, Darren Nakuda, which was the first show

01:29.400 --> 01:37.000
in this series, please be sure to check it out at twimlai.com slash talk slash 57.

01:37.000 --> 01:44.960
And of course, be sure to visit them at www.mty.ai to learn more and follow them on Twitter

01:44.960 --> 01:48.200
at Mighty underscore AI.

01:48.200 --> 01:52.080
Before we jump in, if you're in New York City next week, we hope you'll join us at

01:52.080 --> 01:55.440
the NYU Future Labs AI Summit.

01:55.440 --> 01:59.280
As you may remember, we attended the inaugural summit back in April.

01:59.280 --> 02:04.040
This year's event features more great speakers, including Karina Cortez, head of research

02:04.040 --> 02:10.400
at Google New York, David Venturelli, Science Operations Manager at NASA Ames Quantum AI

02:10.400 --> 02:16.400
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.

02:16.400 --> 02:26.320
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off of all tickets, use the

02:26.320 --> 02:29.160
code twimla25.

02:29.160 --> 02:33.440
And now on to the show.

02:33.440 --> 02:42.120
All right, everyone, I am on the line with Katie Driggs Campbell.

02:42.120 --> 02:48.600
Katie is a postdoc at Stanford in the Intelligent Systems Laboratory in the Department of Aeronautics

02:48.600 --> 02:50.400
and Astronautics.

02:50.400 --> 02:53.040
Katie, welcome to this week in machine learning and AI.

02:53.040 --> 02:55.120
Yeah, thanks for having me.

02:55.120 --> 02:57.680
I am really looking forward to this conversation.

02:57.680 --> 03:05.080
So as you know, we are kind of in the midst of a series of podcasts on autonomous vehicles.

03:05.080 --> 03:11.480
And usually we end up talking to folks that are in CS or electrical engineering or other

03:11.480 --> 03:17.040
disciplines, but you are out of the Department of, again, Aeronautics and Astronautics.

03:17.040 --> 03:23.640
And so I'm really looking forward to digging into the connection between that and autonomous

03:23.640 --> 03:24.960
vehicles.

03:24.960 --> 03:29.840
And I guess a good way to start is to have you tell us a little bit about your background

03:29.840 --> 03:32.960
and your path to what you're doing now.

03:32.960 --> 03:33.960
Yeah, yeah, sure.

03:33.960 --> 03:37.800
So I'll tell you a little bit about, I guess, how I ultimately ended up in an aerospace

03:37.800 --> 03:38.800
department.

03:38.800 --> 03:43.160
I started out in electrical engineering, actually.

03:43.160 --> 03:49.000
So I did my undergrad at Arizona State University, but I was really interested in control systems

03:49.000 --> 03:51.000
and ultimately robotics.

03:51.000 --> 03:56.080
So I was really interested in how we can control and interact ultimately with people.

03:56.080 --> 03:59.960
So when I applied to grad school, I applied to mostly robotics programs.

03:59.960 --> 04:05.680
So then I ended up at UC Berkeley, where I worked with Ruzuna Baichi in robotics, and

04:05.680 --> 04:08.840
that was in electrical engineering and computer science.

04:08.840 --> 04:11.440
So I slowly shifted to computer science.

04:11.440 --> 04:14.320
And that's, I guess, when I first started working on intelligent vehicles.

04:14.320 --> 04:19.160
So when we first got started on this, this was, you know, six some years ago.

04:19.160 --> 04:23.720
So autonomous vehicles weren't, I guess, they hadn't really quite hit the road.

04:23.720 --> 04:25.720
So we were actually looking at...

04:25.720 --> 04:26.720
More pun intended.

04:26.720 --> 04:27.720
Exactly.

04:27.720 --> 04:28.720
Exactly.

04:28.720 --> 04:32.440
We were actually working on semi-autonomous vehicles, and we found that the big problem

04:32.440 --> 04:36.080
with semi-autonomous vehicles was how do you model the human?

04:36.080 --> 04:39.560
So we started to think about how we can model the human, how we can design control systems

04:39.560 --> 04:40.560
to keep people safe.

04:40.560 --> 04:45.240
So we thought a lot about texting while driving, how different environmental influences

04:45.240 --> 04:49.640
will affect your driving abilities, and how we can ultimately, you know, design active

04:49.640 --> 04:51.480
safety systems to keep you safe.

04:51.480 --> 04:57.000
And slowly over the years, this shifted to autonomous vehicles like everyone else.

04:57.000 --> 04:58.640
We're still the human had a huge impact.

04:58.640 --> 05:03.000
So we were still thinking about how people either interact with autonomy in the vehicle,

05:03.000 --> 05:07.960
or how the autonomous vehicle can interact with other people or human drivers on the road

05:07.960 --> 05:08.960
around them.

05:08.960 --> 05:11.560
That's sort of how I slowly shifted into autonomous vehicles.

05:11.560 --> 05:12.560
Yeah.

05:12.560 --> 05:16.800
So then I finished my PhD, and I started working in an aerospace lab.

05:16.800 --> 05:21.640
So we're the AI section of the aerospace department here.

05:21.640 --> 05:27.440
And so the lab I work in is famous for air flight and collision avoidance systems, actually.

05:27.440 --> 05:28.440
Okay.

05:28.440 --> 05:29.440
Yeah.

05:29.440 --> 05:33.920
So thinking about how we can design safety systems for planes, which isn't all that different

05:33.920 --> 05:34.920
from vehicles, actually.

05:34.920 --> 05:40.040
So the lab I'm in is actually mostly funded by vehicle companies and working on autonomous

05:40.040 --> 05:41.880
vehicles and decision making for vehicles.

05:41.880 --> 05:48.080
So it's sort of a different than maybe you're what you might expect from an aerospace

05:48.080 --> 05:50.840
department, but a lot of the systems are pretty similar.

05:50.840 --> 05:58.000
And I imagine that aerospace departments have been working on this problem of, you know,

05:58.000 --> 06:04.360
auto pilot, right, comes from, you know, piloting a plane and, you know, space programs and

06:04.360 --> 06:09.680
things like that have been trying to have autonomous or semi-autonomous remote vehicles

06:09.680 --> 06:13.120
on, you know, like the lunar land or things like that.

06:13.120 --> 06:18.480
I imagine it was that there's quite a history in aeronautics and aerospace departments

06:18.480 --> 06:19.480
around this kind of work.

06:19.480 --> 06:20.480
Is that the case?

06:20.480 --> 06:21.480
Yes.

06:21.480 --> 06:22.480
Yes, definitely.

06:22.480 --> 06:27.960
So I've heard aerospace described as the department that is or studies the system of systems.

06:27.960 --> 06:31.760
Which is exactly what autonomous vehicles are, exactly what planes are, but it's really

06:31.760 --> 06:36.040
thinking about how all these different components and the pilots and autonomy and all these things

06:36.040 --> 06:37.040
sort of fit together.

06:37.040 --> 06:39.200
So yeah, definitely.

06:39.200 --> 06:43.480
And exactly what you said, there's a long, long history of dealing with automation in

06:43.480 --> 06:44.480
the aerospace industry.

06:44.480 --> 06:49.120
So it really sort of fits nicely into a lot of the same curtain.

06:49.120 --> 06:50.120
Awesome.

06:50.120 --> 06:51.120
Awesome.

06:51.120 --> 06:53.520
So what's your research focus there?

06:53.520 --> 06:58.520
So here I am currently working on generally autonomous vehicles.

06:58.520 --> 07:02.200
So a lot of the decision making and control.

07:02.200 --> 07:06.880
So how can we use things like deep learning to come up with very human-like decisions?

07:06.880 --> 07:07.880
Okay.

07:07.880 --> 07:13.080
And how then do we use these sort of, perhaps not the most trustworthy systems like deep

07:13.080 --> 07:14.560
learning can be?

07:14.560 --> 07:18.160
How can we design that robust controllers to execute these decisions?

07:18.160 --> 07:22.560
So how do we still get some of the robustness from the more traditional learning control techniques

07:22.560 --> 07:25.800
while using these more advanced sort of AI tools?

07:25.800 --> 07:26.800
Hmm.

07:26.800 --> 07:30.520
So what do you mean when you say human-like driving?

07:30.520 --> 07:31.520
Right.

07:31.520 --> 07:35.680
So I think, well, personally, when I think about autonomous vehicles, hitting or coming

07:35.680 --> 07:40.280
onto the road relatively soon, sorry, no more puns.

07:40.280 --> 07:43.400
We need to think about how they'll integrate with the human drivers that are currently

07:43.400 --> 07:44.400
on the road.

07:44.400 --> 07:50.200
We can't expect that there's going to be homogeneous autonomous vehicles anytime soon.

07:50.200 --> 07:54.200
So they need to be able to interact with other humans on the road, and the other humans

07:54.200 --> 07:57.440
on the road need to be able to interact with that autonomous vehicle.

07:57.440 --> 08:01.040
So you can't have the autonomous vehicle doing things that are unexpected, even if they

08:01.040 --> 08:07.880
might be optimal in some sense, they need to be optimal in the social context that it

08:07.880 --> 08:09.520
will be driving in.

08:09.520 --> 08:10.520
Mm-hmm.

08:10.520 --> 08:15.680
And so responding to erratic St. Louis or New York drivers, those are the ones that I know

08:15.680 --> 08:20.160
the best, but I'm sure everyone has their, I guess, probably wherever you are.

08:20.160 --> 08:21.160
Yep.

08:21.160 --> 08:22.160
The drivers that you hate.

08:22.160 --> 08:23.160
Yep.

08:23.160 --> 08:24.160
California stops.

08:24.160 --> 08:25.160
Exactly.

08:25.160 --> 08:26.160
Exactly.

08:26.160 --> 08:27.160
So you mentioned that.

08:27.160 --> 08:34.960
And then you also mentioned an aspect of the research that is, so we're doing all these

08:34.960 --> 08:37.560
things to build out these deep learning models.

08:37.560 --> 08:42.400
And then maybe we think about the deep learning models as directly controlling things, but

08:42.400 --> 08:45.880
it sounds like you're suggesting that a big part of your research is, well, maybe if we

08:45.880 --> 08:52.880
put some other stuff between the deep learning models and the drive by wire systems, you

08:52.880 --> 08:54.880
know, we can get better results.

08:54.880 --> 08:55.880
Yes.

08:55.880 --> 08:56.880
Yes.

08:56.880 --> 09:02.080
And I think, from my perspective, it's really important, until you can really prove things

09:02.080 --> 09:05.880
about how the deep learning and the decision making that comes from these learning models

09:05.880 --> 09:08.200
or how they're going to function.

09:08.200 --> 09:13.240
I think it is important that you have sort of some more reliable method for actually executing

09:13.240 --> 09:18.440
these maneuvers and making sure that they're safe and interpretable even.

09:18.440 --> 09:26.560
So can you maybe walk us through some of the specifics about the research in each of

09:26.560 --> 09:27.560
these camps?

09:27.560 --> 09:34.600
I guess we can start with what are all of the research challenges associated with autonomous

09:34.600 --> 09:39.000
vehicles interacting with human driven vehicles?

09:39.000 --> 09:40.000
Yeah.

09:40.000 --> 09:41.000
Sure.

09:41.000 --> 09:49.280
I think one of the really big problems is that you basically, you can model people, but

09:49.280 --> 09:54.000
every model is only going to, or it's going to have places where it fails.

09:54.000 --> 09:59.960
So coming up with a way to balance, you know, a general model of how people typically behave,

09:59.960 --> 10:05.640
while still capturing sort of the crazy things that people also do, is a really hard problem.

10:05.640 --> 10:11.160
As you get something like, you're either not accounting for the places where you really

10:11.160 --> 10:12.600
need to be safe.

10:12.600 --> 10:16.960
So when people do something really unexpected or maybe do something kind of crazy, but

10:16.960 --> 10:20.440
on the flip side, you'll be over-conservative if you only model those things.

10:20.440 --> 10:22.480
So you still want to be able to get to your destination.

10:22.480 --> 10:27.880
So there's some balance between, you know, being safe and actually thriving normally.

10:27.880 --> 10:33.080
So figuring out how to balance that in an intelligent way is really, really difficult.

10:33.080 --> 10:37.680
And what's the general approach to that that you've taken in your research?

10:37.680 --> 10:41.880
So the general approach that I've taken is trying to do something like switching maneuvers.

10:41.880 --> 10:45.920
So if you have, you try and detect basically when people are starting to deviate from the

10:45.920 --> 10:47.920
normal sort of expected behaviors.

10:47.920 --> 10:52.520
If you have a typical model of how people behave, you can use that for the most part, but

10:52.520 --> 10:56.360
you have to always be aware and always be monitoring people for when they start, as I said,

10:56.360 --> 10:57.360
deviating.

10:57.360 --> 11:00.400
So you look for sort of like the anomaly driver.

11:00.400 --> 11:03.000
And then you can start saying, okay, this person's acting a little bit weird.

11:03.000 --> 11:07.320
So I'm going to be more conservative around this driver.

11:07.320 --> 11:11.080
What does it even mean to have a model of how people behave?

11:11.080 --> 11:18.240
Like I guess I think of, I think of the stuff that I've seen, you know, around deep learning

11:18.240 --> 11:25.640
is maybe like a lower level, you know, kind of lower level models or capturing lower level,

11:25.640 --> 11:30.760
you know, ideas about how to interact, how a vehicle might interact with the world.

11:30.760 --> 11:36.120
Is there also a part of the system that is, you know, trying to capture at large, like

11:36.120 --> 11:38.600
the behavior of people?

11:38.600 --> 11:39.600
Yeah.

11:39.600 --> 11:46.120
So at least in my work, there is a lot of my PhD work was coming up with general models

11:46.120 --> 11:47.280
of how people behave.

11:47.280 --> 11:51.440
So how do you take all these big data sets and come up with models that are useful from

11:51.440 --> 11:52.440
that?

11:52.440 --> 11:53.440
So yeah.

11:53.440 --> 11:59.160
So whether some examples of things that you can get a model to, that you can kind of capture

11:59.160 --> 12:04.480
in a model about a person's behavior with regard to driving in particular.

12:04.480 --> 12:08.560
So in driving in particular, we've really been thinking about lane changing behaviors.

12:08.560 --> 12:14.200
So it's a pretty common maneuver and it's pretty easy to find examples of.

12:14.200 --> 12:19.120
So in this work, we've been thinking about how people respond to merging behaviors.

12:19.120 --> 12:23.040
So if you try to cut someone off, how are they likely to respond?

12:23.040 --> 12:28.240
If you want to execute a maneuver, what sort of cues do you need to send to that person

12:28.240 --> 12:32.320
to make sure that they will actually let you in if the gap is not big enough?

12:32.320 --> 12:36.960
So how do you sort of handle these like social nuances in your motion and in your trajectory

12:36.960 --> 12:37.960
planning?

12:37.960 --> 12:38.960
Okay.

12:38.960 --> 12:42.680
What's the general approach you've taken to address that kind of thing?

12:42.680 --> 12:48.640
Like is it changes to the way you model or is it, you know, a set of heuristics that

12:48.640 --> 12:54.120
you kind of build around the model or are you injecting things into the system otherwise?

12:54.120 --> 12:55.120
Right.

12:55.120 --> 12:59.280
So in this original work for modeling how people behave and how people respond, we were

12:59.280 --> 13:02.240
really trying to think about how we can come up with a robust prediction.

13:02.240 --> 13:07.120
So we actually came up with a new modeling method to capture these both behaviors in

13:07.120 --> 13:09.040
a sort of a more general fashion.

13:09.040 --> 13:14.320
So instead of thinking about trying to predict a person by guessing their exact trajectory,

13:14.320 --> 13:18.400
we started thinking about how we can come up with basically sets that they might follow.

13:18.400 --> 13:21.640
So we think of an area that they might enter basically.

13:21.640 --> 13:25.240
So when you start thinking about things in terms of set behavior instead of just an exact

13:25.240 --> 13:27.840
trajectory, you get a much more robust prediction.

13:27.840 --> 13:31.360
So you might be off a little bit but you'll still capture the general behavior.

13:31.360 --> 13:36.120
And this is sort of where what I was mentioning before with balancing being over conservative

13:36.120 --> 13:39.120
and being quite precise kind of comes in.

13:39.120 --> 13:43.440
So when you start basically reducing the uncertainty in your prediction, you'll shrink

13:43.440 --> 13:46.400
this set down to something that's smaller and more precise.

13:46.400 --> 13:50.840
But if you're more uncertain or you start detecting some anomalies, you can grow this

13:50.840 --> 13:53.520
set out and capture more of the uncertainty.

13:53.520 --> 13:56.480
And that will just automatically influence how you change.

13:56.480 --> 14:00.440
So if you basically take these sets and incorporate them into your low level controller that

14:00.440 --> 14:05.120
is planning and trying to keep you in safe regions, this will sort of automatically be captured

14:05.120 --> 14:06.600
by that.

14:06.600 --> 14:13.480
And in this work, does the set represent are you evaluating the set in terms of kind

14:13.480 --> 14:21.920
of, you know, in or out or likelihood of in or out or are you looking at like geographic

14:21.920 --> 14:26.600
regions as probabilistic fields that are maybe more continuous?

14:26.600 --> 14:27.600
Right.

14:27.600 --> 14:28.600
Right.

14:28.600 --> 14:29.600
So a little bit of both.

14:29.600 --> 14:35.560
So the ultimate or the output of this model is something like confidence intervals.

14:35.560 --> 14:39.680
So you get these sort of strict boundaries and a probability associated with these different

14:39.680 --> 14:43.640
basically level sets of trajectories. So it's a little bit of both.

14:43.640 --> 14:48.440
So once you pick what confidence interval you would like to pick, you have a strict set

14:48.440 --> 14:51.840
and you can basically evaluate this by inner out.

14:51.840 --> 14:56.760
But you can also look at this as basically a series of confidence intervals.

14:56.760 --> 15:00.120
So then you get something like an empirical distribution back out.

15:00.120 --> 15:01.120
Okay.

15:01.120 --> 15:02.120
Okay.

15:02.120 --> 15:09.280
When you're implementing this in the lane changing context, is it, I guess I'm trying

15:09.280 --> 15:15.600
to to picture the, I guess, dimensionality is overloaded.

15:15.600 --> 15:21.520
But like if you are you thinking about it from the perspective of a car and like the lane

15:21.520 --> 15:27.760
ahead, you know, how many feet from the vehicle, another vehicle is likely to intrude on.

15:27.760 --> 15:33.920
So like a two dimensional kind of interaction or is it more a three dimensional interaction

15:33.920 --> 15:40.000
where you are thinking about the distance between your car and your lane and the other car

15:40.000 --> 15:44.680
in the other lane as well as where it might intrude into your lane?

15:44.680 --> 15:46.360
That's a great question.

15:46.360 --> 15:52.920
So in most of the work that I've been doing, I've been looking at basically a set of three

15:52.920 --> 15:54.160
vehicle interactions.

15:54.160 --> 15:58.080
So if you think about predicting the vehicle that you want to merge in front of, you can

15:58.080 --> 16:02.160
think about the influences of that vehicle basically being what your actions will be in

16:02.160 --> 16:03.760
a vehicle in front of them.

16:03.760 --> 16:08.200
We've been sort of looking at that network, but we can expand this out by sort of looking

16:08.200 --> 16:10.240
at things like clusters of behavior.

16:10.240 --> 16:16.800
So if you think about different vehicles, sort of inserting more influences on vehicle,

16:16.800 --> 16:20.440
you can think about how this sort of generally happens by looking at clusters of behavior.

16:20.440 --> 16:24.600
So if there's more vehicles, you can basically do a look up and get more clusters or similar

16:24.600 --> 16:26.600
clusters, similar situations.

16:26.600 --> 16:30.960
And by the similarity metric, you can expand this out to more vehicles and larger networks

16:30.960 --> 16:32.440
and things like that.

16:32.440 --> 16:33.440
Okay.

16:33.440 --> 16:34.440
Yeah.

16:34.440 --> 16:37.880
I've seen some interesting videos of how to driver behavior.

16:37.880 --> 16:46.040
And I think even autonomous vehicle interactions with drivers where, you know, the vehicle isn't

16:46.040 --> 16:48.800
given a certain level of aggressiveness.

16:48.800 --> 16:53.560
It will just get stuck like it cannot execute a lane change.

16:53.560 --> 17:00.000
And so there's, you know, there's this need that you're describing for the vehicle to,

17:00.000 --> 17:06.240
you know, not just be able to anticipate human behavior, but to start to emulate human behavior

17:06.240 --> 17:10.960
because like signaling to go into another lane isn't just, you know, putting on the blinkers.

17:10.960 --> 17:14.240
It's also like starting to go into the other lane.

17:14.240 --> 17:15.240
Exactly.

17:15.240 --> 17:16.240
Exactly.

17:16.240 --> 17:19.560
I was talking to someone in LA about this and they said, if you turn your blinker on that

17:19.560 --> 17:21.640
is a cue for the other driver to just speed up.

17:21.640 --> 17:22.640
Exactly.

17:22.640 --> 17:24.440
So you don't just want that.

17:24.440 --> 17:25.440
Right.

17:25.440 --> 17:31.240
So you've got, you've got this ability to kind of predict other driver's behaviors and

17:31.240 --> 17:40.360
when they're likely to enter your lane and you can kind of expand that to multiple vehicles.

17:40.360 --> 17:42.440
I, how do you, what's the next step?

17:42.440 --> 17:46.680
How do you kind of build on that to create a more robust system?

17:46.680 --> 17:47.680
Right.

17:47.680 --> 17:52.720
So now that we have a good model of our environment, if we believe this is a good model of our environment.

17:52.720 --> 17:56.960
So the next layer that we were thinking about is how can we safely execute sort of high

17:56.960 --> 17:57.960
level decisions.

17:57.960 --> 18:00.440
And this is where we started using deep learning.

18:00.440 --> 18:04.800
So if we have a good representation of the environment we want to think about, should

18:04.800 --> 18:06.240
I execute a lane change?

18:06.240 --> 18:08.800
Will this help me achieve my goal?

18:08.800 --> 18:13.400
And also there is some, we wanted to see if we could capture things like sort of implicit

18:13.400 --> 18:14.400
rules.

18:14.400 --> 18:19.360
So there are some rules that aren't necessarily captured by the letter of the law.

18:19.360 --> 18:22.560
So like at intersections, there's a strict ordering.

18:22.560 --> 18:23.560
That happens.

18:23.560 --> 18:27.080
So if one vehicle comes first, they get to pass through.

18:27.080 --> 18:29.880
There's some yielding rules, but people don't always follow these rules.

18:29.880 --> 18:32.760
There's so much certainty in these rules.

18:32.760 --> 18:37.520
So we've been using deep learning and simulation to try and capture some of these sort of nuance

18:37.520 --> 18:40.880
behaviors to come up with these sort of high level decisions.

18:40.880 --> 18:42.320
Like what high level action should I do?

18:42.320 --> 18:46.280
So these high level actions can be things like execute a lane change or go ahead and move

18:46.280 --> 18:48.200
through the intersection now.

18:48.200 --> 18:50.720
And so we've been doing that with, as I mentioned, deep warning.

18:50.720 --> 18:54.040
But deep learning in and of itself is not very trustworthy.

18:54.040 --> 18:58.200
So a lot of what we've been doing is trying to think about how we can develop new tools

18:58.200 --> 19:03.360
and new learning algorithms to try and make these systems more, or give some insight to

19:03.360 --> 19:05.000
the confidence of the system.

19:05.000 --> 19:09.840
So can we determine when our deep learning algorithm is uncertain about its decision?

19:09.840 --> 19:12.480
And if we know when it's uncertain, we can decide whether or not we should listen

19:12.480 --> 19:14.120
to it or not.

19:14.120 --> 19:18.120
And this example, what's the training data?

19:18.120 --> 19:23.440
So in this example, we've been putting a lot of effort into coming up with good simulated

19:23.440 --> 19:24.720
traffic models.

19:24.720 --> 19:28.840
So we can basically train in simulation and then transfer it to a real vehicle, which

19:28.840 --> 19:32.360
is a whole other problem that we're also working on.

19:32.360 --> 19:33.360
Okay.

19:33.360 --> 19:41.920
So the simulated traffic data is, I mean, I'm envisioning like the video game Frogger.

19:41.920 --> 19:46.800
You've just got all this traffic and you're, it's kind of moving at different speeds

19:46.800 --> 19:53.200
and is that how you're not literally, but is, are you just generating several lanes

19:53.200 --> 19:59.000
of traffic and how are you representing the vehicles in this training data set, for example?

19:59.000 --> 20:00.000
Right.

20:00.000 --> 20:04.160
So there's a lot of work in the lab that I'm in that goes into coming up with good driver

20:04.160 --> 20:07.160
models for generating traffic.

20:07.160 --> 20:11.560
So there's lots of really interesting work going on and actually using deep learning to

20:11.560 --> 20:16.360
mimic these driving behaviors so you can validate your autonomous vehicle.

20:16.360 --> 20:18.400
So how do you generate these scenarios?

20:18.400 --> 20:21.160
How do you generate realistic behavior?

20:21.160 --> 20:25.760
So then you can either train your system or just do some validation.

20:25.760 --> 20:30.120
So if you just need to make sure it works or you get some metric for how likely this

20:30.120 --> 20:34.200
should crash, you can use these validation tools to do that.

20:34.200 --> 20:35.200
Mm-hmm.

20:35.200 --> 20:38.480
I'm still trying to visualize the training data set.

20:38.480 --> 20:45.840
Is it like are you looking at a car as a, like a two dimensional kind of representation

20:45.840 --> 20:51.600
of points or space or something like that or are there some simplifying assumptions

20:51.600 --> 20:56.920
or is it maybe more complex than that, you know, based on camera imagery or something?

20:56.920 --> 21:00.880
So these ones we are just using, we have a simulated LiDAR sensor.

21:00.880 --> 21:05.560
So from our ego vehicle, we basically just use these detection points, which are pretty

21:05.560 --> 21:09.920
similar to what we can extract from sensors on a real vehicle.

21:09.920 --> 21:14.560
So from the ego vehicle's perspective in simulation, you basically just get something

21:14.560 --> 21:18.880
like a LiDAR image, but projected down to just a 2D plane.

21:18.880 --> 21:19.880
Okay.

21:19.880 --> 21:20.880
Got it.

21:20.880 --> 21:29.200
So is it the 2D plane from above the vehicle or the 2D plane like looking ahead from a vehicle?

21:29.200 --> 21:32.600
So for us, you can think of it as like an occupancy grid so you can like look down at the

21:32.600 --> 21:37.320
world, your vehicles in the center of it, you get sort of this distance around the vehicle

21:37.320 --> 21:38.320
there.

21:38.320 --> 21:39.320
Okay.

21:39.320 --> 21:40.320
Okay.

21:40.320 --> 21:45.280
So the question was around the objective function, like how did you, how did you construct

21:45.280 --> 21:48.840
the objective function for this model that you trained?

21:48.840 --> 21:49.840
Right.

21:49.840 --> 21:55.800
So for these initial models to start, what we were using was a, some tools called imitation

21:55.800 --> 21:56.800
learning.

21:56.800 --> 22:03.120
So we basically wanted to figure out how we can imitate a model or imitate some expert

22:03.120 --> 22:04.120
model.

22:04.120 --> 22:06.320
So we have some sort of expert behavior that we want to mimic.

22:06.320 --> 22:08.760
So this can be a human, for example.

22:08.760 --> 22:13.040
So if we have some examples of how the human is going to behave or some example trajectories

22:13.040 --> 22:18.880
of this person driving, we basically want to try it and be as similar to this driver as

22:18.880 --> 22:21.120
possible or similar to this human.

22:21.120 --> 22:26.840
So mimicking the expert in training and transferring this knowledge over to our novice or our

22:26.840 --> 22:28.360
deep learning algorithm.

22:28.360 --> 22:29.360
Okay.

22:29.360 --> 22:31.880
So you've got your driver behavior model.

22:31.880 --> 22:39.480
You've trained a deep learning model that can try to optimize an objective relative

22:39.480 --> 22:44.960
to, you know, some expert that it's imitating what, what's next.

22:44.960 --> 22:49.840
So then once we have a model that can effectively mimic these high level decisions, hopefully

22:49.840 --> 22:53.200
pretty well and hopefully in a safe way, that's when we started thinking about how we can

22:53.200 --> 22:55.520
actually implement this in a robust controller.

22:55.520 --> 23:01.600
So we've been using some pretty standard tools from control, like model predictive control

23:01.600 --> 23:07.480
and robust control so we can take these high level commands and turn them into trajectories

23:07.480 --> 23:10.800
that the vehicle then can follow quite smoothly.

23:10.800 --> 23:14.800
So basically using these commands or these high level commands from the deep learning,

23:14.800 --> 23:19.040
now you can sort of start thinking about different models and sort of some of the differences

23:19.040 --> 23:21.480
between the simulation and the real world car.

23:21.480 --> 23:26.760
So you can sort of extract the high level information and execute it in the real vehicle.

23:26.760 --> 23:28.160
So that's actually what we're testing now.

23:28.160 --> 23:32.320
So we're putting this on a real vehicle and testing all of that.

23:32.320 --> 23:33.320
Nice, nice.

23:33.320 --> 23:39.640
You mentioned a couple of disciplines in control systems, robust control and model, something

23:39.640 --> 23:40.640
control.

23:40.640 --> 23:41.640
Model predictive control.

23:41.640 --> 23:42.640
Model predictive control.

23:42.640 --> 23:47.120
Can you walk us through what those are and the assumptions that they're making and what

23:47.120 --> 23:48.440
they're trying to accomplish?

23:48.440 --> 23:49.440
Yeah, yeah.

23:49.440 --> 23:53.640
So the heart of all of what we do is model predictive control and then model predictive control.

23:53.640 --> 23:56.840
So basically using a model of your vehicle and the environment.

23:56.840 --> 24:01.480
It basically solves an optimization problem to give you an optimal trajectory over some

24:01.480 --> 24:03.360
finite time horizon.

24:03.360 --> 24:07.680
So say two seconds in the future, I'm going to plan the optimal trajectory to achieve

24:07.680 --> 24:08.680
my goal.

24:08.680 --> 24:13.520
And since this is just an optimization program, effectively you can easily put in things

24:13.520 --> 24:17.560
like safety constraints, you can tune your trajectory.

24:17.560 --> 24:23.000
So you have a smooth trajectory and basically by solving this problem, you can come up with

24:23.000 --> 24:24.680
your optimal trajectory.

24:24.680 --> 24:28.560
And the kind of cool thing about model predictive control is even though it's a finite time

24:28.560 --> 24:29.560
horizon.

24:29.560 --> 24:33.440
So it only works for about two seconds in the future, whatever your finite time is.

24:33.440 --> 24:35.040
And it's an open loop trajectory.

24:35.040 --> 24:38.440
You basically take one step in the future and then you re-solve the problem.

24:38.440 --> 24:42.960
So you re-solve this trajectory or for this trajectory at each time step.

24:42.960 --> 24:47.400
And so by doing this, receding horizon by sort of sliding along and constantly planning

24:47.400 --> 24:51.320
some time in the future, you actually approximate things like the infinite horizon.

24:51.320 --> 24:53.880
So basically the optimal control policy.

24:53.880 --> 24:57.680
So it's an open loop policy executed in a close up action.

24:57.680 --> 25:05.720
I was recently reading some reviews of some of the production driver assistance, like

25:05.720 --> 25:10.920
autonomous driver assistance, if you will, technologies like Cadillac has one, Mercedes

25:10.920 --> 25:11.920
has one.

25:11.920 --> 25:13.800
Obviously Tesla, I forget the other one.

25:13.800 --> 25:16.600
I think it was infinity that was in this article.

25:16.600 --> 25:23.160
And they talked about how one of the things that they noticed was that for most of these

25:23.160 --> 25:26.280
systems, I think Tesla was the only exception.

25:26.280 --> 25:30.440
Like the car would basically bounce back and forth between the lane markers.

25:30.440 --> 25:38.000
And it, and I'm speculating a little bit, but it sounds like what you're doing with model

25:38.000 --> 25:44.520
predictive control would tend to smooth out that kind of effect as opposed to, you know,

25:44.520 --> 25:48.760
maybe deriving a path straight out of a deep learning model.

25:48.760 --> 25:51.320
Like is that a reasonable kind of intuition about this?

25:51.320 --> 25:53.720
Or does it show up in other ways?

25:53.720 --> 25:55.080
No, that's exactly right.

25:55.080 --> 25:59.920
So by using this basically constant smoothing and optimization technique, you do come up with

25:59.920 --> 26:01.880
much smoother trajectories.

26:01.880 --> 26:06.480
It not only addresses things like the sort of jerky chattering, I guess that happens when

26:06.480 --> 26:11.560
you sort of oscillate between lanes, but it also helps for things like overshoot and

26:11.560 --> 26:14.800
things like that as well, or is jumping back and forth when you do things like turning,

26:14.800 --> 26:18.920
that usually comes from common planning and techniques like that.

26:18.920 --> 26:23.960
And now you're making me remember, you know, grad school control systems courses with

26:23.960 --> 26:30.440
like dampening and all these other things that you need to think about to avoid, you know,

26:30.440 --> 26:31.640
overshooting and oscillation.

26:31.640 --> 26:32.640
Yep, yep.

26:32.640 --> 26:33.640
Yeah.

26:33.640 --> 26:37.640
It's amazing how now that I'm implementing things on a real vehicle, how all of these

26:37.640 --> 26:41.880
original controls that are really coming back, I think I haven't thought about like 10

26:41.880 --> 26:42.880
years.

26:42.880 --> 26:43.880
Nice.

26:43.880 --> 26:44.880
Nice.

26:44.880 --> 26:45.880
Very important.

26:45.880 --> 26:50.560
What are you seeing as you're trying to, you know, go from these models to implementing

26:50.560 --> 26:52.320
them on a real vehicle?

26:52.320 --> 26:57.520
Are you finding that, are you surprised by anything or are there things that, you know,

26:57.520 --> 27:01.880
were working fine in simulation, but, you know, you needed to be tweaked as you moved

27:01.880 --> 27:03.320
to a real vehicle?

27:03.320 --> 27:04.320
Yes.

27:04.320 --> 27:10.920
I think one of my favorite quotes was, or is, everything is doomed to work in simulation.

27:10.920 --> 27:11.920
So.

27:11.920 --> 27:12.920
Nice.

27:12.920 --> 27:17.640
We have all these nice simulation tools, but, and we think we have things working really

27:17.640 --> 27:22.720
well, but a lot of it is still has to be hand-tuned and it's amazing how some of these simple

27:22.720 --> 27:26.800
things are, especially when you kind of get caught up in a lot of the really cool stuff

27:26.800 --> 27:30.600
that's happening in AI, you think a lot of the really cool stuff comes out of the deep

27:30.600 --> 27:34.680
learning, but when you go to actually test things on the vehicle, so much of it comes

27:34.680 --> 27:37.120
down to this low-level control, actually.

27:37.120 --> 27:42.400
So it is kind of amazing how much time is actually spent on the, I guess, more traditional

27:42.400 --> 27:43.400
things.

27:43.400 --> 27:46.320
So as you said, like, the dampening and making sure everything is smooth and tuning

27:46.320 --> 27:47.880
cost functions and things like that.

27:47.880 --> 27:53.760
So I think there's still a lot of work that has to go into the end to end stuff to make

27:53.760 --> 27:55.840
sure it works well.

27:55.840 --> 28:06.320
And is there kind of research into applying some of the ways we optimize machine learning

28:06.320 --> 28:13.200
to optimizing, you know, these control systems, like, you know, can you think of, can you

28:13.200 --> 28:18.640
apply hyperparameter optimization to these control systems to find the right dampening

28:18.640 --> 28:24.400
constants and all that kind of stuff, or is it, are the, you know, traditional techniques

28:24.400 --> 28:25.760
kind of good enough there?

28:25.760 --> 28:30.120
Yeah, so I think you can do some of those things, but again, you still have this problem

28:30.120 --> 28:36.080
where you ultimately have to put all this on a vehicle, you have to test it there,

28:36.080 --> 28:41.040
and because you're testing on a real vehicle that has had a lot of very expensive equipment

28:41.040 --> 28:42.040
on it.

28:42.040 --> 28:46.840
You have to be kind of careful with what you are willing to test and make sure you're

28:46.840 --> 28:47.840
very confident in this.

28:47.840 --> 28:52.640
So you can't do a full, you know, cross validation set on there unless you're confident

28:52.640 --> 28:55.120
that it's going to work really well.

28:55.120 --> 28:59.520
But yeah, there's some work that we've been working on in robust reinforcement learning.

28:59.520 --> 29:05.200
So trying to do things like take into account some of the uncertainty that we think we might

29:05.200 --> 29:09.720
see or some of the things like model mismatch and trying to create basically make sure

29:09.720 --> 29:15.000
that our control systems and our deep learning algorithms are robust to these uncertainties.

29:15.000 --> 29:21.040
So I think that should help it, but we'll see if that actually works or not when we actually

29:21.040 --> 29:23.440
go to the real vehicle without work.

29:23.440 --> 29:25.240
Can you elaborate on that work a little bit more?

29:25.240 --> 29:26.240
Yeah, sure.

29:26.240 --> 29:31.040
So we're building up with some tools in from what's called robust adversarial reinforcement

29:31.040 --> 29:32.040
learning.

29:32.040 --> 29:36.720
So in this framework, the basic idea is if you have some uncertainties in your model

29:36.720 --> 29:41.120
or uncertainties in your environment, you can actually treat this as a game between

29:41.120 --> 29:42.120
two players.

29:42.120 --> 29:47.200
So if you have your controller, which is we'll call it the protagonist and your antagonist,

29:47.200 --> 29:51.000
which is basically these disturbances or these uncertainties that are trying to sort

29:51.000 --> 29:53.600
of perturb your model in a negative way.

29:53.600 --> 29:57.560
So if you basically treat these both as agents that you want to train a model for, you can

29:57.560 --> 30:03.240
train your protagonist to achieve some goal and then you can train your antagonist to basically

30:03.240 --> 30:06.160
get you to not achieve that goal.

30:06.160 --> 30:11.480
So by training these two, basically, your antagonist and your protagonist iteratively,

30:11.480 --> 30:14.560
you develop something that approximates a robust controller.

30:14.560 --> 30:20.880
So you have your ultimate system is able to handle things like uncertainty in your model

30:20.880 --> 30:26.360
because you've been training it against a adversarial uncertainty or disturbance.

30:26.360 --> 30:30.200
So that's one way we're trying to deal with these model mismatches.

30:30.200 --> 30:32.000
We'll see how it works.

30:32.000 --> 30:33.960
I'll get back to you on that.

30:33.960 --> 30:34.960
Yeah.

30:34.960 --> 30:35.960
Yeah.

30:35.960 --> 30:46.120
Are you finding that there's a kind of a cultural mismatch between the folks that come

30:46.120 --> 30:54.560
from the traditional control systems perspective where you've got established practices

30:54.560 --> 31:05.600
and well-defined guarantees or at least laws of physics to kind of rely on versus the folks

31:05.600 --> 31:08.680
that are more of the deep learning camp where you're kind of just throwing a bunch of

31:08.680 --> 31:12.600
data against the wall and this thing is miraculously training itself.

31:12.600 --> 31:13.600
Yes.

31:13.600 --> 31:14.600
Yeah.

31:14.600 --> 31:17.280
There's a huge cultural gap there.

31:17.280 --> 31:20.560
So one of the things I try and do is try and sort of bridge that gap.

31:20.560 --> 31:25.800
But I think there's just a, it's almost like they're speaking different languages.

31:25.800 --> 31:29.680
These two communities tend to get a little bit territorial in what they can and can't

31:29.680 --> 31:30.680
do.

31:30.680 --> 31:31.680
So yes.

31:31.680 --> 31:32.680
In short.

31:32.680 --> 31:33.680
Awesome.

31:33.680 --> 31:44.480
Well, to close us out, what's next for you beyond getting your vehicle working, not

31:44.480 --> 31:49.120
that that's like a short task or a foregone conclusion, what are some of the other things

31:49.120 --> 31:51.400
on the horizon for you and your work?

31:51.400 --> 31:52.400
Yeah.

31:52.400 --> 31:55.760
So some of the other things I've been thinking about is thinking about autonomous vehicles

31:55.760 --> 31:58.720
at a little bit of a higher level or a broader perspective.

31:58.720 --> 32:03.360
So some of the recent work that I'm just starting has been on how can you use autonomous vehicles

32:03.360 --> 32:08.360
and some of these tools from planning to do things like assistant evacuation and disaster

32:08.360 --> 32:09.360
responses.

32:09.360 --> 32:12.160
So using some of these same tools to help there.

32:12.160 --> 32:17.480
I've also been thinking about how you can start applying some of these tools to fleets.

32:17.480 --> 32:22.960
So you can think about optimizing overall performance by making small changes on a minor level,

32:22.960 --> 32:28.200
have to do things when you have many, many vehicles all operating together.

32:28.200 --> 32:29.200
So.

32:29.200 --> 32:33.040
And so are these things like cooperative autonomy and swarming behaviors and the like?

32:33.040 --> 32:34.040
Yeah.

32:34.040 --> 32:35.040
Yeah.

32:35.040 --> 32:36.040
A little bit of that.

32:36.040 --> 32:39.200
And so I think the evacuation planning is a little bit more of the cooperative side for

32:39.200 --> 32:42.360
the fleet performance if you still have individual operators.

32:42.360 --> 32:45.560
So if you think of like a delivery fleet, you still have an individual there driving the

32:45.560 --> 32:46.880
vehicle and making decisions.

32:46.880 --> 32:52.760
So how can you optimize maybe their behaviors on a minor level and then have greater performance

32:52.760 --> 32:53.760
overall?

32:53.760 --> 32:54.760
Interesting.

32:54.760 --> 32:55.760
Interesting.

32:55.760 --> 33:00.360
So what's the best way for folks to learn more about your research or connect with you?

33:00.360 --> 33:01.360
Yeah.

33:01.360 --> 33:04.360
You can find me on the internet.

33:04.360 --> 33:06.120
So you can look at my website.

33:06.120 --> 33:13.200
So just stampord.edu, backstop, tilde, krdc, or you can shoot me an email, krdc at stampord.edu.

33:13.200 --> 33:14.200
Awesome.

33:14.200 --> 33:15.200
Awesome.

33:15.200 --> 33:21.640
Thank you so much for joining me to discuss this is really interesting research and I'm looking

33:21.640 --> 33:24.040
forward to keeping tabs on what you're up to.

33:24.040 --> 33:25.040
Thanks.

33:25.040 --> 33:26.040
Yeah.

33:26.040 --> 33:27.040
It was a great conversation.

33:27.040 --> 33:28.040
Thank you for having me.

33:28.040 --> 33:29.040
Thank you.

33:29.040 --> 33:32.840
All right, everyone.

33:32.840 --> 33:35.160
That's our show for today.

33:35.160 --> 33:40.320
Thanks so much for listening and for your continued feedback and support.

33:40.320 --> 33:45.080
For more information on Katie or any of the topics covered in this episode, head on

33:45.080 --> 33:49.440
over to twomolei.com slash talk slash 59.

33:49.440 --> 33:57.280
To follow along with the autonomous vehicle series, visit twomolei.com slash AV 2017.

33:57.280 --> 34:03.000
Of course, you can send along your feedback or questions via Twitter to at twomolei or

34:03.000 --> 34:07.960
at Sam Charrington or just leave a comment on the show notes page.

34:07.960 --> 34:11.560
Thanks again to my DAI for their sponsorship of this series.

34:11.560 --> 34:18.040
Be sure to check out my interview with their co-founder and CEO Darren Nikuda at twomolei.com slash

34:18.040 --> 34:26.520
talk slash 57 and take a look at what the company's up to at www.mty.ai.

34:26.520 --> 34:44.720
Thanks again for listening and catch you next time.

