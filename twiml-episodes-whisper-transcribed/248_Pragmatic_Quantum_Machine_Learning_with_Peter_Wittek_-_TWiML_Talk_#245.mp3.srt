1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,800
I'm your host, Sam Charrington, before we dive in I'd like to quickly share a few updates

4
00:00:34,800 --> 00:00:37,160
about our ongoing giveaways.

5
00:00:37,160 --> 00:00:42,600
If you haven't done so already you've got until 11.59pm Pacific time tonight to enter

6
00:00:42,600 --> 00:00:46,600
into our AI conference New York Bronze Pass giveaway.

7
00:00:46,600 --> 00:00:52,240
Visit twimlai.com slash AI and Y giveaway to get your entry in.

8
00:00:52,240 --> 00:00:58,760
Also the hackers among us should be sure to visit twimlai.com slash TF giveaway to enter

9
00:00:58,760 --> 00:01:01,520
our TensorFlow Edge Kit giveaway.

10
00:01:01,520 --> 00:01:06,640
Three lucky winners will receive a coral edge TPU device and the Spark Fund Edge Development

11
00:01:06,640 --> 00:01:09,400
Board amongst other goodies.

12
00:01:09,400 --> 00:01:13,320
The contest ends on April 8th.

13
00:01:13,320 --> 00:01:17,640
Today we're joined by Peter Whittick, assistant professor at the University of Toronto who

14
00:01:17,640 --> 00:01:21,880
works on quantum enhanced machine learning and the application of high performance learning

15
00:01:21,880 --> 00:01:24,840
algorithms and quantum physics.

16
00:01:24,840 --> 00:01:28,880
Peter and I caught up back in November to discuss the presentation he gave at the AWS

17
00:01:28,880 --> 00:01:33,880
Reinvent Conference, pragmatic quantum machine learning today.

18
00:01:33,880 --> 00:01:37,160
In our conversation we start with a bit of background including the current state of

19
00:01:37,160 --> 00:01:41,560
quantum computing, a look ahead to what the next 20 years of quantum computing might

20
00:01:41,560 --> 00:01:45,400
hold and how current quantum computers are flawed.

21
00:01:45,400 --> 00:01:49,400
We then dive into our discussion on quantum machine learning and Peter's new course on

22
00:01:49,400 --> 00:01:52,200
the topic which debuted in February.

23
00:01:52,200 --> 00:01:54,200
I'll link to that in the show notes.

24
00:01:54,200 --> 00:01:59,240
Finally, we briefly discuss the work of E1 Tang, a PhD student from the University of

25
00:01:59,240 --> 00:02:04,440
Washington whose undergrad thesis a quantum inspired classical algorithm for recommendation

26
00:02:04,440 --> 00:02:07,640
systems made quite a stir last summer.

27
00:02:07,640 --> 00:02:12,200
As a special treat for those interested, I'm also sharing my interview with E1 as a bonus

28
00:02:12,200 --> 00:02:14,560
episode alongside this one.

29
00:02:14,560 --> 00:02:18,520
I'd love to hear your thoughts on how you think quantum computing will impact machine

30
00:02:18,520 --> 00:02:21,080
learning in the next 20 years.

31
00:02:21,080 --> 00:02:24,520
Send me a tweet or leave a comment on the show notes page.

32
00:02:24,520 --> 00:02:29,680
I'd like to thank Pegasystems, this episode sponsor, and join them in inviting you to

33
00:02:29,680 --> 00:02:35,200
Pegaworld, the company's annual digital transformation conference to be held at the MGM Grand

34
00:02:35,200 --> 00:02:38,840
in Las Vegas from June 2nd to 5th.

35
00:02:38,840 --> 00:02:43,400
Pegasystems puts AI at the center of its customer engagement software so that every customer

36
00:02:43,400 --> 00:02:49,480
touchpoint on every channel is optimized in real time, so that customers find each interaction

37
00:02:49,480 --> 00:02:54,560
relevant and timely, whether a sales call, a marketing campaign, or a customer service

38
00:02:54,560 --> 00:02:55,880
chat.

39
00:02:55,880 --> 00:03:00,160
At Pegaworld, you'll hear great stories of AI applied to the customer experience at

40
00:03:00,160 --> 00:03:02,240
real Pegacustomers.

41
00:03:02,240 --> 00:03:06,600
The event is a great way to learn from a who's who of the Fortune 500, and of course,

42
00:03:06,600 --> 00:03:10,080
I'll be there, and we'll be speaking as well.

43
00:03:10,080 --> 00:03:15,720
To register, visit Pegaworld.com and use the promo code Twimble19 when you sign up for

44
00:03:15,720 --> 00:03:16,720
$200 off.

45
00:03:16,720 --> 00:03:20,760
Again, that's Twimble19, it's as easy as that.

46
00:03:20,760 --> 00:03:23,000
Hope to see you there.

47
00:03:23,000 --> 00:03:25,920
And now onto the show.

48
00:03:25,920 --> 00:03:33,280
All right, everyone, I am on the line with Rob Walker. Rob is the vice president of

49
00:03:33,280 --> 00:03:36,600
decision management and analytics with Pegasystems.

50
00:03:36,600 --> 00:03:44,080
You may remember Rob's name from TwimbleTalk number 127 on hyperpersonalizing the customer

51
00:03:44,080 --> 00:03:46,360
experience with AI.

52
00:03:46,360 --> 00:03:49,000
Rob, welcome back to this week in machine learning and AI.

53
00:03:49,000 --> 00:03:50,000
Yeah, thanks, Sam.

54
00:03:50,000 --> 00:03:52,000
Glad to be back.

55
00:03:52,000 --> 00:03:53,000
Absolutely.

56
00:03:53,000 --> 00:03:54,000
Glad to have you back.

57
00:03:54,000 --> 00:03:59,760
I'm looking forward to what I believe will be a really interesting conversation on the

58
00:03:59,760 --> 00:04:03,240
role of empathy in AI.

59
00:04:03,240 --> 00:04:08,760
For folks that want to learn more about you, I'll refer them back to the previous episode,

60
00:04:08,760 --> 00:04:12,480
but give us a quick overview of your focus at Pegasystems.

61
00:04:12,480 --> 00:04:14,680
Yes, I'm happy to do that.

62
00:04:14,680 --> 00:04:23,160
So within Pegas, I'm responsible for our AI space, but we really try, I mean, there's

63
00:04:23,160 --> 00:04:28,240
so much hype around AI and we don't do AI just for AI sake.

64
00:04:28,240 --> 00:04:35,880
We really try to focus on making AI work for typically pretty large enterprises and

65
00:04:35,880 --> 00:04:39,240
typically in the area of customer engagement, right?

66
00:04:39,240 --> 00:04:44,800
So in the previous episode, we talked about hyperpersonalization and hyperpersonalization

67
00:04:44,800 --> 00:04:52,760
is really trying to be one-to-one conversations with customers for companies to do that.

68
00:04:52,760 --> 00:04:56,840
And that requires a lot of AI.

69
00:04:56,840 --> 00:05:01,920
It also requires lots of other things, but AI is an important aspect of that.

70
00:05:01,920 --> 00:05:04,640
And that's what I mostly worry about.

71
00:05:04,640 --> 00:05:07,800
And also areas around AI.

72
00:05:07,800 --> 00:05:09,520
It's not just, hey, AI is cool.

73
00:05:09,520 --> 00:05:13,000
Let's use that in customer engagement to make customer engagement better, but it's

74
00:05:13,000 --> 00:05:16,800
stuff like, can we trust it?

75
00:05:16,800 --> 00:05:20,040
Who in the organization should be responsible for it?

76
00:05:20,040 --> 00:05:24,840
If it makes weird decisions, if there is a bias, those kind of things.

77
00:05:24,840 --> 00:05:28,120
So that's typically what I worry about.

78
00:05:28,120 --> 00:05:33,720
The concept that kept coming up in our last conversation, I think this is pretty central

79
00:05:33,720 --> 00:05:40,720
to the way you think about applying AI to optimizing customer experiences.

80
00:05:40,720 --> 00:05:47,000
This idea of helping your customers figure out the next best action to take with their

81
00:05:47,000 --> 00:05:48,000
customers.

82
00:05:48,000 --> 00:05:49,000
Is that right?

83
00:05:49,000 --> 00:05:50,000
That is correct.

84
00:05:50,000 --> 00:05:51,000
Yes.

85
00:05:51,000 --> 00:05:52,000
Yes.

86
00:05:52,000 --> 00:05:58,520
So the companies we work with typically implement like this customer decision hub, right?

87
00:05:58,520 --> 00:06:04,920
And it's a centralized decision authority that across all the different channels that

88
00:06:04,920 --> 00:06:13,160
companies may have, figures out the next best action during conversations, right?

89
00:06:13,160 --> 00:06:14,680
So what should we say?

90
00:06:14,680 --> 00:06:16,840
What should we not say?

91
00:06:16,840 --> 00:06:24,600
What price should we mention if it's commercial, basically trying to have very reasonable

92
00:06:24,600 --> 00:06:30,720
conversations, but at the same time, because most of the companies we work with are not

93
00:06:30,720 --> 00:06:32,680
just charities, right?

94
00:06:32,680 --> 00:06:37,600
So at the same time, you need to sort of improve customer value, right?

95
00:06:37,600 --> 00:06:43,240
So the next best action, the best in next best action, is typically some metric about customer

96
00:06:43,240 --> 00:06:49,800
value and trying to improve that over time by doing the best thing possible to optimize

97
00:06:49,800 --> 00:06:50,800
that.

98
00:06:50,800 --> 00:06:58,600
So this concept of empathy in AI is something that you'll be speaking about at the next

99
00:06:58,600 --> 00:07:04,000
Pegaworld, an event that your company hosts annually.

100
00:07:04,000 --> 00:07:07,920
I attended the last one and I'll be attending the next one.

101
00:07:07,920 --> 00:07:15,400
How did this idea of empathy, introducing empathy into these kinds of transactions or customer

102
00:07:15,400 --> 00:07:17,200
experiences?

103
00:07:17,200 --> 00:07:18,880
Where did that come from?

104
00:07:18,880 --> 00:07:21,440
Well, so I've always been interested.

105
00:07:21,440 --> 00:07:27,480
So before I joined Pegaw at some point before that I was a scientist, right, in AI.

106
00:07:27,480 --> 00:07:35,160
I did my PhD in that area and I've always been interested in not just all the cool things

107
00:07:35,160 --> 00:07:41,440
AI can do around predicting customer behavior and things like that, but also potentially

108
00:07:41,440 --> 00:07:43,080
the not so cool things, right?

109
00:07:43,080 --> 00:07:45,440
So can you trust it?

110
00:07:45,440 --> 00:07:46,640
Is it transparent?

111
00:07:46,640 --> 00:07:48,400
Is it opaque?

112
00:07:48,400 --> 00:07:50,240
Is there a bias?

113
00:07:50,240 --> 00:07:51,240
Can it go rogue?

114
00:07:51,240 --> 00:07:53,680
You know, those kind of things.

115
00:07:53,680 --> 00:07:59,800
So over the last years, we've really tried to sort of guard the moral high ground if you

116
00:07:59,800 --> 00:08:06,360
will around AI and I'm just look at what it can, the value it can bring, but also mitigate

117
00:08:06,360 --> 00:08:11,320
the risk that you can have with AI.

118
00:08:11,320 --> 00:08:15,600
And following sort of that path, empathy was a very natural thing.

119
00:08:15,600 --> 00:08:22,320
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,

120
00:08:22,320 --> 00:08:29,560
which is, you know, that's a big beast and sort of more ethical behavior and empathy

121
00:08:29,560 --> 00:08:36,800
seem to be something that was just about tangible enough to try to really put it into the

122
00:08:36,800 --> 00:08:41,440
product and into the vision of best practice around using AI.

123
00:08:41,440 --> 00:08:47,240
So we've been spending quite some time thinking about that and how you can operationalize

124
00:08:47,240 --> 00:08:48,320
that kind of thing.

125
00:08:48,320 --> 00:08:56,760
Now, when you start talking about the morality of AI, certainly, and even to large degree

126
00:08:56,760 --> 00:09:02,240
empathy, I start to, you know, the picture in my head starts to form around, you know,

127
00:09:02,240 --> 00:09:07,640
what some of us will call AGI, artificial general intelligence, you know, what we talk

128
00:09:07,640 --> 00:09:11,240
about more commonly is like sci-fi AI, right?

129
00:09:11,240 --> 00:09:12,880
Is that what we're talking about here?

130
00:09:12,880 --> 00:09:18,200
Or are we talking about something that, you know, how can you make this more concrete

131
00:09:18,200 --> 00:09:19,200
for us?

132
00:09:19,200 --> 00:09:23,080
Yeah, because because I'm definitely not talking about that kind of thing.

133
00:09:23,080 --> 00:09:26,720
I mean, that's really, well, it's, it's, it's interesting.

134
00:09:26,720 --> 00:09:27,720
Right?

135
00:09:27,720 --> 00:09:33,760
I mean, I think everybody in AI is thinking about how that, how that, how that works.

136
00:09:33,760 --> 00:09:38,440
But I think just as a human species, if I just, you know, just even reading the news, you

137
00:09:38,440 --> 00:09:44,440
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,

138
00:09:44,440 --> 00:09:48,400
very clear for, for, for everyone, at least there is a lot of discussion about what would

139
00:09:48,400 --> 00:09:52,920
be moral judgment and not a moral judgment to even expect that of AI.

140
00:09:52,920 --> 00:09:59,920
I think it's already a, a tough act, but I'm certainly not talking about it in that kind

141
00:09:59,920 --> 00:10:03,720
of realm quite yet, although it's a very interesting topic, right?

142
00:10:03,720 --> 00:10:06,680
I was just thinking about, you know, self-driving cars, right?

143
00:10:06,680 --> 00:10:12,720
And, and, and sort of the moral judgments, they may, one day have to make, right?

144
00:10:12,720 --> 00:10:14,880
In extreme circumstances.

145
00:10:14,880 --> 00:10:21,920
But this is very much sort of a smaller subset of those challenges where we're talking about

146
00:10:21,920 --> 00:10:25,220
customer engagement and those kinds of things.

147
00:10:25,220 --> 00:10:34,000
And I think in that area, empathy really shows in, in, in, in, in, in, in pretty clear dimensions.

148
00:10:34,000 --> 00:10:38,960
Like stuff like, is this, if we are talking to me as a company, and it's an AI-driven

149
00:10:38,960 --> 00:10:41,760
conversation, is that, is that a relevant?

150
00:10:41,760 --> 00:10:42,760
Right?

151
00:10:42,760 --> 00:10:44,680
Or are you wasting my time?

152
00:10:44,680 --> 00:10:46,600
Stuff like, is it, is it appropriate?

153
00:10:46,600 --> 00:10:47,600
Right?

154
00:10:47,600 --> 00:10:50,640
So you're talking to me about something and I, it might be interesting, it may be interesting

155
00:10:50,640 --> 00:10:59,840
to me but it may not still be appropriate. Maybe you shouldn't be selling me a gun or a car

156
00:10:59,840 --> 00:11:07,520
or a credit card that I actually can't pay back when I'm in debt. So it's those kind of thing

157
00:11:07,520 --> 00:11:12,880
and is there mutual value? Are you talking to me for something that can we have a transaction that

158
00:11:12,880 --> 00:11:19,840
has a mutual value or is it just about the company? And I think if companies implement those

159
00:11:19,840 --> 00:11:29,120
kind of considerations well, I think that will do pretty well on an empathy scorecard for starters.

160
00:11:30,000 --> 00:11:35,840
Now that's interesting and actually somewhat different from the, I don't know if it's different

161
00:11:35,840 --> 00:11:41,920
from the direction that I thought we were going to go here or that you know the picture that formed

162
00:11:41,920 --> 00:11:51,440
in my mind when we were talking earlier about empathy or if one is part of the other. But I'll tell

163
00:11:51,440 --> 00:11:56,480
you I'll kind of recount the picture that I have in my money. Let me know where it fits into

164
00:11:56,480 --> 00:12:04,480
this world. You know I was envisioning primarily the kinds of interactions that you might have

165
00:12:04,480 --> 00:12:12,800
via a chat bot or you know a chat kind of interface and you often have or even you know to the

166
00:12:12,800 --> 00:12:21,520
extent to which AI is driving a call a call center agent and their responses because that's some

167
00:12:21,520 --> 00:12:30,000
of that or IVR. You know some of that is starting to happen. But I was envisioning kind of this

168
00:12:30,000 --> 00:12:38,160
set of capability where you know maybe the you know whether the chat bot or the IVR you know IVR

169
00:12:38,160 --> 00:12:44,480
is a great example right it's like you can IVR should be able to tell from my voice or could

170
00:12:44,480 --> 00:12:49,840
be could tell from my voice that I'm getting frustrated navigating the 50 million you know menus

171
00:12:49,840 --> 00:12:55,280
and maybe escalate me you know a little bit more quickly to someone or you know you can imagine

172
00:12:55,280 --> 00:13:02,000
the same kind of thing happening in a chat interaction where you know I'm interacting with this

173
00:13:02,800 --> 00:13:09,360
with this virtual agent it's not getting what I need to do or it's you know I'm needing asking

174
00:13:09,360 --> 00:13:15,440
me to repeat myself you know multiple times you know there's a there's a degree of empathy and

175
00:13:15,440 --> 00:13:21,680
all that where it's understanding my I guess I'm kind of simplifying that is understanding my

176
00:13:21,680 --> 00:13:28,080
emotional state and using that as part of the the decisioning around what the next best action

177
00:13:28,080 --> 00:13:33,760
to take is yeah but it sounds like maybe that's a piece of what you're saying but you're also

178
00:13:33,760 --> 00:13:41,360
talking about you know maybe about the broader AI ethics conversation your example around you

179
00:13:41,360 --> 00:13:48,640
know should we offer the the credit card to the person who can't afford it is one that kind of

180
00:13:48,640 --> 00:13:54,960
you know resonates and kind of drives me in that direction yeah now I think so so I think the

181
00:13:54,960 --> 00:14:01,600
example that you gave right like is is somebody getting frustrated and those kind of things I think

182
00:14:01,600 --> 00:14:07,840
are a very important part of of empathy but I think it's part of the delivery mechanism so I think

183
00:14:07,840 --> 00:14:13,200
what we're trying to do is sort of take that in in in in in in two different layers if you will

184
00:14:13,200 --> 00:14:17,840
right one is when you're talking to someone and somebody is getting frustrated or if you do voice

185
00:14:17,840 --> 00:14:24,160
detection and or inflection and you you sort of notices that somebody is getting upset

186
00:14:25,920 --> 00:14:32,240
you may want to change that may influence the next best action as part of the context so I would

187
00:14:32,240 --> 00:14:38,480
call that although it's called more of the sort of the superficial way of empathy right it's trying

188
00:14:38,480 --> 00:14:45,120
to feel somebody's mood and use it as a context but that can become from a lot of different senses

189
00:14:45,120 --> 00:14:50,640
it could be the you know as I said the inflection of a voice it could be when this is face to face

190
00:14:50,640 --> 00:14:55,600
or you're in front of a camera right can be that you know people can sort of read their face or

191
00:14:55,600 --> 00:15:02,080
the system can read the face of the of the customer and see that's not going well but that's not

192
00:15:02,080 --> 00:15:09,280
the same as the underlying level of making sure that the next best action that you are contemplating

193
00:15:09,280 --> 00:15:17,360
is one that is empathetic or even moral right so I see that as two different different thing I think

194
00:15:17,360 --> 00:15:21,680
people think about empathy a lot like you were just describing it like hey I see this is

195
00:15:21,680 --> 00:15:27,440
making you upset so I need to you know hurry this along or ask you wrong and that's all

196
00:15:27,440 --> 00:15:34,480
cool very human stuff but it's under the livery of a particular action but determining what you're

197
00:15:34,480 --> 00:15:40,800
going to do that also requires empathy and that's more along the line of is this is this

198
00:15:40,800 --> 00:15:47,920
relevant is this appropriate is this suitable does kind of things and do you specifically use

199
00:15:47,920 --> 00:15:56,720
the word empathy to distinguish it from ethics or are those ideas different in your mind or

200
00:15:56,720 --> 00:16:03,840
are they you know it's just a different word in this case yeah now I think I think the if I think

201
00:16:03,840 --> 00:16:10,560
about ethics that sort of you know ethical behavior I think empathy is basically the step where

202
00:16:10,560 --> 00:16:17,600
humans for now maybe AI at some point will basically be able to you know please themself in someone

203
00:16:17,600 --> 00:16:25,280
else's place and say hey if this is happening to me you know is this going to make me you know

204
00:16:25,280 --> 00:16:30,960
happier and things like that so I think they are in that sense very very related

205
00:16:30,960 --> 00:16:39,040
hmm so in that sense ethics is kind of relevant to some broader set of you know societal norms

206
00:16:39,040 --> 00:16:44,640
whereas empathy we don't have to figure all that out it's just about you know this particular

207
00:16:44,640 --> 00:16:50,880
customer and I am I doing the right thing by this customer yes am I doing the right thing and I

208
00:16:50,880 --> 00:16:56,640
think there are few dimensions to that right am I doing the right thing in terms of am I forcing

209
00:16:56,640 --> 00:17:02,560
you to take like a risk that I actually think you know is too high I already know that you won't

210
00:17:02,560 --> 00:17:07,280
be able to pay back that mortgage or that credit card or that thing or you don't really need this

211
00:17:07,280 --> 00:17:13,200
kind of thing right so that's that's that's that's part of that decision but also is it relevant

212
00:17:13,200 --> 00:17:18,800
in the first place right it's it's an empathetic thing to not try and waste somebody's time right

213
00:17:18,800 --> 00:17:23,280
I mean if you don't I don't know about you Sam but if you like all these ads all the stuff that you

214
00:17:23,280 --> 00:17:29,440
get if you you know you know browse the internet and and look at all of these pages it's it's

215
00:17:29,440 --> 00:17:34,400
we're used to it it doesn't show a lot of empathy right everybody's trying to get your attention

216
00:17:34,400 --> 00:17:41,200
to do things that you know maybe 1% of the time you're vaguely interested in right so that's

217
00:17:41,200 --> 00:17:48,160
that's that's that's part of it is it relevance is it not wasting my time is it do you remember the

218
00:17:48,160 --> 00:17:53,680
context do you remember that I just spoke to you in another channel right I just walked into the

219
00:17:53,680 --> 00:17:59,920
branch I just visited your website and now I'm going into the IPR do you even remember that or

220
00:17:59,920 --> 00:18:06,560
do are you forcing me to basically repeat everything I did I did you know in the previous channel

221
00:18:06,560 --> 00:18:14,400
that's empathy right empathy with customers that are trying to solve a problem or that want to

222
00:18:14,400 --> 00:18:21,280
get value out of their interaction with the company this is clearly an issue that is much bigger

223
00:18:21,280 --> 00:18:30,160
than AI right we don't have to you know look very far to recognize that the in many ways the

224
00:18:30,800 --> 00:18:37,760
previous financial crisis with the the mortgage bubble grew out of giving loans to people that

225
00:18:37,760 --> 00:18:44,880
you know weren't qualified for them and there are many many more examples where organizations you

226
00:18:44,880 --> 00:18:49,680
know fail to exhibit the kind of empathy that you are describing that have nothing to do with

227
00:18:49,680 --> 00:18:56,880
artificial intelligence or machine learning you know why why take this on from an AI perspective

228
00:18:57,840 --> 00:19:04,320
well I think that's a good question and I think the the answer to that is that if the way we look

229
00:19:04,320 --> 00:19:09,920
at customer interaction in general is to always do this next best action kind of thing and the next

230
00:19:09,920 --> 00:19:17,840
best action is actually collaboration between humans you know inside the company deciding on you

231
00:19:17,840 --> 00:19:26,720
know rules or thresholds or policies working together with AI where AI is maybe determining the

232
00:19:26,720 --> 00:19:35,440
risk it's determining the the level of likely interest from from the customer and it's that combination

233
00:19:35,440 --> 00:19:41,280
that creates the metric for this is the best thing to do right now right so you're you're quite

234
00:19:41,280 --> 00:19:45,760
right it's the and actually that mortgage example or the bubble that you just described is a great

235
00:19:45,760 --> 00:19:52,080
combination right there are analytic models that should have said listen um for this group of

236
00:19:52,080 --> 00:20:00,160
people um the risk is not really acceptable and um you shouldn't be pushing them on this level

237
00:20:00,160 --> 00:20:07,760
of of of of mortgage right suitability for instance is not taking into account right so it's

238
00:20:07,760 --> 00:20:16,480
that combination of AI and rules um that I you know we we call that decisioning or decision

239
00:20:16,480 --> 00:20:25,360
management um that basically needs to represent empathetic behavior right so it's not just the AI

240
00:20:25,920 --> 00:20:33,120
it's also the the rules but one of the reasons I think the AI aspect is so important is because

241
00:20:33,120 --> 00:20:42,720
the AI is learning right so it can you know have evolved um a particular bias right it may be a

242
00:20:42,720 --> 00:20:48,400
very opaque algorithm that may have evolved that bias and you don't even know right so there are

243
00:20:48,400 --> 00:20:55,040
a lot of aspects of AI that I think really touch on ethics and empathy as well when you're talking

244
00:20:55,040 --> 00:21:02,240
about this at Pega World are you you know are you raising this as an issue that customer should

245
00:21:02,240 --> 00:21:09,440
start thinking about this are you talking about new capabilities that you're unveiling at Pega

246
00:21:09,440 --> 00:21:17,840
uh uh with you know with the product that will help them address these issues is it um you know

247
00:21:17,840 --> 00:21:24,080
or is there something else well I don't want to feel all my own center but uh we'll we'll definitely

248
00:21:24,080 --> 00:21:29,200
so two years ago one of the things that I was talking about in the keynote was about this

249
00:21:29,200 --> 00:21:35,120
single the t switch and the t was you know stands for transparency but also for trust and it's

250
00:21:35,120 --> 00:21:41,440
basically the ability that once you have this centralized next best action capability inside of

251
00:21:41,440 --> 00:21:50,000
your company um that you can have full control over where you allow um sort of opaque algorithms

252
00:21:50,000 --> 00:21:55,600
like deep learning or genetic algorithms or that's kind of fancy stuff or where you insist on

253
00:21:55,600 --> 00:22:00,720
more transparent algorithms that you can actually explain to a customer and that you really

254
00:22:00,720 --> 00:22:07,760
explain or that you really understand yourself right so that was one aspect um next up is the

255
00:22:07,760 --> 00:22:14,720
is that the thing around empathy I won't I think it's a really good thing if companies are aware

256
00:22:14,720 --> 00:22:24,800
at all times how empathetic their behavior is so um think about sort of a dashboard where you would

257
00:22:24,800 --> 00:22:30,960
see of all the actions my company is taking and these are you know the decide companies we work with

258
00:22:30,960 --> 00:22:36,720
these are hundreds of millions a day right hundreds of millions of interactions a day and then being

259
00:22:36,720 --> 00:22:44,480
able to see okay these are actions that we are taken automatically is a combination of AI and rules

260
00:22:45,120 --> 00:22:51,760
that are not empathetic and that means that they are going against the relevance or they are not

261
00:22:51,760 --> 00:22:57,200
appropriate like not suitable so we we're we're talking about this credit card but it's not really

262
00:22:57,200 --> 00:23:04,880
suitable um or it doesn't really create value for the customer right so imagine that while all of

263
00:23:04,880 --> 00:23:10,080
this is running this combination of AI and rules and it's making hundreds of millions of you know

264
00:23:10,080 --> 00:23:15,760
decisions and having all of these conversations with customers that you can just see that as a real

265
00:23:15,760 --> 00:23:23,360
time thing and say hey really we're getting you know less empathetic um let's see in our

266
00:23:23,360 --> 00:23:29,600
strategies in our customers strategies that we have in the algorithms that we use where we're losing

267
00:23:29,600 --> 00:23:35,280
that right are we pushing products that we shouldn't be pushing don't we have the rules that are

268
00:23:35,280 --> 00:23:42,000
determining suitability it's it's it's it's it's those kind of things and then in addition to that

269
00:23:42,000 --> 00:23:49,520
I think we can also determine sort of the cost of not being empathetic right so if you for instance

270
00:23:49,520 --> 00:23:55,440
if you if you if you if you are going against somebody's interests and I don't mean I mean interest

271
00:23:55,440 --> 00:24:01,360
in in in terms of of relevance right so so what a lot of marketing is currently doing right

272
00:24:01,360 --> 00:24:05,840
they're they're they're spamming you with stuff they're wasting your time on things that are

273
00:24:05,840 --> 00:24:12,640
actually not that relevant to you it would be good to not only know the percentage of of of of of

274
00:24:12,640 --> 00:24:19,200
events where that happens it would also be really good if you had a sense of the money um

275
00:24:19,200 --> 00:24:24,960
you're actually losing and and you can calculate that mathematically by for instance saying hey

276
00:24:24,960 --> 00:24:28,400
this is what we actually talked about to this customer we talked about this mortgage

277
00:24:28,960 --> 00:24:34,480
and the customer said no to it because it wasn't relevant um what we could have been talking about

278
00:24:34,480 --> 00:24:40,720
is this particular issue that we spotted um you know in in in in in a in a different channel or

279
00:24:40,720 --> 00:24:47,840
last them uh last week or an hour ago or maybe a much more relevant offer that we didn't think

280
00:24:47,840 --> 00:24:54,400
we wanted to do because the margin was you know not as big as on the mortgage right having making

281
00:24:54,400 --> 00:25:03,120
that a transparent thing having people own the kind of um you know empathy level of the brand

282
00:25:03,120 --> 00:25:11,040
I think is um is a really important thing going going forward it strikes me that that latter point

283
00:25:11,040 --> 00:25:22,480
around quantifying the cost of these non empathetic actions is really a big part of where the problem

284
00:25:22,480 --> 00:25:29,920
lies it's it's easy to it's easy to know the cost of the you know they expected

285
00:25:29,920 --> 00:25:38,640
revenue or profit from offering something but a lot harder to know the cost of just wasting the

286
00:25:38,640 --> 00:25:46,480
the customer's time or you know reducing the the brand goodwill because of some series of you

287
00:25:46,480 --> 00:25:55,120
know less relevant or poor experiences how do you overcome that gap yeah well I think some of the

288
00:25:55,120 --> 00:26:00,560
math actually works out quite nicely right so remember that when we do this next best action and

289
00:26:00,560 --> 00:26:06,880
I said before the best is a function of you know the value that is created in the in the in the

290
00:26:06,880 --> 00:26:14,400
relationship so um the math works out that you actually can know that if you go against the

291
00:26:14,400 --> 00:26:18,240
propensity because for instance you're selling let's use this mortgage example that works really

292
00:26:18,240 --> 00:26:24,720
well um if you um if you are thinking this is not particularly relevant but if the customer says

293
00:26:24,720 --> 00:26:31,280
yes this is the margin this is the money the i as a bank in this case will will will will make

294
00:26:31,760 --> 00:26:38,000
right if you calculate all the times a customer said no because you're basically offering stuff

295
00:26:38,000 --> 00:26:42,080
that's not particularly relevant just you know you're you're hoping that the customer will say yes

296
00:26:42,960 --> 00:26:48,800
um what how could we have used that moment how could we have used that interaction with the

297
00:26:48,800 --> 00:26:54,880
customer in a better way that would have created more value if you just multiply that with a hundred

298
00:26:54,880 --> 00:27:00,720
of millions of of decisions you make you get to a monetary value and you find in examples like

299
00:27:00,720 --> 00:27:08,480
this that you know there's some explicit decision that where the customer is saying hey let's offer

300
00:27:08,480 --> 00:27:15,760
this more profitable thing even though we know it's not as relevant or um does that does that happen

301
00:27:15,760 --> 00:27:23,200
in more subtle ways no the i don't think these ways are particularly subtle right so the way

302
00:27:23,200 --> 00:27:30,160
the way because the way we work um is like so to do this next best action right we would calculate

303
00:27:30,720 --> 00:27:35,120
um every single thing this is also part of this hyper personalization

304
00:27:35,120 --> 00:27:40,960
vision right to be completely one to one it means that of all the possible conversations you could

305
00:27:40,960 --> 00:27:46,880
have right you're going to rate them in real time based on the context and you're going to say

306
00:27:46,880 --> 00:27:52,160
this is the thing we are going to talk about as a combination of what the AI thinks is

307
00:27:52,160 --> 00:27:57,920
particularly appropriate and relevant as well as my rules that I have around profitability

308
00:27:57,920 --> 00:28:03,680
and inventory uh inventory and and all of those kind of things right so it's that combination

309
00:28:03,680 --> 00:28:10,240
but we calculate them all in parallel so it's relatively easy to see okay this is what we chose

310
00:28:10,240 --> 00:28:19,520
to actually talk about but this is what we could have talked about if we had weighted suitability higher

311
00:28:19,520 --> 00:28:25,920
or if we didn't overrule this very low um or this very high propensity and said well even though

312
00:28:25,920 --> 00:28:30,800
that's relevant it's not what we want to talk about right so you can see how you get a drop off

313
00:28:30,800 --> 00:28:38,480
of typical uh or or of specific conversation topics that you decide not to pursue for other reasons

314
00:28:38,480 --> 00:28:46,720
and that's what you can um calculate is the task then starting that to you know build awareness

315
00:28:46,720 --> 00:28:56,400
on the part of customers or users or kind of the the industry uh as as a whole to incorporate

316
00:28:56,400 --> 00:29:07,520
these types of empathy metrics if we call them that into um you know their systems their rules their

317
00:29:08,880 --> 00:29:15,920
algorithms um and start to is it as I don't know if simple is the right word but is it as simple

318
00:29:15,920 --> 00:29:22,480
as you know just you know starting to try to put numbers around uh these you know suitability

319
00:29:22,480 --> 00:29:30,320
context to where relevance you know risk and then feeding them into your uh uh your automation

320
00:29:30,320 --> 00:29:37,280
tooling with uh kind of appropriate weights or is it does it go beyond that I think i think the

321
00:29:37,280 --> 00:29:41,760
way you describe it so what we're trying to do is first of all we will talk about that's like an

322
00:29:41,760 --> 00:29:50,000
easy task no no no no no no no no like that but yeah yeah no that's not easy in itself but what we

323
00:29:50,000 --> 00:29:55,760
we are trying to do is, first of all, make it very explicit. When we talk about next-best

324
00:29:55,760 --> 00:30:05,600
action, there are patterns that we've seen that are repeatedly successful and they include

325
00:30:05,600 --> 00:30:16,080
things like relevance, suitability, mutual value, risk mitigation. The tooling and the methodology

326
00:30:16,080 --> 00:30:21,920
already encourage companies to at least think about it. They may think, okay, well, for suitability,

327
00:30:21,920 --> 00:30:29,520
we really don't care about it, or not as much, or we let profitability, trump, suitability,

328
00:30:29,520 --> 00:30:36,240
anytime, but at least the product, if you follow the product guidance, you will have to take all

329
00:30:36,240 --> 00:30:44,720
of these considerations into account. There is an ethical framework built into the software,

330
00:30:44,720 --> 00:30:50,640
into the strategies that it will generate. That's one aspect of it, and then the other aspect of it

331
00:30:50,640 --> 00:30:57,200
are like the dashboard that you will show. It's basically shaming companies a little bit if they

332
00:30:57,200 --> 00:31:04,960
want, having them self-shame them into appropriate behavior, where they would say, hey, listen,

333
00:31:04,960 --> 00:31:11,120
we cranked up profitability, but it's at the expense of suitability or customer interest.

334
00:31:11,120 --> 00:31:21,200
At least, I think the awareness and the transparency around these things will be leading to better

335
00:31:21,200 --> 00:31:32,000
behavior. How does the company begin to put tangible numbers and costs around things like mutual

336
00:31:32,000 --> 00:31:38,560
value and suitability in contexts, awareness and relevance? Relevance is maybe easier because it

337
00:31:38,560 --> 00:31:46,400
impacts propensity to buy. Risk is something that's fundamentally numerical, but some of these

338
00:31:46,400 --> 00:31:55,760
others are a little bit squishier, maybe. You're right. From what we've seen,

339
00:31:55,760 --> 00:32:05,760
is that the less squishy things, once you are aware that you need to put them in,

340
00:32:05,760 --> 00:32:14,160
and that the AI or the decision in general, touching 100 million, making 100 million decisions

341
00:32:14,880 --> 00:32:18,960
in all the different channels with all of your customers, that that is part of your brand,

342
00:32:18,960 --> 00:32:26,720
and you need to protect that. I think that's a very important thing. I think for the squishier

343
00:32:26,720 --> 00:32:34,240
things, I think what we also encourage and also make possible is continuous experimentation.

344
00:32:34,240 --> 00:32:40,240
There is always control groups, there is all sorts of things where you can, for a small

345
00:32:40,240 --> 00:32:44,800
percentage, a small sample of the customers, you can actually measure if you're having an effect,

346
00:32:44,800 --> 00:32:53,920
if they have a positive response to the brand, and you can see if what kind of strategy changes

347
00:32:53,920 --> 00:32:58,320
would improve that, and that is a best practice kind of thing to do.

348
00:32:58,320 --> 00:33:07,040
Do you have any examples of folks that you've worked with that you can kind of walk us through

349
00:33:07,760 --> 00:33:16,480
how this all plays out and how they went about making, kind of incorporating these ideas

350
00:33:16,480 --> 00:33:24,320
into the way they make decisions? I think even before we invested in all this

351
00:33:24,320 --> 00:33:33,840
around empathy and also before the transparency and opacity, it's not like these big brands are

352
00:33:33,840 --> 00:33:42,880
not aware of these issues. I think 10 years ago, maybe longer ago, I had long conversation

353
00:33:42,880 --> 00:33:51,200
with banks that were sued for miscelling that I think we've seen more recent examples,

354
00:33:51,200 --> 00:34:01,440
where obviously these companies want to sort of control that even if just for their own sake,

355
00:34:01,440 --> 00:34:07,440
to not be part of some class action. And in the next best action methodology,

356
00:34:08,080 --> 00:34:13,520
stuff like relevance, appropriateness, failure, and risk have always been sort of first class

357
00:34:13,520 --> 00:34:22,560
citizens. What we're now trying to do is to make sure that it's much harder to break those

358
00:34:22,560 --> 00:34:32,160
patterns, or if you don't want to be compliant with these kinds of ethics practices, you at least

359
00:34:32,160 --> 00:34:38,880
have to make the effort. I think to your question, I think especially the banks, I think we see

360
00:34:38,880 --> 00:34:45,360
that in other industries as well are getting very worried about their brand image in that regard,

361
00:34:45,360 --> 00:34:50,640
and they are putting suitability criteria, for instance, it's a pretty hot topic right now,

362
00:34:50,640 --> 00:34:56,640
they're putting that as part of their next best action strategy. We just want to help them

363
00:34:56,640 --> 00:35:04,560
by showing the cost of that and the benefits of that. You mentioned compliance in there.

364
00:35:04,560 --> 00:35:11,760
Do you envision a time where an enterprise will have a formal empathy compliance regime?

365
00:35:11,760 --> 00:35:16,080
Will it be called that? Will it be called something else? Does it already exist under some other

366
00:35:16,080 --> 00:35:23,680
guys? I think that definitely will happen in some cases already happens. I don't think it's

367
00:35:23,680 --> 00:35:35,680
called empathy. It's probably more on the ethics board. It's not only about the company itself,

368
00:35:35,680 --> 00:35:42,240
it's also about basically a compliance issue out of self-interest, especially now. This is

369
00:35:42,240 --> 00:35:49,040
where the AI is so important, where there's so much self-learning going on on this incredible scale

370
00:35:49,040 --> 00:35:57,360
that there could be a bias and there could be all sorts of things happening that may not be so

371
00:35:57,360 --> 00:36:06,240
easy to control or even spot. I know that some of the companies I talk to, and these are larger

372
00:36:06,240 --> 00:36:15,760
companies, but they have a board already for all the algorithms that are involved in customer

373
00:36:15,760 --> 00:36:22,960
interaction. I think that's a sensible thing to do. It's part of what inspired this transparency

374
00:36:22,960 --> 00:36:29,440
or trust switch in the software to make sure that all AI is, at least, you can control

375
00:36:30,480 --> 00:36:38,720
the level of transparency that you require in certain circumstances, talking to customers.

376
00:36:38,720 --> 00:36:43,200
Do you envision a chief empathy officer? It sounds like no, it's probably going to be if anything,

377
00:36:43,200 --> 00:36:50,480
it'll be a chief ethics officer or some other role. Where do you see this all sitting?

378
00:36:51,200 --> 00:36:55,440
Yeah, well, I think this is a very interesting topic, because I think this will become

379
00:36:56,000 --> 00:37:03,280
very, very important if it isn't already. I think you will get into a situation where you have

380
00:37:03,280 --> 00:37:15,680
at the first level AI trying to check other AI for biases or an ethical behavior, because

381
00:37:15,680 --> 00:37:25,760
it's just a lot, and it would only escalate if such a bias or other irregularities is detected.

382
00:37:25,760 --> 00:37:32,800
But it's certainly, and again, I'm talking about the larger companies with tens to hundreds of

383
00:37:32,800 --> 00:37:39,520
millions of customers that are very worried about, especially with the level of automation that's

384
00:37:39,520 --> 00:37:46,000
now available, and then AI that is dynamically learning new things or evolving new things

385
00:37:47,040 --> 00:37:54,000
to have an ethics board like that. We try from a product perspective, we try to make sure

386
00:37:54,000 --> 00:38:00,480
that like you have QA tests on quality assurance tests, on performance, and other things that,

387
00:38:00,480 --> 00:38:09,280
as a matter of course, you would do the same thing around biased detection or other irregularities

388
00:38:09,280 --> 00:38:18,560
before you release the next version of your corporate brain, so to speak, to make that easier.

389
00:38:18,560 --> 00:38:31,600
I think the ideas of making these more empathetic types of qualities of your various offers,

390
00:38:32,640 --> 00:38:39,040
as you've suggested throughout this conversation, it's very much connected to this idea of

391
00:38:39,040 --> 00:38:46,080
transparency. There are these dollars and sense things that we build into decision-making algorithms

392
00:38:46,080 --> 00:38:51,680
all the time, but there's all this other stuff that goes into the customer experience, and what we're

393
00:38:51,680 --> 00:39:02,240
doing here, we're calling empathy is really the idea of making a lot of those non-premofacy

394
00:39:02,240 --> 00:39:11,680
financial aspects A, more transparent, and then B, like putting trying to make them more financial

395
00:39:11,680 --> 00:39:17,920
or putting numbers against them, and then incorporating them into decision-making,

396
00:39:17,920 --> 00:39:22,640
dashboarding them so that there's some awareness of them, and so that the organization can

397
00:39:23,360 --> 00:39:33,920
manage against them. It's a really interesting set of ideas around how to make this idea of AI

398
00:39:33,920 --> 00:39:40,000
ethics a lot more tangible. Yeah, I think yeah, yeah, tangible I think is the right word, so can

399
00:39:40,000 --> 00:39:47,440
we are there straightforward ways to make sure that in our customer's strategies, empathy is well

400
00:39:47,440 --> 00:39:51,840
represented, and we can choose to ignore it, but then there are these, as you say, these

401
00:39:51,840 --> 00:40:00,320
desporting, these gauges, these dials that show you, that shame you into some compliance.

402
00:40:00,320 --> 00:40:05,280
And also, let's not forget that, I think the reason we as humans have empathy,

403
00:40:05,280 --> 00:40:11,040
you know, there can be lots of different theories around that, but personally, I think that

404
00:40:11,040 --> 00:40:18,000
evolved, right? It evolved out of a desire to collaborate, right? So, empathy is not like a cost

405
00:40:18,000 --> 00:40:25,840
to the company, empathy is actually establishing your, you know, better relationship and a longer term

406
00:40:25,840 --> 00:40:32,800
relationship with your customers. Well, it would be interesting to kind of follow along with

407
00:40:32,800 --> 00:40:40,080
this work and see, you know, I'd love to hear a case study of, you know, how a customer kind of

408
00:40:40,800 --> 00:40:46,960
implements this end and once you've got this out in the market and have folks working with it.

409
00:40:47,760 --> 00:40:53,360
Yeah, I mentioned PegaWorld earlier, any besides from your own keynote, other things that

410
00:40:53,360 --> 00:40:58,800
you're looking forward to at the conference? Yeah, well, I mean, this will be the biggest effort,

411
00:40:58,800 --> 00:41:08,160
so it's always just very exciting about, you know, to show people, you know, where we are at,

412
00:41:08,160 --> 00:41:14,160
and it's not just about empathy, right? It's about, it's about also making decisions in general

413
00:41:14,160 --> 00:41:19,600
at a huge scale, you know, with this real-time AI on the one hand, and then on the other hand,

414
00:41:19,600 --> 00:41:23,520
and that's also part of empathy, although we didn't talk about it right now, but then

415
00:41:23,520 --> 00:41:30,800
following up on it, right, with the processes, right? So we are really, and you will hear a lot

416
00:41:30,800 --> 00:41:36,480
about that at PegaWorld, we're trying to, you know, have that combination very strongly. So it's

417
00:41:37,440 --> 00:41:43,520
we have the AI and a decision to decide what to do, right? And then we have sort of the end-to-end

418
00:41:43,520 --> 00:41:51,360
automation that will tell the company how to do it and to do it fast and efficiently, which also

419
00:41:51,360 --> 00:41:57,520
plays into empathy. So I really love that interplay between sort of decisions and processes,

420
00:41:58,160 --> 00:42:05,680
so I'm expecting a lot of really good inter discussions and presentations from our customers.

421
00:42:05,680 --> 00:42:10,880
Awesome, awesome. Well, Rob, I'm looking forward to seeing you once again at the event.

422
00:42:10,880 --> 00:42:16,000
Thanks so much for taking the time to jump on and talk this through with us.

423
00:42:16,000 --> 00:42:22,160
Okay, well, you're very welcome. Thank you. Thanks, Rob.

424
00:42:22,160 --> 00:42:27,840
All right, everyone, that's our show for today. For more information on Peter or any of the topics

425
00:42:27,840 --> 00:42:36,480
covered in the show, visit twimmelai.com slash talk slash 245. As always, thanks so much for listening

426
00:42:36,480 --> 00:42:46,480
and catch you next time.

