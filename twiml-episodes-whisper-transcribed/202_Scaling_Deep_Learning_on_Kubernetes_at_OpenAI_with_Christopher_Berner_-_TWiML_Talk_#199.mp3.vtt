WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:35.440
For those challenges with promoting the use of machine learning in an organization and

00:35.440 --> 00:40.480
making it more accessible, one key to success is supporting data scientists and machine

00:40.480 --> 00:44.840
learning engineers with modern processes, tooling and platforms.

00:44.840 --> 00:49.480
This is a topic we're super excited to address here on the podcast with the AI Platforms

00:49.480 --> 00:54.120
Podcast series that you're currently listening to, as well as a series of e-books that will

00:54.120 --> 00:56.600
be publishing on the topic.

00:56.600 --> 01:01.000
The first of these books takes a bottoms up look at AI Platforms and is focused on the

01:01.000 --> 01:06.000
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

01:06.000 --> 01:11.160
at places like Airbnb, booking.com and open AI.

01:11.160 --> 01:15.160
The second book in the series looks at scaling data science and machine learning engineering

01:15.160 --> 01:20.600
from the top down, exploring the internal platforms that companies like Airbnb, Facebook

01:20.600 --> 01:25.440
and Uber have built and what enterprises can learn from them.

01:25.440 --> 01:29.560
If these are topics you're interested in and especially if part of your job involves

01:29.560 --> 01:35.600
making machine learning more accessible, I'd encourage you to visit twimbleai.com slash

01:35.600 --> 01:41.880
AI platforms and sign up to be notified as soon as these books are published.

01:41.880 --> 01:47.080
If you found yourself saying, Cuba, what, Cuba, who, when listening to this intro, this

01:47.080 --> 01:48.800
is the show for you.

01:48.800 --> 01:54.640
In this episode of our AI Platforms series, we're joined by open AI's head of infrastructure,

01:54.640 --> 01:56.360
Christopher Burner.

01:56.360 --> 02:01.280
Chris has played a key role in overhauling open AI's deep learning infrastructure in the

02:01.280 --> 02:04.240
course of his two years with the company.

02:04.240 --> 02:09.000
In our conversation, we discussed the evolution of open AI's deep learning platform, the core

02:09.000 --> 02:14.080
principles which have guided that evolution and its current architecture.

02:14.080 --> 02:18.960
We dig deep into the use of Kubernetes and discuss various ecosystem players and projects

02:18.960 --> 02:22.280
that support running deep learning at scale.

02:22.280 --> 02:24.920
And now on to the show.

02:24.920 --> 02:27.640
All right, everyone.

02:27.640 --> 02:30.280
I am on the line with Christopher Burner.

02:30.280 --> 02:33.600
Chris is the head of infrastructure at open AI.

02:33.600 --> 02:36.360
Chris, welcome to this week in machine learning and AI.

02:36.360 --> 02:37.640
Thanks a lot.

02:37.640 --> 02:42.520
Awesome. So why don't we get started by having you tell us a little bit about your background.

02:42.520 --> 02:46.080
You are coming up on almost two years at open AI now.

02:46.080 --> 02:47.560
How did you get to open AI?

02:47.560 --> 02:48.560
Yeah.

02:48.560 --> 02:49.560
Well, let's see.

02:49.560 --> 02:55.120
So before open AI, I was at Facebook, I worked there for about four and a half years.

02:55.120 --> 02:59.200
And before that, I worked at a couple of startups at Facebook.

02:59.200 --> 03:02.960
I worked on the newsfeed ranking team for about a year.

03:02.960 --> 03:08.720
So it's got just a little bit of introductory experience in machine learning.

03:08.720 --> 03:15.440
And then I worked in the data infrastructure team, worked on a distributed SQL query engine

03:15.440 --> 03:18.040
called Presto, which is open source.

03:18.040 --> 03:23.480
So I have a fair bit of background and big data analytics and data warehousing.

03:23.480 --> 03:28.280
And that has certainly helped me get up to speed with all of the large scale infrastructure

03:28.280 --> 03:31.400
that's required for deep learning these days.

03:31.400 --> 03:37.000
And yeah, I've just been learning all about the infrastructure that you need for machine

03:37.000 --> 03:39.920
learning training and specialized infrastructure there.

03:39.920 --> 03:47.040
And so open AI is certainly doing machine learning at large scale.

03:47.040 --> 03:53.400
Maybe we can start out by having you, you know, we've, I've talked to several people

03:53.400 --> 03:59.680
from open AI on the podcast before, but maybe you can start by providing an overview of

03:59.680 --> 04:09.120
some of the larger projects that kind of stressed the need for a platform for machine learning

04:09.120 --> 04:10.680
at open AI.

04:10.680 --> 04:11.680
Yeah.

04:11.680 --> 04:19.320
Well, certainly our largest project is our data to research project.

04:19.320 --> 04:23.720
And in the past few months, we've announced a bunch of the results with open AI five

04:23.720 --> 04:24.720
there.

04:24.720 --> 04:32.520
And yeah, that's a very large system, trans on hundreds of GPUs and over 100,000 virtual

04:32.520 --> 04:39.600
CPU cores runs in one of our clusters in Google compute engine.

04:39.600 --> 04:41.480
And yeah, that's definitely our largest.

04:41.480 --> 04:47.760
We have several other projects that are also quite large though, our robotics team also

04:47.760 --> 04:49.720
does some very large scale training.

04:49.720 --> 04:55.560
I've actually done a little bit of work on large scale image net training.

04:55.560 --> 05:00.960
And yeah, that's like one or 200 GPUs, I'd say kind of all of our teams have things that

05:00.960 --> 05:05.560
are on the medium to large size, but data is definitely our largest.

05:05.560 --> 05:06.560
Nice.

05:06.560 --> 05:07.560
Nice.

05:07.560 --> 05:10.720
And I spoke to Christy not too long ago about the data project.

05:10.720 --> 05:16.400
So I'd refer anyone who wants to hear more about that to that interview.

05:16.400 --> 05:24.520
In terms of supporting these types of projects, when you started at open AI, was there much

05:24.520 --> 05:29.480
established or have you kind of had a hand in building it up from scratch?

05:29.480 --> 05:32.600
Yeah, some of both.

05:32.600 --> 05:38.960
So definitely we already had Kubernetes clusters set up when I joined.

05:38.960 --> 05:43.880
They were definitely architected differently than they are today.

05:43.880 --> 05:48.360
And yeah, I've been involved in changing the architecture of them to make them scale

05:48.360 --> 05:51.640
better and also isolate faults better.

05:51.640 --> 05:54.480
So that was in place.

05:54.480 --> 06:00.800
We've also made some changes in terms of our storage technologies and a bunch of the frameworks

06:00.800 --> 06:02.800
that we use for running experiments.

06:02.800 --> 06:08.480
I think that's actually one of the areas that we've seen the largest changes we had.

06:08.480 --> 06:12.680
Two or three different frameworks that existed when I joined and I don't think any of those

06:12.680 --> 06:13.680
are still around.

06:13.680 --> 06:17.880
We've moved on to completely different ones, so yeah, so a mix of things that still exist

06:17.880 --> 06:19.120
and a bunch that has changed.

06:19.120 --> 06:26.240
You mentioned that a big part of the change was driven around scalability and I think it

06:26.240 --> 06:28.680
was reliability.

06:28.680 --> 06:33.240
Those are a couple of things that have driven change, but taking a step back are there when

06:33.240 --> 06:40.280
you think about the characteristics that you're trying to be able to provide to the research

06:40.280 --> 06:44.960
teams from an infrastructure perspective.

06:44.960 --> 06:51.640
Those are the two or most important things or are there a set of things that you think

06:51.640 --> 06:57.320
that they require of you to be able to do their jobs effectively?

06:57.320 --> 07:01.280
Yeah, so those are definitely two very important ones.

07:01.280 --> 07:04.520
There's a couple others that I would add.

07:04.520 --> 07:12.880
One is access to cutting-edge hardware and we run some machines in our own data centers

07:12.880 --> 07:16.840
because we want access to hardware that's not available in the cloud and so yeah, that's

07:16.840 --> 07:23.400
another dimension and then yeah, I guess sort of another aspect, but it's more on the

07:23.400 --> 07:29.960
tooling side is just ease of use that people want to be able to launch their experiments

07:29.960 --> 07:35.760
quickly and interact quickly and so that's another important aspect of infrastructure.

07:35.760 --> 07:43.480
And it sounds like the support for the framework of choice for the researchers is another

07:43.480 --> 07:48.000
one in that you've already kind of gone through a transition on the framework side.

07:48.000 --> 07:51.320
You don't want to lock them into a specific choice there.

07:51.320 --> 07:56.720
Yeah, we try to provide as much flexibility as possible to our researchers.

07:56.720 --> 08:01.200
Like I mentioned, there were some old research tools that I don't think people are using

08:01.200 --> 08:10.320
now because they've invented new ones that are better and similarly we don't want to

08:10.320 --> 08:14.640
be limited to running experiments just in the cloud and that's one of the benefits that

08:14.640 --> 08:20.080
we see from Kubernetes is that it's very easy to port your experiments from the cloud

08:20.080 --> 08:25.200
to our on-premise clusters where we may have access to different hardware and it makes

08:25.200 --> 08:29.360
that transition pretty seamless and the so the frameworks that you mentioned that folks

08:29.360 --> 08:35.080
weren't using that folks that you transition from are those with those internal frameworks

08:35.080 --> 08:39.440
or these all open source frameworks, but you're just using different ones now.

08:39.440 --> 08:43.280
Are we talking about things like TensorFlow and PyTorch or are we talking about internal

08:43.280 --> 08:45.320
tools that have been developed at OpenAI?

08:45.320 --> 08:48.480
Yeah, so those particular ones were internal.

08:48.480 --> 08:49.480
Okay.

08:49.480 --> 08:54.960
It was a few different frameworks for running and managing experiments and visualizing the

08:54.960 --> 08:59.920
results from them and things like that and now I'd say like TensorFlow board is pretty

08:59.920 --> 09:00.920
popular.

09:00.920 --> 09:07.680
That's kind of replaced some of the need for our custom tooling in one particular area.

09:07.680 --> 09:13.880
But yeah, also TensorFlow, I'd say that's definitely our most popular sort of machine learning

09:13.880 --> 09:14.880
framework.

09:14.880 --> 09:20.000
I'd guess that 90% of our code is probably TensorFlow, but there's also starting to be

09:20.000 --> 09:25.800
significant contingent of people who are using PyTorch, so some amount of change in that

09:25.800 --> 09:26.800
area too.

09:26.800 --> 09:27.800
Okay.

09:27.800 --> 09:38.280
Before we dive into kind of an architectural overview of the platform that you've set

09:38.280 --> 09:47.560
up, can you walk us through kind of the level above that, like the workflow or the processes

09:47.560 --> 09:51.320
as you see it, that you're trying to support.

09:51.320 --> 09:56.760
So for example, you mentioned experiment management and how some of the tooling there is shifted

09:56.760 --> 09:59.520
from internal to tensor board.

09:59.520 --> 10:04.920
What are the ways in which you think about the functional requirements of the machine learning

10:04.920 --> 10:07.000
researchers that you're supporting?

10:07.000 --> 10:08.000
Yeah, certainly.

10:08.000 --> 10:10.400
Yeah, I guess at a really high level.

10:10.400 --> 10:17.360
Pretty much everything that we do is research and experimentation and training models.

10:17.360 --> 10:22.760
Which makes us a little bit different than a lot of other companies that are doing machine

10:22.760 --> 10:30.040
learning because they're often then going on to integrate those into a product and deploy

10:30.040 --> 10:34.520
that to all of their users, whereas we're really just focused on the research side of things.

10:34.520 --> 10:41.640
And so we try to optimize our tools for iterating quickly and exploring a lot of different research

10:41.640 --> 10:49.400
directions and providing the flexibility to do experiments in all different areas.

10:49.400 --> 10:53.800
So we've got a number of teams that do reinforcement learning.

10:53.800 --> 10:58.760
But we also have teams doing supervised learning and unsupervised learning.

10:58.760 --> 11:03.160
So a whole bunch of different types of problems that they're trying to solve.

11:03.160 --> 11:08.600
But kind of all of it comes back to training models and collecting the results from them

11:08.600 --> 11:12.840
and then being able to interact quickly on the next version of their research ideas.

11:12.840 --> 11:13.840
Okay.

11:13.840 --> 11:21.480
So whereas in a non-research enterprise, something, some of the front end work of integrating

11:21.480 --> 11:25.760
with data warehouses and things like that isn't so critical for you.

11:25.760 --> 11:31.800
And some of the backend model management tasks where you're trying to manage productionized

11:31.800 --> 11:32.800
models.

11:32.800 --> 11:38.680
It's also not as critical for you or not at all critical for you.

11:38.680 --> 11:44.480
The part in the middle, this experiment management piece is, that has to be done really well

11:44.480 --> 11:50.440
to make sure that the folks who are supporting are able to work most efficiently.

11:50.440 --> 11:51.440
Yeah, exactly.

11:51.440 --> 11:53.720
Because that's pretty much all that we do.

11:53.720 --> 11:56.120
And so that's really where we focus our effort.

11:56.120 --> 12:02.920
And when you think of that job from a quote unquote head of infrastructure perspective,

12:02.920 --> 12:04.680
are you thinking of it?

12:04.680 --> 12:10.080
I guess I have this picture on my head of infrastructure looking down like hardware

12:10.080 --> 12:18.760
and platforms and frameworks and infrastructure looking up like tools that manage that process

12:18.760 --> 12:21.200
of experiment management, for example.

12:21.200 --> 12:26.760
Are you kind of, you know, top to bottom there or are you, you know, just the bottom part

12:26.760 --> 12:27.760
or?

12:27.760 --> 12:32.040
Yeah, I would say that we do kind of a bit of everything, but for the most part, we're

12:32.040 --> 12:35.920
focused on providing the compute clusters.

12:35.920 --> 12:41.000
And yeah, I guess also running like the storage clusters and other things that are related,

12:41.000 --> 12:46.320
but to sort of providing, yeah, the bottom layers of making sure that everyone has access

12:46.320 --> 12:49.520
to the compute and the hardware that they need.

12:49.520 --> 12:54.360
We do also work a bit on the tooling side.

12:54.360 --> 13:00.120
Most of our experimentation management tools have been developed by our research teams,

13:00.120 --> 13:07.160
but we certainly help maintain a few of those and help support them with the new features.

13:07.160 --> 13:15.800
And so I think infrastructure's role has kind of expanded and actually in some ways,

13:15.800 --> 13:21.200
I suppose, contracted to, we used to have other tools that we ran and managed, and now

13:21.200 --> 13:25.600
it's said that those aren't so useful, so we no longer run those.

13:25.600 --> 13:31.560
But kind of the main focus of the team is on providing really big compute clusters, but

13:31.560 --> 13:35.400
then we've also got a lot of other responsibilities on tooling side and other places that we work

13:35.400 --> 13:36.400
on.

13:36.400 --> 13:40.280
One of the things that I found interesting when I look at what you're doing, you've

13:40.280 --> 13:45.800
published OpenAI as published quite a bit on its infrastructure and some of the things

13:45.800 --> 13:52.840
that it's done, and one of the things that I found really interesting is the very strong

13:52.840 --> 13:55.600
commitment to kind of this multi-cloud world.

13:55.600 --> 13:59.400
You've got experiments running on Google, you've got experiments running on Azure, you've

13:59.400 --> 14:03.640
got experiments running on AWS, and of course, on premises.

14:03.640 --> 14:08.680
Can you talk a little bit about the motivation there?

14:08.680 --> 14:16.240
And what's driven the need to support, it's got to be a more complex environment than

14:16.240 --> 14:18.320
just standardizing in one place?

14:18.320 --> 14:20.200
Yeah, for sure.

14:20.200 --> 14:22.760
I guess there's kind of a few different factors.

14:22.760 --> 14:29.920
So one, like I mentioned, is access to kind of a chartware, so we can get whatever we want

14:29.920 --> 14:35.720
in our on-prem clusters, but the cloud also has a lot of benefits, you can quickly scale

14:35.720 --> 14:36.720
things up.

14:36.720 --> 14:43.920
They've got lots of nice APIs and auxiliary services that you can take advantage of, and

14:43.920 --> 14:49.600
so we'd like to have a presence with all of the major cloud providers so that when they

14:49.600 --> 14:57.440
launch a new type of NVIDIA GPU, or when Google announces their new TPUs, we want to be

14:57.440 --> 15:00.160
sure that we have access to those and that we can use them.

15:00.160 --> 15:05.360
And so that's one thing that drives our multi-cloud strategy.

15:05.360 --> 15:10.720
Another part of it is strategic partnerships and economics.

15:10.720 --> 15:17.240
We want to be able to take advantage of whatever cloud we can get the best pricing in, and

15:17.240 --> 15:25.640
so that certainly has advantages to being able to move our workloads between clouds and

15:25.640 --> 15:27.440
our own cluster, of course.

15:27.440 --> 15:32.520
And so you've got these disparate research workloads.

15:32.520 --> 15:40.760
You've got these multiple infrastructure environments, clouds plus your on-premises cluster.

15:40.760 --> 15:49.360
And as we've alluded to, Kubernetes is a part of the platform that ties all this together.

15:49.360 --> 15:54.640
Taking a step back from that, when you describe the platform for machine learning and deep

15:54.640 --> 15:57.760
learning at OpenAI, how do you describe it?

15:57.760 --> 15:59.240
What are the major pieces?

15:59.240 --> 16:01.400
Do you have names for things?

16:01.400 --> 16:05.720
What kind of walk us through the platform?

16:05.720 --> 16:07.720
Yeah, for sure.

16:07.720 --> 16:10.680
Yeah, so Kubernetes is a big piece of it.

16:10.680 --> 16:14.520
We run three different production clusters.

16:14.520 --> 16:19.360
We name all of our production clusters, animals alphabetically.

16:19.360 --> 16:24.640
So the latest one, we're up to J now, which is Jaguar.

16:24.640 --> 16:26.000
So that's our newest one.

16:26.000 --> 16:29.560
We've got two others, Horus and Ibus.

16:29.560 --> 16:38.080
So each of those is a Kubernetes cluster limited to a specific cloud and geographic region

16:38.080 --> 16:40.680
or our on-prem cluster.

16:40.680 --> 16:45.840
And then sort of outside of that, we've got a number of tools.

16:45.840 --> 16:52.720
So the rapid experimentation tool is the one that our data research team developed.

16:52.720 --> 16:59.400
And that does sort of all of their experimentation management and provides a bunch of core building

16:59.400 --> 17:02.360
pieces for building their experiments as well.

17:02.360 --> 17:06.000
And that's one that our robotics team uses as well.

17:06.000 --> 17:09.240
And a couple other teams have experimented with.

17:09.240 --> 17:13.960
So that has really helped us, I'd say, move quickly and be able to launch new research

17:13.960 --> 17:18.400
experiments in the reinforcement learning space because that's what that tool is optimized

17:18.400 --> 17:20.120
for.

17:20.120 --> 17:24.880
Those core building pieces, can you give us some examples of what those are?

17:24.880 --> 17:33.520
Yeah, so they've got an implementation of the PPO algorithm of the what algorithm?

17:33.520 --> 17:35.720
PPO, proximal policy optimization.

17:35.720 --> 17:36.720
Got it.

17:36.720 --> 17:37.720
Okay.

17:37.720 --> 17:38.720
Yeah.

17:38.720 --> 17:44.080
They've got a special implementation for synchronizing multiple GPUs that are spread across

17:44.080 --> 17:49.640
machines to do distributed training, a bunch of infrastructure just kind of for tracking

17:49.640 --> 17:56.200
the experiments, managing optimizers, managing tiers of machines that are running the environment

17:56.200 --> 18:01.800
like the data to game or a physics simulator, streaming that experience back to the optimizer

18:01.800 --> 18:03.960
machines.

18:03.960 --> 18:09.160
So kind of managing the application level distributed system that has to do all the training.

18:09.160 --> 18:16.720
Is that tool used for multiple experiments and what's the I'm curious how customized

18:16.720 --> 18:24.240
that is for a specific application or experiment or if it's, you know, can it can a new experiment

18:24.240 --> 18:28.840
kind of plug into it pretty easily and take advantage of all the different sub components

18:28.840 --> 18:30.240
that it offers?

18:30.240 --> 18:31.240
Yeah.

18:31.240 --> 18:38.160
So it started out quite specialized for the data team and then has expanded into a more

18:38.160 --> 18:42.880
general research tool that we're now using in a number of teams across the company.

18:42.880 --> 18:46.960
So the robotics team was the second team that started using it and they've applied it

18:46.960 --> 18:54.760
to some of their robotic hand experiments and that kind of turned out to be actually a

18:54.760 --> 18:55.760
similar problem.

18:55.760 --> 19:02.680
You replace the data to game with a physics simulator and then a lot of the other components

19:02.680 --> 19:04.880
really fit together nicely.

19:04.880 --> 19:09.480
And now we've got at least one other team that's starting to use it and they also I believe

19:09.480 --> 19:13.160
we're able to get started really quickly, adapt it for their experiments.

19:13.160 --> 19:18.800
So I think a lot of our teams that are doing reinforcement learning are able to just

19:18.800 --> 19:20.920
move their experiments over to it really quickly.

19:20.920 --> 19:21.920
Okay.

19:21.920 --> 19:22.920
Cool.

19:22.920 --> 19:27.600
And so you mentioned you've got these three production clusters and just to clarify,

19:27.600 --> 19:33.120
those are the three clusters on prem and then you've got additional clusters in the

19:33.120 --> 19:34.120
cloud.

19:34.120 --> 19:35.120
Is that right?

19:35.120 --> 19:37.720
Oh no, those are our three Kubernetes clusters.

19:37.720 --> 19:40.520
So one of them is on prem, the other two are on the cloud.

19:40.520 --> 19:41.520
Okay.

19:41.520 --> 19:50.680
And do the three clusters support presumably different research workloads simultaneously

19:50.680 --> 19:57.160
or are they dedicated to a specific research workload at a time?

19:57.160 --> 19:58.160
Yeah.

19:58.160 --> 20:01.440
So it varies a bit.

20:01.440 --> 20:07.640
And because research projects can do the change in scope over time.

20:07.640 --> 20:11.520
And yeah, they kind of ebb and flow in terms of their compute needs.

20:11.520 --> 20:13.840
It changes over time.

20:13.840 --> 20:17.520
Right at the moment, I believe all three clusters are being shared.

20:17.520 --> 20:21.760
I don't think any of them are dedicated to a single team at the moment.

20:21.760 --> 20:27.320
But it tends to be that like two or three teams will share one cluster and that kind of

20:27.320 --> 20:33.800
makes it easier for them to know how much capacity they're going to be able to use.

20:33.800 --> 20:37.560
And it's just kind of easier for the infrastructure team also to know who it is.

20:37.560 --> 20:40.040
It tends to be using each cluster.

20:40.040 --> 20:43.360
But at the moment, we run all three of them, the shared cluster is where anyone is free

20:43.360 --> 20:44.680
to use the capacity.

20:44.680 --> 20:45.680
Taking a step back.

20:45.680 --> 20:50.280
So Kubernetes deals with containerized workloads.

20:50.280 --> 20:55.600
So the training workload, for example, needs to be containerized.

20:55.600 --> 21:00.200
That's not necessarily a skill that the typical data scientist has.

21:00.200 --> 21:06.520
One of the things that I read is that you've built some tooling that kind of does that

21:06.520 --> 21:12.080
for the researchers so they don't have to get into the weeds in terms of containerizing

21:12.080 --> 21:13.080
their workloads.

21:13.080 --> 21:14.800
Can you talk a little bit about that?

21:14.800 --> 21:15.800
Yeah.

21:15.800 --> 21:19.320
So one of the things that we try to do in general is provide a lot of flexibility in terms

21:19.320 --> 21:25.760
of tooling because different teams and different researchers may have a different level of comfort

21:25.760 --> 21:27.680
with different tool sets.

21:27.680 --> 21:33.960
And so yeah, people can use Kubernetes directly and build their Docker containers and there's

21:33.960 --> 21:36.080
a bunch of people who are comfortable with that.

21:36.080 --> 21:37.080
That's what they do.

21:37.080 --> 21:43.760
But then, yeah, there's also a set of people that prefer different types of interfaces

21:43.760 --> 21:48.840
and maybe something but abstracts way all of the container details.

21:48.840 --> 21:57.720
And so another tool that we have called ARCALL, it handles all of the Docker containers

21:57.720 --> 22:05.200
and, in fact, abstracts away sort of all of the Kubernetes details so that you just

22:05.200 --> 22:11.320
have your Python code in a directory and it knows where to find it, uploads it to the

22:11.320 --> 22:17.000
cluster, launches your containers, launches all your experiments, and then you can just

22:17.000 --> 22:23.200
focus on writing your TensorFlow and other Python code and not even need to necessarily

22:23.200 --> 22:27.120
know that it's running inside a Docker container or really that it's a Kubernetes cluster

22:27.120 --> 22:28.120
even.

22:28.120 --> 22:33.880
And are the researchers there typically working with like Git repositories?

22:33.880 --> 22:38.960
So are they checking in code and are you pulling from those Git repos when you're containerizing

22:38.960 --> 22:43.040
or is it all just slurping stuff up from directories on the desktop?

22:43.040 --> 22:48.480
Yeah, we use Git quite extensively here.

22:48.480 --> 22:54.440
And I guess, again, kind of a mix of things depending on the workflow that researchers

22:54.440 --> 23:00.440
and most convenient, yeah, some of them are launching their experiments from code that

23:00.440 --> 23:05.520
they have locally, although that's almost always in a Git repository just from their local

23:05.520 --> 23:14.520
checkout for other ones, yeah, maybe that it's built by a container, a build engine

23:14.520 --> 23:18.800
like QA, and then we're deploying a container from that registry.

23:18.800 --> 23:26.240
Earlier on you mentioned that your use of Kubernetes is kind of evolved from when you started

23:26.240 --> 23:35.360
to today, can you talk a little bit about that journey in general, how your use of Kubernetes

23:35.360 --> 23:41.840
has evolved as Kubernetes has evolved, what some of the big challenges with the different

23:41.840 --> 23:48.040
phases that you've gone through with your architecture has been and how you've overcome

23:48.040 --> 23:49.040
those?

23:49.040 --> 23:52.080
Yeah, it has changed in a number of ways.

23:52.080 --> 23:56.080
Certainly one is the scale, our clusters have gotten way bigger than when I first started

23:56.080 --> 24:02.080
but they've also sort of transitioned in their design.

24:02.080 --> 24:08.800
So when I first joined the design that we had was a single production Kubernetes cluster

24:08.800 --> 24:18.840
and it was a single multi-cloud cluster, so it spanned both AWS and Azure and it sort

24:18.840 --> 24:24.000
of led to some behavior that was difficult to reason about, I don't think for how many

24:24.000 --> 24:29.040
people run Kubernetes clusters where a single cluster is cross-cloud and especially when

24:29.040 --> 24:35.160
we wanted to scale that up significantly to thousands of machines, we switched over

24:35.160 --> 24:45.280
to a model where we have multiple Kubernetes clusters and each one is limited to a specific

24:45.280 --> 24:53.280
physical region, so either are on-prem clusters or availability zone in the cloud.

24:53.280 --> 24:58.640
And that made it a little easier to reason about some of the network aspects of it and

24:58.640 --> 25:03.000
not need to worry about some of the cross-cloud issues that we'd run into.

25:03.000 --> 25:05.200
So that really allowed us to scale more.

25:05.200 --> 25:09.680
Can you give me an example of some of the issues that you ran into that kind of led to that

25:09.680 --> 25:10.680
shift?

25:10.680 --> 25:12.360
What were the kinds of things that you'd see?

25:12.360 --> 25:13.360
Yeah, for sure.

25:13.360 --> 25:22.000
Also because we were needed to run this cross-cloud, we had the control plan in AWS and then we

25:22.000 --> 25:29.440
had a bunch of IP sect tunnels that connected to the other availability zones where the

25:29.440 --> 25:31.040
workers were running in.

25:31.040 --> 25:35.840
So we would have one set of workers running in like Azure's East Coast region and one

25:35.840 --> 25:42.640
West Coast region and all of these would talk back to the Kubernetes control plan over

25:42.640 --> 25:45.720
this IP sect tunnel back in AWS.

25:45.720 --> 25:52.720
But this meant that that IP sect tunnel was kind of a single point of failure if that went

25:52.720 --> 25:53.720
down.

25:53.720 --> 25:57.080
Not necessarily the entire cluster, but a large section of the cluster because that whole

25:57.080 --> 26:01.560
group of workers would now be cut off from the control plan.

26:01.560 --> 26:04.800
So yeah, there was that problem.

26:04.800 --> 26:12.240
Then there was also the problem of if you wanted to use something like Amazon's file system

26:12.240 --> 26:17.840
like in on the name right now, EFS, I think it's called EBS or S3.

26:17.840 --> 26:25.320
Yeah, S3 I guess was a little bit less problematic, but certainly EBS, that's something that you'd

26:25.320 --> 26:26.320
like to use.

26:26.320 --> 26:30.480
But now if some of your workers are in Azure, then you can only use it in half your cluster

26:30.480 --> 26:35.440
and then that's really confusing if you end up with sometimes you can use it and sometimes

26:35.440 --> 26:44.440
you can't and then I guess one of the most difficult to overcome problems was that if you

26:44.440 --> 26:48.680
scheduled your job into the cluster, it was possible for your job to get split so that

26:48.680 --> 26:51.840
half of it was in Azure and half of it was in AWS.

26:51.840 --> 26:56.240
And now if there was any network communication between them, that would just go very, very

26:56.240 --> 26:57.240
poorly.

26:57.240 --> 26:58.240
Right.

26:58.240 --> 26:59.240
Right.

26:59.240 --> 27:04.600
So you had to be very careful to specify that your job should only be in one cloud and

27:04.600 --> 27:05.600
not the other.

27:05.600 --> 27:06.600
Okay.

27:06.600 --> 27:11.760
But kind of all of that led to us just changing the model so that one Kubernetes cluster, one

27:11.760 --> 27:14.600
availability is out, then you don't have to worry about any of that.

27:14.600 --> 27:15.600
Right.

27:15.600 --> 27:16.600
Yeah, it's interesting.

27:16.600 --> 27:22.240
I think in the Kubernetes community, certainly there's a lot of talk about hybrid cloud and

27:22.240 --> 27:29.240
multi cloud and even the ability to run a single Kubernetes cluster spanning multiple

27:29.240 --> 27:30.240
clouds.

27:30.240 --> 27:35.680
It's actually document and not necessarily thoroughly, but it's there, but I don't get

27:35.680 --> 27:40.760
the impression that a lot of people do it for reasons like the ones that you're describing.

27:40.760 --> 27:41.760
Yeah.

27:41.760 --> 27:46.040
So some tough problems and we were trying to solve them pretty early in the development

27:46.040 --> 27:47.040
of Kubernetes.

27:47.040 --> 27:53.200
I imagine if we tried to do it now, the experience might be a little smoother, but some of those

27:53.200 --> 27:56.160
are just difficult problems to solve.

27:56.160 --> 28:00.840
And then I guess the other area that our Kubernetes clusters have evolved significantly

28:00.840 --> 28:05.240
since I joined is our on-prem cluster.

28:05.240 --> 28:08.400
So that did not exist when I joined.

28:08.400 --> 28:13.520
And as one that we've been continuing to scale up, just as we have more and more demand

28:13.520 --> 28:18.400
for cutting edge hardware that's not available in the clouds.

28:18.400 --> 28:26.080
And so with that particular motivation, front and center, in particular cutting edge hardware,

28:26.080 --> 28:32.640
for example, you have access to an Nvidia DGX, I'm sure you have access to all the latest

28:32.640 --> 28:34.520
and greatest GPUs.

28:34.520 --> 28:42.200
Are you able to utilize Kubernetes features like labels and things like that to target

28:42.200 --> 28:48.200
workloads to these specific cutting edge hardware or do you just kind of throw them all

28:48.200 --> 28:53.120
into the pool and let them land wherever they land and kind of each incremental hardware

28:53.120 --> 28:56.120
piece just adds what it can bring?

28:56.120 --> 28:57.760
Yeah, so we definitely do a bit of both.

28:57.760 --> 29:05.640
We use labels pretty extensively to provide finer grand control of exactly what hardware

29:05.640 --> 29:08.040
you get scheduled on as much as possible.

29:08.040 --> 29:14.680
We also try to keep our clusters homogeneous or mostly homogeneous so that you don't need

29:14.680 --> 29:19.480
to put too much thought into specifying exactly what you want, but yeah, it's a bit of

29:19.480 --> 29:20.480
both.

29:20.480 --> 29:26.440
Do you find in general that the out of the box Kubernetes scheduler does what you need

29:26.440 --> 29:28.440
for these types of workloads?

29:28.440 --> 29:35.120
Yeah, so we've made some various tweaks to it, although we're still using the upstream

29:35.120 --> 29:39.720
scheduler and I've just made like config changes and then sort of built some services on

29:39.720 --> 29:42.640
top of Kubernetes.

29:42.640 --> 29:50.360
We now have a system that uses the mutating webhooks feature along with a controller

29:50.360 --> 29:55.240
and taints that sort of manage the resources in our cluster so that certain groups of

29:55.240 --> 29:58.240
machines can be dedicated to a certain team.

29:58.240 --> 30:04.120
And then when members of that team submit their pods, those pods will receive a toleration

30:04.120 --> 30:06.720
that allows them to run on those reserved machines.

30:06.720 --> 30:10.560
Is that an alternative to using namespaces or is that something that works in conjunction

30:10.560 --> 30:13.560
with namespaces to provide that feature?

30:13.560 --> 30:17.280
Yeah, so it works in conjunction with namespaces.

30:17.280 --> 30:24.280
So here of every researcher their own namespace and then most teams also get their own namespace

30:24.280 --> 30:28.080
and they can have one if they ask for it, just kind of depends on how they like to structure

30:28.080 --> 30:29.560
their workflow.

30:29.560 --> 30:37.040
And then we integrate all of this back into our directory service so that it knows which

30:37.040 --> 30:43.560
people are on which team and then the pods in those namespaces can then be granted a

30:43.560 --> 30:47.160
toleration to run on capacity that's reserved for that team.

30:47.160 --> 30:51.760
So yeah, that's an area where we consider building all of this into a custom scheduler

30:51.760 --> 30:56.800
but it seemed much easier to support if we just built it as a feature on top of the Kubernetes

30:56.800 --> 31:01.240
APIs that could then interact with the scheduler in the normal way.

31:01.240 --> 31:06.880
And I guess the one significant config change that we've made to the default Kubernetes

31:06.880 --> 31:14.400
scheduler is that the default one prefers to spread out the pods in like a deployment

31:14.400 --> 31:18.120
or a job as much as possible, which is good for fault tolerance.

31:18.120 --> 31:23.720
But for us, we tend to want the opposite, it would actually like as many pods as possible

31:23.720 --> 31:29.720
to be packed into single machines so that it leaves other machines completely empty.

31:29.720 --> 31:35.200
And that's good for two reasons one is that if someone submits a bunch of small pods,

31:35.200 --> 31:38.000
we don't want them to fill up tiny pieces of every machine.

31:38.000 --> 31:43.160
We want them to packed into a few so that their whole machines available for people that

31:43.160 --> 31:45.320
want to use an entire machine.

31:45.320 --> 31:49.520
And then the other thing that it really helps us with is auto scaling.

31:49.520 --> 31:54.800
So in our cloud clusters, we auto scaled them up and down depending on demand.

31:54.800 --> 32:00.160
And so you want to have your machines utilized as heavily as possible so that you can scale

32:00.160 --> 32:02.640
in the unused machines.

32:02.640 --> 32:09.880
What does a small mean in that context is it's based on kind of a real time metric like utilization

32:09.880 --> 32:11.960
or is it something else?

32:11.960 --> 32:12.960
Oh, yeah.

32:12.960 --> 32:17.600
So yeah, in terms of small, I mean like how many resources are being requested.

32:17.600 --> 32:25.200
So I would consider it small if you were requesting only one GPU or maybe you're only requesting

32:25.200 --> 32:33.520
like half a dozen CPU cores, whereas like our bigger pods, those are usually requesting

32:33.520 --> 32:40.680
HGPUs or maybe four GPUs and then they're requesting dozens or maybe even 50 or 60 CPU

32:40.680 --> 32:41.680
cores.

32:41.680 --> 32:53.280
How do you manage if at all the issue of folks submitting jobs, IE pods and reserving

32:53.280 --> 32:58.120
essentially these resources, but the pods not actually, you know, being idle essentially.

32:58.120 --> 33:00.480
Is that something that you actively deal with?

33:00.480 --> 33:04.000
Yeah, so yeah, I guess there's kind of two parts of it.

33:04.000 --> 33:10.920
We just kind of trust all of our researchers to use the clusters resources well.

33:10.920 --> 33:16.600
And so we don't have any like checking of are you using the resources that you requested.

33:16.600 --> 33:26.000
So what we do have is a sort of internal billing system where we monitor how many pods you

33:26.000 --> 33:29.800
had running and how many resources they requested and how long those resources were requested

33:29.800 --> 33:31.280
for.

33:31.280 --> 33:34.920
And then it rolls up your billing every day.

33:34.920 --> 33:42.360
And you can see a report of this is how much money effectively you spent on experiments.

33:42.360 --> 33:46.280
And so yeah, if you were leaving a bunch of pods running the ride, it would show up in

33:46.280 --> 33:47.280
your bill.

33:47.280 --> 33:52.560
And is that something that you had to build or was that a project in the Kubernetes ecosystem

33:52.560 --> 33:56.200
that you were just able to kind of turn on and point to your clusters?

33:56.200 --> 34:02.200
Yeah, it's something that we built, although it's actually very little code will probably

34:02.200 --> 34:06.240
open source it if someone else doesn't open source a better solution that we end up switching

34:06.240 --> 34:07.240
to.

34:07.240 --> 34:15.520
But it's based on Prometheus, so Prometheus already monitors all of the metrics of what

34:15.520 --> 34:19.880
pods were running when were they running for how much did they request.

34:19.880 --> 34:26.280
And so really this is just a pretty simple Python script that query is Prometheus aggregates

34:26.280 --> 34:33.080
all of that data groups it by namespace and then integrates with our directory service

34:33.080 --> 34:38.400
to figure out which team and people's namespace is belong to and that rolls up all the cost

34:38.400 --> 34:39.400
by team.

34:39.400 --> 34:40.400
Okay, cool.

34:40.400 --> 34:46.200
And it sounds like you're running upstream and you mentioned Prometheus, what other tools

34:46.200 --> 34:51.880
are you running or projects in conjunction with your use of Kubernetes?

34:51.880 --> 34:53.560
Yeah, let's see.

34:53.560 --> 35:01.800
So we use the cluster auto-scaler, so we use that for scaling our clusters.

35:01.800 --> 35:11.200
It's Prometheus, we use Heapster, we use a flannel for an Overland network, we've got

35:11.200 --> 35:18.880
some deployments of the cluster, although that's sort of lusterfully related to Kubernetes.

35:18.880 --> 35:21.800
And I guess those are kind of the main ones.

35:21.800 --> 35:24.440
Okay, you mentioned Gluster.

35:24.440 --> 35:30.480
What are the different ways that you deal with storage in the context of these clusters?

35:30.480 --> 35:36.400
Yeah, so that's one of the areas that we're, I feel like we have an OK solution now, but

35:36.400 --> 35:39.040
I'd really like us to have a great solution.

35:39.040 --> 35:41.360
It's a tough problem.

35:41.360 --> 35:43.960
It is a very tough problem.

35:43.960 --> 35:50.160
So we've got Gluster deployed and it's been reasonably good.

35:50.160 --> 35:56.360
We haven't quite gotten the performance that we want out of it, but it's definitely convenient.

35:56.360 --> 36:03.480
And yeah, teams certainly have some data storage in S3 and Google Cloud Store and Azure's

36:03.480 --> 36:04.480
Blob Store.

36:04.480 --> 36:08.320
Yeah, so I spread out over those.

36:08.320 --> 36:17.920
For our Cloud Kubernetes clusters, we use whatever sort of EBS equivalent they have.

36:17.920 --> 36:21.680
Google, that's their persistent volumes, that's really convenient.

36:21.680 --> 36:26.680
For our on-prem cluster, I think what we're going to end up deploying is Saf, possibly through

36:26.680 --> 36:31.800
the REC project, but we haven't started on that yet.

36:31.800 --> 36:35.560
So it's a little bit hard to say what we're going to end up with, but that seems like

36:35.560 --> 36:37.280
it has a lot of promise.

36:37.280 --> 36:46.920
It sounds like then there's a strong inclination towards a distributed storage solution that

36:46.920 --> 36:53.160
kind of runs on the existing compute infrastructure as opposed to some standalone monolithic kind

36:53.160 --> 36:55.720
of NFS type of thing.

36:55.720 --> 37:02.240
Yeah, so I guess what we have found, so I guess Gluster sort of falls more into the single

37:02.240 --> 37:04.920
shared file system like NFS.

37:04.920 --> 37:10.200
And what we've found is that the performance hasn't been quite as good as what we want,

37:10.200 --> 37:14.640
although maybe that could be resolved by tuning some of the configuration settings or adding

37:14.640 --> 37:22.400
more hardware, but the other issue has been fault domains that when one team really starts

37:22.400 --> 37:29.760
hammering the storage, then it causes a disruption in service for everyone.

37:29.760 --> 37:35.080
And so one of the potential benefits that we see of something like Saf is hopefully better

37:35.080 --> 37:42.400
fault isolation where that won't happen because you'll have your separate or weblock devices.

37:42.400 --> 37:50.120
You mentioned a flannel for networking, how has that dealing with the networking been

37:50.120 --> 37:54.160
since you've gone away from the multi-cloud?

37:54.160 --> 38:00.440
Is that kind of addressed all your issues or is it still a challenge for you?

38:00.440 --> 38:06.680
Yeah, I'd say that that really simplified things and these days I don't worry too much

38:06.680 --> 38:08.720
about our networking.

38:08.720 --> 38:16.720
So flannel has worked out nicely, we have integrated all of our clusters together via VPN so the

38:16.720 --> 38:22.280
researchers can connect all of them very easily and that's worked out well.

38:22.280 --> 38:27.560
And with our on-prem cluster we've kind of figured out some of the performance aspects originally

38:27.560 --> 38:33.560
when we had deployed flannel, we had been deployed it in a way that wasn't as performant

38:33.560 --> 38:38.280
but when we switched over to, so we're now using their direct routing feature.

38:38.280 --> 38:41.800
And that really resolved the performance problems that we were seeing there.

38:41.800 --> 38:52.320
So yeah, it's pretty much been a non-issue since we made some re-architectures to our clusters.

38:52.320 --> 39:01.120
And one of the advantages of the single Kubernetes cluster as opposed to a federated would

39:01.120 --> 39:06.600
be that the researchers don't need to think about where a particular workload goes.

39:06.600 --> 39:10.960
Have you found another way to abstract that or do they just have to, you know, do they

39:10.960 --> 39:14.920
target a specific cluster when they're deploying workloads?

39:14.920 --> 39:18.120
Yeah, for sure.

39:18.120 --> 39:25.360
So our experimentation tools handle a little bit of that of trying to abstract it away so

39:25.360 --> 39:29.160
that the researchers don't need to worry about it so much.

39:29.160 --> 39:35.600
Longer term I've been sort of loosely following the upstream work that's being done in federated

39:35.600 --> 39:40.000
Kubernetes and how that all is going to work.

39:40.000 --> 39:44.520
I think at some point in the future, we're really hoping to switch over to a model like

39:44.520 --> 39:49.760
that where there is a federated control plane that can then handle a schedule length that

39:49.760 --> 39:54.640
looks like a single cluster, but yeah, not there yet.

39:54.640 --> 39:56.480
Right, right, right.

39:56.480 --> 40:02.920
Speaking of which, what's been your experience with Kubernetes as it evolves?

40:02.920 --> 40:08.520
How have changes on the Kubernetes side changed the way you use it?

40:08.520 --> 40:15.280
Maybe starting, was there any, did you run into any hurdles early on that you had to

40:15.280 --> 40:22.480
maybe develop around or hack or hound that Kubernetes eventually caught up with what

40:22.480 --> 40:29.160
your needs were, whether it's in terms of its operation or scalability or reliability

40:29.160 --> 40:33.720
or things like that, or how's that whole experience been for you?

40:33.720 --> 40:35.520
Yeah, definitely.

40:35.520 --> 40:39.200
So I'd say in general, super positive.

40:39.200 --> 40:45.040
The direction that Kubernetes has been going in has really just continued to address more

40:45.040 --> 40:48.640
and more of the pain points that we've seen.

40:48.640 --> 40:55.000
So yeah, I guess a few specific ones, and we were probably one of the first people using

40:55.000 --> 41:04.480
Kubernetes to schedule GPU devices, and so we were using the alpha feature that added support

41:04.480 --> 41:12.200
for that, I believe as soon as it was released, and so we did have to build up some additional

41:12.200 --> 41:20.360
tooling for handling that and making it work seamlessly, like in particular, the device

41:20.360 --> 41:23.320
drivers need to be mounted into your container.

41:23.320 --> 41:27.480
And so we had kind of a whole way of managing that with Simbling so that people could do

41:27.480 --> 41:28.480
it.

41:28.480 --> 41:32.320
Now that Nvidia has released their device plugin, that's kind of all been automated for

41:32.320 --> 41:37.520
us, and so we switched over to that in all of our clusters, and it has simplified all

41:37.520 --> 41:43.520
of the management of the GPU devices there, and also like on the scalability side, I think

41:43.520 --> 41:49.640
Kubernetes now officially has been tested up to like 5,000 nodes in the past.

41:49.640 --> 41:58.400
That was not as true, and we had to do something to work around various scalability issues,

41:58.400 --> 42:04.160
but now a lot of that has just been handled by upstream where they have optimized things.

42:04.160 --> 42:09.840
You mentioned in terms of things that you're looking forward to, you mentioned some of the

42:09.840 --> 42:14.640
work around federated Kubernetes, are there other things that you see on the horizon

42:14.640 --> 42:21.000
for Kubernetes or more broadly that ecosystem that you're excited about?

42:21.000 --> 42:24.280
Yeah, definitely federated Kubernetes.

42:24.280 --> 42:29.040
Yeah, earlier I mentioned the Ruck project, and that's definitely one that I've been

42:29.040 --> 42:34.240
following and have quite a bit of an interest in.

42:34.240 --> 42:39.480
It looks like it might make it really easy to run Seth at our on-prem clusters, so that

42:39.480 --> 42:42.080
would be really exciting for us.

42:42.080 --> 42:47.280
So I'm hoping that that project continues to get more traction.

42:47.280 --> 42:54.760
And beyond that, one of the things that we're looking at next is on the network side.

42:54.760 --> 43:01.720
We're planning to enable Rocky, the RDMA over-converged Ethernet protocol in our on-prem

43:01.720 --> 43:07.760
clusters, and so this is one of the things that we can't get in the cloud, that's why

43:07.760 --> 43:09.320
we have our on-prem cluster.

43:09.320 --> 43:16.440
So I'm not familiar with that one, you said Rocky, and RDMA would be for remote memory

43:16.440 --> 43:17.440
access.

43:17.440 --> 43:18.440
Yeah, exactly.

43:18.440 --> 43:19.440
Are you familiar with Infiniband?

43:19.440 --> 43:20.640
Oh, yeah, sure.

43:20.640 --> 43:27.440
Yeah, okay, so Rocky is basically the same thing as Infiniband, but it goes over Ethernet

43:27.440 --> 43:31.800
instead of the custom Infiniband protocol.

43:31.800 --> 43:32.800
Oh, cool.

43:32.800 --> 43:39.040
Yeah, so it's really cool, because then you can have all of your normal Ethernet workloads

43:39.040 --> 43:43.440
that are doing TCP or UDP or whatever.

43:43.440 --> 43:49.800
And then you can also have your RDMA applications that are going over exactly the same links.

43:49.800 --> 43:51.840
And yeah, so we're pretty excited about that.

43:51.840 --> 43:58.120
I think that's really going to improve both throughput that we can get, and also really

43:58.120 --> 43:59.880
cut down on latency.

43:59.880 --> 44:07.160
And Infiniband has become pretty standard for some of these high performance computing workloads,

44:07.160 --> 44:09.240
because of its performance.

44:09.240 --> 44:12.280
Are you able to, how close do you get to that?

44:12.280 --> 44:14.600
And this is over 100G Ethernet?

44:14.600 --> 44:21.080
Yeah, so we've done some testing on 100G Ethernet, and I believe have seen up to like

44:21.080 --> 44:23.400
97% like utilization, so.

44:23.400 --> 44:24.400
Oh, wow.

44:24.400 --> 44:25.400
Pretty awesome.

44:25.400 --> 44:31.880
If I remember correctly, on the Infiniband side, correct me if I'm mischaracterizing this,

44:31.880 --> 44:40.080
my impression is that it's been limited to kind of these niche use cases like HPC, because

44:40.080 --> 44:45.840
it requires that the applications need to be modified to take advantage of this memory

44:45.840 --> 44:46.920
access.

44:46.920 --> 44:47.920
Is that true?

44:47.920 --> 44:51.120
And will the same thing need to happen with Rocky?

44:51.120 --> 44:53.440
Yeah, so it is true.

44:53.440 --> 44:56.600
And yes, the same thing will need to happen.

44:56.600 --> 44:59.360
But at least you don't need a whole separate network infrastructure.

44:59.360 --> 45:00.360
Right.

45:00.360 --> 45:05.560
At least you don't need to buy completely new network cards and new switches and everything.

45:05.560 --> 45:08.040
So then it can just happen at the application level.

45:08.040 --> 45:13.120
And really, what I'm hoping will happen is that we can handle all of it in the framework.

45:13.120 --> 45:18.080
And then you'll just have like a, you know, op that you plug into your TensorFlow graph,

45:18.080 --> 45:25.400
which is the, you know, all reduce over our DMA operation.

45:25.400 --> 45:31.000
And then people that are doing research won't have to worry about any of that, but that's

45:31.000 --> 45:34.920
still to be seen how that works out and how all of this is going to integrate into the

45:34.920 --> 45:35.920
Kubernetes ecosystem.

45:35.920 --> 45:36.920
Nice.

45:36.920 --> 45:37.920
Nice.

45:37.920 --> 45:42.440
And you mentioned something that made me realize that one area that we skipped was how

45:42.440 --> 45:45.320
you're doing distributed training.

45:45.320 --> 45:46.520
Can you talk a little bit about that?

45:46.520 --> 45:51.360
There's a number of solutions for TensorFlow, for example, there's out of the box, there's

45:51.360 --> 45:55.960
porovod, there are some others like how have you approached distributed training?

45:55.960 --> 45:56.960
Yeah.

45:56.960 --> 46:02.800
So that's one where we have a number of different approaches.

46:02.800 --> 46:06.080
So we're definitely using porovod.

46:06.080 --> 46:07.080
I've used it a bit.

46:07.080 --> 46:09.240
I know a bunch of other teams are using it.

46:09.240 --> 46:12.440
It's worked out quite well for us.

46:12.440 --> 46:17.880
We have some people that are just like directly using MPI for Pi.

46:17.880 --> 46:22.040
I think that has also worked well.

46:22.040 --> 46:28.480
I believe there's at least one custom implementation of all reduce, and one of our teams has written.

46:28.480 --> 46:33.600
So yeah, kind of a few different solutions haven't converged on anything particular.

46:33.600 --> 46:39.760
And the idea is, in part, because everything is containerized, they just containerized

46:39.760 --> 46:45.840
those components of their distributed training, deploy them out via Kubernetes, and it all

46:45.840 --> 46:48.240
kind of works like anything else.

46:48.240 --> 46:49.240
Exactly.

46:49.240 --> 46:50.240
Cool.

46:50.240 --> 46:56.520
Well, any thoughts or words of wisdom for folks that are interested in Kubernetes as a platform

46:56.520 --> 47:00.680
for ML and DL and just getting started?

47:00.680 --> 47:01.680
Yeah.

47:01.680 --> 47:09.280
I'd say that the portability aspect of it and just the use of running it and scaling it is

47:09.280 --> 47:12.400
something that has really benefited us a lot.

47:12.400 --> 47:16.640
Especially if those are important factors that are recommended.

47:16.640 --> 47:23.440
And in terms of ease of running it, sometimes Kubernetes or at least early on Kubernetes had

47:23.440 --> 47:26.640
this reputation for not being particularly easy.

47:26.640 --> 47:34.040
What's the support burden on your team for supporting the cluster bin like or the clusters?

47:34.040 --> 47:36.680
Granted that you're already had a pretty tremendous scale.

47:36.680 --> 47:45.880
Yeah, well, and I would say that that early reputation did reflect our experience too.

47:45.880 --> 47:50.880
When I joined, we had a lot of problems that we were resulting.

47:50.880 --> 47:55.400
And there was a pretty non-trivial support burden.

47:55.400 --> 48:02.460
But both as Kubernetes has evolved and as we have evolved our way of managing it and

48:02.460 --> 48:08.440
understood some of the aspects they need to pay attention to, like EtsyD in particular,

48:08.440 --> 48:14.840
really want to make sure that you've provisioned enough IOPS and everything for that.

48:14.840 --> 48:22.000
Now we've gotten to a point where I would say there's a very, very little maintenance burden.

48:22.000 --> 48:29.440
We have an on-call rotation, of course, but in terms of critical issues, I'd say it's

48:29.440 --> 48:33.240
rare that we have even one problem a month.

48:33.240 --> 48:34.240
Oh, wow.

48:34.240 --> 48:39.400
So let me re-ask that last question about words of wisdom and light of all of the kind of

48:39.400 --> 48:45.280
revealed wisdom over the past coming up on two years now that's gone into allowing

48:45.280 --> 48:48.640
it to be easy to maintain.

48:48.640 --> 48:53.720
You mentioned, make sure you provision enough IOPS to your EtsyD.

48:53.720 --> 48:58.920
What are some of the other things that folks need to do in order to have a good experience

48:58.920 --> 49:01.760
managing Kubernetes for these kinds of workloads?

49:01.760 --> 49:02.760
Yeah.

49:02.760 --> 49:09.320
So we actually published a whole blog post back in January or February that talks about some

49:09.320 --> 49:10.560
of the top ones.

49:10.560 --> 49:15.080
So yeah, definitely, EtsyD is one of those.

49:15.080 --> 49:18.760
And most of these, there are really issues that you're only going to hit once you go

49:18.760 --> 49:20.600
beyond a few hundred machines.

49:20.600 --> 49:23.960
So you can run it at a pretty good scale and not have to worry about much of it.

49:23.960 --> 49:28.200
But yeah, once you get into the thousand or multiple thousands of machines, there's a bunch

49:28.200 --> 49:29.200
of stuff to be aware of.

49:29.200 --> 49:36.560
So, yeah, EtsyD was one of them ran into some funny network issues.

49:36.560 --> 49:42.400
I spent a good couple of days learning all about ARP caches and how those work.

49:42.400 --> 49:47.200
And it was predical that I had learned about back in college and figured out never needs

49:47.200 --> 49:48.200
to know about again.

49:48.200 --> 49:53.600
But very important to make sure that the ARP cache is big enough.

49:53.600 --> 50:00.320
The combination of using flannel means that you need a significant amount of space in

50:00.320 --> 50:01.640
your ARP cache.

50:01.640 --> 50:06.480
If you've got thousands of containers that are talking to each other or talking to a single

50:06.480 --> 50:09.400
one, like the CUBE DNS service.

50:09.400 --> 50:15.840
So there's a few other issues that we enumerated in that blog post, but it's really not stuff

50:15.840 --> 50:18.440
that you're going to hit until you've got many hundreds of machines.

50:18.440 --> 50:21.480
Well, we'll link to that in the show notes for sure.

50:21.480 --> 50:25.760
Chris, thanks so much for taking the time to chat with me about this super interesting

50:25.760 --> 50:26.760
stuff.

50:26.760 --> 50:33.480
Likewise, yeah, thanks so much for having me.

50:33.480 --> 50:36.280
Alright everyone, that's our show for today.

50:36.280 --> 50:41.080
For more information on Christopher or any of the topics covered in this episode, head

50:41.080 --> 50:46.880
on over to twimmelai.com slash talk slash 199.

50:46.880 --> 50:53.000
To learn more about our AI platform series or to get our e-books, visit twimmelai.com slash

50:53.000 --> 50:54.840
AI platforms.

50:54.840 --> 51:24.800
As always, thanks so much for listening and catch you next time.

