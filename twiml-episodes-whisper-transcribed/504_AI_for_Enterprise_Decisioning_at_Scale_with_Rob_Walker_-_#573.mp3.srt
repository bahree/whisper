1
00:00:00,000 --> 00:00:11,200
All right, everyone. Welcome to another episode of the Twinkle AI podcast. I am your host, Sam

2
00:00:11,200 --> 00:00:17,200
Charrington. And today I'm joined by Rob Walker, VP of Decisioning and Analytics and General

3
00:00:17,200 --> 00:00:23,280
Manager of one to one customer engagement at Pegasystems. Before we get going, be sure to take

4
00:00:23,280 --> 00:00:29,600
a moment to hit that subscribe button wherever you're listening to today's show. Rob, it's been

5
00:00:29,600 --> 00:00:34,800
almost three years to the day since the last time we spoke. Welcome back to the podcast.

6
00:00:35,520 --> 00:00:39,520
Yeah, thank you. Yeah, glad to be here. Incredible three years ago, my God.

7
00:00:40,560 --> 00:00:46,960
It has been a crazy three years. Why don't we have you introduce yourself or reintroduce

8
00:00:46,960 --> 00:00:54,720
yourself to our audience? It's been a while. Tell us how you got involved in AI and we'll jump in

9
00:00:54,720 --> 00:01:01,680
from there. Yeah, no, happy to, happy to do that. Yeah, well, AI has been a real passion of me

10
00:01:01,680 --> 00:01:08,240
for a long time. I'm not prepared to admit how long it is. But I did a PhD in AI a long time ago.

11
00:01:08,240 --> 00:01:14,720
Before it was really fashionable, I was just very intrigued by the promise. But obviously,

12
00:01:14,720 --> 00:01:19,360
at that point, it was, well, we have neural networks, not the deep learning quite yet,

13
00:01:19,360 --> 00:01:24,320
but definitely the learning. But it was also expert systems and rules. But that got me really into

14
00:01:24,320 --> 00:01:30,400
that, into that, into that space. So I did my PhD then work for a large consultancy firm

15
00:01:31,280 --> 00:01:35,840
around mostly predictive analytics. What we probably would now call data science and

16
00:01:35,840 --> 00:01:41,600
consultancy around that. And then really, really, really wanted to, you know, set up my own,

17
00:01:41,600 --> 00:01:47,920
my own company in an area called Decisioning, which was not quite a verb, is not quite a

18
00:01:47,920 --> 00:01:54,720
verb I think. So maybe we coined it, but it's pretty popular right now. And that sort of is like

19
00:01:54,720 --> 00:01:59,920
an applied way of, you know, of using data science and predictive models. So I've always been

20
00:01:59,920 --> 00:02:04,400
intrigued by sort of, you know, apart from the science of it, but also, you know, how to make it

21
00:02:04,400 --> 00:02:12,960
like really, really practical. So that company I sold and then through another acquisition,

22
00:02:12,960 --> 00:02:19,920
you know, got, got to be part of, of, of, of Bega, where I'm responsible for, you know,

23
00:02:19,920 --> 00:02:24,880
that part of the, of the business. So basically, long wind away saying, I've never done anything else,

24
00:02:24,880 --> 00:02:32,000
then sort of AI, then applied, applied AI. So, yeah, I'm always excited to, to talk about that.

25
00:02:32,960 --> 00:02:39,360
Awesome. Well, let's, let's talk a little bit more about the use of AI and ML in the context of

26
00:02:39,360 --> 00:02:45,440
customer engagement and customer decisioning. What are the types of problems that you find yourself

27
00:02:45,440 --> 00:02:50,160
helping people solve? Yeah. And, and, and this is actually sort of even, even way back. I've

28
00:02:50,160 --> 00:02:54,240
always tried, maybe that's the consultancy background. I've always been very careful, I think,

29
00:02:54,240 --> 00:02:58,480
to align this with outcomes because AI and machine learning is obviously used and very, very

30
00:02:58,480 --> 00:03:04,480
broad. Even in customer engagement, it's still relatively broad because it can go from creating

31
00:03:04,480 --> 00:03:12,320
auto generating text or images that you may want to use in marketing to stuff that, that we do,

32
00:03:12,320 --> 00:03:18,640
which is really around determining the next best action or the next best experience, next best

33
00:03:18,640 --> 00:03:26,240
conversation. There's a lot of next best now in the market. But have AI in combination with

34
00:03:26,240 --> 00:03:33,840
business rules, determine what you should be talking about with a particular customer to make

35
00:03:33,840 --> 00:03:41,920
that super relevant and, and very, you know, contextual. So the, the business outcomes are usually

36
00:03:41,920 --> 00:03:46,240
around, you know, mutual value creation, right? So it's obviously, you know, it can be about

37
00:03:46,240 --> 00:03:51,440
revenue. But in these times, you know, during the pandemic, actually, especially, but also in,

38
00:03:51,440 --> 00:03:58,080
you know, in other sort of economic downturns, it's also about maybe coaching or counseling,

39
00:03:58,080 --> 00:04:04,000
customers, you know, towards, you know, financial resilience. It's really not just about selling,

40
00:04:04,000 --> 00:04:10,320
next best action is, is really about, like, you know, across the whole customer life cycle,

41
00:04:10,320 --> 00:04:17,280
you know, whether it's nurturing retention, risk management, sales or, or actually determining

42
00:04:17,280 --> 00:04:21,920
in real time that nothing is really relevant enough. So we shouldn't, you know, steal the moment

43
00:04:21,920 --> 00:04:27,280
from the customer to talk about something. But, but it is, it's really about, so where the

44
00:04:27,280 --> 00:04:34,480
machine learning and AI comes in is in part to determine what's best in next best action, right?

45
00:04:34,480 --> 00:04:40,800
So that, that metric, because that sounds very simple, but in practice, it means that,

46
00:04:41,360 --> 00:04:48,800
if you combine that to another big thing that's really important to, to, you know, to our vision,

47
00:04:48,800 --> 00:04:54,560
is the sort of the one-to-one approach, right? So it's not working off averages and segments,

48
00:04:54,560 --> 00:04:59,920
but it is really like, you know, for this particular customer, you know, Sam Charrington is,

49
00:05:00,640 --> 00:05:08,640
you know, calling or browsing or swiping on his mobile app. Now, what do we do based on what

50
00:05:08,640 --> 00:05:13,040
we know about him, based on what he's currently doing, maybe what he's even doing another, you know,

51
00:05:13,040 --> 00:05:18,640
another devices or other systems, what should we be really talking about? And then the moment you

52
00:05:18,640 --> 00:05:25,520
do something and react to it, we need to, you know, recalculate that in the moment. And the

53
00:05:25,520 --> 00:05:30,400
relevance, I think, is a, is a key thing. And that's where we use the AI for. Like, what are you

54
00:05:30,400 --> 00:05:36,320
likely, you know, going to be favorably disposed to, or what is top of mind for you? Yeah. Yeah.

55
00:05:36,880 --> 00:05:44,560
How do you distinguish this idea of next best action and determining it from a recommender system,

56
00:05:44,560 --> 00:05:50,480
you know, which we're all familiar with and interact with, with all the time, both from a kind of

57
00:05:50,480 --> 00:05:55,760
high level use case perspective, as well as, you know, from a technical perspective.

58
00:05:55,760 --> 00:06:01,200
It's interesting, because the, is there some media distinction there?

59
00:06:01,760 --> 00:06:07,120
No, there is a media distinction, because we actually work in a few, you know, in some from

60
00:06:07,120 --> 00:06:15,120
eco, e-customer or e-commerce clients, where both are being used, right? And, and typically,

61
00:06:15,120 --> 00:06:21,200
I mean, in next best action is really at a customer level, right? Whereas the recommender system

62
00:06:21,200 --> 00:06:26,240
is an input to that on, you know, doing a basket analysis and those kind of things. And then saying,

63
00:06:26,240 --> 00:06:32,000
hey, people like you have built, have bought this based on transactions. And that's only part of

64
00:06:32,000 --> 00:06:37,680
it, right? Because for instance, the next best action would, on top of that recommendation, say,

65
00:06:37,680 --> 00:06:42,080
yeah, that's really, you know, what that person would be interested in, what that customer is

66
00:06:42,080 --> 00:06:48,320
interested in, but it's too expensive. Or we believe that if he took out a loan on that,

67
00:06:48,320 --> 00:06:54,800
then he wouldn't repay it. Or he is actually not interested in this at all, because there's a

68
00:06:54,800 --> 00:06:59,440
surface issue that we should really solve. And that's what, you know, is actually the most pressing

69
00:06:59,440 --> 00:07:07,360
thing right now. So it's an input to a next best action. And it's an inside that is useful,

70
00:07:07,360 --> 00:07:12,240
but can easily be overruled or augmented by, you know, other more important insights.

71
00:07:12,240 --> 00:07:17,600
Got it. So what I'm hearing there is that recommender systems tend to be kind of broader,

72
00:07:17,600 --> 00:07:25,760
maybe product and offering based. And this idea of next best action is very personalized

73
00:07:25,760 --> 00:07:31,200
to the, to the individual. Yeah. And very personalized to the individual. And I think that is the,

74
00:07:31,200 --> 00:07:36,800
that is sort of, I think, the important thing. And also looks at a lot of different things.

75
00:07:36,800 --> 00:07:42,000
Well, as an example, right, the recommender system might say, hey, this is like, you know,

76
00:07:42,000 --> 00:07:49,120
your 4k television that you would likely be interested in because people like you would have,

77
00:07:49,120 --> 00:07:55,280
would have bought that, right? But then on top of that, you know, you would, you would really have to look

78
00:07:55,280 --> 00:08:00,400
at like, you know, can you afford it? Did you already maybe, you know, buy that particular

79
00:08:00,400 --> 00:08:04,880
television? You already have it. So you may be interested because you already actually bought it.

80
00:08:04,880 --> 00:08:10,480
And we should do something else, maybe an accessory to that television. There is like all of these

81
00:08:10,480 --> 00:08:17,120
things get into, you know, come into consideration. So to us, it's just one of the many, many

82
00:08:17,120 --> 00:08:24,000
propensities you would get, product propensities. That's the, that's the outcome. And in the case

83
00:08:24,000 --> 00:08:29,600
where we work with like retailers, actually, next best action is often used sort of at the category

84
00:08:29,600 --> 00:08:35,280
level. So it is using, hey, like, hey, we have this, we have this person, we think based on everything

85
00:08:35,280 --> 00:08:39,520
she's been doing, this is the category she's interested in, which may be, you know, high

86
00:08:39,520 --> 00:08:46,800
definition televisions. And then which particular brand and model of television might be the output

87
00:08:46,800 --> 00:08:52,320
of a recommender system. And technically, because she also asked technically, the other thing

88
00:08:52,320 --> 00:08:57,840
is that the recommender system doesn't really have to, you know, refresh all the time, right? So

89
00:08:57,840 --> 00:09:01,920
in these scenarios where, you know, next best action works with the recommender system,

90
00:09:01,920 --> 00:09:08,480
the recommender system, it's fine to, you know, overnight churn and create 10 million different

91
00:09:08,480 --> 00:09:14,000
probabilities at the product level, at the skew level, right? Whereas next best action will just

92
00:09:14,000 --> 00:09:20,080
grab that from the database, but then adds all the contextual insights and behavioral insights

93
00:09:20,080 --> 00:09:24,400
to decide, yeah, this is the thing we need to actually talk about. Sounds a little bit like the

94
00:09:24,400 --> 00:09:31,920
fusing of a CRM and a recommender and, you know, all of the possibilities that are created by

95
00:09:31,920 --> 00:09:37,760
machine learning. It's a great combination. And the, yeah, it's, but it is really at the, because

96
00:09:37,760 --> 00:09:43,040
a recommender system is really at sort of the skew level recommendation, which in a customer

97
00:09:43,040 --> 00:09:48,400
experience, customer engagement context is really only one of the things you should be looking, you

98
00:09:48,400 --> 00:09:53,280
know, you should be looking at if you want to be, you know, customer centric. Got it, got it. And

99
00:09:53,280 --> 00:09:58,000
so if you're building a system like this, what are some of the things that you need to be thinking

100
00:09:58,000 --> 00:10:05,200
about from a machine learning perspective to deliver these types of next best actions?

101
00:10:05,200 --> 00:10:09,760
Yes, next, well, next best action recommendations. Yes, it's like it's fine. The recommender system

102
00:10:09,760 --> 00:10:14,560
is just a particular class, you know, of algorithms that just an input. It's very confusing.

103
00:10:14,560 --> 00:10:18,400
But I think the, the important thing, and this is obviously an open door, but I just want to sort of

104
00:10:18,400 --> 00:10:24,320
say it's still, you know, it's not like, you know, doing AI or machine learning for, you know,

105
00:10:24,320 --> 00:10:29,760
AI shake, right? So we really, really start, and I've always started from the business side,

106
00:10:29,760 --> 00:10:34,240
right? So what are the outcomes in the customer engagement you want to achieve? Is it, is it

107
00:10:34,240 --> 00:10:40,160
revenue growth or is it revenue growth with like constraints on the risk if you're a bank,

108
00:10:40,160 --> 00:10:44,640
you know, you can, you know, you can hand out as we've seen a lot of mortgages, but, you know,

109
00:10:44,640 --> 00:10:51,040
what does your risk profile look like? So, so you look at sort of the outcomes and KPIs,

110
00:10:51,760 --> 00:10:56,880
and once you have established sort of those broad business goals, then you can sort of see the

111
00:10:56,880 --> 00:11:02,000
sort of the scaffolding of, you know, the best metric in next best action, right? And the best,

112
00:11:02,000 --> 00:11:09,040
then say, if this is the outcome, what kind of thing are we, for instance, if this is about

113
00:11:09,040 --> 00:11:14,640
retention initially, right? And we want to retain customers that are, you know, dear to us,

114
00:11:14,640 --> 00:11:21,840
and profitable to us, but, you know, we see sort of, you know, the, the bells of churn,

115
00:11:21,840 --> 00:11:26,800
and we need to, you know, we need to see if we can, you know, save them. From that metric,

116
00:11:26,800 --> 00:11:32,640
you then have to decide, okay, what kind of rules apply? So that's human judgment or, or, or,

117
00:11:32,640 --> 00:11:38,560
or economic factors and profitability factors, right? Like, hey, we want to retain you, Sam,

118
00:11:38,560 --> 00:11:44,400
right? But how much is that worth to us? What can we actually do that would be, you know, that,

119
00:11:45,040 --> 00:11:51,120
that we have in stock in terms of, you know, convince you to stay longer with us, for instance?

120
00:11:51,120 --> 00:11:57,920
So that's, that's past one. And then you're sort of start at the machine learning AI level,

121
00:11:57,920 --> 00:12:03,360
where now we need to figure out, you know, what is on your mind, right? What you would be interested

122
00:12:03,360 --> 00:12:10,640
in, what you would likely say yes to, right? And, and, and, and, and then one level below that is

123
00:12:10,640 --> 00:12:14,880
another level of machine learning and AI. And that is like, okay, once we have decided that we

124
00:12:14,880 --> 00:12:19,840
know what to talk to you about right now, right? And that can change on a dime, but probably talk

125
00:12:19,840 --> 00:12:24,400
about that later when we talk about like real time and those kind of things. But once you have,

126
00:12:25,360 --> 00:12:29,760
once we've decided this is what we need to talk to you about, now the question is how are we going

127
00:12:29,760 --> 00:12:34,880
to do that? Like, are we using a visual? What's the script? How should we say it? All of these kind

128
00:12:34,880 --> 00:12:40,080
of things. And again, that's where AI and machine learning will come up, you know, based on,

129
00:12:40,080 --> 00:12:46,880
you know, collective learnings on what's best. That's sort of the, the approach very much top

130
00:12:46,880 --> 00:12:52,960
down from the outcomes and then all the way down from, you know, outcomes and KPIs to rules,

131
00:12:52,960 --> 00:12:57,920
to predictions to the data that is then required to actually make those predictions.

132
00:12:57,920 --> 00:13:03,600
And is that last part relatively new? You mentioned kind of creating the script. Are you

133
00:13:04,400 --> 00:13:10,960
now at the point where you're looking at generative AI to kind of assemble the script from a set of

134
00:13:10,960 --> 00:13:18,080
data points? Yeah. We, we, we pick actually sort of more, you know, script or content because I

135
00:13:18,080 --> 00:13:22,960
think that is a particular use case of AI, which is very interesting, you know, to generate,

136
00:13:22,960 --> 00:13:27,120
you know, what is the perfect sentence? What's the perfect image? So we do sort of, you know,

137
00:13:27,120 --> 00:13:34,800
some creative optimization, but it is of a stock, right? Because I think, especially generating it,

138
00:13:34,800 --> 00:13:39,280
right, not picking the best one, but generating it, very interesting. It's not what we do,

139
00:13:39,280 --> 00:13:45,520
but we work with systems like that and say, Hey, we give you the context. We know exactly what we

140
00:13:45,520 --> 00:13:50,640
want to talk about. If, you know, if you can pick or construct more interesting, you know, the

141
00:13:50,640 --> 00:13:58,160
right background image or the right, you know, banner text, then that's great. We don't, we don't

142
00:13:58,160 --> 00:14:03,360
specifically do that. We just pick it from a library of, of actions, which can be thousands of

143
00:14:03,360 --> 00:14:08,560
things. A couple of things that you mentioned jumped out of me. One is this idea of starting from

144
00:14:08,560 --> 00:14:15,440
the, the outcome. Another is part of that. You mentioned that use a combination of machine learning

145
00:14:15,440 --> 00:14:21,440
and heuristics. I'd love to get your take on kind of how those are co-existing in your

146
00:14:21,440 --> 00:14:26,640
engagements nowadays. I think we've kind of, you know, there's been this pendulum swing of,

147
00:14:26,640 --> 00:14:31,280
hey, we're going to do everything using statistical machine learning to, hey, you know, we've got all

148
00:14:31,280 --> 00:14:35,920
this domain knowledge and these rule-based systems. Can we find ways to fuse them together?

149
00:14:35,920 --> 00:14:40,080
I'm curious how this plays out and the types of problems that you're solving.

150
00:14:40,080 --> 00:14:46,800
Yes. And it's, I think it's, it's, it's, it's a very interesting, I think topic to, to discuss

151
00:14:46,800 --> 00:14:53,040
a little bit because the, the combination of sort of, yeah, heuristics or, or even human judgment

152
00:14:53,040 --> 00:14:58,880
in most cases, but mostly, you know, heuristics on, on top of machine learning, I think that

153
00:14:58,880 --> 00:15:06,640
combination is important for, for a few reasons. I think one, there are just things that honestly

154
00:15:06,640 --> 00:15:13,680
require, you know, human policies at least, right? For instance, an AI will not tell you that,

155
00:15:13,680 --> 00:15:18,560
unless that's what it's trying to do, but will not tell you that there is a competitive pressure

156
00:15:18,560 --> 00:15:24,880
or there is an announcement of an acquisition in your industry and you need to really, you know,

157
00:15:24,880 --> 00:15:29,040
prepare for that and change tack. You know, so that's the kind of heuristic that you really want

158
00:15:29,040 --> 00:15:35,040
to change quickly, but you don't necessarily want the AI to do those kind of things for you,

159
00:15:35,040 --> 00:15:42,080
indefinitely not unsupervised. And that's the second topic, is that in the industries,

160
00:15:42,080 --> 00:15:48,160
we do most in, which is, you know, retail banking, insurance, communications, healthcare.

161
00:15:48,160 --> 00:15:56,000
It's typically very important that you can also, to some degree, and in many cases,

162
00:15:56,000 --> 00:16:02,640
to a pretty significant degree, explain what is happening, right? So if you had like AI,

163
00:16:02,640 --> 00:16:08,720
especially, you know, the, the fancy algorithms, try to figure out everything that would not be

164
00:16:08,720 --> 00:16:16,160
an acceptable way of, of doing it, even if it worked, but in combination, so the heuristics on

165
00:16:16,160 --> 00:16:23,120
top of thousands of insights that are generated by AI machine learning, we've learned that just from

166
00:16:23,120 --> 00:16:29,600
a business outcome perspective, that's pretty, you know, um, parallel performance.

167
00:16:29,600 --> 00:16:33,440
And can you talk a little bit about how you make that work? Is it kind of

168
00:16:34,160 --> 00:16:41,360
fusing rules engines with algorithms? It is, um, although that makes it sound a little bit like,

169
00:16:41,360 --> 00:16:47,200
you know, it's like a technical integration. Um, and the thing is, um, we've done this for a very

170
00:16:47,200 --> 00:16:51,360
long time. I mean, you know, we spoke last time three years ago, but I've been in this business,

171
00:16:51,360 --> 00:16:57,360
as I said, much longer than I care to admit. Um, and, and, and we've really been very opinionated

172
00:16:57,360 --> 00:17:03,680
now, uh, in a good way, opinionated sounds maybe a little bad, but it's like, um, we know sort of,

173
00:17:03,680 --> 00:17:09,760
you know, what these large brands or, you know, in particular industries, what's best practice,

174
00:17:09,760 --> 00:17:16,080
right? So it's, it's really like more of, um, of, um, of a declarative way of saying, hey,

175
00:17:16,080 --> 00:17:21,920
let's, let's take you through this process of defining the outcomes, then defining rules,

176
00:17:22,800 --> 00:17:28,720
like, for instance, eligibility rules, right? I mean, whatever the AI thinks, you cannot actually

177
00:17:28,720 --> 00:17:33,920
sell a car to a four-year-old, right? Even if the person would be really, really interested to buy

178
00:17:33,920 --> 00:17:39,360
like a, whatever, there's like all sorts of, all sorts of rules that maybe, you know, inventory,

179
00:17:39,360 --> 00:17:43,760
there's risk and margin rules, there's all sorts of these kind of rules that you need. So that's

180
00:17:43,760 --> 00:17:50,320
what you start to define outcomes, then eligibility rules, suitability rules, maybe some competitive,

181
00:17:50,320 --> 00:17:56,320
you know, priorities that you, that you, that you see. And then you start looking at sort of the,

182
00:17:56,320 --> 00:18:03,200
the full action library. So what are all the actions across, you know, nurturing and onboarding

183
00:18:03,200 --> 00:18:07,840
and selling and retention and, and, and, and, and, and, and, and, and, and, and, and risk management

184
00:18:07,840 --> 00:18:14,800
like collections across all of the actions in those categories. Um, how are we going to arbitrate,

185
00:18:14,800 --> 00:18:20,240
given the top of that pyramid, pyramid that I just described, right? And then automatically,

186
00:18:20,240 --> 00:18:24,320
and automatic is here, one of the key words because there are thousands of these actions. And if you

187
00:18:24,320 --> 00:18:30,960
have the treatments on how to, how to present those actions, it gets into even bigger numbers.

188
00:18:30,960 --> 00:18:36,960
For each of those, you need a predictive model to predict this is the interest in the moment,

189
00:18:36,960 --> 00:18:42,400
right? And, and it's not like you have to import or something or find, you know, a thousand

190
00:18:42,400 --> 00:18:46,800
algorithms in the organization, the way that typically because that would obviously be, you know,

191
00:18:46,800 --> 00:18:54,480
fine, but un, unmanageable. Um, so the way that really works is that we look at the data science

192
00:18:54,480 --> 00:19:00,720
output that's already available, right? They may have a risk model or many risk models or attrition

193
00:19:00,720 --> 00:19:06,480
models or propensity models for certain very important products, which we would actually import

194
00:19:06,480 --> 00:19:14,160
into that framework. So we can execute it, but we almost always compliment that, um, with a massive

195
00:19:14,160 --> 00:19:21,040
machine learning capability to compliment data science, right? So data science, typically with the

196
00:19:21,040 --> 00:19:27,520
kind of, um, brands we work with, uh, would still monitor that. So it's like, they shouldn't really

197
00:19:27,520 --> 00:19:33,680
care whether this is a model they created in, you know, um, you know, Python or whatever it is,

198
00:19:33,680 --> 00:19:39,040
or it's a self-learning model that beg I has supplied to them, you know, they, it's, it's their

199
00:19:39,040 --> 00:19:44,880
responsibility, but it just gives them, uh, uh, much, much higher throughput, right? Because these

200
00:19:44,880 --> 00:19:49,680
models learn on the fly, they look about competitive actions that look about seasonal

201
00:19:49,680 --> 00:19:56,960
winfluences, well, all of these kind of things, um, and, and, and, and, and just back to your question,

202
00:19:56,960 --> 00:20:01,920
so they stop down approach, declarative from the outcomes, then the rules that every business

203
00:20:01,920 --> 00:20:05,760
will understand, like eligibility and profitability and all of these kind of things,

204
00:20:05,760 --> 00:20:10,560
then to all the actions and their propensity models that are generated automatically,

205
00:20:11,200 --> 00:20:16,800
that sort of, you know, I think the state of play currently in the, in the industry, it's not a

206
00:20:16,800 --> 00:20:23,680
patchwork of like costly integration to do this. It's just that's the motion for next best action.

207
00:20:23,680 --> 00:20:30,560
And when you go into a large brand of those propensity models already typically exist or

208
00:20:31,280 --> 00:20:37,200
are there, you know, are there holes that need to be filled or new ones that need to be created

209
00:20:37,200 --> 00:20:42,880
in order to, to fulfill the, the framework that you're trying to create? Yes, two, two, two things

210
00:20:42,880 --> 00:20:49,520
about that. So, uh, usually, um, um, so first of all, from, from a, from a decisioning perspective,

211
00:20:49,520 --> 00:20:55,920
that's at Europe again, um, um, we're, we're really trying to be, uh, agnostic about where the

212
00:20:55,920 --> 00:21:00,480
algorithms come from, right? Because there's like, there's great open source tools,

213
00:21:00,480 --> 00:21:05,600
the big cloud platforms will have, you know, offer increasingly more analytics. So,

214
00:21:05,600 --> 00:21:11,440
I don't really care about any of that, where that comes from. I just care about like, can I have

215
00:21:11,440 --> 00:21:19,360
um, an algorithm behind every single action and treatment? And can I execute it in the moment?

216
00:21:19,360 --> 00:21:23,760
That's very important. So, I, I don't want to retrieve a probability from a database, right?

217
00:21:23,760 --> 00:21:30,240
Because if, if a customer says no, right, then I want to re-evaluate a thousand different

218
00:21:30,240 --> 00:21:36,720
predictive models where that no may have some influence and then re-decide what the next best

219
00:21:36,720 --> 00:21:42,880
action, what the next best action is, right? So, so we usually, and, and I'm, when I say usually,

220
00:21:42,880 --> 00:21:49,680
actually, I mean always, complement the, the existing models, um, the data science models with

221
00:21:49,680 --> 00:21:56,080
this machine learning capability. Um, but one thing I would like to, to add is that on top of that,

222
00:21:56,080 --> 00:22:00,400
and this is, I think what a lot of brands are also asking for is around like, you know, this whole

223
00:22:00,400 --> 00:22:06,880
responsible AI, right? We also want to make sure that every algorithm, whether the data science

224
00:22:06,880 --> 00:22:15,040
department build it or our machine learning generated it for them, um, we'll have to adhere to,

225
00:22:15,040 --> 00:22:21,120
you know, sort of the tenets of responsible AI. Like, is it a fair model? Is it a robust model?

226
00:22:21,120 --> 00:22:27,840
Is it transparent? If you need it to be transparent in a particular, um, as said, is it empathetic,

227
00:22:27,840 --> 00:22:33,280
which is sort of, in my mind, really about like, is it relevant? But anyway, those, those tenets

228
00:22:33,280 --> 00:22:40,000
of responsible AI are much easier to enforce if you have like a central AI policy that you can

229
00:22:40,000 --> 00:22:47,440
apply to, you know, your next best action strategy, wherever the algorithm or originated from.

230
00:22:48,000 --> 00:22:53,280
I think that's important. And, and, and lastly, Sam, just before we move on, it's also like,

231
00:22:53,280 --> 00:23:01,040
because it's this combination of heuristics and AI, we can't just look at an AI algorithm

232
00:23:01,040 --> 00:23:05,680
and say, oh, it's biased, you know, it's, well, if it's biased, it's biased and, you know,

233
00:23:05,680 --> 00:23:11,680
we have a problem. If it's not biased, that doesn't mean that your overall outcome is unbiased.

234
00:23:11,680 --> 00:23:16,640
If you apply your rules to it and, and a hundred other models and then arbitrate,

235
00:23:16,640 --> 00:23:24,560
is the final outcome of that, the final recommendation is that fair, right? That's, that's more

236
00:23:24,560 --> 00:23:31,120
of sort of the challenge that we take on. Okay. Uh, I want to circle back to, uh, responsible

237
00:23:31,120 --> 00:23:36,080
AI. One of the things you mentioned in kind of drawing this distinction between

238
00:23:37,440 --> 00:23:42,320
data science and machine learning or these, uh, propensity models and, and kind of the

239
00:23:42,320 --> 00:23:49,600
decisioning on top is that you don't want to have predictions, uh, in some database because

240
00:23:49,600 --> 00:23:57,200
you want to be able to evaluate them real time across these thousands of models. Uh, talk a

241
00:23:57,200 --> 00:24:04,400
little bit about the doing that at scale. Yeah, that's hard. That's actually, it sounds hard.

242
00:24:05,040 --> 00:24:09,120
It is, it is, it is hard. Yes. So I think part of a little part of, of, of,

243
00:24:09,120 --> 00:24:12,800
certainly, of, of my career, but also, you know, the, the teams that have been, that have been

244
00:24:12,800 --> 00:24:18,640
building this, um, it's, it's all about that scale. Can you do that, um, at scale and can you

245
00:24:18,640 --> 00:24:23,520
do it fast enough, right? Because remember, this is customer engagement, right? So you are, it

246
00:24:23,520 --> 00:24:29,280
means that, that if, if, if, if, if, you know, if, if you were, you know, saying no to something,

247
00:24:29,280 --> 00:24:35,600
we need to within fraction of a second, we need to take that no into account, recalculate 1200

248
00:24:35,600 --> 00:24:41,680
propensities, apply all the rules on top of it and then say, oh, but in that case, you know,

249
00:24:41,680 --> 00:24:48,640
and then, you know, have the next best action ready, right? So that's, um, from a technology

250
00:24:48,640 --> 00:24:54,800
perspective, I think is, is, is quite a feat. Um, and I think it's often confused and, and maybe

251
00:24:54,800 --> 00:25:00,240
even downplayed a little bit in, not, it sort of in the market like, well, do we really need real time?

252
00:25:00,240 --> 00:25:06,320
Um, and, and there are really two different things. One is like, do we have all the data real time?

253
00:25:06,320 --> 00:25:10,800
And that can be a challenge. You don't meet all the data, you know, real time. I don't need to know

254
00:25:10,800 --> 00:25:16,320
your birthday in real time, you know, that's fine. Um, but the, the thing that you're doing in a

255
00:25:16,320 --> 00:25:20,720
particular channel, right? What I'm browsing, what I'm clicking, what I'm not clicking, my mood,

256
00:25:21,360 --> 00:25:27,440
all, all of these things that we can now figure out increasingly accurately, um, all change the

257
00:25:27,440 --> 00:25:34,320
context, right? And I want to have the, the most up to date, really up to the second context,

258
00:25:34,320 --> 00:25:40,800
added to sort of a more static profile, and then apply all of these algorithms to it, right?

259
00:25:40,800 --> 00:25:45,840
If I don't, um, then if you said no, then, you know, the results will be pretty much the same.

260
00:25:45,840 --> 00:25:50,640
Well, I would probably suppress it in a rule. I would say, if he said no, let's not offer this

261
00:25:50,640 --> 00:25:57,280
again in three weeks, right? But in fact, you'll know informs a thousand predictions, right? That

262
00:25:57,280 --> 00:26:01,600
again, you know, maybe some of them will not change. A lot of them may change a little bit. So

263
00:26:01,600 --> 00:26:07,280
may change a lot, right? And then the next best action, that kind of relevance is, I think,

264
00:26:07,280 --> 00:26:12,640
that's sort of definitely our claim to fame, but, you know, relevance drives conversions,

265
00:26:12,640 --> 00:26:19,280
drives drives, drives customer satisfaction, drives a conversation like, you know, interaction.

266
00:26:19,280 --> 00:26:24,160
And that's why it's important to not retrieve a score from a database that is, even if it's,

267
00:26:24,160 --> 00:26:30,960
you know, a minute old, that's already not great. Like it would be like, we were, we would be talking

268
00:26:30,960 --> 00:26:52,160
with like a minute delay. It would be hard.

269
00:27:00,960 --> 00:27:13,840
Yes, it's the way it usually works. So first of all, we do support, you know, both on-premise

270
00:27:13,840 --> 00:27:20,800
and cloud model, but pretty much all we do is, you know, it's all cloud, either our cloud or,

271
00:27:20,800 --> 00:27:26,400
you know, the client's cloud, the private one, whatever, all of these options exist. And it's

272
00:27:26,400 --> 00:27:32,560
actually more, more of a, you know, a data and data governance challenge than it is a technical,

273
00:27:32,560 --> 00:27:40,160
than it is a technical, technical challenge. But what we, what we do is like we do pre-aggregate

274
00:27:41,040 --> 00:27:45,600
all the things we can pre-aggregate. And again, that's completely automated. There's no building.

275
00:27:45,600 --> 00:27:50,720
It all comes from declaring what your business challenge is that you're trying to solve, you know,

276
00:27:50,720 --> 00:27:56,800
from that pyramid that I described from the outcome down, but also all the data transformations,

277
00:27:56,800 --> 00:28:02,080
you know, are also happening in real time. But for instance, what we don't need to calculate in

278
00:28:02,080 --> 00:28:10,480
real time is like, for instance, if you are, you know, with like, you know, like a telecom operator,

279
00:28:10,480 --> 00:28:19,360
right, we don't need to re-calculate your, you know, your text messaging or I am volumes

280
00:28:19,360 --> 00:28:23,440
over the month, is that going up? Is that going down? You know, those kind of things you can,

281
00:28:23,440 --> 00:28:29,280
if that's a, if that's a material predictor, right, you can, you can pre-calculate all of that.

282
00:28:29,920 --> 00:28:35,040
And there are many of these kind of trending things that we, that we would sort of pre-aggregate

283
00:28:35,040 --> 00:28:39,440
to make sure that in the moment we don't have to do that. And we can just take your monthly

284
00:28:39,440 --> 00:28:49,600
effort and your weekly effort and your three monthly effort as pre-calculated, assuming that. In the

285
00:28:49,600 --> 00:28:56,400
machine learning and ML ops ecosystem, there's this notion of a feature store which sounds a little

286
00:28:56,400 --> 00:29:04,320
bit like what you're describing. Do you relate this idea to a feature store? Yes, yes, I would,

287
00:29:04,320 --> 00:29:10,560
I would definitely, you know, call it that. We call it, you know, an extended analytics record,

288
00:29:10,560 --> 00:29:16,720
but that's pretty much, pretty much the same thing. So it's the usual profile data then,

289
00:29:16,720 --> 00:29:22,960
then as much of a transactional data that you have or that, you know, the brand cares to share,

290
00:29:22,960 --> 00:29:29,280
you know, could be part of, you know, of a customer data platform or in, you know, big storage,

291
00:29:29,280 --> 00:29:36,160
big data platforms. And, and then, and then some of these features that you can pre-calculate

292
00:29:36,160 --> 00:29:42,400
and are not two-time sensitive, that's, that would be added to that, to that, to that record.

293
00:29:42,400 --> 00:29:47,920
But not sort of, I just want to make clear because it so nicely top down, you don't have to sort

294
00:29:47,920 --> 00:29:52,880
of boil the ocean. I see a lot of, a lot of organizations that think, hey, we're going to build

295
00:29:52,880 --> 00:29:58,560
our CDP or our data, you know, big data platform and we're going to do that first because we need

296
00:29:58,560 --> 00:30:04,720
that to do analytics and we need to do the decisioning on top of it. So that will have to wait.

297
00:30:04,720 --> 00:30:09,920
That's actually, although it sounds sort of common sense, but that's actually the wrong way of

298
00:30:09,920 --> 00:30:14,080
doing it because if you go from, well, it's not the wrong way of doing it. It's a very expensive

299
00:30:14,080 --> 00:30:19,360
way of doing it, right? Because you can use AI and machine learning to sort of figure out, hey,

300
00:30:19,360 --> 00:30:26,240
first of all, given that we're optimizing business outcomes, maybe the data that we have or can get

301
00:30:26,240 --> 00:30:33,600
relatively quickly, we'll get us already 80% there, right? So you have like the funding and,

302
00:30:33,600 --> 00:30:40,240
and, and, and, and, and, and, and the business case to then iterate and, you know, make your data

303
00:30:41,040 --> 00:30:46,240
better or, you know, increase the number of features that you would, in the feature store.

304
00:30:46,240 --> 00:30:52,960
All of these things are much better informed if you actually know what you have rather than

305
00:30:52,960 --> 00:30:58,720
trying to be complete and, and AI sort of, it's almost a side effect, but not really AI and machine

306
00:30:58,720 --> 00:31:05,360
learning can report very accurately on, hey, this data works, this data works with this kind of data,

307
00:31:05,360 --> 00:31:09,840
or there is a correlation between this and this, so you actually don't even need to store this,

308
00:31:09,840 --> 00:31:15,680
or certainly not, you don't have to make it available in real time, you know, so from, from

309
00:31:15,680 --> 00:31:20,800
thousands and thousands and thousands of customer attributes, for instance, the predictive models

310
00:31:20,800 --> 00:31:27,360
may use a couple of hundred, maybe even tens, you know, in, in, in some cases, and it's really good

311
00:31:27,360 --> 00:31:32,560
to know that, otherwise you will invest a lot of time and money in making the data perfect.

312
00:31:32,560 --> 00:31:38,960
Are these next best action types of problems? Typically, at least the machine learning parts

313
00:31:38,960 --> 00:31:45,520
supervise, like is there a requirement to label a lot of data? It's, it's usually really matched

314
00:31:45,520 --> 00:31:51,760
against, you know, it's supervised because it's running against particular outcomes that we already

315
00:31:51,760 --> 00:31:57,440
know, right? So we are trying to predict, are you going to what's right? Are you going to default

316
00:31:57,440 --> 00:32:01,680
on your loan? Are you going to buy this product? Are you going to make us, you know,

317
00:32:01,680 --> 00:32:07,840
is the value of the relationship going to go up and down? And, and, and that's essentially,

318
00:32:07,840 --> 00:32:13,840
you know, what, what you learn about. So it's, it's in that sense, it's supervised learning,

319
00:32:13,840 --> 00:32:18,560
it's supervised, very highly used from historical data, as opposed to, you know,

320
00:32:18,560 --> 00:32:23,040
have a label, a particular transaction. No end, no end labeling, no, no, no, this is all,

321
00:32:23,040 --> 00:32:27,680
yeah, no good question. Yeah, so that's all, this is all at the, at the, at the very large

322
00:32:27,680 --> 00:32:31,600
scale. I think part of this, it's only part, but part of this is trying to sort of, you know,

323
00:32:31,600 --> 00:32:37,760
industrialize this whole process, right? Because otherwise, it would just be, be daunting,

324
00:32:37,760 --> 00:32:42,880
right? So that's sort of the design phase is actually relatively easy, you know, if you know,

325
00:32:42,880 --> 00:32:46,320
what you're doing, you go from the outcomes and the rules and the data, it's, you know,

326
00:32:46,320 --> 00:32:51,120
and then you don't boil the ocean, but you start at maybe 80% of the value, which will be in

327
00:32:51,120 --> 00:32:57,760
a astronomical figure, usually given, you know, the brands we work with, at least. And then,

328
00:32:57,760 --> 00:33:03,040
you know, you can get from 80 to 85 to 90, maybe to 95, and maybe you don't even worry about the

329
00:33:03,040 --> 00:33:13,120
last, the last 5% because, you know, it may not even be, be worth squeezing the data for, for, for, for, for, you

330
00:33:13,120 --> 00:33:18,800
know, that last bit. Yeah, maybe one last question on this. I think you spoke to this already

331
00:33:19,520 --> 00:33:25,440
from an explainability perspective and kind of the opacity of, quote unquote, fancy algorithms,

332
00:33:25,440 --> 00:33:31,760
which I took to mean deep learning. You know, deep learning gets a lot of attention, but a lot

333
00:33:31,760 --> 00:33:36,720
of the bread and butter problems within enterprises are kind of these tabular data problems that,

334
00:33:37,760 --> 00:33:45,200
that is struggles with our, I'm curious the types of models that you, that you see I'm assuming,

335
00:33:45,200 --> 00:33:51,680
you know, kind of a lot of classical tree-based types of things. Are you also seeing places for

336
00:33:51,680 --> 00:33:59,040
deep learning within the, this next best action problem? Yeah, some, and, and, and, but as you say,

337
00:33:59,040 --> 00:34:04,720
because I get the question a lot like, hey, we want to do deep learning, but again, it's often for

338
00:34:04,720 --> 00:34:10,800
deep learning sake, right, because it's, it is, first of all, it is opaque, so you have to be very

339
00:34:10,800 --> 00:34:16,720
careful in practice of what you let it predict, you know, if this is the background color, okay,

340
00:34:16,720 --> 00:34:22,480
or the position of your billboard in the city, maybe, right, but whether you get like a mortgage or

341
00:34:22,480 --> 00:34:27,600
not or be approved for a loan, there's no way any bank would apply a deep learning model to do

342
00:34:27,600 --> 00:34:33,520
that, but from our perspective, as I said before, we are really agnostic and then allow, so we,

343
00:34:33,520 --> 00:34:37,680
so if you, if you, for instance, if we will work with the bank and the bank says we have like,

344
00:34:37,680 --> 00:34:45,520
you know, an incredible model to predict risk, right, then, then you could actually, so we don't

345
00:34:45,520 --> 00:34:52,080
really care, except we allow them to apply sort of an AI policy. So, transparency is a good one,

346
00:34:52,080 --> 00:34:56,880
there's more, like bias and things like that, but that's part of sort of the AI policies,

347
00:34:56,880 --> 00:35:03,920
not our policy, that's not our place, but we allow these organizations to define an AI policy

348
00:35:03,920 --> 00:35:09,840
that we will automatically check against the algorithms, right, which means that a deep learning

349
00:35:09,840 --> 00:35:17,360
model would not be compliant, so we would flag it as incompliant or not compliant if it is about,

350
00:35:17,360 --> 00:35:24,000
like, you know, the probability to take on a loan or recommend a loan, but we would probably

351
00:35:24,000 --> 00:35:30,160
approve it if it's about the position of a particular ad on the web page. Got it. It's hard to

352
00:35:30,160 --> 00:35:39,520
visualize the user interface of a AI policy system, although maybe it's, it's similar to, you know,

353
00:35:39,520 --> 00:35:44,640
it's, it's another set of rules. Yeah, it is like, well, it's more like the way we look at it,

354
00:35:44,640 --> 00:35:48,960
it is more like, like, like, like a filter, so you have like all of these models that you are,

355
00:35:48,960 --> 00:35:53,840
if you have your libraries, so that's part of part of, you know, of model ups, it's like you have

356
00:35:53,840 --> 00:36:00,640
all of all of the models, and they get tacked by like, you know, their explainability or the

357
00:36:00,640 --> 00:36:07,440
type of algorithm, and then, and then we sort of see, well, this is allowed, and this is part of

358
00:36:07,440 --> 00:36:12,640
the policy, you just define and say, hey, for this particular use case or this particular, you know,

359
00:36:12,640 --> 00:36:20,080
decision, we really have to insist on a very transparent models like a decision tree or regression,

360
00:36:20,080 --> 00:36:27,680
or just rules, whereas here, you know, go a lot, right? So even if we don't really understand it,

361
00:36:28,400 --> 00:36:34,480
but we would flag, before taking something into production, we would flag, you know, the

362
00:36:34,480 --> 00:36:42,320
oh for all logic, which is, you know, the heuristics plus the models as being out of compliance. Now,

363
00:36:42,320 --> 00:36:48,080
transparency is just one aspect of responsible AI, what are some of the other ways that you're helping

364
00:36:48,080 --> 00:36:54,720
customers manage those challenges? Yeah, well, so the way we define responsible AI, yeah,

365
00:36:54,720 --> 00:36:59,680
transparency is I think is clear, and we discussed the other one is I think it's also obvious,

366
00:36:59,680 --> 00:37:05,360
you know, fairness, so is there a bias? But again, it's not a bias in the model, well, as well,

367
00:37:06,000 --> 00:37:12,240
but, but no bias or very acceptable bias, maybe if there is anything acceptable about it,

368
00:37:12,240 --> 00:37:18,240
but in the model, doesn't actually mean that your final decision will be biased, right? It's a very

369
00:37:18,880 --> 00:37:24,480
subtle combination of predictive insights with like, for instance, eligibility or suitability

370
00:37:24,480 --> 00:37:30,720
or profitability rules may actually produce results you didn't expect, right? And that's what we

371
00:37:30,720 --> 00:37:36,880
in that AI policy test against. So you can have 100 models, you can have, you know, a thousand

372
00:37:36,880 --> 00:37:42,320
different predictive models underneath all of the rules, and we check if the next best action

373
00:37:42,320 --> 00:37:48,240
distribution is actually fair, right? And I think that's the kind of thing I think that's that's

374
00:37:48,240 --> 00:37:54,240
important. And then we have sort of softer things that we call it empathy in our responsible AI that's

375
00:37:54,240 --> 00:37:59,440
like, it's maybe not even completely correct because it's, you know, you can be responsible

376
00:37:59,440 --> 00:38:04,080
without being empathetic, but we get and I get when I talk to executives a lot of things like,

377
00:38:04,080 --> 00:38:11,200
well, we don't want to be a clinical brand that, you know, has this robot AI deciding what to do

378
00:38:11,200 --> 00:38:19,760
and it's not warm enough or it doesn't really reflect our brand values and that's another

379
00:38:19,760 --> 00:38:26,080
aspect. So we also calculate sort of the softer elements like, for instance, your level of empathy,

380
00:38:26,080 --> 00:38:34,480
your relevance, like are you, for instance, withholding relevant messages or actions to a particular,

381
00:38:35,120 --> 00:38:42,720
you know, subgroup of your customer base. And if so, why is that? Is that like, because it can

382
00:38:42,720 --> 00:38:51,840
be eligibility rule, like back to the car selling the cars or, you know, to teenagers, for instance.

383
00:38:51,840 --> 00:38:56,080
So in that case, that's okay that you're doing that. They may be interested and the models may

384
00:38:56,080 --> 00:39:02,000
indicate they're interested, but you still can't do it. But it can be a lot more subtle, right?

385
00:39:02,000 --> 00:39:07,040
Maybe it's like a policy way that you've set or you put in competitive pressure or you, you said,

386
00:39:07,040 --> 00:39:12,000
well, the profitability is so important. Like, for instance, we need to get this message out

387
00:39:12,000 --> 00:39:18,240
because we need to sell, you know, 10,000 of these and we are completely spamming our customer

388
00:39:18,240 --> 00:39:23,360
base and nobody's interested. We would flag that kind of meta analysis to say, that's not very

389
00:39:23,360 --> 00:39:29,840
empathetic and you need to decide on, you know, what your brand is. And also what the cost is

390
00:39:29,840 --> 00:39:37,040
of doing those kind of things. And the last one, Sam, is around robustness. And this is a question

391
00:39:37,040 --> 00:39:42,000
I also get a lot. And that is like, if you have your predictive models and they're static predictive

392
00:39:42,000 --> 00:39:46,720
models, so, you know, you've been building them or your data science group has been building these

393
00:39:46,720 --> 00:39:51,760
models and you execute them in real time, the model itself obviously doesn't need to change.

394
00:39:51,760 --> 00:39:55,600
And that's fine. So you can test it, you can simulate it. All of these things are fine.

395
00:39:56,160 --> 00:40:01,680
But if it's a self-learning model, especially if you have thousands of them, you want to make

396
00:40:01,680 --> 00:40:07,680
sure that none of them goes rogue, right? Like, hey, it's it's being exposed to real time or real

397
00:40:07,680 --> 00:40:14,800
life, I should say, real life behavior. And suddenly it's doing weird, weird things. And especially

398
00:40:14,800 --> 00:40:19,120
if you combine that with like, you know, maybe a deep learning algorithm, you know, that can go,

399
00:40:19,120 --> 00:40:25,360
hey, why are you really quick? So that's another aspect that we make sure like, okay, we are testing

400
00:40:25,360 --> 00:40:31,760
these self-learning models. Are they are they drifting? Are they drifting a little bit too quick?

401
00:40:31,760 --> 00:40:35,600
Is there a concern? And there's thousands of them. So you can't manually look at that. So you

402
00:40:35,600 --> 00:40:43,200
need to flag that for a data science or a governance body like the model of his in a large organization

403
00:40:43,200 --> 00:40:47,840
and say, hey, there's something weird going on. It still works. You're not you're still fair.

404
00:40:47,840 --> 00:40:53,280
You know, you're still making value, creating value. But this model is doing weird things and you

405
00:40:53,280 --> 00:40:58,720
need to, you know, look at why that is and we then, you know, help this that discovery process.

406
00:40:58,720 --> 00:41:03,760
Awesome. Awesome. Well, I know that in addition to customer engagement, one of the other things that

407
00:41:03,760 --> 00:41:09,120
you're excited about is your upcoming conference, Pega World. And before we wrap up, I wanted to

408
00:41:09,120 --> 00:41:13,760
give you a chance to share a bit about that. Is that a place where folks can come to learn more

409
00:41:13,760 --> 00:41:19,040
about the kinds of things we've been talking about? Absolutely. So, yeah, Pega World, old virtual.

410
00:41:19,040 --> 00:41:24,080
So everybody can sign up. It's it's it's free. Maybe the last time or one of the last time,

411
00:41:24,080 --> 00:41:29,280
we do it virtually, virtually only. But for now, it's virtual. Everybody can sign up.

412
00:41:29,280 --> 00:41:36,240
It is May 24th in the morning in the in the US. So it won't take up all of your all of your day.

413
00:41:36,240 --> 00:41:43,680
And you will hear, you know, companies like, like, for instance, T-Mobile talk about, you know,

414
00:41:43,680 --> 00:41:50,080
much of this and their vision and how they've been, you know, operating and and and and productizing

415
00:41:50,080 --> 00:41:54,240
next best action in that are obviously, you know, breakdowns and deep dives. It's very interesting.

416
00:41:54,240 --> 00:41:58,400
And we'll talk about a lot of these kind of things. But you can also, as I said, hear it from,

417
00:41:59,360 --> 00:42:04,000
you know, the brands themselves. Awesome. Well, Rob, great to see you once again. And thanks so

418
00:42:04,000 --> 00:42:09,760
much for joining the show and sharing a bit about what you've been up to recently and digging into

419
00:42:09,760 --> 00:42:17,360
this more detail. And thanks for having me, Sam, as always. Okay. Yeah. Bye-bye.

