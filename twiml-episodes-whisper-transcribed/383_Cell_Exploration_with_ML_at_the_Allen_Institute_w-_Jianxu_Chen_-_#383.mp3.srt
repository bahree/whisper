1
00:00:00,000 --> 00:00:13,000
Welcome to the Tumul AI Podcast.

2
00:00:13,000 --> 00:00:25,040
Alright everyone, I am on the line with Jenshu Chen.

3
00:00:25,040 --> 00:00:31,480
Jenshu is a scientist in the assay development group at the Allen Institute for Cell Science.

4
00:00:31,480 --> 00:00:34,120
Jenshu, welcome to the Tumul AI Podcast.

5
00:00:34,120 --> 00:00:35,320
Hi Sam.

6
00:00:35,320 --> 00:00:42,200
Hey, it's great to speak to you and meet you and I'm looking forward to our conversation

7
00:00:42,200 --> 00:00:43,800
to get started.

8
00:00:43,800 --> 00:00:49,040
Why don't you tell us a little bit about your background and how you came to work at

9
00:00:49,040 --> 00:00:52,680
the intersection of machine learning and biology?

10
00:00:52,680 --> 00:01:01,120
Okay, I got my PhD in computer science in 2017 from University of Notre Dame and during

11
00:01:01,120 --> 00:01:09,960
my PhD, I mainly conducted research in machine learning and image analysis and more importantly

12
00:01:09,960 --> 00:01:13,120
and their applications in biology.

13
00:01:13,120 --> 00:01:18,560
So before I came to Notre Dame, I have no clue what I'm going to do in the future and

14
00:01:18,560 --> 00:01:25,280
later I just, all of a sudden, like we're randomly came into a very fascinating problem

15
00:01:25,280 --> 00:01:30,680
in biology and trying to use computer science algorithm to solve it and that is the moment

16
00:01:30,680 --> 00:01:35,800
I say, okay, oh my god, this is so fun and I'm very impactful.

17
00:01:35,800 --> 00:01:43,480
So that's the place where I decide, okay, this is my career.

18
00:01:43,480 --> 00:01:51,920
And then after I graduate in 2017, I came to Allen Institute for Cell Science where we

19
00:01:51,920 --> 00:02:00,680
have very heavy like multi-team science where we have lots of biologists, computer scientists,

20
00:02:00,680 --> 00:02:06,240
computational biologists and software engineer and everyone worked together to solve very

21
00:02:06,240 --> 00:02:13,080
big fundamental cell biology problems and machine learning is playing a critical

22
00:02:13,080 --> 00:02:16,840
role that attracts me here.

23
00:02:16,840 --> 00:02:22,880
When you first got started, were you more coming from a biology perspective or more from

24
00:02:22,880 --> 00:02:25,760
a computer science perspective?

25
00:02:25,760 --> 00:02:34,120
I will say I came from more computer science perspective and I always use a very interesting

26
00:02:34,120 --> 00:02:35,120
example.

27
00:02:35,120 --> 00:02:39,920
I would say like before 2017, I know nothing about mitochondria.

28
00:02:39,920 --> 00:02:45,600
I don't even know that particular English word and I don't even know what does it mean.

29
00:02:45,600 --> 00:02:48,280
So that's really funny.

30
00:02:48,280 --> 00:02:54,440
Right now I'm doing lots of segmentation analysis interpretation on mitochondria.

31
00:02:54,440 --> 00:02:59,520
So that's really something really funny to me actually.

32
00:02:59,520 --> 00:03:01,760
How did you approach that?

33
00:03:01,760 --> 00:03:08,680
A lot of folks that are doing machine learning don't have deep experience in a domain

34
00:03:08,680 --> 00:03:15,240
and have to get up to speed in the domain and the domain that you're getting up to speed

35
00:03:15,240 --> 00:03:21,200
in is one that is particularly complex which has its own many open questions around the

36
00:03:21,200 --> 00:03:22,280
biology.

37
00:03:22,280 --> 00:03:25,400
How did you approach coming up to speed in it?

38
00:03:25,400 --> 00:03:27,760
Yes, that's a fantastic question.

39
00:03:27,760 --> 00:03:30,480
So actually I will probably cover more.

40
00:03:30,480 --> 00:03:35,520
I cover a lot in that particular question in my GTC talk.

41
00:03:35,520 --> 00:03:43,080
So the basic idea is that in computer science, we try to make design abstract problems.

42
00:03:43,080 --> 00:03:49,560
Say we have a very practical problem like in biology, in medical domain or in whatever,

43
00:03:49,560 --> 00:03:52,880
the problem in our real life is more complicated.

44
00:03:52,880 --> 00:03:58,200
But computer scientists sometimes may want to start with a simplified version or abstract

45
00:03:58,200 --> 00:03:59,200
version.

46
00:03:59,200 --> 00:04:05,040
And starting from there, we develop methods, we develop all the kind of deep learning models

47
00:04:05,040 --> 00:04:08,920
and artificial intelligence techniques put in there.

48
00:04:08,920 --> 00:04:14,680
But later, I realized that how much we want to do this simplification or abstract, abstract

49
00:04:14,680 --> 00:04:22,640
is actually a key when we want to put artificial intelligence into biology.

50
00:04:22,640 --> 00:04:25,720
So sometimes the problem is oversimplified.

51
00:04:25,720 --> 00:04:26,720
That's the problem.

52
00:04:26,720 --> 00:04:33,120
That's the place I find myself really struggling with and having lost, I'm struggling a lot

53
00:04:33,120 --> 00:04:38,480
when I do the transition from a computer scientist to like half and half.

54
00:04:38,480 --> 00:04:45,200
So the strategy I took to do this transition is going back and forth between the knowledge

55
00:04:45,200 --> 00:04:51,360
that the biologists and come back to my problem and come back for many times.

56
00:04:51,360 --> 00:04:56,960
When I show my result to biologists, I try to understand how they see my result.

57
00:04:56,960 --> 00:05:01,480
And sometimes, or most of the time, they see the result or they see the image or they

58
00:05:01,480 --> 00:05:07,120
see the figures, the numbers completely different from what I saw it.

59
00:05:07,120 --> 00:05:10,680
So they are viewing it from a very different aspect.

60
00:05:10,680 --> 00:05:17,160
And by analyzing or by learning all such discrepancy between how I view the problem and

61
00:05:17,160 --> 00:05:23,400
how they view the problem, that's the strategy I took to transition myself from computer

62
00:05:23,400 --> 00:05:27,320
scientists to like half computer scientists, half biologists.

63
00:05:27,320 --> 00:05:28,320
That's awesome.

64
00:05:28,320 --> 00:05:33,520
Does any particular example of that kind of interaction come to mind where, you know,

65
00:05:33,520 --> 00:05:36,800
you presented some results and they really saw them very differently from how you did

66
00:05:36,800 --> 00:05:38,960
and you had to kind of close that gap?

67
00:05:38,960 --> 00:05:39,960
Yes, absolutely.

68
00:05:39,960 --> 00:05:42,160
There are tons of cool examples there.

69
00:05:42,160 --> 00:05:48,480
A very simple example is when we know lots of like segmentation, say the basic idea

70
00:05:48,480 --> 00:05:52,120
of segmentation is the computer raised in an image.

71
00:05:52,120 --> 00:05:57,360
We want to find or we want to outline the structure in this image.

72
00:05:57,360 --> 00:06:03,200
And then we can quantify the size of it or how long it is, something like that, right?

73
00:06:03,200 --> 00:06:05,720
So the problem is pretty straightforward.

74
00:06:05,720 --> 00:06:12,200
So for computer scientists, I will just say, hmm, I will do a like this and that method

75
00:06:12,200 --> 00:06:13,680
and extract object.

76
00:06:13,680 --> 00:06:18,320
I will, how good is my output or how good is my algorithm?

77
00:06:18,320 --> 00:06:25,240
I will do that visually by a video assessment or some kind of comparison between my result

78
00:06:25,240 --> 00:06:27,960
and what I see in the image.

79
00:06:27,960 --> 00:06:30,640
So this is my understanding as a computer scientist.

80
00:06:30,640 --> 00:06:34,760
But when I go to the biologist, they are seeing that in a different way.

81
00:06:34,760 --> 00:06:40,320
So a key message is what you see in the image may not be the truth.

82
00:06:40,320 --> 00:06:48,320
You have take the microscope effect into account when you shoot a light to light up a small

83
00:06:48,320 --> 00:06:55,800
ball under the microscope, we are talking about spinning this confocal microscope, where

84
00:06:55,800 --> 00:07:01,960
some particular cellular structure is labeled by some kind of fluorescent protein and then

85
00:07:01,960 --> 00:07:06,000
it will be light up under the microscope and by the laser power.

86
00:07:06,000 --> 00:07:11,720
But what you see in the image is actually you have to take into account the blurry

87
00:07:11,720 --> 00:07:19,000
effect of the microscope because of all those optical details that I have no idea to understand

88
00:07:19,000 --> 00:07:20,320
at the beginning.

89
00:07:20,320 --> 00:07:27,640
So you see a ball with, for example, like 10 pixel wide, the actual object may be only

90
00:07:27,640 --> 00:07:30,160
5 pixel wide, something like that.

91
00:07:30,160 --> 00:07:32,280
What you see is not the truth.

92
00:07:32,280 --> 00:07:34,480
That's what drives me crazy.

93
00:07:34,480 --> 00:07:42,400
So I have to understand what the optical details of that particular microscope and all those

94
00:07:42,400 --> 00:07:47,280
fundamental biology, say, for example, the things cannot be like having two branches.

95
00:07:47,280 --> 00:07:51,040
It has to be a single filament, things like that.

96
00:07:51,040 --> 00:07:57,040
I have to combine my computer knowledge and computer science, also my, the new things

97
00:07:57,040 --> 00:08:03,720
I'm learning about optics and also the new things I'm learning about biology, I glued

98
00:08:03,720 --> 00:08:08,440
these three piece together and reach a new, more accurate solution.

99
00:08:08,440 --> 00:08:09,440
That's great.

100
00:08:09,440 --> 00:08:14,440
And I think that example is one that you're going to come back to when I look through your

101
00:08:14,440 --> 00:08:22,480
slides from GTC, a lot of the work that you're doing in the, the cell explorer toolkit,

102
00:08:22,480 --> 00:08:27,800
which we'll be talking about, look like trying to kind of close this gap computationally

103
00:08:27,800 --> 00:08:31,880
between what you might see in one image and what's really, you know, there, but you're

104
00:08:31,880 --> 00:08:36,320
not seeing it due to some of these artifacts and effects that you're describing.

105
00:08:36,320 --> 00:08:37,320
Exactly.

106
00:08:37,320 --> 00:08:38,320
Yes.

107
00:08:38,320 --> 00:08:39,320
Is that right?

108
00:08:39,320 --> 00:08:40,320
Yes.

109
00:08:40,320 --> 00:08:41,320
Right.

110
00:08:41,320 --> 00:08:49,000
So maybe let's then use that as an opportunity to transition and talk a little bit about the

111
00:08:49,000 --> 00:08:54,360
cell explorer toolkit and, you know, what is it really trying to do?

112
00:08:54,360 --> 00:08:56,040
Oh, yeah, sure.

113
00:08:56,040 --> 00:09:02,320
So at Alan Institute for Cell Science, we are trying to build this concept called Alan

114
00:09:02,320 --> 00:09:09,960
Cell Explore Toolkit, which is a combination of cell creator, cell image generator, cell

115
00:09:09,960 --> 00:09:15,760
image analyzer, cell image view analyzer and cell image simulator.

116
00:09:15,760 --> 00:09:22,880
And in my GTC talk, I focused mostly on three parts, the generator analyzer and view

117
00:09:22,880 --> 00:09:29,760
analyzer, where the computation or the GPU computing is heavily used there.

118
00:09:29,760 --> 00:09:38,080
So the generator, if I want to describe it with one sentence is like how we get a image

119
00:09:38,080 --> 00:09:39,520
to be analyzed.

120
00:09:39,520 --> 00:09:48,600
So apparently we need to get image from microscope, but with the help of GPU computing, we can

121
00:09:48,600 --> 00:09:52,560
generate more image from one single experiment.

122
00:09:52,560 --> 00:09:55,440
So that's the key idea of generator.

123
00:09:55,440 --> 00:10:03,000
The analyzer is, as the name indicates, it's just given this image how we analyze them.

124
00:10:03,000 --> 00:10:06,840
And finally, realization is a big part of cell biology.

125
00:10:06,840 --> 00:10:15,440
So and also for GPU computing is certainly a big, a play a critical role in the modern

126
00:10:15,440 --> 00:10:23,680
realization tools and taking a step back, what's the goal for the Cell Explore Toolkit?

127
00:10:23,680 --> 00:10:31,040
Is this an internal tool that's used at the Alan Center or is it designed to be used

128
00:10:31,040 --> 00:10:39,560
by external folks and who are the main users, presumably biologists or is this something

129
00:10:39,560 --> 00:10:40,560
different?

130
00:10:40,560 --> 00:10:41,560
Yeah, great.

131
00:10:41,560 --> 00:10:42,880
That's a perfect question.

132
00:10:42,880 --> 00:10:48,440
So actually, at Alan Institute for Cell Science, we are doing open science and we want to

133
00:10:48,440 --> 00:10:55,360
make the tool that we develop here to be usable for everyone outside this institute.

134
00:10:55,360 --> 00:11:00,520
And the reason we believe our tool will be useful is that we are doing academic research

135
00:11:00,520 --> 00:11:02,240
at the industrial scale.

136
00:11:02,240 --> 00:11:08,640
By doing research at this level or at this scale, we may realize something we hope to build

137
00:11:08,640 --> 00:11:16,440
some tools that will be more general or more stable or more user friendly for a broader

138
00:11:16,440 --> 00:11:20,840
audience so that everyone can use it.

139
00:11:20,840 --> 00:11:28,080
So that's the basic idea of why we want to build this toolkit.

140
00:11:28,080 --> 00:11:35,480
And so you mentioned that you focused on these three areas, starting with the cell image

141
00:11:35,480 --> 00:11:36,800
generator.

142
00:11:36,800 --> 00:11:41,600
Tell us a little bit more about that and where machine learning comes into play.

143
00:11:41,600 --> 00:11:42,600
Okay.

144
00:11:42,600 --> 00:11:44,000
Cell image generator.

145
00:11:44,000 --> 00:11:49,680
Of course, again, as I just mentioned, we need the microscope to get some image.

146
00:11:49,680 --> 00:11:58,240
However, with the help of GPU computing, we can break some limit where the traditional

147
00:11:58,240 --> 00:12:00,480
microscope has.

148
00:12:00,480 --> 00:12:09,560
For example, in my talk, I showed an example where I took the image as a very low resolution,

149
00:12:09,560 --> 00:12:17,400
which can help me get overview of the whole colony, which is like maybe 500 of cells in

150
00:12:17,400 --> 00:12:24,280
the same image, but each of them has relatively not that detailed because of the resolution.

151
00:12:24,280 --> 00:12:30,160
However, if we change some settings in the microscope, we switch to a different objective

152
00:12:30,160 --> 00:12:37,640
or using a different modality, the same colony, we can get it in a very, very high resolution.

153
00:12:37,640 --> 00:12:43,080
Say, for example, but we may sacrifice something because for example, we can only see maybe

154
00:12:43,080 --> 00:12:49,520
tens cells in that particular image, but each cell has lots of details, which contains

155
00:12:49,520 --> 00:12:52,840
lots of meaningful biological information in there.

156
00:12:52,840 --> 00:12:55,120
But there's a trade-off.

157
00:12:55,120 --> 00:12:59,120
So you cannot get both things at the same time.

158
00:12:59,120 --> 00:13:06,520
So you want either get more cells, but less details, or get less cells, but more details.

159
00:13:06,520 --> 00:13:07,920
There's always a trade-off.

160
00:13:07,920 --> 00:13:13,720
So we are thinking that whether a computational model can help.

161
00:13:13,720 --> 00:13:20,480
So that's what motivates us to design this, what we call the transfer function.

162
00:13:20,480 --> 00:13:27,240
The basic idea is we build a deep learning model, train on some low-res image and high-res

163
00:13:27,240 --> 00:13:28,640
image pair.

164
00:13:28,640 --> 00:13:35,960
So whenever in the real image acquisition, we got a sequence of low quality or low-res

165
00:13:35,960 --> 00:13:42,920
image, which give us like 500 cells in each time step.

166
00:13:42,920 --> 00:13:45,520
And even though there's not that much detail.

167
00:13:45,520 --> 00:13:53,480
We're after applying this fully trained transfer function model on this low resolution images.

168
00:13:53,480 --> 00:14:00,600
We can, like, make it up and we can improve the quality or improve the resolution of

169
00:14:00,600 --> 00:14:07,760
each image so that every single cell, we can put back all the details of each cell into

170
00:14:07,760 --> 00:14:09,280
that low-res image.

171
00:14:09,280 --> 00:14:18,560
In other words, we are seeing high resolution of each cell at a very large colony, or

172
00:14:18,560 --> 00:14:25,880
it's a combination of larger field of view, or maybe seeing more cells in the image and

173
00:14:25,880 --> 00:14:28,840
also seeing more details in the image.

174
00:14:28,840 --> 00:14:32,880
So traditionally, yeah, in the microscope, we cannot do that.

175
00:14:32,880 --> 00:14:37,720
But combining that with the computational model, we can achieve that to a very, very

176
00:14:37,720 --> 00:14:38,720
high accuracy.

177
00:14:38,720 --> 00:14:46,120
I'm envisioning something along the lines of applying style transfer or deoldify kind

178
00:14:46,120 --> 00:14:52,760
of one of these image coloring techniques to the low-resolution images based on the

179
00:14:52,760 --> 00:14:56,240
data from the high-resolution portion of those images.

180
00:14:56,240 --> 00:14:57,240
Yes.

181
00:14:57,240 --> 00:15:04,760
So the underlying techniques is pretty much the same as style transfer was the common models

182
00:15:04,760 --> 00:15:10,360
people used in deep learning and computer science for style transfer for super resolution

183
00:15:10,360 --> 00:15:11,600
and things like that.

184
00:15:11,600 --> 00:15:15,600
But later, at the beginning, I thought, yeah, I have the exact same feeling.

185
00:15:15,600 --> 00:15:20,600
I think if people can do that on natural scene image, I think it's just a piece of cake

186
00:15:20,600 --> 00:15:23,440
and just put it on the microscope image.

187
00:15:23,440 --> 00:15:25,120
There's no big deal.

188
00:15:25,120 --> 00:15:32,760
But later, after doing more and more validation, I realized one important piece.

189
00:15:32,760 --> 00:15:41,440
In the natural scene image, say you are trying to supervise your image of a cat, right?

190
00:15:41,440 --> 00:15:48,080
So in your output, the supervised image, if the cat, the eyes of the cat, looks a little

191
00:15:48,080 --> 00:15:51,120
bit larger or smaller, it doesn't really matter.

192
00:15:51,120 --> 00:15:55,840
Maybe one pixel larger or two pixels smaller, you may not even notice that, right?

193
00:15:55,840 --> 00:15:58,400
That's still a cat.

194
00:15:58,400 --> 00:16:02,720
But for sale, that's different, that will be a totally different story.

195
00:16:02,720 --> 00:16:05,920
I was going to ask that, I mean, a lot of, you know, when I think about those kind of

196
00:16:05,920 --> 00:16:10,360
techniques, I think of generative models that, you know, in a lot of ways, they're just

197
00:16:10,360 --> 00:16:11,360
making stuff up.

198
00:16:11,360 --> 00:16:16,600
But if you're trying to use these images for scientific purposes, you actually want them

199
00:16:16,600 --> 00:16:21,200
to reflect the reality of what's happening happening in that cell population, not just,

200
00:16:21,200 --> 00:16:24,720
you know, a visual, high resolution thing that looks nice.

201
00:16:24,720 --> 00:16:25,720
Yes.

202
00:16:25,720 --> 00:16:31,840
Let's give us two things, first of all, it gives us some direction about how we can improve

203
00:16:31,840 --> 00:16:35,600
our model on top of existing style transform models.

204
00:16:35,600 --> 00:16:42,320
On the other hand, it gives us, it gives us some hint on how we should validate our result

205
00:16:42,320 --> 00:16:47,360
and how much we should trust our generated image.

206
00:16:47,360 --> 00:16:50,200
So why don't you go into those two in more detail?

207
00:16:50,200 --> 00:16:51,560
Yes, for sure.

208
00:16:51,560 --> 00:16:55,560
So for the first part, so after seeing this particular

209
00:16:55,560 --> 00:17:00,840
thing about whether the prediction is to be larger or smaller and whether how much it

210
00:17:00,840 --> 00:17:04,480
affects, it drives us going back to the model.

211
00:17:04,480 --> 00:17:11,360
See, what can we do to, in the model, to make this prediction more accurate?

212
00:17:11,360 --> 00:17:13,360
What's wrong with the model?

213
00:17:13,360 --> 00:17:19,680
So then we realized that in our training data or the way we collect the pair of low-res

214
00:17:19,680 --> 00:17:25,520
image and high-res image, it's very, very hard to get a fully aligned, low-res image

215
00:17:25,520 --> 00:17:27,920
and low-res and high-res pair.

216
00:17:27,920 --> 00:17:33,360
Think about how we capture the image in the real life.

217
00:17:33,360 --> 00:17:40,240
Say in microscope, when we capture a 3D image, you can think of like, if you have a piece

218
00:17:40,240 --> 00:17:42,840
of meat, you want to slice it, right?

219
00:17:42,840 --> 00:17:48,720
So in the low-res image, the same piece of meat, you may slice it like at three different

220
00:17:48,720 --> 00:17:49,720
positions.

221
00:17:49,720 --> 00:17:56,960
But in a high-res image, you may slice that piece of meat like at ten different positions,

222
00:17:56,960 --> 00:18:00,480
much denser, much smaller gap between each step.

223
00:18:00,480 --> 00:18:09,800
So in that case, and also the position used to the slicing, or you take the slice, they

224
00:18:09,800 --> 00:18:11,800
may not at the same place.

225
00:18:11,800 --> 00:18:18,640
So anyway, anyhow, the low-res image and high-res image, they may miss a line in Z.

226
00:18:18,640 --> 00:18:20,080
That is a fundamental issue.

227
00:18:20,080 --> 00:18:21,080
Right.

228
00:18:21,080 --> 00:18:29,880
So you can't necessarily directly match your target image or label image in the high-res

229
00:18:29,880 --> 00:18:36,600
space to the low-res space because they're misaligned in terms of these slices.

230
00:18:36,600 --> 00:18:37,600
Correct.

231
00:18:37,600 --> 00:18:40,120
So we are trying two different ways.

232
00:18:40,120 --> 00:18:49,360
First of all, we modified a model, which we embedded a spatial transformer network into

233
00:18:49,360 --> 00:18:54,360
the model to learn such misalignment and try to correct it.

234
00:18:54,360 --> 00:18:55,880
That's one thing we tried.

235
00:18:55,880 --> 00:19:03,080
The other thing we tried is we tried to develop registration algorithm, which can make

236
00:19:03,080 --> 00:19:06,360
this alignment to its best.

237
00:19:06,360 --> 00:19:13,840
Maybe they are not perfect, but as much as we can using a traditional image registration

238
00:19:13,840 --> 00:19:16,360
algorithm to the line image.

239
00:19:16,360 --> 00:19:23,400
And we tried both, and each has pros and cons, but the good news is, both methods are

240
00:19:23,400 --> 00:19:28,000
improving the quality of the prediction by a lot.

241
00:19:28,000 --> 00:19:34,480
I'm not sure if the image that I'm thinking of is one that corresponds to the point you're

242
00:19:34,480 --> 00:19:35,480
making.

243
00:19:35,480 --> 00:19:42,480
There's an initial reaction I had to that, that you've got these additional slices, and

244
00:19:42,480 --> 00:19:48,040
you're not able to align your slices, and so the approach is maybe one of interpolation,

245
00:19:48,040 --> 00:19:52,400
but there's this one image in there that's showing that actually these different slices

246
00:19:52,400 --> 00:19:54,320
have totally different things in them.

247
00:19:54,320 --> 00:20:00,080
And so you can't necessarily statistical interpolation of some sort probably isn't going to work

248
00:20:00,080 --> 00:20:01,080
very well.

249
00:20:01,080 --> 00:20:05,280
So in terms of the slice, so I'm not sure whether I...

250
00:20:05,280 --> 00:20:11,800
I explained that clearly, but if I would try to put it in a different way.

251
00:20:11,800 --> 00:20:19,280
So if you think about the low-rise image, have three different slices on the same mid.

252
00:20:19,280 --> 00:20:23,600
On the high-rise image, you have ten different slices on the same piece of mid.

253
00:20:23,600 --> 00:20:29,960
So the first step, we need to absent or do some interpolation on low-rise image so that

254
00:20:29,960 --> 00:20:33,480
it will have ten slices.

255
00:20:33,480 --> 00:20:37,880
So now, on both images, we have ten slices, right?

256
00:20:37,880 --> 00:20:47,200
So now, the number of slices match, but the first slice in the up-sample or the interpolated

257
00:20:47,200 --> 00:20:54,680
version, the first slice of that may correspond to the second slice of the true, like, high-rise

258
00:20:54,680 --> 00:20:55,680
slice.

259
00:20:55,680 --> 00:20:57,480
And that's where you're doing your registration.

260
00:20:57,480 --> 00:21:03,240
Yes, that's what we are doing either we want to shift them a little bit and at the beginning

261
00:21:03,240 --> 00:21:09,760
or we can also, we also develop a model that can handle that inside the model.

262
00:21:09,760 --> 00:21:10,760
Okay.

263
00:21:10,760 --> 00:21:15,320
I think the image that I was thinking of is referring to something else.

264
00:21:15,320 --> 00:21:19,360
It was talking about predicted channels, and it has, like, each of the channels has very

265
00:21:19,360 --> 00:21:20,600
different things on it.

266
00:21:20,600 --> 00:21:23,800
And I was interpreting that as slices, but that's probably something different.

267
00:21:23,800 --> 00:21:28,240
Oh, by slice, I mean, the slice along C direction.

268
00:21:28,240 --> 00:21:36,600
Yeah, so you're doing the registration, which allows you to then use your up-sampled image

269
00:21:36,600 --> 00:21:40,040
as a label, is that essentially right?

270
00:21:40,040 --> 00:21:46,840
So use my up-sampled image as the input, use my tool, yeah, use the true, high-rise

271
00:21:46,840 --> 00:21:48,240
image as my target.

272
00:21:48,240 --> 00:21:49,240
Cool.

273
00:21:49,240 --> 00:21:50,840
So that was the first part, correct?

274
00:21:50,840 --> 00:21:54,800
And then the second piece is what?

275
00:21:54,800 --> 00:21:59,440
The second part of the, actually, there's a second part of the image generator, which

276
00:21:59,440 --> 00:22:01,720
we call the label-free method.

277
00:22:01,720 --> 00:22:08,120
So what we have been talking about is from low-rise to high-rise, but there's another type

278
00:22:08,120 --> 00:22:10,440
of generation.

279
00:22:10,440 --> 00:22:17,000
So think about, now we come to multi-channel, is what you are teaching a moment ago.

280
00:22:17,000 --> 00:22:24,600
So yeah, so usually when we do, we collect cell images, we have multiple channels, say,

281
00:22:24,600 --> 00:22:29,240
we should light at different, the light spectrum, we have different types of light.

282
00:22:29,240 --> 00:22:35,520
And then we can have different channels, as they have different dye in there.

283
00:22:35,520 --> 00:22:44,040
So usually we will have an image with some dye to light up the nucleus and some kind of

284
00:22:44,040 --> 00:22:50,200
protein to light up the one of the intercellular structure and another type of dye to light

285
00:22:50,200 --> 00:22:52,680
up the cell boundary.

286
00:22:52,680 --> 00:22:59,240
And there's another type, another special channel, which is from the transmitted light, which

287
00:22:59,240 --> 00:23:02,600
gives us what we call the bright field image.

288
00:23:02,600 --> 00:23:07,960
The bright field image is different from what we talk about in the other three, which

289
00:23:07,960 --> 00:23:13,520
requires some fluorescent tagging, some fluorescent imaging.

290
00:23:13,520 --> 00:23:17,840
The bright field image is just transmitted light, you just shoot the light there, you see

291
00:23:17,840 --> 00:23:26,760
the image without any harmful or photo, any dye that may damage your cell.

292
00:23:26,760 --> 00:23:30,000
So the light will not damage your cell.

293
00:23:30,000 --> 00:23:38,160
So in that sense, this is a general description of what we have in general.

294
00:23:38,160 --> 00:23:46,360
So I mentioned that there's a specific channel tagging only one intercellular structures,

295
00:23:46,360 --> 00:23:54,200
because that's a special cell line, which we did lots of hope, bunch of gene editing,

296
00:23:54,200 --> 00:23:59,200
so that that particular cell can have that property where that particular structure

297
00:23:59,200 --> 00:24:04,320
can be light up with the type of experiment.

298
00:24:04,320 --> 00:24:09,400
Each experiment can only light up one intercellular structures.

299
00:24:09,400 --> 00:24:15,400
And sometimes with very, very special gene editing techniques, we may light up two or

300
00:24:15,400 --> 00:24:20,320
at most three, going beyond three will be super, super hard.

301
00:24:20,320 --> 00:24:27,360
So basically, we can see three structures at the time, at most, in the real experiment.

302
00:24:27,360 --> 00:24:32,840
However, we always have the bright field as a reference, the bright field image, I mean

303
00:24:32,840 --> 00:24:37,160
the image we collect by shooting the transmitted light.

304
00:24:37,160 --> 00:24:43,120
So that is something harmless and can be cheaply acquired.

305
00:24:43,120 --> 00:24:50,360
So what we are thinking is, whether we can predict the special intercellular structure from

306
00:24:50,360 --> 00:24:52,360
the bright field image, right?

307
00:24:52,360 --> 00:25:00,440
So if we can do that, but given any single bright field image, we can predict 20 different

308
00:25:00,440 --> 00:25:04,400
structures at the same time for the same style.

309
00:25:04,400 --> 00:25:06,200
And an non-destructive way?

310
00:25:06,200 --> 00:25:13,000
Yes, that is the most important part, or it's the brand new way of designing your essay.

311
00:25:13,000 --> 00:25:15,720
Or your experimental essay.

312
00:25:15,720 --> 00:25:19,320
So before, we can only tack one structure at a time.

313
00:25:19,320 --> 00:25:25,400
And you cannot see how these 20 structure leads together in the same style.

314
00:25:25,400 --> 00:25:30,160
You will never see that, because every time you can only see one structure or two, right?

315
00:25:30,160 --> 00:25:36,120
So now with this particular technique, with any single bright field image, we can predict

316
00:25:36,120 --> 00:25:44,440
a lot of different structures for the same style, that is a fundamental improvement in imaging

317
00:25:44,440 --> 00:25:47,280
or how we generate images.

318
00:25:47,280 --> 00:25:54,680
So that will allow us to study the cell as a whole, as like with all the different parts

319
00:25:54,680 --> 00:25:57,880
playing together, not individually.

320
00:25:57,880 --> 00:26:06,240
And so in both of these cases, kind of going back to the earlier comments around the

321
00:26:06,240 --> 00:26:12,640
kind of the generative nature of some of these techniques, how do you measure performance

322
00:26:12,640 --> 00:26:23,600
and compare these cells that you're generating to actual cells in a way that ensures that

323
00:26:23,600 --> 00:26:27,160
they are high fidelity to what's actually there?

324
00:26:27,160 --> 00:26:29,480
Yeah, that's a good question.

325
00:26:29,480 --> 00:26:36,680
So the strategy we are taking is applications for the scientific validation.

326
00:26:36,680 --> 00:26:43,360
Say you are carrying, if you are, for example, when we talk about the label free prediction,

327
00:26:43,360 --> 00:26:49,120
say we are predicting 10 different structures from the same bright field image.

328
00:26:49,120 --> 00:26:52,120
And how good they are, how much can we trust them?

329
00:26:52,120 --> 00:26:59,640
And if we are talking about some application, we require the absolute accuracy.

330
00:26:59,640 --> 00:27:06,160
Say whether the pixel-wise accuracy is exactly the same matching the choose, then probably

331
00:27:06,160 --> 00:27:11,240
in the 10 structure, nine of them is not that trustable in that sense.

332
00:27:11,240 --> 00:27:16,000
Because the pixel-wise accuracy may not be that high for some structures.

333
00:27:16,000 --> 00:27:20,680
So maybe one structure will have very high pixel-wise accuracy.

334
00:27:20,680 --> 00:27:27,680
In other applications, where we care about how different parts correlate it to each other

335
00:27:27,680 --> 00:27:29,520
like in the space.

336
00:27:29,520 --> 00:27:33,920
We don't actually need that much pixel-wise accuracy.

337
00:27:33,920 --> 00:27:38,800
What we care more about is whether they are predicting the overall position of that particular

338
00:27:38,800 --> 00:27:41,600
structure in the correct place.

339
00:27:41,600 --> 00:27:49,320
So with that in place, we can study how different structures functioning together or whether

340
00:27:49,320 --> 00:27:52,280
there's any correlation with the structures.

341
00:27:52,280 --> 00:27:59,200
So anyway, that is what the strategy we call, like we care more is whether this result

342
00:27:59,200 --> 00:28:01,800
is suitable for that application.

343
00:28:01,800 --> 00:28:04,000
So this is our strategy.

344
00:28:04,000 --> 00:28:09,400
So kind of taking an application-by-application approach to evaluating performance, some

345
00:28:09,400 --> 00:28:17,880
of which are concerned about absolute characteristics like size and others more concerned with relative

346
00:28:17,880 --> 00:28:20,040
characteristics like position.

347
00:28:20,040 --> 00:28:21,440
Exactly.

348
00:28:21,440 --> 00:28:25,960
That is the image generator component.

349
00:28:25,960 --> 00:28:30,440
The next one you mentioned is the cell image analyzer.

350
00:28:30,440 --> 00:28:38,720
What is that piece trying to do and where, you know, how have you used ML in that component?

351
00:28:38,720 --> 00:28:39,720
Yeah.

352
00:28:39,720 --> 00:28:46,880
For image analyzer, in lots of cases, when we've got an image, where we want to analyze

353
00:28:46,880 --> 00:28:50,960
it, the very first step is to do a segmentation.

354
00:28:50,960 --> 00:28:53,400
We just like binarize the image.

355
00:28:53,400 --> 00:28:58,280
Generate a binary mask indicates where are the structures?

356
00:28:58,280 --> 00:29:00,880
Where are the interstellar structures?

357
00:29:00,880 --> 00:29:04,120
So that's the very first step we call a segmentation.

358
00:29:04,120 --> 00:29:11,360
So doing segmentation in microscopy image or microscopy image of cells, it's actually

359
00:29:11,360 --> 00:29:18,920
different from what we are doing like the semantic segmentation or instant segmentation

360
00:29:18,920 --> 00:29:25,840
or all sorts of segmentation in computer vision in natural scene image.

361
00:29:25,840 --> 00:29:27,560
That could be very different.

362
00:29:27,560 --> 00:29:31,960
So that difference comes from a couple of different sources.

363
00:29:31,960 --> 00:29:34,360
Let's start with a very simple example.

364
00:29:34,360 --> 00:29:41,360
Most of people may know that in deep learning, people can just manually draw the boundary

365
00:29:41,360 --> 00:29:46,920
of that object, so no matter if it's a people or cat or dog, we just manually draw the

366
00:29:46,920 --> 00:29:54,920
boundary of that and we draw that boundary for, say, 500 images or 5,000 images and there

367
00:29:54,920 --> 00:29:55,920
may not be that hard.

368
00:29:55,920 --> 00:29:58,920
It may be take some time, but it's not that hard.

369
00:29:58,920 --> 00:29:59,920
That's possible.

370
00:29:59,920 --> 00:30:00,920
Right?

371
00:30:00,920 --> 00:30:06,360
So we have to have this 5,000 image or even more, we just throw it into a unit or some

372
00:30:06,360 --> 00:30:13,440
kind of rest net or some deep neural network and it will predict the mask for us.

373
00:30:13,440 --> 00:30:18,720
That's what a lot of existing work are doing.

374
00:30:18,720 --> 00:30:28,440
But in microscopy images, especially for cellular structures, if you think about how complicated

375
00:30:28,440 --> 00:30:36,720
the topology or morphology of the structure could be, then you immediately appreciate that

376
00:30:36,720 --> 00:30:42,640
this is not possible to do this kind of manual annotation.

377
00:30:42,640 --> 00:30:48,800
So if you think about, I think when you think about, you know, bounding boxes and how tightly

378
00:30:48,800 --> 00:30:54,280
packed these cells are in some cases, you know, it's clear that that won't work.

379
00:30:54,280 --> 00:30:59,800
But then you have pixel masks that are manually applied and other techniques that could conceivably

380
00:30:59,800 --> 00:31:02,120
be used.

381
00:31:02,120 --> 00:31:07,960
For the pixel mask, even that, that could be hard if you think about it 3D.

382
00:31:07,960 --> 00:31:14,040
By the way, I forgot to mention that everything we are doing is 3D.

383
00:31:14,040 --> 00:31:15,040
Okay.

384
00:31:15,040 --> 00:31:16,040
Yeah.

385
00:31:16,040 --> 00:31:23,320
If you are thinking about a ball in 3D and if you ask me to draw the mask pixel wise, for

386
00:31:23,320 --> 00:31:28,040
that particular ball, yes, 3D, I think I can do a pretty good job.

387
00:31:28,040 --> 00:31:36,760
But if you give me a tree or a more complicated structure where different parts interacting

388
00:31:36,760 --> 00:31:43,280
with different other parts, something like that, or a ball of yarn, you think about how

389
00:31:43,280 --> 00:31:45,640
they tangle up, right?

390
00:31:45,640 --> 00:31:53,280
And I want to you to draw the outline of each single piece of yarn, go through the ball.

391
00:31:53,280 --> 00:31:54,280
Yeah.

392
00:31:54,280 --> 00:31:55,280
It gets a lot harder in 3D.

393
00:31:55,280 --> 00:31:56,280
Yeah.

394
00:31:56,280 --> 00:31:57,280
It's not possible.

395
00:31:57,280 --> 00:31:58,280
Right?

396
00:31:58,280 --> 00:32:05,240
So the strategy we are approaching that is what we called, which we implemented in what

397
00:32:05,240 --> 00:32:12,520
we call the segmenter is we use some classic method there, like more than 20 years of study

398
00:32:12,520 --> 00:32:18,160
of classic image processing techniques that are still very useful and they can give us

399
00:32:18,160 --> 00:32:21,080
a very, very good result to start.

400
00:32:21,080 --> 00:32:28,360
So we make a collection of a classic algorithm to give us some quick start.

401
00:32:28,360 --> 00:32:34,240
So based on that, for example, if you think about the ball of yarn, we design some classic

402
00:32:34,240 --> 00:32:41,640
algorithm can give us a rough segmentation of each piece of yarn, mostly like not very

403
00:32:41,640 --> 00:32:47,120
accurate, but give us a lot of good segmentation here and there.

404
00:32:47,120 --> 00:32:55,160
So instead of manually draw where each piece of yarn is, we use a different way to do the

405
00:32:55,160 --> 00:32:56,160
annotation.

406
00:32:56,160 --> 00:33:02,400
We draw a bounding box here, this area, what the classic algorithm is doing is good.

407
00:33:02,400 --> 00:33:03,960
Let's confirm that.

408
00:33:03,960 --> 00:33:09,040
And then for that particular area, what the classic algorithm is doing is bad.

409
00:33:09,040 --> 00:33:10,880
So throw that away.

410
00:33:10,880 --> 00:33:15,720
So by doing this kind of curation, we say here are good, here are bad, here are good,

411
00:33:15,720 --> 00:33:23,520
and there are bad, when we may have a small amount of good segmentation and use that as

412
00:33:23,520 --> 00:33:30,040
our initial training data for our, to train our deep learning model.

413
00:33:30,040 --> 00:33:35,720
And then you have this model one, you apply it on your data, and then the result will

414
00:33:35,720 --> 00:33:36,720
be better.

415
00:33:36,720 --> 00:33:42,600
Now let's do again, here is doing good job, here is doing not that good, throw it away,

416
00:33:42,600 --> 00:33:45,840
here we are doing a good job, and there we are doing a good job.

417
00:33:45,840 --> 00:33:54,080
Okay, now we have our second round of iteration, we collect a larger set of good segmentation.

418
00:33:54,080 --> 00:33:56,200
Then we use that to train our second model.

419
00:33:56,200 --> 00:34:03,280
Now we keep this iteration going, going like iteration by iteration.

420
00:34:03,280 --> 00:34:08,840
So every iteration, you are model is improving a little bit, more or less.

421
00:34:08,840 --> 00:34:16,040
So we hope to improve our model throughout these different iterations.

422
00:34:16,040 --> 00:34:23,320
And at the end, finally, we will achieve a model that we can never achieve by collecting

423
00:34:23,320 --> 00:34:25,000
a manual annotation.

424
00:34:25,000 --> 00:34:31,960
What I'm hearing is that I know it's not quite the same, but it's making me think a little

425
00:34:31,960 --> 00:34:37,520
bit of like an active learning type of a scenario where you can't be.

426
00:34:37,520 --> 00:34:44,000
You're trying to feed back to the system, the training data that is struggled with the

427
00:34:44,000 --> 00:34:45,000
most.

428
00:34:45,000 --> 00:34:52,440
And in this case, you're using human annotators, but not to identify the actual features

429
00:34:52,440 --> 00:34:59,040
in your image, but rather to identify where your segmentation algorithm didn't do a good

430
00:34:59,040 --> 00:35:00,040
job.

431
00:35:00,040 --> 00:35:01,040
Correct.

432
00:35:01,040 --> 00:35:06,800
So that is a big part of the segmenter, there's another complementary part where you think

433
00:35:06,800 --> 00:35:13,120
about if you have two types of cells in your image, say, mytotic cell and interface

434
00:35:13,120 --> 00:35:14,120
cell.

435
00:35:14,120 --> 00:35:21,440
They are in different cell cycle and they may show up as different morphology.

436
00:35:21,440 --> 00:35:28,320
So one single classic method will not be able to do a good job there, right?

437
00:35:28,320 --> 00:35:30,760
So then we can use two.

438
00:35:30,760 --> 00:35:35,600
We use two methods, each will give us a different segmentation version.

439
00:35:35,600 --> 00:35:40,640
Now we have the same image, we have segmentation version one, we have segmentation version

440
00:35:40,640 --> 00:35:41,640
two.

441
00:35:41,640 --> 00:35:44,840
Each version has its own advantage and disadvantage advantage.

442
00:35:44,840 --> 00:35:45,840
Okay.

443
00:35:45,840 --> 00:35:53,360
Now we just use our human annotator as the follows, say we circle out in cell one, in

444
00:35:53,360 --> 00:35:55,080
this cell, we make a circle.

445
00:35:55,080 --> 00:35:56,080
Okay.

446
00:35:56,080 --> 00:35:59,320
In this cell, we use segmentation version one.

447
00:35:59,320 --> 00:36:03,680
In that cell, we circle out, we use segmentation version two.

448
00:36:03,680 --> 00:36:06,600
In that cell, we use segmentation version one.

449
00:36:06,600 --> 00:36:09,720
In that cell, we use segmentation version two.

450
00:36:09,720 --> 00:36:17,160
So by doing this kind of circling, we are creating emerged ground shoes that can help

451
00:36:17,160 --> 00:36:18,680
us in the model.

452
00:36:18,680 --> 00:36:24,760
At the end, the model is able to segment both type of cells correctly, you know, very single

453
00:36:24,760 --> 00:36:32,560
model or multiple models, single model to deal with two different type of cells.

454
00:36:32,560 --> 00:36:40,760
And so this is again resting on the segment or the part of the image analyzer.

455
00:36:40,760 --> 00:36:49,320
So a little bit like one minute or two minutes down the segment or direction is sometimes

456
00:36:49,320 --> 00:36:57,880
is, we may think of segment segmentation is most of the analysis is about segmentation.

457
00:36:57,880 --> 00:37:00,360
Sometimes it's not.

458
00:37:00,360 --> 00:37:04,360
Sometimes we can do something on the original image.

459
00:37:04,360 --> 00:37:11,160
So that's another part of the analyzer, this is what we call the integrity cell, where

460
00:37:11,160 --> 00:37:19,360
we build a auto encoder on the original image, where we try to learn the underlying correlation

461
00:37:19,360 --> 00:37:25,480
between each part of the cells and how they function as the whole.

462
00:37:25,480 --> 00:37:29,920
And that's another part of the analyzer.

463
00:37:29,920 --> 00:37:35,600
You mentioned the auto encoder part, elaborate on what exactly that's doing.

464
00:37:35,600 --> 00:37:36,600
Okay, sure.

465
00:37:36,600 --> 00:37:46,240
So I showed an example in my slides where I see cell boundaries like this, I see DNA patterns

466
00:37:46,240 --> 00:37:53,640
like this, and then can we use these two pieces of information, just purely taking an image

467
00:37:53,640 --> 00:37:59,000
to predict where the mitochondria should live.

468
00:37:59,000 --> 00:38:08,600
So there might be a correlation between the position of the mitochondria condition on

469
00:38:08,600 --> 00:38:12,880
the shape and shape of the cell and DNA.

470
00:38:12,880 --> 00:38:18,280
So this is how the auto encoder is trying to learn.

471
00:38:18,280 --> 00:38:24,480
So basically I take this, I'm trying to reconstruct it, and if I'm able to reconstruct it, and

472
00:38:24,480 --> 00:38:31,680
I learn throughout this process, I learn some correlation between the mitochondria and

473
00:38:31,680 --> 00:38:35,800
the shape of the cell and the position of the DNA, things like that.

474
00:38:35,800 --> 00:38:41,760
So by doing this, we are trying to model the relationship between the different structures.

475
00:38:41,760 --> 00:38:47,040
I'm not sure if I make it clear, but once you've done that and you've got this hidden

476
00:38:47,040 --> 00:38:51,280
representation in your auto encoder, how are you using that?

477
00:38:51,280 --> 00:38:57,360
Yeah, that is actually, we'll give us lots of, that's something we want to try to make

478
00:38:57,360 --> 00:39:04,360
some biological interpretation, say, what are the correlations, and how should we interpret

479
00:39:04,360 --> 00:39:07,240
from a biological respective?

480
00:39:07,240 --> 00:39:13,840
So that's something we are doing right now to give it a biological interpolation.

481
00:39:13,840 --> 00:39:16,760
Got it, got it, so that part is ongoing.

482
00:39:16,760 --> 00:39:17,760
Yes.

483
00:39:17,760 --> 00:39:23,280
Okay, cool, and then just quickly the third part of the toolkit that you mentioned is

484
00:39:23,280 --> 00:39:25,000
the visualizer.

485
00:39:25,000 --> 00:39:30,160
How to folks use the visualizer is that, you know, just kind of a GUI that, you know, sits

486
00:39:30,160 --> 00:39:34,440
on some data structures and allows you to look at these images that you've generated

487
00:39:34,440 --> 00:39:37,240
or what else going on in there?

488
00:39:37,240 --> 00:39:45,680
Yes, so we build this, I mean, I already instilled, we build this software called Agave, and

489
00:39:45,680 --> 00:39:53,040
that used the GPU technology to do the photorealistic lightening and shading on our 3D

490
00:39:53,040 --> 00:39:55,200
microscopy and data.

491
00:39:55,200 --> 00:40:03,760
So the reason we care about making the realization more realistic and also including something

492
00:40:03,760 --> 00:40:09,600
like dApps of the image and things like that to make it looking amazing, it's not more

493
00:40:09,600 --> 00:40:15,040
than looking amazing, actually by looking this amazing image, it's give us some feedback

494
00:40:15,040 --> 00:40:17,760
from biological perspective.

495
00:40:17,760 --> 00:40:23,280
So when you look at a particular colony, the traditional rendering method will give you

496
00:40:23,280 --> 00:40:28,240
kind of messy realization in 3D, especially in 3D, right?

497
00:40:28,240 --> 00:40:34,360
So you don't have a good first impression about the biological property of the colony,

498
00:40:34,360 --> 00:40:40,920
but with proper shading and the lightening and all these, the retree tracing techniques

499
00:40:40,920 --> 00:40:48,640
that we are using, we make the image looking very clean and very easy to digest so that

500
00:40:48,640 --> 00:40:53,800
biologists have a better understanding what's going on inside the cell and inside this

501
00:40:53,800 --> 00:40:55,600
whole colony.

502
00:40:55,600 --> 00:41:06,000
And we will include a link to your slides from GTC on the show notes page, and I encourage

503
00:41:06,000 --> 00:41:14,280
folks to check it out for this part in particular, if only because the image of the, you've got

504
00:41:14,280 --> 00:41:22,240
one mode in a gavi that's called cinematographic path tracing and the images do look spectacular.

505
00:41:22,240 --> 00:41:30,920
Yes, this is from our animated cell team, so I love their image, they're very beautiful.

506
00:41:30,920 --> 00:41:37,320
Cool, and is there a ML component to this work as well, or is it just kind of raw number

507
00:41:37,320 --> 00:41:40,520
crunching using the GPU?

508
00:41:40,520 --> 00:41:45,920
So far, there's no ML yet, but we are thinking about further integration.

509
00:41:45,920 --> 00:41:48,120
Well, very interesting stuff.

510
00:41:48,120 --> 00:41:55,600
Jen, she thanks so much for taking the time to chat with us about what you're working

511
00:41:55,600 --> 00:41:56,600
on.

512
00:41:56,600 --> 00:41:57,600
Very cool.

513
00:41:57,600 --> 00:42:02,840
Any parting thoughts or words for folks that might be interested in exploring this area

514
00:42:02,840 --> 00:42:03,840
further?

515
00:42:03,840 --> 00:42:12,000
I would say we find that all these machine learning stuff and the GPU computing solutions

516
00:42:12,000 --> 00:42:19,520
sounds like it's much more than faster computation, it's much more than better realization.

517
00:42:19,520 --> 00:42:26,480
It's actually making us thinking about or doing cell biology research in a completely

518
00:42:26,480 --> 00:42:33,280
new way, and we are also exploring different possibilities down this road, and that's

519
00:42:33,280 --> 00:42:40,000
something we are doing now and keep pushing forward into different directions.

520
00:42:40,000 --> 00:42:42,680
Well, once again, thanks so much.

521
00:42:42,680 --> 00:42:44,680
Thank you.

522
00:42:44,680 --> 00:42:50,320
All right, everyone, that's our show for today.

523
00:42:50,320 --> 00:42:56,160
For more information on today's show, visit twomolai.com slash shows.

524
00:42:56,160 --> 00:42:59,880
As always, thanks so much for listening, and catch you next time.

