1
00:00:00,000 --> 00:00:04,880
All right, everyone. Welcome to another episode of the Swimwell AI podcast. I am your host,

2
00:00:04,880 --> 00:00:10,880
Sam Sherrington, and today I'm joined by Johan Braemer. Johan is a research scientist at Qualcomm AI

3
00:00:10,880 --> 00:00:16,240
research in Amsterdam. Before we get going, be sure to take a moment to hit that subscribe button

4
00:00:16,240 --> 00:00:20,720
wherever you're listening to today's show. Johan, welcome to the podcast.

5
00:00:20,720 --> 00:00:25,200
Hi, Sam. Thanks a lot for having me. Super excited to have you. We're going to be talking about one of

6
00:00:25,200 --> 00:00:32,480
the papers that you presented at NURPS on causal representations, as well as some of the things

7
00:00:32,480 --> 00:00:38,000
that your colleagues presented at the conference. But before we do that, I'd love to have you share

8
00:00:38,000 --> 00:00:42,400
a little bit about your background and how you came to work in machine learning. I grew up as a

9
00:00:42,400 --> 00:00:46,800
particle physicist. So I was trying to figure out the best way that we can measure the properties

10
00:00:46,800 --> 00:00:50,800
of all these elementary particles around us, like the Higgs boson, from all the data collected

11
00:00:50,800 --> 00:00:55,200
the large header and collider. Now, that's an inherently statistical problem, right? You need tools

12
00:00:55,200 --> 00:00:59,600
from statistics and from machine learning to solve this problem with all the high-dimensional data

13
00:00:59,600 --> 00:01:03,760
and the many pyramids you're trying to measure. During that time, I figured out that I actually

14
00:01:03,760 --> 00:01:09,040
enjoy working with the methods with the statistics and the machine learning much more than talking

15
00:01:09,040 --> 00:01:13,760
about the theoretical questions I set out to solve in the first place. So from there, it was a

16
00:01:13,760 --> 00:01:19,360
slippery slope. I started working on statistics for particle physics. I then did some machine learning

17
00:01:19,360 --> 00:01:23,760
for the sciences, and suddenly I found myself an Amsterdam working on machine learning problems

18
00:01:23,760 --> 00:01:31,360
that have nothing to do with particle physics with some great colleagues. And initially, or immediately

19
00:01:31,360 --> 00:01:38,400
into communications types of problems and the kind of things that you work on at Qualcomm, or

20
00:01:38,400 --> 00:01:43,200
did you start in another area? So in my first year at Qualcomm, I worked on a video compression

21
00:01:43,200 --> 00:01:47,200
with neural networks. I think my colleague, Alke Viggas, from the team was recently on your

22
00:01:47,200 --> 00:01:52,080
podcast and explained all that much better than I would do justice now. But there was a nice

23
00:01:52,800 --> 00:01:57,440
introduction to Qualcomm and all the research that we're doing there. But then one year in,

24
00:01:57,440 --> 00:02:02,880
we started a new team on causality. Now relative to some of the other folks that I've talked to

25
00:02:04,400 --> 00:02:11,680
on the Qualcomm AI research team, causal work is much less applied than video compression,

26
00:02:11,680 --> 00:02:17,440
for example. Yeah, totally true. I think Qualcomm has kind of this full spectrum of research that

27
00:02:17,440 --> 00:02:22,800
ranges from pretty applied, like model efficiency, neural network quantization, video compression,

28
00:02:22,800 --> 00:02:27,200
these kind of things, to pretty fundamentally like a geometrically learning, a covariance, and now

29
00:02:27,200 --> 00:02:32,560
also causality. But they're all united by this idea of making AI more efficient in some way.

30
00:02:32,560 --> 00:02:37,440
And I think for causality, the idea is that causality may be a framework that helps us solve some

31
00:02:37,440 --> 00:02:41,840
problems that are currently pretty much unsolvable with the standard paradigms in machine. But it may

32
00:02:41,840 --> 00:02:46,960
take some years to get there. And what are some of those types of problems that you think causality

33
00:02:46,960 --> 00:02:52,560
can help us solve in in ways that will be much more efficient? Yeah, so machine learning systems

34
00:02:52,560 --> 00:02:56,480
right now are great, like just by scaling up the datasets and scaling up the models we can solve

35
00:02:56,480 --> 00:03:01,360
many problems much better than we thought we could. I mean, look at ChatGPT. But there's still

36
00:03:01,360 --> 00:03:07,440
some things that are really hard for these problems. And one case is kind of the brittleness under

37
00:03:07,440 --> 00:03:11,520
changes. If you train a model in one type of conditions and then deploy it under different

38
00:03:11,520 --> 00:03:16,080
type of conditions, then often the performance goes down quite a bit. Like the machine learning

39
00:03:16,080 --> 00:03:22,720
systems have a hard time with seem to real with any kind of changing conditions. Now causality

40
00:03:22,720 --> 00:03:28,640
on a very high level is a framework about reasoning about actions, about reasoning about changes,

41
00:03:28,640 --> 00:03:35,520
about robustness under changes. So on an intuitive level, I think it makes a lot of sense that

42
00:03:35,520 --> 00:03:40,960
this can help us address these these open problems. However, I should say that it's not totally

43
00:03:40,960 --> 00:03:45,280
clear concretely how this will work, right? This is an open research field. Causing machine learning

44
00:03:45,280 --> 00:03:51,760
has really just started gaining momentum a few years ago. So let's see where this takes us.

45
00:03:51,760 --> 00:03:59,440
Yeah, before we dive into the paper about your research, Qualcomm, is it entirely focused on

46
00:03:59,440 --> 00:04:05,440
causality or are you covering several areas as well? It's about causality and interactive learning.

47
00:04:05,440 --> 00:04:09,360
So it kind of focuses on a lot of topics that bother on reinforcement learning. We have some

48
00:04:09,360 --> 00:04:13,520
projects on imitation learning. Maybe we'll get into that a little bit later. We have some projects

49
00:04:13,520 --> 00:04:18,640
on unsupervised reinforcement learning. What can you learn in a setting where you have an

50
00:04:18,640 --> 00:04:23,040
interactive environment, but no reward function, and no expert demonstrations, and then there's

51
00:04:23,040 --> 00:04:29,040
kind of the more classical causality. Awesome. And so this particular paper that we wanted to spend

52
00:04:29,040 --> 00:04:34,560
some time talking about is called weekly supervised causal representation learning. unpack that

53
00:04:34,560 --> 00:04:40,720
title a little bit and the goals you have for the paper. Yes, excellent. Let me start a little bit

54
00:04:40,720 --> 00:04:45,040
from the end of it, right? So the paper title ends with representation learning. And one of the

55
00:04:45,040 --> 00:04:52,240
two goals of this paper is to learn meaningful representations of data. What do I mean with that?

56
00:04:52,240 --> 00:04:57,040
If you have data presented to you in some kind of low level format, think the pixels of an image

57
00:04:57,040 --> 00:05:02,720
or maybe of a video feed or anything that we can record with sensors 3D, we want to train

58
00:05:02,720 --> 00:05:08,160
into a network to take that as an input and output a much smaller number of high level variables

59
00:05:08,160 --> 00:05:13,200
that capture the meaningful aspects of a system similar to how humans reason about system.

60
00:05:13,200 --> 00:05:17,040
When I show an image, you're not thinking, oh, these are some really nice collection of pixels

61
00:05:17,040 --> 00:05:22,720
here, right? You think this image shows, for instance, a car on a road in front of a traffic light.

62
00:05:22,720 --> 00:05:26,720
And the relevant variables are probably something like the position of the car, maybe the speed of the

63
00:05:26,720 --> 00:05:31,040
car, and then the state of the traffic light, whether it's red or green. So this is a much smaller

64
00:05:31,040 --> 00:05:35,760
number of meaningful variables and we want to train a new network to put this out. So that's the

65
00:05:35,760 --> 00:05:40,720
representation learning part. Now going one word more to the beginning, there's a causal in there.

66
00:05:40,720 --> 00:05:46,000
The second goal in our work is that we also want to learn a causal model between these high level

67
00:05:46,000 --> 00:05:52,320
variables. And its causal model is about the interactions between concepts. So again, in this

68
00:05:52,320 --> 00:05:57,040
example of a car before a traffic light, as humans, it's very intuitive to us that we think that

69
00:05:57,040 --> 00:06:02,320
the traffic light state, whether it's green or red, influences causes the behavior of the driver,

70
00:06:02,320 --> 00:06:07,520
whether they accelerate the car or break the car. So there's a causal effect from the traffic

71
00:06:07,520 --> 00:06:12,880
light state to the velocity of the car, but not the other way around. And this is important

72
00:06:12,880 --> 00:06:16,640
because it allows us to reason about what if questions. As a human, it's very easy for us to say,

73
00:06:17,600 --> 00:06:22,400
if the traffic light would turn to red, the car would break. If the car would break, it wouldn't

74
00:06:22,400 --> 00:06:27,280
necessarily make the traffic light go red. Now, machine learning systems, at least the majority of

75
00:06:27,280 --> 00:06:32,320
them, do not really reason about this kind of cause and effect relations. Most machine learning

76
00:06:32,320 --> 00:06:37,680
systems are stuck at the level of describing correlation patterns. So for instance, looking at a

77
00:06:37,680 --> 00:06:43,280
data set that shows you images of cars on traffic lights, machine learning systems could figure out

78
00:06:43,280 --> 00:06:50,880
that green lights are correlated with fast velocities and red lights are correlated with braking cars.

79
00:06:51,600 --> 00:06:55,520
But this doesn't allow the machine learning system to answer a question, what happens if the car

80
00:06:55,520 --> 00:07:02,240
breaks? Does the traffic light then suddenly change? So this is really a new kind of capability that

81
00:07:02,240 --> 00:07:09,120
doesn't exist in most kind of machine learning systems. Now, there's the weekly supervised part in

82
00:07:09,120 --> 00:07:14,320
the title. In this paper, what we do is we learn a neural network that that learns that

83
00:07:14,320 --> 00:07:19,280
represents data from pixel level into this kind of low number of meaningful high level variables

84
00:07:19,280 --> 00:07:25,840
and also learns the causal relations between them. But we do this without explicit labels in the

85
00:07:25,840 --> 00:07:32,880
training data. So there's no label that tells us in this image, the velocity of the car is 30 kilometers

86
00:07:32,880 --> 00:07:40,000
per hour, that's 20 miles per hour, or what have you, anything like that. So we just train this from

87
00:07:40,000 --> 00:07:45,760
example on the pixel level. But it's impossible to do this fully unsupervised. There are some papers

88
00:07:45,760 --> 00:07:51,520
that prove that this problem is really underspecified if you have just IID data just images.

89
00:07:52,400 --> 00:07:57,040
And the way that we solve this is by by breaking the ID assumption, we consider non-ID data,

90
00:07:57,040 --> 00:08:02,400
so kind of data with some structure in it. And concretely, we consider the case where we observe

91
00:08:02,400 --> 00:08:07,360
the system kind of pairs of before and after images. So we take a picture of the street,

92
00:08:08,000 --> 00:08:13,760
then something changes in the scene, like some effect is applied to it. For instance,

93
00:08:13,760 --> 00:08:18,880
you switch the traffic light state from red to green and let all the causative effects of that

94
00:08:18,880 --> 00:08:22,720
play out, and then you take another picture of the scene. And we just need this kind of before and

95
00:08:22,720 --> 00:08:27,040
the after image, but no further labels, no information on what happened before and after. If we have

96
00:08:27,040 --> 00:08:33,680
this kind of paired data, then we can show that this is enough to really learn the true causal

97
00:08:33,680 --> 00:08:38,080
variables in the scene and the true causal effects, the true causal graph, all the mechanisms that

98
00:08:38,080 --> 00:08:44,880
come on the scene. Now, I'd love to maybe start talking about how far along you are and how far

99
00:08:45,520 --> 00:08:53,200
the method that you've identified can take us and ask that in the context of this one diagram

100
00:08:53,200 --> 00:08:58,720
that you have at the very beginning of the paper that kind of articulates what you're hoping to do.

101
00:08:58,720 --> 00:09:06,480
It's a picture of some standing dominoes that you're before picture and then you have

102
00:09:06,480 --> 00:09:11,360
some dominoes tipped over and presumably there's an intervention between the two, someone

103
00:09:11,360 --> 00:09:17,760
tipped a domino. And you know, the grand idea is that you, with enough of these domino pictures,

104
00:09:17,760 --> 00:09:22,240
you can start to learn that if you've got some kind of domino tipping intervention,

105
00:09:23,520 --> 00:09:28,720
what the outcome will be after you've applied that intervention.

106
00:09:28,720 --> 00:09:33,200
Yeah, that's exactly right. And yeah, we're not at the point where I can, you know,

107
00:09:33,200 --> 00:09:38,400
giving your trained model, take a picture of some dominoes and apply this intervention somehow

108
00:09:38,400 --> 00:09:45,120
or say that, okay, give me the picture after the dominoes have fallen and get that resulting

109
00:09:45,120 --> 00:09:50,320
picture, correct? That's unfortunately correct. Yes, I think we make a pretty strong claim.

110
00:09:50,320 --> 00:09:55,360
In this paper, right, we have this really strong claim that we can identify the causal structure

111
00:09:55,360 --> 00:10:00,000
and the right variables and everything correctly. But to get such a strong result, we also need

112
00:10:00,000 --> 00:10:04,480
strong assumptions. And the strongest assumption is I think there's this kind of data regime that we

113
00:10:04,480 --> 00:10:09,600
need this data in this before and after pairing. And we also need to assume that nothing else

114
00:10:09,600 --> 00:10:14,320
changes between the before and the after image, just like one thing is intervened upon one action

115
00:10:14,320 --> 00:10:19,360
happens. But we also need to make a couple of more technical assumptions. And unfortunately,

116
00:10:19,360 --> 00:10:24,480
many real world examples violates some of these technical assumptions that we need for our theory,

117
00:10:24,480 --> 00:10:30,080
at least. So in particular, in this domino case, if you have two domino pieces, if you push over one,

118
00:10:30,080 --> 00:10:33,520
it would knock over the second one. If you push over the second, it could knock over the first

119
00:10:33,520 --> 00:10:38,800
one. So they're kind of the graph here in some sense is cyclic, right? Like each domino,

120
00:10:38,800 --> 00:10:44,560
like they affect each other pairwise. And this is something that standard causal frameworks

121
00:10:44,560 --> 00:10:50,160
can't really deal with so easily because we always assume that the graph of causal interactions

122
00:10:50,160 --> 00:10:55,040
needs to be a cyclic. Something can be the cause of something else, but that can't also be the

123
00:10:55,040 --> 00:11:00,560
effect of that variable. There's no back and forth relation in classical causal models. Now,

124
00:11:00,560 --> 00:11:05,440
there's some ways to resolve that. But in this work, we just assume that the causal structure is

125
00:11:05,440 --> 00:11:10,320
indeed a cyclic. So there's a clear causal ordering of mechanisms. The traffic light causes the

126
00:11:10,320 --> 00:11:14,560
velocity, but the velocity doesn't cause the traffic light, just one of these two things is possible.

127
00:11:14,560 --> 00:11:22,400
And so this is one setting that that that domino example violates. But I think it's really good

128
00:11:22,400 --> 00:11:25,600
to look at these cases that we can't do with yet because ultimately what we want to do is we want

129
00:11:25,600 --> 00:11:29,600
to solve real world problems. So we really need to kind of look at what what our theory can't

130
00:11:29,600 --> 00:11:35,040
describe yet and push there. And I think the way forward maybe to give up a little bit on the

131
00:11:35,040 --> 00:11:42,080
theoretical rigor to maybe not aim for theorems and proofs, but aim for just algorithms that empirically

132
00:11:42,080 --> 00:11:48,400
work ultimately. But in this current paper, we are kind of at the closer to the fundamental side

133
00:11:48,400 --> 00:11:52,720
of things. We have some theory. I think we understood some things better, but we don't have this.

134
00:11:52,720 --> 00:11:58,160
One size fits all algorithm that you can deploy to real world examples. And certainly not we haven't

135
00:11:58,160 --> 00:12:05,280
solved autonomous driving yet. Yeah, for sure, for sure. But what you've done is you've demonstrated

136
00:12:05,280 --> 00:12:14,480
that it is possible to identify these causal variables from, for example, from just pixels,

137
00:12:14,480 --> 00:12:20,160
which is pretty impressive in and of itself. Does the paper just demonstrate that it is possible

138
00:12:20,160 --> 00:12:24,080
or does the paper demonstrate how to derive the causal variables themselves?

139
00:12:25,040 --> 00:12:29,200
It does both. So I think a large fraction of the contribution of this paper is the theory. So

140
00:12:29,200 --> 00:12:34,720
that it's basically mostly one theorem and the proof that it accompanies it. But we also have a

141
00:12:34,720 --> 00:12:39,520
practical implementation. And essentially it consists of a variational autoencoder. So there's

142
00:12:39,520 --> 00:12:43,920
an encoder that takes as input pixels and it outputs some latent variables. And these latent

143
00:12:43,920 --> 00:12:47,920
variables are the causal variables of the system. And then there's a decoder that knapset back to the

144
00:12:47,920 --> 00:12:53,360
pixel space. And what's new about this kind of variation autoencoder is that we describe some causal

145
00:12:53,360 --> 00:12:59,360
structure in the latent space. So the latent variables are not just described by some, you know,

146
00:12:59,360 --> 00:13:04,960
IID Gaussian prior, like most fear is to it. But we have a prior that really incorporates

147
00:13:04,960 --> 00:13:09,520
the causal structure, the graph between the variables, this graph is learned during training.

148
00:13:10,640 --> 00:13:14,480
And also the exact mechanism, how each variable affects each other very well.

149
00:13:16,080 --> 00:13:21,440
And yeah, this kind of VAE, we trained this on these paired data sets before and after

150
00:13:22,160 --> 00:13:27,600
the intervention images. And then we show in a series of experiments with some, we start with

151
00:13:27,600 --> 00:13:32,160
some very similar toy data sets as we worked towards a simple image data sets that this works in

152
00:13:32,160 --> 00:13:39,760
practice. Is it interesting or kind of obvious and not interesting that the parallels between

153
00:13:39,760 --> 00:13:44,640
the video compression stuff that we talked about before that might use an encoder decoder type

154
00:13:44,640 --> 00:13:51,360
of architecture and that you're able to do the same thing here. You're kind of compressing

155
00:13:51,360 --> 00:14:02,640
the dynamics of the system happening before and after the pixel space into some kind of reduced

156
00:14:02,640 --> 00:14:10,080
dimensionality, you know, causally semantic space. Yeah, that's a good point. It definitely has

157
00:14:10,080 --> 00:14:15,600
some parallels like the overall architecture is very similar. But I think there's one important

158
00:14:15,600 --> 00:14:19,600
difference and that is that the kind of goal that we are trying to achieve, right? And compression

159
00:14:19,600 --> 00:14:24,720
is really about this aspect of compression, but then kind of how we use these different bits,

160
00:14:24,720 --> 00:14:28,320
how we use the representation of the latent space, doesn't really matter like the model can do

161
00:14:28,320 --> 00:14:33,680
whatever at once. And if you visualize the latent representation that compression autoencoder

162
00:14:33,680 --> 00:14:38,560
learns, you'll see that it's really not interpreted by humans. Like varying one latent variable

163
00:14:38,560 --> 00:14:45,440
leads to some very weird outputs in image space. And contrast, our model doesn't really care so

164
00:14:45,440 --> 00:14:49,120
much about the bit rate used in the end. I mean, it does play a role in the loss function a little

165
00:14:49,120 --> 00:14:55,680
bit, but it's not really the goal. The goal is really that the variables in the end have some

166
00:14:55,680 --> 00:15:01,360
meaning. The latent variables are exactly the true latent variables up to some permutations

167
00:15:01,360 --> 00:15:06,560
and rescalings. There's definitely some kind of machine learning knowledge that we gained

168
00:15:06,560 --> 00:15:10,800
on this compression work that we could use in the causality work, but I think the interpretation

169
00:15:10,800 --> 00:15:14,720
of the results is very different. Can you maybe talk a little bit broadly about

170
00:15:14,720 --> 00:15:19,520
the, you mentioned that there's kind of this one theorem that's at the heart of the paper. Can

171
00:15:19,520 --> 00:15:25,920
you talk a little bit more detail about that theorem? Yeah, I'm happy to. So this basically says,

172
00:15:25,920 --> 00:15:32,560
if you have two models, and with model, I now mean kind of a causal model between some variables,

173
00:15:32,560 --> 00:15:37,440
and then a map that takes these causal variables and maps them to some data space, so think pixels.

174
00:15:38,080 --> 00:15:43,280
So we call this a latent causal model. Now, what the theorem says, if you have two latent causal

175
00:15:43,280 --> 00:15:47,600
models, such that both of these latent causal models give rise to the same kind of data set,

176
00:15:47,600 --> 00:15:53,840
if you look at them. So if you kind of look at what kind of images you would get from the first

177
00:15:53,840 --> 00:15:57,440
latent causal model and what kind of images before and after images you'd get from the second causal

178
00:15:57,440 --> 00:16:02,800
model, they are the same. There's the same distribution. Then the two latent causal models are the

179
00:16:02,800 --> 00:16:06,720
same in the sense that they have the same latent variables, they have the same causal structure

180
00:16:06,720 --> 00:16:10,080
up to some equivalence class. This equivalence class basically tells you that for instance,

181
00:16:10,080 --> 00:16:17,360
we can't resolve permutations. If one model has variable one and two, and then the second model

182
00:16:17,360 --> 00:16:22,960
has variable two and one in the opposite order, then there's just something we can't further resolve

183
00:16:22,960 --> 00:16:28,960
because we never get labels. But up to this kind of small, and one might say irrelevant effects,

184
00:16:29,520 --> 00:16:35,120
those two models need to be the same. Now, why does this theorem matter? Because if you think of one

185
00:16:35,120 --> 00:16:40,640
of these two models as the generative process, the ground truth process, the generated observed data

186
00:16:40,640 --> 00:16:46,160
in some setting, maybe it's like the physical laws of the universe or it's like the rules of

187
00:16:46,160 --> 00:16:50,400
the traffic scene or maybe there's some human psychology and there would have some ground truth

188
00:16:50,400 --> 00:16:55,520
process that generates a data set. And the second model is some neural implementation of

189
00:16:55,520 --> 00:17:00,480
the latent causal model. So we use a VAE, we use neural permutations of the causal structure,

190
00:17:00,480 --> 00:17:06,960
and we train it to maximize our training objective. And then another some additional assumption

191
00:17:06,960 --> 00:17:10,080
that we have a good optimizer that we have enough data and so on, like the usual machine learning

192
00:17:10,080 --> 00:17:15,280
assumptions, then our theorem implies that in the end our learned model will recover the ground

193
00:17:15,280 --> 00:17:19,600
truth causal variables and the ground truth causal structure. Of course, the hidden assumption

194
00:17:19,600 --> 00:17:25,680
here is that indeed nature operates as such a causal model. And as we've already discussed

195
00:17:25,680 --> 00:17:30,640
with this domino example, there's sometimes some subtleties where our assumptions are not satisfied.

196
00:17:31,600 --> 00:17:38,080
And what are the mechanisms that you use to prove this theorem?

197
00:17:39,920 --> 00:17:44,480
Oh, that's a fun question and I like to talk more about this, but maybe I try to keep this

198
00:17:44,480 --> 00:17:49,440
kind of short. My two collaborators, my two main collaborators on this paper, Pim de Han and

199
00:17:49,440 --> 00:17:56,640
Takako and are both believers and practitioners of a field of mathematics called category theory.

200
00:17:56,640 --> 00:18:03,760
This is like the most abstract of mathematics. I really try to find like the most general structures

201
00:18:03,760 --> 00:18:08,000
that exist and try to unify many different fields of mathematics. We are not really using

202
00:18:08,000 --> 00:18:13,360
category theory, but we're using one tool, one particular graphical language developed in

203
00:18:13,360 --> 00:18:19,520
category theory, called string diagrams. And we use string diagrams to kind of represent

204
00:18:19,520 --> 00:18:24,400
different probabilistic equations, kind of relations between different probabilistic distributions

205
00:18:24,400 --> 00:18:29,520
graphically. And then you can use some nice manipulations on these and then use that to prove

206
00:18:29,520 --> 00:18:34,240
the theorem. I have to say, when I started out of this project, I was a bit skeptical about this

207
00:18:35,120 --> 00:18:40,560
and I like to make fun of my colleagues. And category theory in general? No, I think category

208
00:18:40,560 --> 00:18:43,920
theory is beautiful, but I was skeptical that it would give us practical advantages for this

209
00:18:43,920 --> 00:18:50,960
concrete project. But then it did simplify the proof here quite a bit. And I say by colleagues

210
00:18:50,960 --> 00:18:58,000
really showed me the way a little bit there. That's awesome. Did that just happen to be the

211
00:18:58,000 --> 00:19:04,240
approach that worked or was the project conceived with that as part of the approach that you

212
00:19:04,240 --> 00:19:11,840
would, was the project conceived as, you know, with that solution in mind? No, I would say we started

213
00:19:11,840 --> 00:19:16,400
out just from the actual kind of question. Can we, can we identify, cause a variables, can we

214
00:19:16,400 --> 00:19:21,040
identify, cause a structure just from pixelabid data? And then our first version of the proof

215
00:19:21,040 --> 00:19:26,160
looked much longer and much less elegant and did not involve any fancy diagrams, but then that

216
00:19:26,160 --> 00:19:31,680
evolved over time into something prettier. So it was more of a later stage of the project.

217
00:19:31,680 --> 00:19:38,160
Okay, okay. And you're careful to distinguish the causal variables and the causal structure

218
00:19:38,160 --> 00:19:45,760
or the causal relationships. Can you kind of more carefully distinguish the two and really talk

219
00:19:45,760 --> 00:19:50,640
about the relationships and how, how causal relationships are expressed? Oh, yeah, that's

220
00:19:50,640 --> 00:19:57,440
a great question. So in causality, one of the really important things is that we really don't

221
00:19:57,440 --> 00:20:01,840
just reason about variables, but reason about variables and the mechanisms that generate them.

222
00:20:01,840 --> 00:20:06,000
And what I mean with that is that the causal model, or there's different frameworks, but the one

223
00:20:06,000 --> 00:20:10,960
that's most widely used these days, describes the relation between variables is kind of factorized

224
00:20:10,960 --> 00:20:15,920
into a number of, if you want cause and effect relations, each of them takes the form of some,

225
00:20:15,920 --> 00:20:21,200
some, some mechanism. So in the, the root example that we had, one mechanism could, for instance,

226
00:20:21,200 --> 00:20:26,640
be how the, the traffic light is programmed, right? It probably has some frequency, how often it's

227
00:20:26,640 --> 00:20:30,880
green, how often it's red, how the changes, that's, that's one mechanism. And then a separate

228
00:20:30,880 --> 00:20:36,480
mechanism is how the car behavior adapts to, to the traffic light and maybe also the other things

229
00:20:36,480 --> 00:20:42,320
it sees on the road. So that kind of determines the car velocity as a function of all the other

230
00:20:42,320 --> 00:20:46,480
inputs, in particular, as the function of the traffic lights date. This is something that depends

231
00:20:46,480 --> 00:20:52,080
now on, on the particular driver. And what's kind of cool about this is that these mechanisms

232
00:20:52,080 --> 00:20:56,480
are independent in the sense that if you think about a different version of the scene, let's say we

233
00:20:56,480 --> 00:21:01,600
move to a different city, then maybe the, the traffic light frequency is totally different. So you

234
00:21:01,600 --> 00:21:06,000
can kind of change this one mechanism, but the way that humans react to traffic light will stay

235
00:21:06,000 --> 00:21:10,480
the same, right? That's kind of universal. So we only have to swap one mechanism when we change

236
00:21:10,480 --> 00:21:15,760
the scenery, why can keep, why we can keep the other one. This as fast mechanism shift hypothesis

237
00:21:15,760 --> 00:21:21,520
at its call sometimes is the essential reason why people believe that causality can help us with

238
00:21:21,520 --> 00:21:25,280
things like domain shift. Because we can think that if everything is composed of mechanisms,

239
00:21:25,280 --> 00:21:30,560
it's quite likely that when you change conditions, if you go from seem to real or from one country

240
00:21:30,560 --> 00:21:34,080
to another one, just a few of these mechanisms change, while other things are more universal,

241
00:21:34,080 --> 00:21:37,840
like the laws of physics are the same everywhere. Some human behaviors are the same everywhere.

242
00:21:39,040 --> 00:21:42,720
Yeah, but other things change, like go to the UK and suddenly you have to drive on the left side

243
00:21:42,720 --> 00:21:48,800
of the road. Now going back to the exchange we had about the parallel between compression and

244
00:21:48,800 --> 00:21:56,560
what you're doing here, identifying causal representations. You mentioned that in the case of

245
00:21:56,560 --> 00:22:06,240
compression, the goal is to find kind of the minimalist representation of this function, if you will.

246
00:22:06,880 --> 00:22:14,320
In this case, you're trying to identify a representation that is true to the causal nature that

247
00:22:14,320 --> 00:22:25,280
you're trying to model. I don't know if it's related to the, I guess intuitively, it strikes me

248
00:22:25,280 --> 00:22:32,560
that the causal representation would also be minimal. In the case of the traffic scenario we're

249
00:22:32,560 --> 00:22:38,800
talking about, really, the only things that matter are the traffic light and the cars and everything

250
00:22:38,800 --> 00:22:45,440
else is noise that might be captured by some other model, but the fundamental things that are

251
00:22:45,440 --> 00:22:51,440
causing the system to behave the way it's behaving are the traffic light and the car behavior.

252
00:22:51,440 --> 00:22:58,000
If you can model those and just those as a set of causal relationships, that would seem like it

253
00:22:58,000 --> 00:23:04,800
should be minimal. Where does that break down? Yeah, that's a really interesting point. I am actually

254
00:23:04,800 --> 00:23:11,760
not sure if anyone has tried exactly this, but it's true that most people find it likely that

255
00:23:11,760 --> 00:23:16,880
the causal description of the system has the most or relatively simple components as opposed to

256
00:23:16,880 --> 00:23:20,720
a non-causal description of the system which could have very complicated probability distributions.

257
00:23:20,720 --> 00:23:27,760
So, yeah, that's a great question. It is true that in some sense causal mechanisms are generally

258
00:23:27,760 --> 00:23:32,480
assumed to be simple, especially for humans. We assume that if we factor a probability

259
00:23:32,480 --> 00:23:35,840
distribution along the causal mechanisms, so if we describe everything with a kind of

260
00:23:35,840 --> 00:23:41,920
causal mechanisms, it's a much simpler thing than if we parameterize it in some other way.

261
00:23:41,920 --> 00:23:47,200
Now, I think where this breaks down is that ultimately what's simple to humans and what's

262
00:23:47,200 --> 00:23:51,600
simple to machine learning algorithms, at least right now, isn't quite the same yet. So,

263
00:23:53,040 --> 00:24:01,520
the way that machine learning capacity works translates into different notions of simplicity

264
00:24:01,520 --> 00:24:07,440
for a totally neural prior and then a neural decoder, then notions of simplicity for kind of,

265
00:24:07,440 --> 00:24:12,160
when humans read an expression, what do they think of the complexity of something?

266
00:24:12,160 --> 00:24:18,640
But I do think it's very interesting to explore this further and think more about things like

267
00:24:18,640 --> 00:24:23,200
the minimal description length principle and how that could be applied maybe to compression.

268
00:24:24,000 --> 00:24:30,480
Maybe indeed there is a deeper relation how we can use some of these ideas of simplicity that

269
00:24:30,480 --> 00:24:36,560
pop up both in causality and in compression. There was some prior work that attempted to do

270
00:24:37,360 --> 00:24:44,160
something similar, identify causal representations, but found that it was not possible.

271
00:24:44,800 --> 00:24:51,200
Is that the case? That's exactly right. There's a very well done paper by Francesco Locatello.

272
00:24:51,200 --> 00:24:57,200
I think it won a best paper award at ICML, I want to say 2020. They basically show that if you just

273
00:24:57,200 --> 00:25:03,200
have IID data, if you just have individual images, but kind of just one image from each scene,

274
00:25:04,240 --> 00:25:08,400
that then you can't even identify the variables, even if you assume that the causal relations are

275
00:25:08,400 --> 00:25:13,360
in some sense trivial and that kind of implies that for more complicated causes structures you can

276
00:25:13,360 --> 00:25:18,880
also not do it. So, the way that we get around this is by introducing this non-IID setting,

277
00:25:18,880 --> 00:25:23,120
where we have these pairs of observations. And actually for full fairness here, I would point out

278
00:25:23,120 --> 00:25:26,800
that this is not a totally novel idea. There was another paper by Francesco Locatello

279
00:25:26,800 --> 00:25:33,360
at ICML 2021, I think, where they already think about this weekly supervised setting in observing

280
00:25:33,360 --> 00:25:38,000
a system before and after something has changed. But they focus on the trivial case where

281
00:25:38,960 --> 00:25:42,800
trivial sounds are negative. What I mean is the case where all the causal variables are

282
00:25:42,800 --> 00:25:46,800
independent. So, there's no causal relations between anything. You just have statistically

283
00:25:46,800 --> 00:25:51,840
independent latent variables and they shall show that those can be identified. What we do in our

284
00:25:51,840 --> 00:25:55,440
papers, we extend that to the setting where there's actually non-trivial causal structure

285
00:25:55,440 --> 00:25:59,600
and that we can even then learn the variables and even more that we can also identify the causal

286
00:25:59,600 --> 00:26:06,560
structure. Is there an experimental component to your paper at all? So, in the sense, yes,

287
00:26:06,560 --> 00:26:11,680
that we evaluate our via algorithm on synthetic data. We start with some simple toy data sets and

288
00:26:11,680 --> 00:26:19,200
then we scale up to image data sets. But also these data sets are not real photos of some real

289
00:26:19,200 --> 00:26:22,800
scenes or anything like that. We start with a standard benchmark for causal representation

290
00:26:22,800 --> 00:26:27,840
learning, the causal 3D-ident data set or a variation of that was introduced in a paper by

291
00:26:27,840 --> 00:26:33,120
Julius van Kugelgen and others. And then we introduce our own data set. We call the causal

292
00:26:33,120 --> 00:26:37,840
circuit because we weren't quite satisfied with the available benchmarks. And in this causal

293
00:26:37,840 --> 00:26:44,480
circuit data set, you see a robotic finger operating a bunch of buttons that are connected to lights

294
00:26:44,480 --> 00:26:48,560
in a way. And we show that in this data set, you can correctly identify kind of which pixels

295
00:26:48,560 --> 00:26:53,120
map to the buttons and to the robotic finger and how everything is related. I guess the

296
00:26:53,120 --> 00:27:01,680
salient point in this data set is that the robot, when the robot interacts with a button and it

297
00:27:01,680 --> 00:27:07,200
turns on that light, that doesn't impact any of the other lights, for example. Is that true?

298
00:27:08,000 --> 00:27:12,160
Yeah, kind of. So we actually, just for fun, we added a little causal relation there that like if

299
00:27:12,160 --> 00:27:17,600
you press one of the lights, it turns on one of the other ones as well. But in principle, you could

300
00:27:17,600 --> 00:27:21,600
also have the kind of independent thing. But I think one very obvious thing is like the robot

301
00:27:22,640 --> 00:27:26,240
which is also in the scene that causes the lights, right? It's not the other way around. You could

302
00:27:26,240 --> 00:27:30,880
in principle also think there's some kind of magnet on the table, if whenever one light activates

303
00:27:30,880 --> 00:27:34,960
that magnet pulls the robot, I'm closer to it. Just from looking at individual pictures, there's

304
00:27:34,960 --> 00:27:40,240
no way of telling these two apart. But the, yeah, with these kind of paired data,

305
00:27:42,400 --> 00:27:46,160
this data setting that we introduced and we show that we can actually resolve this and we can

306
00:27:46,160 --> 00:27:50,880
learn that the robot causes the lights to go on another way around. I think there's a really

307
00:27:50,880 --> 00:27:56,960
interesting paper and I'm personally interested in following along with kind of how that work evolves

308
00:27:56,960 --> 00:28:02,960
and how far we can push this idea of just pulling causal representation from images and

309
00:28:05,200 --> 00:28:11,040
you know, woodling down the set of constraints that need to be applied to the model that you create

310
00:28:11,040 --> 00:28:17,440
from them. But I also wanted to talk to you about some of the papers that your colleagues presented

311
00:28:17,440 --> 00:28:25,120
at NURBS and there were several. A couple of those were on combinatorial optimization broadly.

312
00:28:25,120 --> 00:28:30,320
Can you talk a little bit about those? Yeah, I'm happy to. So my colleagues wrote two papers on

313
00:28:30,320 --> 00:28:35,760
combinatorial optimization that we accepted at NURBS, one that was written by Mukul Gagrani and

314
00:28:35,760 --> 00:28:42,080
Karado Rhinona and some others on neural topological ordering for computation graphs. And a second

315
00:28:42,080 --> 00:28:46,960
that was on a batch-based optimization over permutations led by Chang-Yung-Oh,

316
00:28:48,960 --> 00:28:53,200
Cuba students who student at the University of Amsterdam in a Qualcomm sponsored lab there.

317
00:28:54,080 --> 00:28:59,120
What both of these papers have in common is that they are about the problem of finding some

318
00:28:59,120 --> 00:29:03,840
optimal ordering of some objects under constraints. So where this shows up, for instance,

319
00:29:03,840 --> 00:29:09,120
is in computation graphs when you have some, for instance, neural network that has a number of

320
00:29:09,120 --> 00:29:13,440
operations that need to be performed and you want to execute them on your hardware in the right order,

321
00:29:13,440 --> 00:29:18,800
where right order means you want to be as fast as possible, you want to use as little memory as

322
00:29:18,800 --> 00:29:24,320
possible, but you also have to satisfy some precedence constraints that you don't execute an

323
00:29:24,320 --> 00:29:29,520
operation while all the inputs aren't available yet. And this is a hard problem because the number

324
00:29:29,520 --> 00:29:34,800
of operations can be large and the number of possible orders of operations grows very quickly

325
00:29:34,800 --> 00:29:42,160
with the number of operations. In this neural topological ordering paper, my colleagues develop a new

326
00:29:42,160 --> 00:29:47,600
attention-based graph neural network with a suitable message passing algorithm to solve this problem.

327
00:29:47,600 --> 00:29:54,160
It's called a topoformer. I think what's really key about this idea is that this graph-based

328
00:29:54,160 --> 00:30:02,480
transformer combines both the local topology of the computation graphs, so kind of for each

329
00:30:02,480 --> 00:30:07,280
operation you have the information kind of what are the preceding operations that you require and

330
00:30:07,280 --> 00:30:12,080
for which other operations is the output required, but also in the computation you're readily available,

331
00:30:12,080 --> 00:30:17,040
you have readily available global information about the structure of the graph. So each node can

332
00:30:17,040 --> 00:30:21,120
kind of talk to each part of the graph at the same time in this message passing algorithm.

333
00:30:21,120 --> 00:30:26,080
Now there was a bit technical, but the bottom line is that this works really well. If you just

334
00:30:26,080 --> 00:30:31,840
compare to similarly fast algorithms, this is really state of the art, both on synthetic graphs

335
00:30:31,840 --> 00:30:36,880
and on a few real-world examples. And I think it's still competitive with algorithms that are some

336
00:30:36,880 --> 00:30:43,920
orders of magnitude slower than this method. In this context, when you compare synthetic graphs

337
00:30:43,920 --> 00:30:56,240
versus real-world, elaborate on that, real-world problems will be kind of represented as a graph,

338
00:30:56,240 --> 00:31:02,960
the graph itself isn't real-world necessarily. That's a good point, but what I mean with

339
00:31:02,960 --> 00:31:06,000
the real-world problem here is more something like you have some neural network that somebody

340
00:31:06,000 --> 00:31:11,920
really trains, so usually people use some kind of standard neural networks. And then if you look

341
00:31:11,920 --> 00:31:16,160
at the structure of the neural network, you can extract the computation graph from that. So

342
00:31:17,360 --> 00:31:22,720
the graph is as much real-world as the neural network is. It's not that you can see it on the

343
00:31:22,720 --> 00:31:26,880
street, of course, but there's kind of some consensus in the field of what these networks look like.

344
00:31:27,760 --> 00:31:32,800
So this is a problem that appears for real-world compilers when these networks are executed.

345
00:31:33,680 --> 00:31:37,200
While synthetic graphs is really you want to generate a graph with, you say, I don't know,

346
00:31:37,200 --> 00:31:43,360
I want 10,000 nodes and I want like each edge to exist with the probability of 0.1 and I want

347
00:31:43,360 --> 00:31:47,040
the graph to be a cyclic and those are kind of the constraints you put in. And then you just

348
00:31:47,760 --> 00:31:50,960
put on your random number generator. So typically these have a little bit less structure than

349
00:31:52,480 --> 00:31:56,480
real-world graphs. Got it. You're not generating synthetic

350
00:31:57,120 --> 00:32:01,280
programs and extracting the graphs from those programs or models or something like that.

351
00:32:01,280 --> 00:32:04,880
How about the other paper? The other paper, batch-based optimization on

352
00:32:04,880 --> 00:32:09,760
permutations using acquisition weighted kernels is essentially on the same problem. So

353
00:32:09,760 --> 00:32:14,800
it's still about finding the right order for a number of objects. But now they consider the case

354
00:32:14,800 --> 00:32:20,640
where computing performance for each order, so the cost function or whatever you want to call it,

355
00:32:21,600 --> 00:32:26,960
is really expensive. I see you can only run it a few times. So this setting you have to be smart

356
00:32:26,960 --> 00:32:32,240
about how you perform your training, which different configuration, which permutations you query.

357
00:32:32,240 --> 00:32:36,720
And Bayesian optimization is one framework that allows you to do this in a principled way. So you

358
00:32:36,720 --> 00:32:43,520
kind of at each step you try to find that permutation that allows you to gain the most information

359
00:32:43,520 --> 00:32:50,480
on the optimal configuration. Without getting into too many details here, I think my colleagues

360
00:32:50,480 --> 00:32:57,120
here introduced the first batch-based optimization method for optimizing over permutations.

361
00:32:57,120 --> 00:33:01,920
This batch means that you can kind of evaluate the number of configurations in parallel,

362
00:33:01,920 --> 00:33:05,760
which is really practical if you, for instance, have multiple devices, multiple GPUs during training.

363
00:33:06,560 --> 00:33:11,680
And again, like this other paper, I think what really stands out is after developing the algorithm

364
00:33:11,680 --> 00:33:17,520
that this is just state-of-the-art, it beats existing methods on standard benchmarks.

365
00:33:18,720 --> 00:33:22,400
As we mentioned earlier, a number of your colleagues work on

366
00:33:22,400 --> 00:33:30,240
a core variant deep learning, and a couple of the papers at this year's NURBS were on that topic.

367
00:33:30,240 --> 00:33:31,920
Can you talk a little bit about those?

368
00:33:31,920 --> 00:33:38,800
Yeah, we have two papers on a key variant deep learning at NURBS. The first is led by Gabriela

369
00:33:38,800 --> 00:33:43,520
Chesa, and three of the four authors on that paper were actually previously on your podcast.

370
00:33:43,520 --> 00:33:50,400
I believe there was Arash, Taco, and Max were all colleagues, or in Max's case, former colleague

371
00:33:50,400 --> 00:33:56,880
of mine, and who you talked at some point. Anyway, this paper is about a cryo-electron microscopy.

372
00:33:56,880 --> 00:33:57,920
Have you heard about that?

373
00:33:57,920 --> 00:34:00,080
No, tell me more about that.

374
00:34:00,080 --> 00:34:04,400
Yes, it's a really cool and pretty new imaging method for structural biology,

375
00:34:04,400 --> 00:34:09,680
and it has really revolutionized this field a little bit, and I think it also has one

376
00:34:09,680 --> 00:34:16,320
Nobel Prize in 2017. So the idea is, you're biologist, you have some molecules, say a protein

377
00:34:16,320 --> 00:34:19,600
or something, and you want to reconstruct the 3D structure of that.

378
00:34:19,600 --> 00:34:24,480
Now that's hard to do because molecules are kind of small. So what you can do is take a bunch

379
00:34:24,480 --> 00:34:30,000
of these molecules, solve them in some solution, freeze the solution in a very thin layer,

380
00:34:30,000 --> 00:34:36,560
and then bombard it with electrons, and image kind of the size of this. This gives you a bunch

381
00:34:36,560 --> 00:34:43,680
of really noisy two-dimensional images. Now the task is, of course, you have kind of have to combine

382
00:34:43,680 --> 00:34:49,440
these noisy two-dimensional images into the 3D structure of the molecule that you started with,

383
00:34:49,440 --> 00:34:54,320
and that's really difficult, one, because of the noise, and two, because you don't know the

384
00:34:54,320 --> 00:34:59,440
orientation of the molecules, right? They are frozen in some liquid, but you have no idea which

385
00:34:59,440 --> 00:35:02,960
way around. So you need to infer the pose for each of these molecules first.

386
00:35:03,760 --> 00:35:07,280
And this is the problem that my colleagues tackle here. They develop a new algorithm

387
00:35:07,280 --> 00:35:14,240
for inferring the pose of three-dimensional molecules from these trial EM images,

388
00:35:14,240 --> 00:35:20,560
and they tackle this using some strategy from group theory. I think it's called a group synchronization

389
00:35:20,560 --> 00:35:26,640
framework. It's quite technical, but the bottom line here is that they incorporate the structures,

390
00:35:26,640 --> 00:35:32,400
the geometric symmetries of the problem, into the algorithm as much as possible, and then they,

391
00:35:33,200 --> 00:35:38,080
those are smart people, they figure out that you can infer the 3D poses of these molecules

392
00:35:38,080 --> 00:35:44,640
from these 2D images a little bit better than before. This is the starting point to then plug

393
00:35:44,640 --> 00:35:50,240
the residing post-estimates into some 3D reconstruction algorithm and then get better

394
00:35:50,240 --> 00:35:54,560
reconstructions of these molecules. Maybe on a slightly higher level, I think this is another

395
00:35:54,560 --> 00:35:59,600
example for how when you take into account the geometric structure of your problem, the symmetries

396
00:35:59,600 --> 00:36:04,880
of a problem can really make your machine learning algorithms or classical algorithms more efficient.

397
00:36:04,880 --> 00:36:11,840
And that's a lot of what the group focusing on the aquarium and stuff focuses on those symmetries

398
00:36:12,480 --> 00:36:17,280
applied in all different places. Yeah, that's right, and maybe that's a good transition to this

399
00:36:17,280 --> 00:36:24,560
other paper by Arash Béberodi, also a Gabriele and others called a PEC Bayesian generalization bound

400
00:36:25,280 --> 00:36:32,880
for aquarium networks. Now here, they really study how much you can win when you include

401
00:36:32,880 --> 00:36:37,280
in corporate aquarium variants or kind of the geometric structure into the architectural

402
00:36:37,280 --> 00:36:44,400
choices in NREL network. And they do this using a framework called PEC Bayes, so the idea is

403
00:36:44,400 --> 00:36:49,680
basically that you can develop a theoretical upper bound on how much your loss can become worse

404
00:36:49,680 --> 00:36:56,160
when you go from training data to test data. And what they do is that they show that this bound

405
00:36:56,160 --> 00:37:03,760
on the loss, so this kind of generalization error becomes better, the more symmetry constraints

406
00:37:03,760 --> 00:37:09,040
you incorporate in the architecture of your neural network. So in some sense, they theoretically show

407
00:37:09,040 --> 00:37:12,400
that aquarium neural networks generalize better than non-aquivariant neural networks.

408
00:37:13,120 --> 00:37:17,840
What I really like about this paper is that the result of this this theoretical bound also depends

409
00:37:17,840 --> 00:37:22,240
a little bit on the architectural choices you're making your neural network, so you can use this

410
00:37:22,240 --> 00:37:26,480
to provide some guidance for the design of aquivariant neural networks.

411
00:37:27,200 --> 00:37:30,080
How are the architectural choices parameterized?

412
00:37:30,960 --> 00:37:37,120
So I think this is all about what kind of representations of your group you use in the hidden

413
00:37:37,120 --> 00:37:42,240
layers of your neural network. So representations of a group are kind of a classification of

414
00:37:42,240 --> 00:37:48,000
the different ways that groups can manifest themselves on mathematical objects

415
00:37:48,000 --> 00:37:53,760
in particular on vector spaces. So if you have some latent variables, some hidden variables

416
00:37:53,760 --> 00:38:00,400
in a neural network, you can think about them, for instance, as vectors or as tensors or higher

417
00:38:00,400 --> 00:38:07,440
order objects, and they all transform differently under group transformations. Now, that was

418
00:38:08,160 --> 00:38:12,720
quite dense sentence I just said, but basically if you think about, for instance, translation

419
00:38:12,720 --> 00:38:17,040
symmetries, you kind of the idea that if you shift the input a little bit, the output should also

420
00:38:17,040 --> 00:38:22,480
shift a little bit in some way. Then one representation of this is kind of just shifting in the same

421
00:38:22,480 --> 00:38:27,280
way, the standard representation. Another representation is kind of the trivial representation,

422
00:38:27,280 --> 00:38:33,040
where you just stay constant, no matter what happens. This is exactly invariant. So if the input

423
00:38:33,040 --> 00:38:37,200
shifts your hidden layer or your output would kind of stay at the same point. And then there is

424
00:38:37,200 --> 00:38:43,760
higher order representations where when you shift something, the hidden variables or the outputs,

425
00:38:43,760 --> 00:38:47,360
we have in a more complicated way, but still in a way that reflects the symmetry structure of your

426
00:38:47,360 --> 00:38:55,680
problem. And what my colleagues find here is that these representations are that you choose for

427
00:38:55,680 --> 00:38:59,920
for the design of your network are related to the bound on the generalization error. So they give

428
00:38:59,920 --> 00:39:06,720
you some guidance on how you can build a query networks in a way that have as small as possible

429
00:39:06,720 --> 00:39:14,880
bound on the generalization error. The question again. There was a paper on quantization as well.

430
00:39:14,880 --> 00:39:21,920
Yes. Our last paper at NURBs called FP8 quantization, the power of the exponent. This was led by

431
00:39:21,920 --> 00:39:26,480
my colleagues Andrei Kusmin and Mart van Bealen. And I think one of the authors' time has also been

432
00:39:26,480 --> 00:39:32,320
on your podcast before is indeed about a neural network quantization. As you know, a neural

433
00:39:32,320 --> 00:39:38,560
quantization is maybe the most efficient way of making neural networks more efficient. And with

434
00:39:38,560 --> 00:39:45,520
that, I mean, run faster, useless memory, useless power are these things. That's quite important

435
00:39:45,520 --> 00:39:49,360
when you run to run things on device. And that's something that Qualcomm cares a great deal about.

436
00:39:49,360 --> 00:39:53,120
But it's also maybe just important from a sustainability perspective.

437
00:39:54,800 --> 00:39:59,680
Now, one problem with quantization is that many different formats for the representation of

438
00:39:59,680 --> 00:40:04,080
weights and activations have been proposed. So there are integer representations and there are

439
00:40:04,080 --> 00:40:07,600
floating point representations, for instance. And for floating point representations, there's also

440
00:40:07,600 --> 00:40:14,240
many different ways you could construct the representation. What my colleagues do in this paper

441
00:40:14,240 --> 00:40:22,080
is that they compare 8-bit representations in theory and practice. In particular, they compare 8-bit

442
00:40:22,080 --> 00:40:28,480
integer representations and 8-bit floating point representations. And they train neural networks

443
00:40:28,480 --> 00:40:34,080
both in a quantization-aware way, so they kind of take into account how your quantitative

444
00:40:34,080 --> 00:40:38,240
already doing training. And for post-training quantization, where you run the quantization as an

445
00:40:38,240 --> 00:40:44,240
afterburner after your neural network has converged. The result of this paper is a very clear

446
00:40:44,240 --> 00:40:52,960
it depends. So if you have, if you want to use post-training quantization, and especially if you have

447
00:40:52,960 --> 00:40:57,360
large neural networks, let's say large transformer, then a floating point quantization can be

448
00:40:57,360 --> 00:41:02,560
beneficial. But they also find that if you use quantization-aware training, so if you already

449
00:41:03,280 --> 00:41:06,560
include the knowledge that you're going to quantize your neural networks during training,

450
00:41:06,560 --> 00:41:12,080
then the difference becomes much smaller. And another point they make is that the performance

451
00:41:12,080 --> 00:41:18,560
of floating point quantization really depends on the hyper parameter structures. And this is

452
00:41:18,560 --> 00:41:24,720
much more sensitive to the choice of hypermills than int8 quantization. Finally, I think it's important

453
00:41:24,720 --> 00:41:30,240
to say that if the performance of the things is kind of equal, integer quantization has the

454
00:41:30,240 --> 00:41:35,840
advantage that the hardware that is compatible with running that is often much more energy efficient.

455
00:41:35,840 --> 00:41:42,160
So all of the things being equal, I think the go-to recommendation is maybe still int8 quantization here.

456
00:41:42,160 --> 00:41:48,800
Awesome, awesome. How about we quickly run through workshops and demos? Were there any of those as

457
00:41:48,800 --> 00:41:55,040
well this year? Yeah, I'm happy to talk about those as well. We had a couple of workshop papers.

458
00:41:55,040 --> 00:41:58,880
I'm a little bit biased here, but I'd like to cherry-pick just one of them because I was involved

459
00:41:58,880 --> 00:42:04,480
with it personally. And this is a paper called Deconfirmed Immutation Learning, led by my colleague

460
00:42:04,480 --> 00:42:11,760
Risto for Oreo, and presented at the deep reinforcement learning workshop. Now, you know

461
00:42:11,760 --> 00:42:16,800
imitation learning is about the problem of training a policy to not just maximize the reward

462
00:42:16,800 --> 00:42:21,040
function, but imitate behaviors in some offline data set, generate by some experts, right?

463
00:42:21,040 --> 00:42:24,720
And this is advantageous because it doesn't require you to come up with a reward function,

464
00:42:24,720 --> 00:42:28,480
but also it can be beneficial for safety, for instance, in autonomous driving.

465
00:42:28,480 --> 00:42:33,040
And where does the confounding problem that's apparent in the title come in?

466
00:42:33,040 --> 00:42:39,280
Yeah, yeah, exactly. We're getting to that. The confounding problem appears when the expert

467
00:42:39,280 --> 00:42:44,000
and the imitator don't see exactly the same data. Let's say the expert has access to more data

468
00:42:44,000 --> 00:42:49,120
in his inputs than the imitator. Maybe let me give you a silly example for this. So,

469
00:42:49,120 --> 00:42:54,480
let's say we are trying to solve autonomous driving and we are doing this through imitation

470
00:42:54,480 --> 00:42:58,800
learning where the data set is generated by some human drivers. It's a reasonable setting,

471
00:42:58,800 --> 00:43:02,720
but these human drivers generally will have lots of additional information that then later the

472
00:43:03,440 --> 00:43:07,440
autonomous agent will not have. For instance, imagine a human driver listens to the weather

473
00:43:07,440 --> 00:43:12,240
forecast on the morning of generating the training data. And maybe on the weather forecast,

474
00:43:12,240 --> 00:43:16,400
they hear that the road conditions are going to be icy. And then even if you don't see anything

475
00:43:16,400 --> 00:43:20,400
on the road, I bet that their driving will be a little bit slow and a little bit more careful.

476
00:43:20,400 --> 00:43:26,640
A little more cautious. Yeah. Exactly. And that's a problem because then later the agent in the data

477
00:43:26,640 --> 00:43:31,920
set observed multiple examples where the inputs look the same or the behavior of the agent is different.

478
00:43:31,920 --> 00:43:35,600
So what are they going to choose? Are they going to drive slowly? Are they going to drive fast?

479
00:43:35,600 --> 00:43:40,000
They have no way of inferring this. So they must make some random guess here. And then it gets

480
00:43:40,000 --> 00:43:46,800
worse because once you make an initial guess, later the agent can use its own past behavior

481
00:43:46,800 --> 00:43:52,240
as evidence for what the correct behavior is. This is something known as causal delusions and was

482
00:43:52,240 --> 00:44:00,960
pointed out in a series of papers by Pedro Ortega who used to be at DeepMind. So if you just train

483
00:44:00,960 --> 00:44:06,320
an imitation learning agent with a standard algorithm and it kind of decides to go fast initially

484
00:44:06,320 --> 00:44:11,440
and then a few time steps later, it can kind of look back and see, okay, I used to drive fast

485
00:44:11,440 --> 00:44:16,080
and in all the training examples where early on the driver drives fast, he continues to drive fast.

486
00:44:16,080 --> 00:44:20,240
So then this is kind of evidence for continuing with a high speed. Of course, that's not

487
00:44:20,240 --> 00:44:25,520
ideal if the road is potentially icy. So this is a slightly exaggerated example. Of course,

488
00:44:25,520 --> 00:44:30,080
autonomous vehicles right now do not really have exactly this problem, but maybe you get the idea.

489
00:44:30,080 --> 00:44:34,960
And what we do with this paper and what particular is to us in this paper is study this problem

490
00:44:34,960 --> 00:44:40,240
theoretically and characterize the different conditions under which we can solve it in imitation learning.

491
00:44:40,240 --> 00:44:47,600
And mostly limited kind of simple toy scenarios, but we also develop an algorithm that we can run

492
00:44:47,600 --> 00:44:52,880
on reinforcement learning or imitation learning settings and we demonstrate it on a few

493
00:44:53,920 --> 00:45:02,400
continuous control problems. Interesting. How toy? So for instance, we have one environment,

494
00:45:02,400 --> 00:45:07,840
it's one of these standard open eye gym environments where you have like a robotic arm with a few

495
00:45:07,840 --> 00:45:13,840
degrees of freedom and it can strike a ball. And there are some parameters that the agent doesn't know

496
00:45:13,840 --> 00:45:19,920
like the weight of the ball or how much it slows down later kind of the friction.

497
00:45:21,920 --> 00:45:25,360
And the expert knows these things. You can imagine that the expert maybe picked up the ball and

498
00:45:25,360 --> 00:45:30,400
placed it on the table before they hit it to play some mini golf or whatever you want to call it.

499
00:45:30,400 --> 00:45:35,440
So this is something if you just do it once, there's just, if you just train it on imitation learning

500
00:45:35,440 --> 00:45:39,360
and out there in your test environment, you have no way of knowing how hard to hit the ball.

501
00:45:39,360 --> 00:45:44,880
But we learn a policy that kind of does it twice and based on kind of observing how the

502
00:45:44,880 --> 00:45:49,520
ball behaves the first time, then then figures out the value of the friction and the mass in these

503
00:45:49,520 --> 00:45:55,920
things and corrects the behavior for that in the next time. So it's still pretty toy, but I think it's

504
00:45:55,920 --> 00:46:01,360
at least not a kind of tabular data set anymore, but it's slowly moving towards bigger problems.

505
00:46:01,360 --> 00:46:07,280
Awesome. How about demos? Yeah, it was a great fun. It was the first time for me to be at the

506
00:46:07,280 --> 00:46:12,960
Qualcomm booth at least physically. And a lot of people came by and looked at the four demos

507
00:46:12,960 --> 00:46:19,040
that we had there. I talk to you about what exactly they were in a second, but I think what they all

508
00:46:19,040 --> 00:46:27,520
have in common is kind of maybe Qualcomm AI research core competency, which is really

509
00:46:29,120 --> 00:46:33,760
making AI ubiquitous in the sense that we want to run machine learning systems on any kind of

510
00:46:33,760 --> 00:46:40,960
device. So we don't just run our systems on GPUs or even bigger systems, but we had two phones

511
00:46:40,960 --> 00:46:48,880
there. We had one tablet there and we had one AR goggle there and we were running our AI systems on

512
00:46:48,880 --> 00:46:54,400
all of these devices and of course all of these included Qualcomm ships inside. Now more

513
00:46:54,400 --> 00:47:02,400
concretely, we had one demo on super resolution. And here the main novelty was that we quantized

514
00:47:02,400 --> 00:47:07,760
the super resolution network to in four to four bit integers, which is quite a bit small and

515
00:47:07,760 --> 00:47:13,120
quite a bit more efficient than what has been done before at least in terms of real world demos.

516
00:47:15,120 --> 00:47:22,240
We have another demo on conditional compute for video understanding on device. So here the idea

517
00:47:22,240 --> 00:47:27,440
is that we want to solve an action recognition problem. So you see some video data and you want to

518
00:47:27,440 --> 00:47:32,560
recognize what is being done in that video. And most methods do this by kind of parsing the full

519
00:47:32,560 --> 00:47:38,080
video all frames and then classifying the action. But my colleagues developed this method called

520
00:47:38,080 --> 00:47:44,080
frame exit. I think it was presented in CVPR last year that it basically just looks at a few frames

521
00:47:44,080 --> 00:47:48,160
and then the size of it is seen enough and can make the decision already. And once it is seen enough

522
00:47:48,160 --> 00:47:53,520
it kind of stops the computation. It doesn't require processing of all frames and thus makes

523
00:47:53,520 --> 00:47:58,560
a still very efficient decision but exactly with much less computation time also running on

524
00:47:58,560 --> 00:48:05,120
device yet. And we had one demo on multi-model future learning on teach your AI. So we had a

525
00:48:05,920 --> 00:48:11,760
cute simulated robot doc on a tablet and you try to teach gestures that we are performing

526
00:48:11,760 --> 00:48:16,960
to this dog. So you talk to the dog and at the same time you perform some gesture to tell the

527
00:48:16,960 --> 00:48:22,720
dog I don't know something with your hands and and it would kind of after seeing it on your

528
00:48:22,720 --> 00:48:27,680
few times it would kind of learn what these gestures meant by matching what you said and what

529
00:48:27,680 --> 00:48:34,160
what kind of past as audio input with with the video feed. Also running on device.

530
00:48:35,200 --> 00:48:43,600
And then finally we had a demo on a 3D reconstruction on AR goggles. This is about the problem

531
00:48:43,600 --> 00:48:48,560
of depth estimation in AR. So when you have your AR goggles on to do anything with your environment

532
00:48:48,560 --> 00:48:52,160
you really need to kind of reconstruct the 3D scene around here right you need to know how far

533
00:48:52,160 --> 00:48:56,880
away are these pixels that the cameras recording from you. And in particular you want to be able to

534
00:48:56,880 --> 00:49:03,520
do this with very strict hardware constraints. Ideally even just from a single camera feed.

535
00:49:04,720 --> 00:49:10,720
So here my colleagues developed a monocular depth estimation network trained with

536
00:49:10,720 --> 00:49:17,440
a self-supervised learning and then quantized that and ran it on device in a different way.

537
00:49:17,440 --> 00:49:21,840
The processor and the AR goggles roughly equivalent to something that might be in a mobile phone.

538
00:49:21,840 --> 00:49:26,800
I think it's a little bit more restricted. I'm not an expert on this but I believe that this

539
00:49:26,800 --> 00:49:32,400
thing was a Qual Snapdragon 888 chip in there and I think these are less powerful than the

540
00:49:32,400 --> 00:49:36,400
you know the Snapdragon 8 Gen 2 that we just announced that was running in our one of our phone

541
00:49:36,400 --> 00:49:42,240
demos. Okay. Awesome. So I would like to add one thing to this and I think what's really

542
00:49:43,200 --> 00:49:47,200
unites these four demos and what what makes them work in addition to kind of these these ideas

543
00:49:47,200 --> 00:49:53,840
on the method sides is this philosophy of full stack optimization. So at Qualcomm and I research

544
00:49:53,840 --> 00:49:58,960
we do everything from developing the architectures thinking about the training algorithm

545
00:49:58,960 --> 00:50:04,320
to quantizing the neural network. We use this AMAT AI model efficiency toolkit that's

546
00:50:04,320 --> 00:50:09,520
open source and then finally also running it on our chips actually that you can hold in your hand

547
00:50:09,520 --> 00:50:14,800
at the demo booth. I think that's quite neat. It's awesome and I'll link to an interview that I

548
00:50:14,800 --> 00:50:21,920
did with one of your colleagues Morale on that full stack philosophy as well as some of the other

549
00:50:21,920 --> 00:50:28,320
interviews that you've mentioned as well. With all that said it sounds like quite a successful

550
00:50:28,320 --> 00:50:35,680
nerves for you and your colleagues so congratulations. I'd love to have you maybe close us up by

551
00:50:35,680 --> 00:50:43,680
talking a little bit about kind of where your research goes in the in the future and kind of what

552
00:50:43,680 --> 00:50:48,320
you're most excited about across the various things that we've talked about. Yeah thanks for

553
00:50:48,320 --> 00:50:53,360
the question. So this causal representation learning paper that we talked about at length

554
00:50:53,360 --> 00:51:00,400
earlier is really one example of how action and perception should inform each other. In the

555
00:51:00,400 --> 00:51:04,080
sense that in this paper we show that if we can observe the effect of actions it gives us a

556
00:51:04,080 --> 00:51:09,760
principled way of learning perception learning representations. But I think this this relation

557
00:51:09,760 --> 00:51:14,320
between action and perception goes goes further than the justice one direction and in the future

558
00:51:14,320 --> 00:51:18,320
my colleagues and I would really like to continue working on this and maybe also going in the

559
00:51:18,320 --> 00:51:25,680
other direction. Can we learn principled ways of of learning to act learning policies in an

560
00:51:25,680 --> 00:51:32,320
interactive environment based on ideas about causality based on ideas about structure of a system

561
00:51:32,320 --> 00:51:38,880
based on maybe ideas of how representations should behave under actions and that's one direction

562
00:51:38,880 --> 00:51:44,000
I'm very excited about. More broadly we talked about this already before but I really think we

563
00:51:44,000 --> 00:51:51,040
should try to move away from identifiability theorems and more towards thinking about downstream

564
00:51:51,040 --> 00:51:58,320
applications. But I you know what I've had really beautiful in machine learning is the following

565
00:51:58,320 --> 00:52:03,440
like sometimes you have this hammer based research where you have some elegant idea and some

566
00:52:03,440 --> 00:52:08,560
beautiful theory and then you just try to make this theory somehow work in some way and sometimes

567
00:52:08,560 --> 00:52:12,480
you have this nail based research where you have a really well problem you really want to solve it

568
00:52:12,480 --> 00:52:16,320
and the working is the only way that matters and often it ends up being more of an engineering

569
00:52:16,320 --> 00:52:20,800
effort than then really kind of beautiful. But every now and then these two things come together

570
00:52:20,800 --> 00:52:25,360
and you can really kind of hit a hammer with an air you can really use something elegant

571
00:52:25,360 --> 00:52:29,120
from the theory side to solve a really well problem and that's that's where the beauty is

572
00:52:29,120 --> 00:52:34,880
that's that's where machine learning is really fun. Awesome. Awesome. Well Johan thanks so much for

573
00:52:34,880 --> 00:52:40,320
taking the time to walk us through your paper and some of your colleagues work at NURPS.

574
00:52:40,320 --> 00:53:10,160
Yeah thanks a lot thanks for having me this was fun.

