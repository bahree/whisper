1
00:00:00,000 --> 00:00:16,320
All right, everyone. I am here with Francesque Riera. Francesque is an applied machine learning

2
00:00:16,320 --> 00:00:21,280
engineer at the Lego group. Francesque, welcome to the Twomo AI podcast.

3
00:00:22,000 --> 00:00:26,720
Thanks. And thanks a lot for having me as well. I think it's a pleasure and super excited

4
00:00:26,720 --> 00:00:33,600
about the talk tonight as well. Same year. Same year. And thanks for taking the call at night

5
00:00:33,600 --> 00:00:39,600
or being on at night. It's a bit later for you. You're in Denmark. Yeah, that's correct. Denmark,

6
00:00:40,400 --> 00:00:44,560
well, as you know, headquarters for Lego was born in the small town of Pilon,

7
00:00:44,560 --> 00:00:51,280
here in Denmark. So living just across it. Very nice. Very nice. Why don't we get started by

8
00:00:51,280 --> 00:00:56,400
having you share a little bit about your background and how you came to work in machine learning

9
00:00:56,400 --> 00:01:03,600
and at Lego? Yeah. So, well, I think it's, it's not going to be a very long story,

10
00:01:03,600 --> 00:01:08,720
because I've been on the, I guess, on the market for roughly three years now.

11
00:01:09,520 --> 00:01:16,800
But I think my enthusiasm for ML and actually I should say my enthusiasm for computer vision

12
00:01:16,800 --> 00:01:23,680
started back in my bachelor's in industrial electronics in Spain. And that's just because

13
00:01:23,680 --> 00:01:30,320
I was studying the, the bachelor's in electronics. And then the last semester was focusing on robotics

14
00:01:30,320 --> 00:01:36,160
and then robotics. We had an introduction to computer vision. And I don't know why, but I thought

15
00:01:36,160 --> 00:01:41,840
this is then impressive. It's super interesting what you can do well in in reality with mathematics.

16
00:01:41,840 --> 00:01:47,360
And then matrices, right? For all the pictures and pictures and all of these sort of things.

17
00:01:47,360 --> 00:01:54,240
And then that drove me to then actually catching up on, on a master's in Denmark. So that's

18
00:01:54,240 --> 00:01:59,120
when I moved to Denmark to do a full on masters in computer vision and machine learning in the

19
00:01:59,120 --> 00:02:05,680
University of all work, which is in the north of Denmark. And after that, well, I guess I

20
00:02:05,680 --> 00:02:12,880
become sort of an expert in the matter. I hope so at least. And that got me then. I got a,

21
00:02:12,880 --> 00:02:17,280
a small job as a supporting engineer for a couple of months. It was not my thing. And then I

22
00:02:17,280 --> 00:02:24,320
found this splendid opportunity to live where I got to actually work with ML on active products,

23
00:02:24,320 --> 00:02:30,240
running in the cloud as well. Awesome. Awesome. It was speaking of industrial robotics and

24
00:02:30,240 --> 00:02:38,720
computer vision. One of the early, I think this was, I think this was early many years ago,

25
00:02:38,720 --> 00:02:52,160
demos of AI was like somebody built a Lego sorter using a treadmill kind of thing and a paddle

26
00:02:52,160 --> 00:02:56,160
or some kind of robot that would like sort the Legos into different pieces. Did you ever

27
00:02:56,160 --> 00:03:02,560
have you ever seen that one? It's funny. You mentioned it. And we are trying to

28
00:03:02,560 --> 00:03:09,040
and did that one from the internet to maybe exploring the options also to use it as a product

29
00:03:09,040 --> 00:03:14,160
for us. Oh, really? So it's quite funny that you actually mentioned it. I think we talked about it

30
00:03:14,960 --> 00:03:25,440
two, three weeks ago. Oh, that's funny. If I remember correctly, I remember reading a hacker news thread

31
00:03:26,320 --> 00:03:32,000
when this was published. Again, I think it was probably like five years ago or four years ago or

32
00:03:32,000 --> 00:03:40,080
something. There were apparently a bunch of people that would like go on eBay and buy these

33
00:03:40,080 --> 00:03:46,240
gigantic bags of miscellaneous Legos and they were talking about using these kinds of devices

34
00:03:46,240 --> 00:03:52,480
to sort them and then resell them. Apparently, there's a bit of a cottage market, so to speak,

35
00:03:52,480 --> 00:04:01,040
in remarketing Legos. And it's also one of the big campaigns that we're also running is

36
00:04:02,080 --> 00:04:08,640
even though Lego is primarily made of plastic wrap, so you want to give your bricks a

37
00:04:08,640 --> 00:04:14,720
longest life ever, right? And I mean, if you had some Lego set from the 50s or the 60s, you

38
00:04:14,720 --> 00:04:20,480
would still probably use it today. So I think that's also what drives this enthusiasm, right?

39
00:04:20,480 --> 00:04:26,240
On you being able to, all right, let's try and make this, you know, circular economy,

40
00:04:26,240 --> 00:04:33,360
you know, way right for the brick. Yeah. Yeah. Well, I brought that up not thinking that it was

41
00:04:33,360 --> 00:04:43,840
something that you were actually thinking about. But you know, maybe we can jump into how ML is actually

42
00:04:43,840 --> 00:04:54,400
used today at Lego. Yeah, so maybe I can just start putting it aside my area. So my team going

43
00:04:54,400 --> 00:05:02,160
from, I guess, the bottom to the highest. So I'm part of a team where we are three ML engineers

44
00:05:02,160 --> 00:05:09,440
and then full stack and then all of the surroundings right. And in my team, we have two main

45
00:05:09,440 --> 00:05:15,440
products, which I can just quickly describe now. We're going to go into details a bit later if you

46
00:05:15,440 --> 00:05:23,120
want as well. So the main product that started also now to two and a half years ago, approximately,

47
00:05:23,120 --> 00:05:31,440
it's a moderation service. So as you know, Lego is of course a brand for aimed at children, right?

48
00:05:32,400 --> 00:05:38,560
And children are our main responsibility. So of course, all of these, for example, social media

49
00:05:38,560 --> 00:05:46,560
applications or games or anything where children can upload their content, let's say images,

50
00:05:47,520 --> 00:05:52,800
text, videos, anything that can be shared and can be created by them on the phone or on the computer

51
00:05:53,440 --> 00:05:59,520
needs to be moderated by law before it's actually available or online for all the children to

52
00:05:59,520 --> 00:06:06,560
sit right as to avoid the obvious things right. And I mean, we know Facebook does that. We know

53
00:06:06,560 --> 00:06:11,680
Instagram does that. A lot of companies do it, but not in the level I would say as level as it.

54
00:06:12,800 --> 00:06:21,520
And that's because in Facebook, you could post something that is obscene and it's not necessarily

55
00:06:21,520 --> 00:06:26,080
going to be deleted as you upload it, but it's going to be deleted after the fact, maybe because

56
00:06:26,080 --> 00:06:32,000
nobody reported you, right? That cannot happen in our applications because the damage could already

57
00:06:32,000 --> 00:06:36,640
be inflicted and that's not, it's definitely not, you know, the image you want to give to the brand.

58
00:06:37,680 --> 00:06:44,000
So yeah, so our product, what it does is receive the content created by the users and we do an

59
00:06:44,000 --> 00:06:50,080
initial profiting of these content. So on images, on text and on videos, we try to project already

60
00:06:50,080 --> 00:06:53,840
what is the most obvious things that should not be in the app. Like if you uploaded a very bad,

61
00:06:53,840 --> 00:07:00,640
bad image, if you showed your face or some sort of identifiable information, you cannot also

62
00:07:00,640 --> 00:07:08,000
share that. So we also remove that. Of course, the obvious profanities. And we are working also

63
00:07:08,000 --> 00:07:14,080
in extending a couple more detectors. And detectors, by detectors, I mean machine learning algorithms,

64
00:07:14,080 --> 00:07:21,600
actually. So we do this profiltering, what gets rejected by us just gets feedback to the users

65
00:07:21,600 --> 00:07:28,000
and hey, we are very sorry, but your creation is not allowed because of this. Try to take a new

66
00:07:28,000 --> 00:07:33,040
picture or try to be nice writing the text. So some some some post different for that.

67
00:07:34,160 --> 00:07:37,920
If the content is approved by us, it then goes to manual moderation. So there's a company

68
00:07:38,560 --> 00:07:44,160
moderating every single piece of content before it is published. So what we accomplish with our

69
00:07:44,160 --> 00:07:50,560
product, right, is what for the students, right, one of them being monetary aspect because

70
00:07:51,520 --> 00:07:56,880
each piece of content that is very obvious does not need to go to a moderator that

71
00:07:56,880 --> 00:08:04,160
costs per piece, right? So in that sense, what that's what we manage to prove. And of course,

72
00:08:05,120 --> 00:08:12,960
if let's say, you know, there's like, I don't know, a big fair with Lego and people are

73
00:08:13,920 --> 00:08:19,120
asked to upload pictures because they are in the fair. It's like, you need to build a car and then

74
00:08:19,120 --> 00:08:24,160
you need to post a picture of the car. So of course, they would like to post a picture, but if the

75
00:08:24,160 --> 00:08:29,200
picture was bad for some reason, it would be nice that they get the feedback instantly rather than

76
00:08:29,200 --> 00:08:34,160
having to wait. So when when it goes to manual moderation, there is a time frame of five minutes.

77
00:08:34,160 --> 00:08:40,800
I think it's five to 15. I'm not 100% sure because it's not my part. But the thing is our

78
00:08:40,800 --> 00:08:47,680
our service provides a response on 10 seconds for whatever time. So if you uploaded something,

79
00:08:47,680 --> 00:08:52,800
let's say you take a selfie with your car, like, oh, we really like your car, but you cannot have

80
00:08:52,800 --> 00:08:58,080
your face there. So they get the response in seconds, like, oh, my car is still here. I take another

81
00:08:58,080 --> 00:09:06,240
picture, life's good again, right? And then the second product we have, so after everything has

82
00:09:06,240 --> 00:09:15,520
been approved. So when your creation is live on the social media app, then we have an automatic

83
00:09:15,520 --> 00:09:22,640
job every morning that will take all of the creations from the users. And then again, with two

84
00:09:22,640 --> 00:09:29,920
sorts of algorithms, so one for image and one for text, we try to identify what is the theme,

85
00:09:29,920 --> 00:09:37,360
the predominant theme on the on the on the creation. So for example, you have a big Star Wars,

86
00:09:37,360 --> 00:09:41,600
you have a big Star Wars fan, then you have a lot of Star Wars sets, you take picture of your

87
00:09:41,600 --> 00:09:48,800
Star Wars set, we probably will identify it as a Star Wars. And then what we do is like, okay,

88
00:09:49,440 --> 00:09:55,680
this kid has uploaded the Star Wars picture or Star Wars album collection. So what we do is then

89
00:09:56,560 --> 00:10:04,320
we send the like with an NPCs on the application, and we also use different NPCs to send comments,

90
00:10:04,320 --> 00:10:09,440
reinforcing them and encouraging them to keep building, saying, oh, that's a very nice

91
00:10:09,440 --> 00:10:14,000
Star Wars creation you have. I would love to see more from you and see more like that.

92
00:10:14,560 --> 00:10:21,280
And the NPCs are bots or characters and, yeah, they've already existed in the app,

93
00:10:22,560 --> 00:10:29,680
and they've been used. So before we set up this environment for this product, it was used by

94
00:10:30,640 --> 00:10:39,040
my people. So me as a, I guess, consumer, engagement person on the team, I would go in,

95
00:10:39,040 --> 00:10:45,600
I would log in as the, let's say Chihuacca, and then I would comment out on a couple of things.

96
00:10:46,480 --> 00:10:51,200
So now what we're doing is we are automating the job. Of course, there's still some people

97
00:10:51,200 --> 00:10:56,400
looking at the comments and maybe doing more specific comments, depending on the scenario.

98
00:10:58,000 --> 00:11:02,400
But it's been proven also to actually give a lot of value because we have received a lot of

99
00:11:02,400 --> 00:11:09,920
uploads saying, oh, look, legal life, Emmett, like my comment, like my post, and they are super happy

100
00:11:09,920 --> 00:11:16,240
right? So it's also very, it's very fulfilling for us as well to see that the reactions are positive,

101
00:11:16,240 --> 00:11:23,680
right? And you mentioned there are there are three ML engineers on that team.

102
00:11:23,680 --> 00:11:34,240
And how, what's kind of the broader size or scope of ML and data at like a?

103
00:11:35,920 --> 00:11:45,360
Well, so back in March, we had also a big reorganization where we then found that also what

104
00:11:45,360 --> 00:11:51,360
is now the department called the data office. So my team resides within the data office and

105
00:11:51,360 --> 00:11:56,320
then the data science products. But from the data science products, for example, we have a couple

106
00:11:56,320 --> 00:12:02,560
of others. So of course, in the legal.com page, where you go to the shop, right? And you buy

107
00:12:02,560 --> 00:12:08,560
a start-wall set, we have, of course, a recommended engine saying, oh, you like the dits there,

108
00:12:08,560 --> 00:12:15,200
maybe you're all generated in the X-wing, right? So the recommended team is also, I guess, our

109
00:12:15,200 --> 00:12:23,840
sister team working using data as well and developing ML solutions. That's also true. Yeah,

110
00:12:23,840 --> 00:12:31,440
two other product teams, one working on marketing effectiveness. So for example, if you have a

111
00:12:31,440 --> 00:12:38,320
new release of Legonin Chago Netflix, how is that performing compared to, I don't know, Barbie or

112
00:12:38,320 --> 00:12:46,160
or whatever other cartoon is for children at the time, right? And then the last one is the

113
00:12:46,160 --> 00:12:52,960
man forecasting. So trying to be ahead of the curve, which I guess the team had a lot of fun in

114
00:12:52,960 --> 00:12:59,200
Corona times because it's very difficult now. There was no curve. It was just a pick and it's like,

115
00:12:59,200 --> 00:13:05,600
well, we are running out of stock everywhere. So that's it. Yeah. And then a way, of course,

116
00:13:05,600 --> 00:13:10,560
having an incubation team as well. So the incubation team is trying to analyze different

117
00:13:10,560 --> 00:13:17,600
areas, sectors and departments at Lego where automation could be beneficial, for example,

118
00:13:17,600 --> 00:13:23,920
using machine learning. And one of the examples is this brick shorter using a treadmill. Well,

119
00:13:24,640 --> 00:13:30,480
it's a bit more high-scale. That's the idea, right? Nice, nice.

120
00:13:30,480 --> 00:13:40,400
With the, you mentioned the moderation app, it reminds me of a recent interview that we published

121
00:13:41,760 --> 00:13:47,680
with someone who works in cybersecurity. And you know, in that environment, as you can imagine,

122
00:13:47,680 --> 00:13:52,800
it's kind of very adversarial. You, you know, plug one hole, someone's trying to poke another.

123
00:13:52,800 --> 00:14:00,160
And moderation in your environment is it, you know, quite the case that there are bad actors,

124
00:14:00,160 --> 00:14:06,320
as opposed to like, are you, are you, is the task primarily identifying passive,

125
00:14:09,120 --> 00:14:15,040
passive behavior that you don't want, or are there bad actors, so to speak, that are trying to

126
00:14:15,040 --> 00:14:24,240
abuse the system? I think there's a, there's a bit of both actually. Generally speaking, we see

127
00:14:24,240 --> 00:14:30,160
the cases where, you know, maybe, maybe you had the phone in your pocket, right? And accidentally,

128
00:14:30,160 --> 00:14:35,120
it took a picture. I mean, it happens to everybody, right? And then you post a picture by mistake.

129
00:14:37,120 --> 00:14:42,560
And that happens quite often. Sometimes we also actually caught some bugs in the app, because it's

130
00:14:42,560 --> 00:14:48,000
like, well, is it not about we're getting, I don't know, a thousand black images or a full white

131
00:14:48,000 --> 00:14:52,880
images, you know, no, it should not make any sense. It's like, okay, well, maybe you should look

132
00:14:52,880 --> 00:15:01,840
into the latest police, you made, and so that was a funny, funny fact. But also, and that's maybe a

133
00:15:01,840 --> 00:15:06,960
bit above the, where the product, our protocols, the migration product, and that's more on the,

134
00:15:06,960 --> 00:15:15,200
on the manual moderation, because they do track behaviors and bad actors. And as far as I

135
00:15:16,320 --> 00:15:21,760
know, I think there's been a couple of cases where somebody got banned, they opened more accounts,

136
00:15:21,760 --> 00:15:29,040
they keep doing the same, got banned again, the game. So there's a little bit of that. I was curious

137
00:15:29,040 --> 00:15:38,320
if that was, if that was a something that you had to deal with how that played into the way you

138
00:15:38,320 --> 00:15:45,840
approach the model development, but it sounds like that's kind of downstream of what you're doing.

139
00:15:46,560 --> 00:15:53,520
It is because it is, again, we are not Facebook or Instagram, right? We need to make sure nothing

140
00:15:53,520 --> 00:16:00,000
back gets published. Right. So that's why there's the extra layers of security here. Do you

141
00:16:00,000 --> 00:16:09,600
incorporate the input of the human moderators? Like, is it a kind of a human and a loop type of

142
00:16:09,600 --> 00:16:18,000
situation where you're using their judgments to evolve your models? Yeah, yeah, that's right. And

143
00:16:18,000 --> 00:16:24,880
that's something we started doing, I think, at the beginning of this year, maybe the end of last

144
00:16:24,880 --> 00:16:30,320
year, you know, with the Corona year, everything specifically started for me and the calendar. But

145
00:16:31,920 --> 00:16:38,400
what we managed to do, yeah, link was this external moderation company. So when they've

146
00:16:38,400 --> 00:16:45,600
reject an image or a text or a video, we get the feedback that saying, okay, this content was

147
00:16:45,600 --> 00:16:54,080
rejected because of this. And what we did is we made a feature store. And it's a very simple

148
00:16:54,080 --> 00:17:00,160
reason for making a feature store rather than just a database, I guess. It's that the data by

149
00:17:00,160 --> 00:17:08,960
law, so GDPR, it cannot be stored forever. Meaning that let's say you are a user of the social media

150
00:17:08,960 --> 00:17:14,400
and one day you call to consumer service and say, hey, I want all my data to be deleted. We cannot

151
00:17:14,400 --> 00:17:20,720
have this link because we are in a way a standalone product, right? So we are not linked to the app.

152
00:17:21,920 --> 00:17:26,560
Meaning if you call to consumer service, you said, I want to delete it. Your data is deleted.

153
00:17:27,440 --> 00:17:31,760
But nobody's going to tell us, hey, this user called because, well, first of all,

154
00:17:31,760 --> 00:17:36,000
I don't have your user data, for example. So it will be impossible for me to track who you are.

155
00:17:36,000 --> 00:17:43,680
But what we accomplished with this also is we generate features and the features are generated by

156
00:17:43,680 --> 00:17:52,320
well-known machine learning algorithms. For images, we are talking ResNet50. For text samples,

157
00:17:52,320 --> 00:17:58,240
we're talking about multilingual bird, for example. So in that sense, we get the images,

158
00:17:58,240 --> 00:18:06,640
text samples, we get the features out of it, which are anonymized because if you have pulling

159
00:18:06,640 --> 00:18:12,560
layers in a neural network, right, you cannot undo a pulling. You could, but you could not get

160
00:18:12,560 --> 00:18:18,640
a one-mile script. And then those features are saved and then labeled with the human interloop.

161
00:18:19,840 --> 00:18:25,760
Got it. So rather than saving the original images, you're saving these representations from

162
00:18:25,760 --> 00:18:37,680
these neural nets. And one of the driving reasons behind that is to kind of avoid the GDPR

163
00:18:38,640 --> 00:18:42,160
responsibilities if that was personally identifiable data.

164
00:18:43,360 --> 00:18:51,760
Yeah. And whether it is or it is not, if the user calls and wants the data deleted, it needs to be

165
00:18:51,760 --> 00:18:58,400
deleted. Otherwise, yeah, you are reaching the contract. I guess, yeah, I guess that's also within

166
00:18:58,400 --> 00:19:06,960
the GDPR. Meaning if they call whatever the consumer GDPR line is and ask for their data to be

167
00:19:06,960 --> 00:19:12,880
deleted, these representations get deleted from the feature store. Not the representations,

168
00:19:12,880 --> 00:19:17,680
because they have anonymized data. So that's fully anonymized data. There's no way to backtrack

169
00:19:17,680 --> 00:19:25,760
where and what it comes from. Yeah. Yeah. And so you said that, you described that as a rationale

170
00:19:25,760 --> 00:19:31,840
for a feature store versus a database, but you could still put those representations in a database.

171
00:19:31,840 --> 00:19:37,360
Well, yeah. I guess we like to make a fancy words, so I can just call it a feature store rather than

172
00:19:37,360 --> 00:19:44,400
a database, right? Yeah. And I say that to probe a little bit deeper into that and are there other

173
00:19:44,400 --> 00:19:54,800
capabilities that you've built into this feature store that are specific to the way you're using it

174
00:19:54,800 --> 00:20:01,440
for machine learning. Yeah. Well, so as I said, right, so the features, which is anonymized data,

175
00:20:01,440 --> 00:20:08,000
gets labeled by regulators. In this case, it's, so actually we have labels for the moderation part,

176
00:20:08,000 --> 00:20:15,840
but we also have labels for the theme part. Now, and as I said before, so the images that are

177
00:20:15,840 --> 00:20:21,920
approved, they are published on the application right, which I mean are open source, not open source,

178
00:20:21,920 --> 00:20:28,480
but they are open to everybody because they come from a backend that publicly shares the images,

179
00:20:29,680 --> 00:20:34,960
so then what we do for this, for example, is that those rather than keeping the features because

180
00:20:34,960 --> 00:20:42,000
that model is a bit more complex, we keep a pointer to where the image is stored in the database

181
00:20:42,000 --> 00:20:47,200
for the application. And then we'll reuse that if the image was deleted by the customer,

182
00:20:47,200 --> 00:20:52,000
well, we don't have the image. It's a pity that it's not there, but at least it's not our problem anymore.

183
00:20:53,760 --> 00:20:58,880
What we do with this feature store and actually something where currently working on, hopefully,

184
00:20:58,880 --> 00:21:05,920
to be done by the end of the year, it's a sort of retraining pipeline plus a bit testing framework

185
00:21:05,920 --> 00:21:12,640
as well with both actually. So the idea is checking this feature store ever now and then, right?

186
00:21:13,200 --> 00:21:18,960
Okay, we have no features for retraining, you know, the personal detector, right? Yeah, we don't,

187
00:21:18,960 --> 00:21:25,280
all right. Then let's retrain. Before we launch live, even though we think the model is better,

188
00:21:25,280 --> 00:21:29,200
right? Let's have a maybe testing where both models are running the production at the same time.

189
00:21:30,080 --> 00:21:36,720
And then it's up to of course us as developers, but also part of the business side of moderation

190
00:21:36,720 --> 00:21:40,560
to decide, all right, the new model is better or no, the new model is no better.

191
00:21:40,560 --> 00:21:55,680
With the AB testing framework, I'm curious how you're kind of packaging and deploying the models.

192
00:21:57,040 --> 00:22:04,720
Yeah, so for model experimentation and model registry, for example, we use the open

193
00:22:04,720 --> 00:22:13,120
source version of MLflow, which is the, I guess, the package from Databricks. We, what we did is

194
00:22:13,120 --> 00:22:20,320
we created our own, the Moster with the MLflow backend. And then within the model registry,

195
00:22:20,320 --> 00:22:27,840
so we have the most in either production or staging or of nothing. And in the core of moderation,

196
00:22:27,840 --> 00:22:35,200
the core of moderation is built upon a step function in AWS. So what we have in this step

197
00:22:35,200 --> 00:22:39,680
function is, okay, I have the person detector here. And if I have a staging model for the person

198
00:22:39,680 --> 00:22:47,600
detector, then the task is a parallel task where the inputs will flow towards both so that I can

199
00:22:47,600 --> 00:22:53,040
have a one-to-one analysis to say, okay, which model is performing better in all of the images, right?

200
00:22:53,040 --> 00:23:05,040
Okay, and so you mentioned step functions. Do you use serverless technologies pretty broadly

201
00:23:05,040 --> 00:23:12,240
in deploying ML? Yeah, I think, and that's also one of the biggest changes we did. I think

202
00:23:12,240 --> 00:23:19,680
this year, it's been quite a, quite a change here, I think. We are full on serverless, so everything

203
00:23:19,680 --> 00:23:26,960
is deployed with AWS sound, for example. And then everything is, yeah, step function, lambda,

204
00:23:27,760 --> 00:23:35,440
we have also an ECS Fargo running for the biggest model. But that's generally its own. Yeah,

205
00:23:35,440 --> 00:23:41,920
you could say everything is so interesting. And so I'm curious how you're, you mentioned,

206
00:23:41,920 --> 00:23:50,880
it sounds like you're evolving the infrastructure. You've evolved it quite a bit over the past year

207
00:23:50,880 --> 00:23:58,880
with COVID year or so. And I'm wondering if you can share kind of what the prior state was,

208
00:23:58,880 --> 00:24:07,840
and what were some of the drivers for moving to serverless and container service?

209
00:24:07,840 --> 00:24:14,560
Yeah, so, well, as I said, so the moderation products started two and a half years ago,

210
00:24:14,560 --> 00:24:21,680
and when we started, we thought it would be best if we tried to manage everything ourselves,

211
00:24:21,680 --> 00:24:27,760
which meant, all right, let's go full on lambdas, and then rather than, you know, having step

212
00:24:27,760 --> 00:24:34,560
function logic or things like that, let's go SQL, SNSs, Dynamos with streams.

213
00:24:34,560 --> 00:24:42,000
If I had a picture of the old setup, I think it would be scared, and many people may be

214
00:24:42,000 --> 00:24:48,000
listening also to be scared to see that image. But then that evolved a bit into, all right,

215
00:24:48,000 --> 00:24:54,000
let's go ECS, so let's go full on Fargo desks. The problem in that scenario is that

216
00:24:55,520 --> 00:25:00,880
we, in the moderation setup, because the application is, I wasn't going to say deployed,

217
00:25:00,880 --> 00:25:06,800
I guess, the moderation exists in 26 markets, and we get there constantly, all day, everything.

218
00:25:07,840 --> 00:25:13,520
It makes no sense to have a Fargo task that is stopped, and then has to be started when new

219
00:25:13,520 --> 00:25:19,520
data comes, because there's data coming in maybe a bit true for a second, which meant that the

220
00:25:19,520 --> 00:25:26,480
Fargo task I run at 24 hours a day, and then for the Fargo task, which is basically a Docker container,

221
00:25:26,480 --> 00:25:33,120
right, to receive whatever needs to be moderated, is that we needed to then enable a queue,

222
00:25:33,120 --> 00:25:38,640
so then that queue would get some data, and then at some point in the thread looping, right,

223
00:25:38,640 --> 00:25:44,800
you would get the message. I mean, it worked very well, everything was perfectly fine, and

224
00:25:46,160 --> 00:25:53,360
it was not a big deal, but then we consider that whenever we get new employees, new colleagues

225
00:25:53,360 --> 00:26:01,520
in my team, it's hard to explain the flow, it's hard to explain or maybe try to understand what

226
00:26:01,520 --> 00:26:10,720
is happening, right, and I think it was in December 2020 that AWS Sam came with an update,

227
00:26:11,600 --> 00:26:17,520
no, not AWS Sam, Lambda came with an update, where you can deploy your custom Docker images to Lambda,

228
00:26:17,520 --> 00:26:25,280
and that just, yeah, that made our work much easier. Life was better that day,

229
00:26:27,520 --> 00:26:31,360
because what we did is, well, we have to do very, which is really, let's just transfer them to Lambda,

230
00:26:31,360 --> 00:26:36,640
because we know how Lambda works, Lambda is amazing. AWS, not sponsored, I'm gonna say now.

231
00:26:40,000 --> 00:26:44,560
But having the Docker container running in Lambda, then we could also integrate it into

232
00:26:44,560 --> 00:26:50,400
the step functions without having to have task tokens waiting for the callbacks and a lot of

233
00:26:50,400 --> 00:26:55,920
complications, right, and effectively with this meant, and what this means up to today is that

234
00:26:55,920 --> 00:27:01,840
today we can deliver responses under 10 seconds when we talk for, you know, many images with comments

235
00:27:02,560 --> 00:27:10,080
and everything. Whereas before, we were below the minute, so we actually managed to reduce the

236
00:27:10,080 --> 00:27:15,040
average times from a minute to 10 seconds, which is quite the accomplishment, I think, as well.

237
00:27:16,160 --> 00:27:26,400
And do you run into runtime constraints for using Lambda when you're doing image inference?

238
00:27:27,840 --> 00:27:34,240
No, no, and that's because so what we do, and as I said, right, so the feature store, right,

239
00:27:34,240 --> 00:27:40,960
again, and clicking back to the feature store, so the images, they are passed through a resident 50

240
00:27:40,960 --> 00:27:44,800
without the specification layer, which is, I think, a very common approach, right, transfer

241
00:27:44,800 --> 00:27:49,120
learning. You've already got the representation, and you're just doing classification.

242
00:27:50,640 --> 00:27:58,240
And it's funny, because there was this Tesla AI event, a couple of weeks ago, maybe a couple of

243
00:27:58,240 --> 00:28:04,960
months ago, where they mentioned they are doing classification, image classification with something

244
00:28:04,960 --> 00:28:12,320
called Hydra. And what is Hydra? I mean, it's just a resident network, where then you have

245
00:28:12,320 --> 00:28:16,800
different heads, which classify different things. And I was like, well, this is exactly what we

246
00:28:16,800 --> 00:28:27,520
do. Why didn't you think of cool name like that before? We've talked quite a bit about the feature store.

247
00:28:30,000 --> 00:28:36,800
Can you talk a little bit about how that evolved or any challenges you have run into

248
00:28:38,240 --> 00:28:44,400
in bringing that project to fruition? Yeah, so the feature store was, I think it was,

249
00:28:44,400 --> 00:28:53,040
one of the biggest problems was from idea to reality, actually, because we were like, of course,

250
00:28:53,040 --> 00:28:58,720
a feature store makes sense, right? I mean, if you want our models to improve, you need data,

251
00:28:58,720 --> 00:29:04,800
you need labeled data, right? And I guess that is the suffering of every ML engineer, right? It's

252
00:29:04,800 --> 00:29:08,960
not about the data. The spread of data in the world is not, but it's not labeled, right? That's

253
00:29:08,960 --> 00:29:17,600
the problem. So the aviation was there, but we were, it took us quite a, quite a period of time

254
00:29:17,600 --> 00:29:23,840
to try to figure out what's the best approach. How do we do this best, you know, also future thinking?

255
00:29:24,400 --> 00:29:27,760
Because it's like, well, I mean, I could just, you know, get all the data from the manual

256
00:29:27,760 --> 00:29:35,680
moderation and stack it up in some one drive and, you know, let it work there forever,

257
00:29:35,680 --> 00:29:39,280
which is what a lot of people and a lot of companies do with the data, I'm afraid to say.

258
00:29:40,720 --> 00:29:46,080
But then we came to the realization, and I think it was also really some posts on other companies,

259
00:29:46,080 --> 00:29:50,880
how I think maybe it was Cooper, actually, how they did feature stores in Uber.

260
00:29:54,080 --> 00:29:59,680
So then we came to that to making a feature store client, everything we do is in Python,

261
00:29:59,680 --> 00:30:07,440
so it was in Python too. Well, when using the backend as AWS, because we are full on AWS,

262
00:30:08,160 --> 00:30:13,520
but of course, we tried to make it also cloud-agnostic so that if one day we move to

263
00:30:13,520 --> 00:30:19,920
Azure, or we'll move to, I don't know, any other else, then there's not that many problems,

264
00:30:19,920 --> 00:30:25,600
or it can also be integrated, right? And I think one of the learnings also we've got in the

265
00:30:25,600 --> 00:30:29,840
feature store in the first steps is that, all right, I have features, so let's say the feature,

266
00:30:29,840 --> 00:30:36,320
right? It's an image that went through Resident 50, so we go from a couple of megabytes image

267
00:30:36,320 --> 00:30:41,840
to seven kilobytes, a feature array, right? And I know it because I've seen many of them,

268
00:30:41,840 --> 00:30:42,960
so I know it's seven kilobytes.

269
00:30:45,280 --> 00:30:51,600
With all right, because we're going to get a lot of data, I think on average, we get around

270
00:30:51,600 --> 00:31:01,360
10 to 15,000 images per day on the evaluation platform, so you can think, right? So 10 to 15,000

271
00:31:01,360 --> 00:31:09,040
images per day, 10 to 15,000 features. Let's store them in S3, so we store them in S3,

272
00:31:09,040 --> 00:31:14,880
and then we have a catalog in no SQL database, which is DynamoDB and Amazon Work Services.

273
00:31:14,880 --> 00:31:22,960
Everything was fine. The data is starting, it's increasing in numbers, everything looks to be

274
00:31:22,960 --> 00:31:27,840
wonderful. Until the day we want to query the data for the first time, because we sell that.

275
00:31:28,480 --> 00:31:35,600
Let's see how the query works. Well, it turns out that S3 is not the ideal scenario when you want

276
00:31:35,600 --> 00:31:43,760
to query a lot of small files, because that is, and I'm not an expert in this sort of

277
00:31:43,760 --> 00:31:48,880
infrastructure things, but there's all of these handshake things that come between requests and

278
00:31:48,880 --> 00:31:53,520
get into data. That was taken longer than actually fetching the data itself, and because there's a

279
00:31:53,520 --> 00:32:02,800
lot of... Why was the query against S3 and not the Dynamo? Well, the DynamoDB was to keep a catalog,

280
00:32:02,800 --> 00:32:08,560
so in the Dynamo, you would be like, okay, this is the feature path on S3, and this is the label,

281
00:32:08,560 --> 00:32:16,240
so you just need to get the data from S3, because in our eyes, it's where data is supposed to be

282
00:32:16,240 --> 00:32:22,960
stored, it's in a bucket rather than a table. The problem that you ran into was just the latency

283
00:32:22,960 --> 00:32:31,920
for requesting a single file in S3 when your bucket had a lot of files. Yeah, exactly, and also

284
00:32:31,920 --> 00:32:37,840
requesting a big dataset, or a big feature dataset, because I guess for a couple of hundreds,

285
00:32:37,840 --> 00:32:42,960
for features doesn't matter, but I think that one of my colleagues did the calculation right,

286
00:32:42,960 --> 00:32:49,600
so it was, I think, 40 milliseconds per feature, and I'm not exactly sure of the number now,

287
00:32:49,600 --> 00:32:55,360
but what we learned is that we're probably into something else, so what we did is migrate the data

288
00:32:55,360 --> 00:33:01,680
itself as well to DynamoDB, so that the catalog, because I mean, the feature can also be converted

289
00:33:01,680 --> 00:33:07,760
to a bytes array, so then the bytes array can also be stored as, yeah, as no SQL entry write.

290
00:33:08,560 --> 00:33:15,120
So by moving the data into the catalog itself, what we managed to do is that the data now is

291
00:33:15,120 --> 00:33:23,520
fetched 16 milliseconds per feature, which is, you know, howling at the time, and that

292
00:33:23,520 --> 00:33:32,240
that actually proved to be very efficient. So is the data community there at LEGO? You know,

293
00:33:32,240 --> 00:33:38,640
is everyone kind of full stack, or do you have your more traditional data scientists, and then,

294
00:33:38,640 --> 00:33:45,360
you know, folks that are more ML engineers, how does the, what's kind of the range of skill sets

295
00:33:45,360 --> 00:33:53,360
or culture there in terms of full stackness, if that's a thing? We are very happy

296
00:33:53,360 --> 00:33:58,240
we have full stack development in our team, because she's a mastermind in doing our front-end

297
00:33:58,240 --> 00:34:07,760
tool. Before we had him, our think our front-end app was maybe something out of university,

298
00:34:07,760 --> 00:34:17,600
you could say. But so we are split actually, so we have, like myself, so we have machine learning

299
00:34:17,600 --> 00:34:24,400
slash data engineers, you would say, right? We also have the more standard data scientists

300
00:34:24,400 --> 00:34:36,240
as well, and then we have data management, folks, and platform people, so that once that are helping

301
00:34:36,240 --> 00:34:44,640
with the more standard infrastructure, like setting as the account, the security, and of the

302
00:34:44,640 --> 00:34:52,560
basic things that you need when you are an enterprise company. Interesting. We haven't talked

303
00:34:53,200 --> 00:34:59,360
in detail about the engagement tool, or some of the interesting challenges you ran into

304
00:34:59,360 --> 00:35:06,640
in developing that tool. Well, yeah, there's also been a couple, right? Because when you come out,

305
00:35:06,640 --> 00:35:16,160
fresh from university, right? You think data sets are beautiful, clean, and, you know, I think

306
00:35:16,160 --> 00:35:20,320
that's what a lot of people think about them, because when you read research papers, the data is

307
00:35:20,320 --> 00:35:26,880
just beautiful, right? And you have, I don't know, 16 GPUs, and all the money in the world, apparently.

308
00:35:28,560 --> 00:35:33,200
Well, that's not our case, even though it also goes well, I guess monetarily speaking of labor,

309
00:35:33,200 --> 00:35:40,320
but I think one of the, so the main issue, this is this all started, so the whole idea for

310
00:35:40,320 --> 00:35:47,840
this engagement tool started also, maybe two, two and a half, three years ago, and the initial idea

311
00:35:47,840 --> 00:35:54,480
was that we would get the data to then train only image, so we started on with images, to train

312
00:35:54,480 --> 00:36:00,640
an image classifier that could recognize themes. Then the constraint was we want this algorithm

313
00:36:00,640 --> 00:36:10,080
to be on device. So what we did is we went from the smallest network known, which is, well,

314
00:36:10,080 --> 00:36:18,640
at the time was mobile net actuators, and that's three, five megabytes, and that was to be for them.

315
00:36:20,240 --> 00:36:27,600
So, so that we had a problem because it's a wall. We can try to go down, we can try to minimize,

316
00:36:27,600 --> 00:36:32,960
we can try to reduce weights between layers, we can try to do a lot of this sort of optimizations,

317
00:36:32,960 --> 00:36:40,160
but the results were already a bit clumsy. So, you know, you maybe had a Jurassic World build,

318
00:36:40,160 --> 00:36:46,880
and I could recognize it as styles. So it goes far from idea, and I actually the idea of the project

319
00:36:46,880 --> 00:36:53,680
stopped there. Until a couple of months after somebody said, well, when we tried to do this theme,

320
00:36:53,680 --> 00:37:00,800
the texture for actually interacting with the customers or with the users, because then

321
00:37:00,800 --> 00:37:05,280
it's a wall. You have room to do the network you want, we don't really care because it's just

322
00:37:05,280 --> 00:37:10,560
going to run in the cloud, and it's going to run after the fact, right? So it's going to run

323
00:37:10,560 --> 00:37:15,760
at some point in the day to just, you know, get the themes and send some likes and comments, right?

324
00:37:18,800 --> 00:37:23,280
And that went very fine. I think we started with five themes, which is the top five categories,

325
00:37:23,280 --> 00:37:29,680
also, from the application, and probably the biggest issue there again was the data quality, right?

326
00:37:31,360 --> 00:37:37,920
So you can think of our application like the level life app as Instagram, right? So when you're a

327
00:37:37,920 --> 00:37:43,360
user, you take a picture of your level build, you will write a title in the description,

328
00:37:43,360 --> 00:37:49,120
say, you know, this is my cool creation, this is a Star Wars save file. And you can also then

329
00:37:49,120 --> 00:37:55,840
add the hashtags back on Instagram. So those hashtags could be, you know, hashtags, Star Wars,

330
00:37:55,840 --> 00:38:05,840
hashtags, a lot of other things. So because this data is available in the social media

331
00:38:05,840 --> 00:38:12,480
as backend, in this sort of, you know, you could say clumsy labeling, which is all right,

332
00:38:12,480 --> 00:38:16,880
well, we could try maybe running the first iteration, trying to crawl all of this data

333
00:38:16,880 --> 00:38:24,640
with some specific themes and maybe also even keywords. We know are used for specific themes,

334
00:38:24,640 --> 00:38:32,400
right? So let's say, okay, if a user has published a Star Wars image and says, this is Chihuahua,

335
00:38:32,400 --> 00:38:39,840
I know Chihuahua is Star Wars minifigure. Well, I can probably make it in my dataset saying it's

336
00:38:39,840 --> 00:38:45,600
Star Wars. So that's how we collected data the first time for five classes. And it worked

337
00:38:45,600 --> 00:38:51,920
wonderfully well. There were also a lot of misclassifications, but that was also expected from us.

338
00:38:52,960 --> 00:38:57,040
And we tried to clean, so we tried to, I think we did a couple of iterations from the keywords

339
00:38:57,040 --> 00:39:04,080
and the hashtags we used as well. Because you could think, if you said the spaceship,

340
00:39:04,080 --> 00:39:07,760
the spaceship could be Star Wars, but it could also be in Jailor, it could also be City.

341
00:39:08,640 --> 00:39:12,560
There's a lot of different spaces. That's the problem at labor, right? That imagination is the

342
00:39:12,560 --> 00:39:18,240
limit, right? So it's not the real world unfortunately or fortunate. It makes it a bit more fun.

343
00:39:20,560 --> 00:39:26,000
But yeah, so then we got the data. The first model with five classes worked. It was accepted by

344
00:39:26,000 --> 00:39:32,960
the business side. And after that happened, then we extended to 10, no nine classes. And then

345
00:39:32,960 --> 00:39:38,640
now we're up to 13 classes. And I think the latest issues we've been having are not issues,

346
00:39:38,640 --> 00:39:45,760
but the latest challenges we have to overcome was growing to 15 classes. Now we have a three third

347
00:39:45,760 --> 00:39:49,920
by data set of images, which you might think, well, it's not that much. You know, there's like

348
00:39:49,920 --> 00:39:57,280
ImageNet has 15 million images, I guess. It's quite a, it's quite a bunch when it's the first time

349
00:39:57,280 --> 00:40:05,520
we work with such a big data set for a production-ready solution, right? And using SageMaker,

350
00:40:05,520 --> 00:40:11,600
we also learned a lot of SageMaker because we don't have on on premise GPUs. So well,

351
00:40:11,600 --> 00:40:19,680
I have it here, but I have it for gaming on my free time. And so we also have to do some learning

352
00:40:19,680 --> 00:40:25,840
on how to do to let SageMaker the best possible. And to be fair, that also was very helpful when

353
00:40:25,840 --> 00:40:32,240
we also got support from them from the enterprise side. Because then yeah, so we were using SageMaker

354
00:40:32,240 --> 00:40:40,800
notebooks, which is just a full on Jupyter notebook. It turns out that even though you can choose

355
00:40:41,440 --> 00:40:47,840
GPU instance for the notebook, it is not the recommended scenario. The training on a GPU has to

356
00:40:47,840 --> 00:40:54,880
be done on the SageMaker training instead. The notebook is more just for data prep and the

357
00:40:54,880 --> 00:41:03,120
visualization. Well, we didn't know that. Until of course, we needed to run this 13-class training,

358
00:41:03,840 --> 00:41:09,280
actually the 9-class 13 training, we did it on the SageMaker notebook. And I think it took

359
00:41:10,320 --> 00:41:18,960
a week, maybe, a week and a half. Whereas for the 15-class, we then got these estimators

360
00:41:18,960 --> 00:41:25,440
running on the training jobs. And that took, I think, half a day. So more data less time.

361
00:41:25,440 --> 00:41:34,160
It was a good trade-off, I think. Got it. Got it. Very cool. Very cool. What are the,

362
00:41:34,960 --> 00:41:41,760
kind of what's next at LEGO with ML? What are you excited about or looking forward to?

363
00:41:41,760 --> 00:41:50,720
I think it's a very open question, right? Because again, so we are a company that makes a lot of,

364
00:41:51,680 --> 00:41:57,360
I guess, out of the ordinary things, right? And one of the projects, for example, we've

365
00:41:57,360 --> 00:42:05,040
been looking at is a taxonomious, right? So let's say how many cards we have, how many

366
00:42:05,040 --> 00:42:12,320
motorbikes, how many cats we have at LEGO, right? And you could say, well, we are LEGO, we own

367
00:42:12,320 --> 00:42:18,240
all of our data, we know what we have, and that's true. But what is, well, who is telling me that

368
00:42:18,240 --> 00:42:29,280
tomorrow there's not going to be a unicorn with a fish legs, for example, right? So this makes this

369
00:42:29,280 --> 00:42:34,080
taxonomious complicated, right? Because how do you evolve an algorithm, or a set of algorithms

370
00:42:34,080 --> 00:42:40,000
that can get a new class that has never seen before, it does not look like anything, right? Because

371
00:42:40,000 --> 00:42:45,520
you can have a fish in a unicorn, but if they are combined, who's the winner here? How do you

372
00:42:46,240 --> 00:42:50,400
make sure that your algorithm can learn the new class if you're coming? And it's something we're

373
00:42:50,400 --> 00:42:57,920
looking into, and I think it's hopefully coming next year. It requires a lot of prep work, that's

374
00:42:57,920 --> 00:43:05,280
for sure. But there's also a couple of other things. So we're also trying to, to upgrade some of the,

375
00:43:06,080 --> 00:43:15,920
of the A4, which is A4 is an adult fan of LEGO. So all of these adult oriented experiences,

376
00:43:17,200 --> 00:43:22,480
we're also trying to put on a bit more spice to them for making it all exciting for the adults to

377
00:43:22,480 --> 00:43:29,840
use, saying, you can have this specific application here, can you recognize what you're building,

378
00:43:29,840 --> 00:43:37,680
things like that. So trying to interact with the users as well. But it's complicated, I think,

379
00:43:37,680 --> 00:43:43,120
right? Because you can have very big dreams, but it's always the, it's always bureaucratic,

380
00:43:43,120 --> 00:43:48,800
right? And it's always about the data, right? It's like, well, sure, you can ask me to classify

381
00:43:48,800 --> 00:43:53,840
all of the red LEGO bricks you have, but I need to know what a LEGO brick is first, right? And what is

382
00:43:53,840 --> 00:44:04,160
red? Right. Very good, very good. Well, Francesc, thanks so much for joining us. It was great

383
00:44:04,160 --> 00:44:11,040
learning about your projects, and especially how you've built out the platform and infrastructure

384
00:44:11,040 --> 00:44:19,280
to support them at LEGO. Yeah, and well, thanks again for having me. And I guess maybe one last learning,

385
00:44:19,280 --> 00:44:49,200
you don't need to go to Kubernetes, you know, you can always go sellers. Awesome, thanks so much. Yes, thank you.

