1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,400
I'm your host Sam Charrington.

4
00:00:31,400 --> 00:00:36,320
Today we're joined by Solan Barokas, Assistant Professor of Information Science at Cornell

5
00:00:36,320 --> 00:00:37,800
University.

6
00:00:37,800 --> 00:00:43,200
Solan is also the co-founder of the Fairness Accountability and Transparency in Machine Learning Workshop

7
00:00:43,200 --> 00:00:47,040
that's hosted annually at conferences like ICML.

8
00:00:47,040 --> 00:00:51,080
Solan and I caught up to discuss his work on model interpretability and the legal and

9
00:00:51,080 --> 00:00:55,160
policy implications of the use of machine learning models.

10
00:00:55,160 --> 00:01:00,320
In our conversation, we discussed the gap between law, policy, and ML and how to build

11
00:01:00,320 --> 00:01:04,600
the bridge between them, including formalizing ethical frameworks for machines to look at

12
00:01:04,600 --> 00:01:09,520
his paper, the intuitive appeal of explainable machines, which proposes that explainability

13
00:01:09,520 --> 00:01:14,480
is really two problems, inscrutability and non-intuitiveness, and that disentangling

14
00:01:14,480 --> 00:01:19,560
the two allows us to better reason about the kind of explainability that's really required

15
00:01:19,560 --> 00:01:21,960
in any given situation.

16
00:01:21,960 --> 00:01:26,560
And now on to the show.

17
00:01:26,560 --> 00:01:27,920
All right, everyone.

18
00:01:27,920 --> 00:01:30,360
I am on the line with Solan Barokas.

19
00:01:30,360 --> 00:01:35,800
Solan is an Assistant Professor of Information Science at Cornell University.

20
00:01:35,800 --> 00:01:39,080
Solan, welcome to this week in Machine Learning and AI.

21
00:01:39,080 --> 00:01:40,560
Thanks so much for having me.

22
00:01:40,560 --> 00:01:43,440
Why don't we get started by having you tell us a little bit about your background and

23
00:01:43,440 --> 00:01:46,640
how you got involved in machine learning?

24
00:01:46,640 --> 00:01:49,440
Yeah, it's a nice story, at least.

25
00:01:49,440 --> 00:01:53,520
I have a think unusual story in the way that I found my way to machine learning.

26
00:01:53,520 --> 00:02:00,200
So I got my PhD at NYU a couple years ago now, and I actually graduated from what is essentially

27
00:02:00,200 --> 00:02:01,720
a media studies department.

28
00:02:01,720 --> 00:02:06,880
I worked with a particular professor named Helen Nissenbaum, who for many years has been

29
00:02:06,880 --> 00:02:12,360
one of the kind of leading people doing work on the ethics of information technology.

30
00:02:12,360 --> 00:02:16,440
So I had spent time there specifically to work with her, but I knew early on when I was

31
00:02:16,440 --> 00:02:21,320
a graduate student, that I was interested in what was more commonly called data mining

32
00:02:21,320 --> 00:02:28,320
then, and decided to actually take some classes and got to know some of the faculty at NYU,

33
00:02:28,320 --> 00:02:32,440
who then really kind of brought me up to speed in the fundamentals of machine learning.

34
00:02:32,440 --> 00:02:37,200
And it became very clear to me that there were enormous number of interesting ethical and

35
00:02:37,200 --> 00:02:41,880
policy questions that were raised by machine learning, and that those were quite different

36
00:02:41,880 --> 00:02:47,040
than the kinds of topics that people were discussing quite speculatively without knowing

37
00:02:47,040 --> 00:02:50,560
very much about how machine learning actually worked in practice.

38
00:02:50,560 --> 00:02:54,360
And so I went off to do a dissertation that was really trying to translate some of these

39
00:02:54,360 --> 00:03:00,040
interesting questions that grow out of the technical aspects of machine learning into

40
00:03:00,040 --> 00:03:07,000
ethics and policy questions, and it has meant over a kind of a long period of time becoming

41
00:03:07,000 --> 00:03:11,880
more and more familiar with machine learning and both the community of researchers and practitioners

42
00:03:11,880 --> 00:03:17,440
in this area to the extent now that I feel pretty comfortable and at home in the broader

43
00:03:17,440 --> 00:03:20,280
machine learning community in general.

44
00:03:20,280 --> 00:03:25,360
And a lot of that work is also taking the form of trying to bring some of these normative

45
00:03:25,360 --> 00:03:32,680
concerns around privacy or fairness or transparency further into the machine learning community.

46
00:03:32,680 --> 00:03:39,240
So making these issues first order questions for the field and that has included staging

47
00:03:39,240 --> 00:03:44,920
workshops for about five years now attached to the main machine learning conferences like

48
00:03:44,920 --> 00:03:51,080
NIPS and ICML that were explicitly attempts to bring questions of fairness accountability

49
00:03:51,080 --> 00:03:53,920
and transparency into the mainstream of the community.

50
00:03:53,920 --> 00:03:58,560
In fact, the event we started was still called fairness accountability and transparency

51
00:03:58,560 --> 00:04:00,440
and machine learning.

52
00:04:00,440 --> 00:04:05,320
So that has been one of the main mechanisms by which I've kind of entered the community.

53
00:04:05,320 --> 00:04:09,240
I think people, if they know me at all, probably know me through that work.

54
00:04:09,240 --> 00:04:15,080
And it's been great actually because five years ago when we started this kind of conversation,

55
00:04:15,080 --> 00:04:22,240
it was a pretty niche group, some great people from the start but still quite a small community.

56
00:04:22,240 --> 00:04:26,680
And over the past two years in particular, these have just become kind of mainstream issues

57
00:04:26,680 --> 00:04:32,160
for the field, such that papers are now being accepted at these conferences, a bunch

58
00:04:32,160 --> 00:04:36,080
of best paper awards have gone to papers on fairness recently.

59
00:04:36,080 --> 00:04:39,640
And if you follow the news at all, a lot of companies, a lot of the big tech companies

60
00:04:39,640 --> 00:04:44,720
have also begun to take these issues really seriously and begin to work on them in practice.

61
00:04:44,720 --> 00:04:47,760
And I've had opportunity to kind of see how that's progressed.

62
00:04:47,760 --> 00:04:51,720
So that's really the role I've been filling in this community the past couple of years.

63
00:04:51,720 --> 00:04:58,040
And while I really appreciate that aspect of kind of bridging these two communities, I find often

64
00:04:58,040 --> 00:05:07,000
or found that a lot of people in machine learning care about these issues of ethics, but there

65
00:05:07,000 --> 00:05:10,720
aren't a lot of people that have both the machine learning background and a background

66
00:05:10,720 --> 00:05:11,720
in ethics.

67
00:05:11,720 --> 00:05:17,800
And so you often observe, appears to be reinventing the wheel, coming up with ethical frameworks

68
00:05:17,800 --> 00:05:18,800
from scratch.

69
00:05:18,800 --> 00:05:24,880
In fact, there's clearly been a lot of work put into that kind of thing on the ethics side.

70
00:05:24,880 --> 00:05:30,880
And it's just an issue of like connecting these dots or like connecting the two sides of

71
00:05:30,880 --> 00:05:34,040
the bridge or something like, do you experience that as well?

72
00:05:34,040 --> 00:05:35,400
It's a great question.

73
00:05:35,400 --> 00:05:42,400
I think what's been exciting about this area of research and policy discussion is people

74
00:05:42,400 --> 00:05:48,560
sort of recognized early on that at least when it comes to questions of fairness and discrimination,

75
00:05:48,560 --> 00:05:54,520
there were some existing ways of thinking about the issue which lent themselves to formalization.

76
00:05:54,520 --> 00:06:01,360
So just to give a kind of quick anecdote here, it's sort of a rule of thumb in discrimination

77
00:06:01,360 --> 00:06:07,480
law that if there's a disparity in outcome greater than 20% between, let's say, men and

78
00:06:07,480 --> 00:06:13,120
women who are applying for a job, that itself can be a trigger for a case to investigate

79
00:06:13,120 --> 00:06:15,480
whether in fact there is discrimination.

80
00:06:15,480 --> 00:06:20,800
It's called the four fifth rule, it comes from the, the regulator, the employment regulator.

81
00:06:20,800 --> 00:06:24,960
And even though it's not enough to establish that some kind of decision making process, there's

82
00:06:24,960 --> 00:06:30,400
discriminatory, it's often used as a heuristic to say like, well, we should try to minimize

83
00:06:30,400 --> 00:06:33,800
the disparity so that it's never greater than 20%.

84
00:06:33,800 --> 00:06:38,080
And because this is an existing way of thinking about this in quantitative terms, it really

85
00:06:38,080 --> 00:06:43,240
lent itself to the kind of work that machine-linear people like to do, which is this kind of constrained

86
00:06:43,240 --> 00:06:44,400
optimization problem.

87
00:06:44,400 --> 00:06:49,280
So you know, how can we build classifiers or predictive models that aim to achieve, you

88
00:06:49,280 --> 00:06:53,440
know, a maximum performance, subject to this additional constraints.

89
00:06:53,440 --> 00:06:57,360
And some of the earliest work on fairness and machine learning grew directly out of some

90
00:06:57,360 --> 00:07:00,040
of these principles from the law.

91
00:07:00,040 --> 00:07:04,480
Since then, what's happened though is that there's been a bunch of new ways of trying to

92
00:07:04,480 --> 00:07:08,720
formalize notions of fairness that are really different and pretty much detached from

93
00:07:08,720 --> 00:07:12,680
the way that the law and policy community think about them.

94
00:07:12,680 --> 00:07:17,480
And you could maybe think about that as being reinventing the wheel or not being sufficiently

95
00:07:17,480 --> 00:07:21,360
sensitive to how much work has been done already.

96
00:07:21,360 --> 00:07:27,680
But part of what's been kind of exciting and fascinating is that some of these new formalizations

97
00:07:27,680 --> 00:07:31,600
are actually just sort of interesting new ways of thinking about the problem that haven't

98
00:07:31,600 --> 00:07:36,120
come up in the law and policy discussion in part because perhaps people haven't had

99
00:07:36,120 --> 00:07:41,120
reasons to think about them in the way that these new formalizations actually parse the

100
00:07:41,120 --> 00:07:42,120
issue.

101
00:07:42,120 --> 00:07:47,800
And so an example of this is, you know, there's a debate now whether differences, differences

102
00:07:47,800 --> 00:07:52,080
not only in the accuracy of the model between different groups matter, but whether differences

103
00:07:52,080 --> 00:07:54,080
in the error rates matter.

104
00:07:54,080 --> 00:07:58,520
So, you know, it might matter very much in different settings that you're subject to a false

105
00:07:58,520 --> 00:08:04,920
positive or false negative and perhaps you want to equalize across these rates, but which

106
00:08:04,920 --> 00:08:08,840
one of those actually matters or do these matter at all.

107
00:08:08,840 --> 00:08:13,320
And that's sort of the direction that the field is going, proposing these new ideas that

108
00:08:13,320 --> 00:08:17,400
don't have a neat or obvious analog in law.

109
00:08:17,400 --> 00:08:19,120
And I think there's a lot that can come out of that.

110
00:08:19,120 --> 00:08:23,160
Part of it might be that maybe these aren't, in fact, good ways to think about it given

111
00:08:23,160 --> 00:08:28,480
what people actually care about from a normative perspective, but it also can potentially

112
00:08:28,480 --> 00:08:32,920
reveal that some of the ways that law and policy have thought about these problems are incomplete

113
00:08:32,920 --> 00:08:35,560
or incoherent or we could actually do better.

114
00:08:35,560 --> 00:08:37,160
So I think there's a lot of opportunity here.

115
00:08:37,160 --> 00:08:41,080
That's a fantastic take on that.

116
00:08:41,080 --> 00:08:48,920
What are, you kind of mentioned, you know, broadly different formalisms and ways to parse

117
00:08:48,920 --> 00:08:49,920
this issue?

118
00:08:49,920 --> 00:08:55,160
Are there some other examples that come to mind of the way folks have formalized ethical

119
00:08:55,160 --> 00:08:58,920
and fairness frameworks applying them to machine learning?

120
00:08:58,920 --> 00:09:03,680
Yeah, I think at a high level, there's a way to maybe categorize some of them.

121
00:09:03,680 --> 00:09:10,600
So I think a set of concerns have to do with just straight up differences in the performance

122
00:09:10,600 --> 00:09:14,600
of models for different groups, for men and women, for people of different races or what

123
00:09:14,600 --> 00:09:16,720
have you.

124
00:09:16,720 --> 00:09:20,360
And you could think about that using any number of different metrics, right?

125
00:09:20,360 --> 00:09:24,800
So not just accuracy, not just these kind of false positive or false negative rates,

126
00:09:24,800 --> 00:09:29,000
but a bunch of other things as well, including calibration and other terms that might be familiar

127
00:09:29,000 --> 00:09:30,320
to you folks.

128
00:09:30,320 --> 00:09:35,480
What's interesting about that work is that it sort of sets aside the question, whether

129
00:09:35,480 --> 00:09:39,200
the data, the underlying training data itself is reliable.

130
00:09:39,200 --> 00:09:44,480
It says, even if the data is perfect, there might be circumstances where we still observe

131
00:09:44,480 --> 00:09:48,960
disparities in the performance of these models between groups.

132
00:09:48,960 --> 00:09:50,880
And if that's the case, you know, what can we do about it?

133
00:09:50,880 --> 00:09:53,400
How do we actually try to avoid those situations?

134
00:09:53,400 --> 00:09:56,160
A separate bucket, though, actually starts from a different position.

135
00:09:56,160 --> 00:10:02,360
It says, we should never actually believe that the training data we have is reliable.

136
00:10:02,360 --> 00:10:06,720
And in fact, there's good reason to believe that it's systematically unreliable in a particular

137
00:10:06,720 --> 00:10:07,720
way.

138
00:10:07,720 --> 00:10:13,720
So an example of this is, you know, you might think about how does the accuracy of a predictive

139
00:10:13,720 --> 00:10:16,840
policing model differ between groups?

140
00:10:16,840 --> 00:10:19,800
That's the sort of question you get asked in the first camp.

141
00:10:19,800 --> 00:10:23,320
And the second camp, which is concerned with the underlying quality of the data, they might

142
00:10:23,320 --> 00:10:29,280
say, yeah, you actually don't have data that is a good representation of the true incidence

143
00:10:29,280 --> 00:10:30,800
of crime in society.

144
00:10:30,800 --> 00:10:35,040
So anything you might do around the edges concerning performance is really not addressing the

145
00:10:35,040 --> 00:10:39,760
fundamental problem, which is that you're learning from highly biased data.

146
00:10:39,760 --> 00:10:46,480
And a slightly more serious version of this concern might be that maybe you are even

147
00:10:46,480 --> 00:10:49,920
using data that encodes some kind of prejudice.

148
00:10:49,920 --> 00:10:54,280
So it's not just that the selection or sample of data is biased, but that the training

149
00:10:54,280 --> 00:10:59,760
data is itself some past decision made by someone, which was made in a prejudicial way.

150
00:10:59,760 --> 00:11:04,280
Though the example I tend to give for this is something like an employer trying to use

151
00:11:04,280 --> 00:11:09,640
machine learning to help find good people to hire in the future.

152
00:11:09,640 --> 00:11:14,440
And you might say, let's take a look at which employees that we've hired in the past

153
00:11:14,440 --> 00:11:17,080
have gone on to be particularly successful.

154
00:11:17,080 --> 00:11:22,800
The target variable we might select in this model is the annual review score that we've

155
00:11:22,800 --> 00:11:25,800
given our past employees.

156
00:11:25,800 --> 00:11:31,600
And so what you want the model to do is to predict what the likely annual review score

157
00:11:31,600 --> 00:11:36,960
would be for any new job applicant given the training data from your past employees.

158
00:11:36,960 --> 00:11:42,000
The problem, of course, is that even though one of these annual review scores is meant

159
00:11:42,000 --> 00:11:48,360
to be carefully considered and people are asked to go through a pretty systematic process

160
00:11:48,360 --> 00:11:52,760
to assign this score invariably, and there's even research empirical research to show

161
00:11:52,760 --> 00:11:58,120
this, that score is going to be inflected either potentially by conscious prejudice or

162
00:11:58,120 --> 00:12:02,840
implicit bias, where people don't even really realize that their assessment is somehow swayed

163
00:12:02,840 --> 00:12:06,800
by these preexisting beliefs about gender or race or what have you.

164
00:12:06,800 --> 00:12:11,240
And so what ends up happening is that the training data actually encodes that prior prejudice

165
00:12:11,240 --> 00:12:15,400
or bias, and so the model is not learning to predict how people would actually do on

166
00:12:15,400 --> 00:12:16,400
the job.

167
00:12:16,400 --> 00:12:21,200
The model is learning to predict how human assessors who had previously been given, who had

168
00:12:21,200 --> 00:12:26,920
been previously giving these annual review scores, would likely review this future person.

169
00:12:26,920 --> 00:12:31,720
So these give you a taste for some of the subtle differences, and each of these problems

170
00:12:31,720 --> 00:12:34,560
really require quite different responses, right?

171
00:12:34,560 --> 00:12:38,440
The very first type of problem that's describing, you're trying to figure out how to deal with

172
00:12:38,440 --> 00:12:43,400
the fact that there are situations where your model just won't perform as well, and these

173
00:12:43,400 --> 00:12:50,040
other situations where the underlying data is actually unreliable, require more creative,

174
00:12:50,040 --> 00:12:56,160
potentially kind of more ambitious solutions, which try to just compensate for what you

175
00:12:56,160 --> 00:13:00,880
believe to be the underlying problem with the data, even though you might have no direct

176
00:13:00,880 --> 00:13:03,120
way of measuring the thing you really care about.

177
00:13:03,120 --> 00:13:10,240
Just following that thread of the job performance reviews, to what degree has that specific problem

178
00:13:10,240 --> 00:13:15,720
been, explored, and what kinds of approaches or solutions have you seen folks taking

179
00:13:15,720 --> 00:13:16,720
with that?

180
00:13:16,720 --> 00:13:17,720
That's a great question.

181
00:13:17,720 --> 00:13:23,720
So I don't think there's any concrete case yet where we've been able to establish that

182
00:13:23,720 --> 00:13:32,200
the training data actually encodes the past discriminatory decisions of management.

183
00:13:32,200 --> 00:13:37,720
But as a kind of thought experiment, and given other empirical work on how humans actually

184
00:13:37,720 --> 00:13:43,080
do assign these kinds of scores to their employees, there's very good reason to believe

185
00:13:43,080 --> 00:13:46,040
that that training data would, in fact, whatever training data people might put together

186
00:13:46,040 --> 00:13:49,200
would suffer from that kind of problem.

187
00:13:49,200 --> 00:13:55,240
There's a lot of people who are becoming more and more familiar with this as a potential

188
00:13:55,240 --> 00:14:01,440
concern, and so that's resulted both in lawyers trying to find cases to potentially bring

189
00:14:01,440 --> 00:14:03,080
against employees.

190
00:14:03,080 --> 00:14:07,960
But separately, there's also a lot of companies who specialize in machine learning, who are

191
00:14:07,960 --> 00:14:13,920
trying to integrate these concerns into the products they then sell to clients.

192
00:14:13,920 --> 00:14:18,480
So these are sort of recruitment machine learning specialists to then develop tools that

193
00:14:18,480 --> 00:14:23,200
they license or sell to employers.

194
00:14:23,200 --> 00:14:28,440
And part of the solution to these problems can vary considerably.

195
00:14:28,440 --> 00:14:34,440
So on the one hand, a lot of the simplest interventions involve reporting performance

196
00:14:34,440 --> 00:14:39,800
in a way that just breaks apart overall performance into performance by group.

197
00:14:39,800 --> 00:14:45,600
So can we just observe that this model does a much worse job for certain people than others?

198
00:14:45,600 --> 00:14:48,040
And are there ways that we can try to compensate for that?

199
00:14:48,040 --> 00:14:52,480
And my sense is that that's becoming an increase in the more common approach.

200
00:14:52,480 --> 00:14:57,000
The problem with that approach, of course, is that you actually need to know those details

201
00:14:57,000 --> 00:14:59,320
about the people you're trying to score.

202
00:14:59,320 --> 00:15:04,640
So in order to separate out the performance for men and women or for people with different

203
00:15:04,640 --> 00:15:08,360
of different races, you actually have to collect information about that.

204
00:15:08,360 --> 00:15:15,680
And as a kind of standard practice, employers at least really have good incentive not to

205
00:15:15,680 --> 00:15:20,640
collect that information because they don't want to even create the possibility that of

206
00:15:20,640 --> 00:15:23,960
being accused of having considered it when making decisions.

207
00:15:23,960 --> 00:15:26,000
So it's an interesting tension, right?

208
00:15:26,000 --> 00:15:32,000
We might want the information to prevent discrimination, but historically, companies have been instructed

209
00:15:32,000 --> 00:15:36,680
not to collect it to limit the likelihood that they could ever even consider it.

210
00:15:36,680 --> 00:15:41,600
The problem with encoding or your training data is just past decisions which might have

211
00:15:41,600 --> 00:15:44,040
been influenced by human bias.

212
00:15:44,040 --> 00:15:45,480
That's much more difficult.

213
00:15:45,480 --> 00:15:49,920
And it's not even obvious what a principled approach to that would be.

214
00:15:49,920 --> 00:15:54,160
Some of the positions that people have put forward is to sort of just make certain assumptions

215
00:15:54,160 --> 00:16:01,160
about what you think is a more reasonable distribution of attributes across the population so that

216
00:16:01,160 --> 00:16:07,640
you sort of override what is the actual distribution and the training data you're dealing with.

217
00:16:07,640 --> 00:16:12,240
In a way, you're basically saying, I suspect the data to be systematically flawed and I'm

218
00:16:12,240 --> 00:16:16,280
going to kind of put my thumb on the scale to compensate for that.

219
00:16:16,280 --> 00:16:21,520
And some people now have pretty rigorous mathematical methods for doing this without necessarily

220
00:16:21,520 --> 00:16:23,960
sacrificing performance.

221
00:16:23,960 --> 00:16:28,680
But I'm just not so familiar yet with what is happening in practice on that front.

222
00:16:28,680 --> 00:16:32,920
And the final thing I would say is that, you know, in a situation where you have all

223
00:16:32,920 --> 00:16:37,840
their mechanisms to potentially measure these dynamics, you might take a different approach.

224
00:16:37,840 --> 00:16:41,760
You might say, I'm not going to treat this as a pure prediction problem.

225
00:16:41,760 --> 00:16:47,400
I'm going to try to do some kind of empirical study to see if in fact, you know, there's

226
00:16:47,400 --> 00:16:49,160
a problem in my workplace.

227
00:16:49,160 --> 00:16:55,240
So rather than just relying exclusively on these annual review scores, maybe you go and

228
00:16:55,240 --> 00:17:02,240
try to find some other thing to measure, which is not as vulnerable to human bias.

229
00:17:02,240 --> 00:17:05,840
So an example of this might be something like, well, it's pretty hard to argue with the

230
00:17:05,840 --> 00:17:10,640
fact that you've achieved some sales figure at the end of the year if you're a sales person.

231
00:17:10,640 --> 00:17:16,720
Like that seems like a metric that's much less vulnerable to this kind of bias assessment.

232
00:17:16,720 --> 00:17:20,280
You can challenge that, but the argument anyway is that maybe we can begin to measure

233
00:17:20,280 --> 00:17:25,560
other things and then compare that to the kinds of assessment people get at their annual

234
00:17:25,560 --> 00:17:26,560
review.

235
00:17:26,560 --> 00:17:31,560
And that might reveal some kind of underlying disparity or sort of misalignment between

236
00:17:31,560 --> 00:17:34,440
people's true performance and the score they're given.

237
00:17:34,440 --> 00:17:37,880
And then given that finding, you can go about trying to correct things.

238
00:17:37,880 --> 00:17:42,600
But there's, you know, even in this very long answer, I haven't even exhausted the list

239
00:17:42,600 --> 00:17:43,600
of possibilities.

240
00:17:43,600 --> 00:17:49,480
I mean, I guess it's obvious that a lot of these challenges are like fundamental human

241
00:17:49,480 --> 00:17:57,520
organizational people issues and technology is but a small part of the overall picture.

242
00:17:57,520 --> 00:17:59,040
That's right.

243
00:17:59,040 --> 00:18:04,760
And I think what's interesting about the current state of the research is that a lot of it

244
00:18:04,760 --> 00:18:10,520
is sort of head-to-head comparison between existing decision-making process and some

245
00:18:10,520 --> 00:18:13,240
model under perfect conditions.

246
00:18:13,240 --> 00:18:19,280
And I think a lot of progress will depend really ultimately, I think, on figuring out

247
00:18:19,280 --> 00:18:25,680
how to think more kind of formally and carefully about these fairness concerns and integrate

248
00:18:25,680 --> 00:18:28,040
them into the model development process.

249
00:18:28,040 --> 00:18:34,600
But separately, also think about how to then put that model into practice and potentially

250
00:18:34,600 --> 00:18:37,840
reform the institution itself, as you were saying, right?

251
00:18:37,840 --> 00:18:42,840
Like really consider how this fits into the bureaucratic decision-making process, how it

252
00:18:42,840 --> 00:18:46,560
figures into the dynamics of the workplace that would have you.

253
00:18:46,560 --> 00:18:54,000
So even if we are successful in trying to deal with these concerns within the model itself,

254
00:18:54,000 --> 00:18:58,840
that is certainly not sufficient to achieve these broader fairness or justice goals we might

255
00:18:58,840 --> 00:18:59,840
have.

256
00:18:59,840 --> 00:19:05,480
And so I asked about that, about an example in the context of performance reviews, but

257
00:19:05,480 --> 00:19:13,400
are there more broadly any examples that you've come across of kind of this process taking

258
00:19:13,400 --> 00:19:21,760
place full circle within an organization or some political structure where the algorithmic

259
00:19:21,760 --> 00:19:29,440
or data biases were observed, some sets of adjustments were made to modeling as well

260
00:19:29,440 --> 00:19:37,040
as the underlying organization, organizational practices, and that leading to a better

261
00:19:37,040 --> 00:19:38,040
outcome?

262
00:19:38,040 --> 00:19:39,560
Yeah, it's a great question.

263
00:19:39,560 --> 00:19:44,720
And I don't think there's some shiny and example to point to, unfortunately, at least

264
00:19:44,720 --> 00:19:45,720
not yet.

265
00:19:45,720 --> 00:19:51,600
I mean, there's certainly some examples of changes that have been made that were a little

266
00:19:51,600 --> 00:19:52,640
bit more straightforward.

267
00:19:52,640 --> 00:19:59,400
So some scholars were able to show that there were disparities in the performance of

268
00:19:59,400 --> 00:20:05,360
kind of off-the-shelf facial recognition software and that these disparities were along the

269
00:20:05,360 --> 00:20:09,920
lines of both race and gender, such that these systems did a much, much worse job, for

270
00:20:09,920 --> 00:20:13,240
instance, for black women than for white men.

271
00:20:13,240 --> 00:20:18,240
And the result of this research, which I certainly encourage your listeners to take a look

272
00:20:18,240 --> 00:20:25,720
at, was some pretty rapid changes on the part of the companies that provided these often

273
00:20:25,720 --> 00:20:30,560
API facial recognition services, so they just basically went about trying to figure out

274
00:20:30,560 --> 00:20:33,160
how to improve the performance for these populations.

275
00:20:33,160 --> 00:20:37,200
You know, that's a different story because it doesn't involve this entire kind of workflow

276
00:20:37,200 --> 00:20:42,280
and bureaucracy, but an example I could still point to, which I think is an interesting

277
00:20:42,280 --> 00:20:51,160
one, is in Allegheny County, which houses Pittsburgh, the county was working with some academic

278
00:20:51,160 --> 00:20:56,320
researchers to not only use machine learning, but just in general, you sort of more data-driven

279
00:20:56,320 --> 00:21:02,400
approaches to the way that it handled its child protective welfare agency.

280
00:21:02,400 --> 00:21:09,520
And this got written up as a kind of long future article in the New York Times magazine

281
00:21:09,520 --> 00:21:10,880
a few months back.

282
00:21:10,880 --> 00:21:15,680
It's also actually the focus and the chapter of an excellent book called Automate in

283
00:21:15,680 --> 00:21:23,080
Equity both described this pretty careful process by which researchers engage with the city,

284
00:21:23,080 --> 00:21:30,880
but also with agency workers, with advocates for children and families, for people affected

285
00:21:30,880 --> 00:21:35,600
by these systems, and really try to take into consideration all the different interests

286
00:21:35,600 --> 00:21:39,960
that this agency was charged with serving.

287
00:21:39,960 --> 00:21:44,000
And the people developing the tool actually observed that there were, in fact, some of

288
00:21:44,000 --> 00:21:47,520
these disparities and how well the model performed.

289
00:21:47,520 --> 00:21:53,680
It was likely to produce these kinds of kind of disparate impact in the way that it would

290
00:21:53,680 --> 00:22:00,000
suggest people for scrutiny when it came to potential child maltreatment or child abuse.

291
00:22:00,000 --> 00:22:04,880
And one of the interesting things to think about when focusing on this story is that despite

292
00:22:04,880 --> 00:22:11,280
the fact that this effort really involves a considerable amount of community engagement

293
00:22:11,280 --> 00:22:16,960
and consultation, and even really explicitly took into consideration some of these questions

294
00:22:16,960 --> 00:22:21,600
around fairness, that people will nevertheless still have, I think, some legitimate concerns

295
00:22:21,600 --> 00:22:23,480
around the use of these tools.

296
00:22:23,480 --> 00:22:30,800
So it's hard to say that this is like a clear model for what everyone should be doing

297
00:22:30,800 --> 00:22:37,480
in similar situations, but it gives some sense of just how difficult it may be to kind

298
00:22:37,480 --> 00:22:42,640
of more fundamentally address questions of fairness, even if you've gone to the effort

299
00:22:42,640 --> 00:22:45,680
of integrating them into the model development process.

300
00:22:45,680 --> 00:22:52,240
I think what it points to for me is the need for kind of lots of people and perspectives

301
00:22:52,240 --> 00:22:57,560
to get involved in understanding this issue and how it applies to the problems that they

302
00:22:57,560 --> 00:23:04,200
care about and taking on the little pieces of it that they can take on, even if they're

303
00:23:04,200 --> 00:23:08,520
not exposed to the full cycle, if you will.

304
00:23:08,520 --> 00:23:13,600
Yeah, you know, the way you think about this is heavily influenced by the fact that one

305
00:23:13,600 --> 00:23:19,240
of the people who introduced me to machine learning was someone who had more than a decade

306
00:23:19,240 --> 00:23:25,120
of experience and practice, this is Foster Provo who's a professor at NYU.

307
00:23:25,120 --> 00:23:29,400
And his way of teaching machine learning really emphasized this kind of first step in the

308
00:23:29,400 --> 00:23:33,400
process, very different than what you get in kind of traditional academic machine learning

309
00:23:33,400 --> 00:23:34,400
education.

310
00:23:34,400 --> 00:23:38,760
You know, he really emphasized that one of the main challenges is how do you translate

311
00:23:38,760 --> 00:23:41,440
a business problem into a machine learning problem?

312
00:23:41,440 --> 00:23:47,360
So, you know, how do you specify the target variable correctly or in a way that's reasonable?

313
00:23:47,360 --> 00:23:51,920
And for people who I think have experience and practice, that is a familiar problem, you

314
00:23:51,920 --> 00:23:57,720
know, it's like not that easy to know exactly how to solve some general business problem

315
00:23:57,720 --> 00:24:01,160
by figuring out how to specify the target variable.

316
00:24:01,160 --> 00:24:06,000
And my sense is that when people do think about that, they think about it in terms of, you

317
00:24:06,000 --> 00:24:10,960
know, what is the appropriate thing to try to predict when you're in online marketing,

318
00:24:10,960 --> 00:24:11,960
right?

319
00:24:11,960 --> 00:24:15,000
Clearly, like, click through is not the best thing.

320
00:24:15,000 --> 00:24:19,240
We want someone to convert and so you can have a pretty obvious debate around what is

321
00:24:19,240 --> 00:24:21,880
the right thing that you should be predicting.

322
00:24:21,880 --> 00:24:27,000
I think there's a similar thing that happens in these domains that involve much more high

323
00:24:27,000 --> 00:24:33,760
stakes decisions like, you know, child welfare or employment or credit, right?

324
00:24:33,760 --> 00:24:38,960
We can have, I think, a pretty straight forward conversation about what it is that we're

325
00:24:38,960 --> 00:24:45,320
actually trying to achieve and does the way the problem has been specified actually correspond

326
00:24:45,320 --> 00:24:47,760
to our normative goals.

327
00:24:47,760 --> 00:24:54,040
And the advice that you get from some of the early data money, literature, I think is

328
00:24:54,040 --> 00:24:58,920
really relevant here, it's sort of about, as you say, you really want to have a pretty

329
00:24:58,920 --> 00:25:02,720
deep and thoughtful conversation with your stakeholders.

330
00:25:02,720 --> 00:25:06,560
You want to understand the problem that you're being charged with solving and you want

331
00:25:06,560 --> 00:25:10,280
to understand whether or not it really, the way you've kind of set up the problem really

332
00:25:10,280 --> 00:25:13,680
reflects the concerns of the people it's supposed to serve.

333
00:25:13,680 --> 00:25:20,320
So my sense is that, you know, a lot of existing ways of doing machine learning well in practice,

334
00:25:20,320 --> 00:25:25,360
those insights, the kind of ideas people have from their experience on the ground, could

335
00:25:25,360 --> 00:25:29,320
be super helpful to the conversation people are having now about ethics.

336
00:25:29,320 --> 00:25:37,080
So you recently published a paper that looked at the applicability of some of the work that's

337
00:25:37,080 --> 00:25:46,280
happening around transparency and explainability to various regulatory frameworks like GDPR and

338
00:25:46,280 --> 00:25:47,600
others.

339
00:25:47,600 --> 00:25:50,400
Can you give us an overview of that work?

340
00:25:50,400 --> 00:25:51,400
Sure.

341
00:25:51,400 --> 00:25:56,960
So this is a forthcoming paper and Ford and Law Review and my co-author Andrew Selbst and

342
00:25:56,960 --> 00:26:02,120
I were trying to sort of bring together a couple different conversations that were happening.

343
00:26:02,120 --> 00:26:08,320
So in the law and policy world, there's a lot of anxiety around the use of machine learning

344
00:26:08,320 --> 00:26:12,760
for high-stakes decision making because the fear is that you won't be able to explain

345
00:26:12,760 --> 00:26:17,880
the outcome of some decision-making process and for people who come from machine learning,

346
00:26:17,880 --> 00:26:22,280
you'll know that there's a long history of working on interpretability in machine learning

347
00:26:22,280 --> 00:26:27,040
and that this has become especially hot area as deep learning has become more successful

348
00:26:27,040 --> 00:26:31,720
and more dominant and there's been some really interesting research breakthroughs in trying

349
00:26:31,720 --> 00:26:36,160
to be able to explain what is happening with deep learning models.

350
00:26:36,160 --> 00:26:40,600
So what we were trying to kind of show is that there's ways to sort of have these two

351
00:26:40,600 --> 00:26:43,880
things speak to each other a little bit.

352
00:26:43,880 --> 00:26:50,080
Part of it is about explaining exactly what it is that the existing laws and regulations

353
00:26:50,080 --> 00:26:56,440
actually require when they require explanations and then part of it is also trying to show

354
00:26:56,440 --> 00:27:01,040
that there are in many cases tools for satisfying those laws.

355
00:27:01,040 --> 00:27:08,840
So although GDPR which is the European general data protection regulation is the thing that

356
00:27:08,840 --> 00:27:14,520
has really generated a lot of attention around these issues recently, there are laws here

357
00:27:14,520 --> 00:27:19,880
in the United States that also will require explanations for automated decisions and

358
00:27:19,880 --> 00:27:25,600
the key example of that is the Equal Credit Opportunity Act and the Fair Credit Reporting

359
00:27:25,600 --> 00:27:26,600
Act.

360
00:27:26,600 --> 00:27:33,120
These are both laws that regulate credit scoring and credit decision-making and the acronyms

361
00:27:33,120 --> 00:27:40,520
are ECOA and FICRA and these laws when they stipulate that when a person applies for

362
00:27:40,520 --> 00:27:45,880
credit and the creditor denies that person, the creditor actually has to give reasons

363
00:27:45,880 --> 00:27:49,400
for why they denied the loan.

364
00:27:49,400 --> 00:27:54,160
And this law actually dates from the 1970s so this is not a new law, it's been around

365
00:27:54,160 --> 00:28:00,320
for a very, very long time and it has actually really structured the credit industry.

366
00:28:00,320 --> 00:28:04,480
If you speak to people in practice, you'll know that this really is the way that they've

367
00:28:04,480 --> 00:28:09,120
had to orient all the work that to make sure that they could always give reasons for

368
00:28:09,120 --> 00:28:14,520
their decisions regardless of the mechanism by which they got to their decision about

369
00:28:14,520 --> 00:28:16,320
whether they issued the loan.

370
00:28:16,320 --> 00:28:23,560
And the concern is that can you give reasons for a decision around credit if your model

371
00:28:23,560 --> 00:28:26,400
is using something like deep learning?

372
00:28:26,400 --> 00:28:31,240
Well, if you know the work in interpretability and machine learning at all, you'll know

373
00:28:31,240 --> 00:28:39,400
that a lot of the recent proposals involve for going any attempt to actually provide global

374
00:28:39,400 --> 00:28:46,080
transparency into the model, meaning forget any effort to describe the full relationship

375
00:28:46,080 --> 00:28:47,960
that the model maps out.

376
00:28:47,960 --> 00:28:54,760
And instead, let's use some other mechanisms to see if we can say what in any given decision

377
00:28:54,760 --> 00:29:00,080
actually seem to be the most salient variable or set of variables, so which features in

378
00:29:00,080 --> 00:29:06,240
the model really account for this particular classification or outcome.

379
00:29:06,240 --> 00:29:11,880
And those methods have proven pretty powerful in general for purposes of kind of any deep

380
00:29:11,880 --> 00:29:17,040
learning or machine learning model, but certainly it's not hard to imagine that they would

381
00:29:17,040 --> 00:29:23,440
be well suited to satisfying this existing requirement in eco and Fickra, which literally

382
00:29:23,440 --> 00:29:30,420
say you have to give specific and the actual reasons why someone was denied a loan.

383
00:29:30,420 --> 00:29:33,720
So what's interesting about this is it feels sort of like a silver bullet.

384
00:29:33,720 --> 00:29:41,400
It says like, oh, you can go off and build an arbitrarily complex model so long as you

385
00:29:41,400 --> 00:29:45,560
can provide specific reasons for any particular decision.

386
00:29:45,560 --> 00:29:51,840
And it feels that there might be a way to avoid the longstanding perceived tradeoff between

387
00:29:51,840 --> 00:29:56,720
the performance of the models you build and the interpretability of those models.

388
00:29:56,720 --> 00:29:57,920
So that seems like a good outcome.

389
00:29:57,920 --> 00:30:02,840
It seems like progress in kind of the research domain of machine learning has really helped

390
00:30:02,840 --> 00:30:08,920
solve a longstanding issue with regulation.

391
00:30:08,920 --> 00:30:13,960
The paper with my co-author kind of goes into some detail about why this might not always

392
00:30:13,960 --> 00:30:16,920
be so helpful in practice.

393
00:30:16,920 --> 00:30:17,920
And it's a bit complicated.

394
00:30:17,920 --> 00:30:21,160
So I'm not sure if I should carry on or I should let you ask another question.

395
00:30:21,160 --> 00:30:24,160
Well, we definitely want to dig into what makes it complicated.

396
00:30:24,160 --> 00:30:35,080
But I'm curious with what you stated, kind of the broader history of explainability or

397
00:30:35,080 --> 00:30:42,920
transparency requirement by other regulations, did you generally feel like all of the

398
00:30:42,920 --> 00:30:49,760
hullabaloo about GDPR and its implications for machine learning and deep learning and

399
00:30:49,760 --> 00:30:56,120
innovation and all this stuff like do you feel it was overblown or based on some of the

400
00:30:56,120 --> 00:30:59,920
challenges that you're aware of appropriate?

401
00:30:59,920 --> 00:31:07,560
So I think the reactions to GDPR in general is appropriate in the sense that the law

402
00:31:07,560 --> 00:31:12,680
is not radically different from the existing national laws that the regulation is meant

403
00:31:12,680 --> 00:31:13,680
to replace.

404
00:31:13,680 --> 00:31:18,520
So in the European Union, a regulation refers to a law that is standardized across all

405
00:31:18,520 --> 00:31:19,880
member states.

406
00:31:19,880 --> 00:31:24,280
There was something called the data protection directive for more than 15 years I think

407
00:31:24,280 --> 00:31:28,760
that was a sort of earlier version of what is now GDPR.

408
00:31:28,760 --> 00:31:33,040
The difference is that the directive was sort of guidance from member states and they

409
00:31:33,040 --> 00:31:37,720
were expected to sort of follow similar rules, but it wasn't standardized across all

410
00:31:37,720 --> 00:31:38,720
of Europe.

411
00:31:38,720 --> 00:31:44,360
The regulation was updated to kind of deal with some new problems, but the main radical

412
00:31:44,360 --> 00:31:50,920
change between existing laws and the regulation was financial penalty.

413
00:31:50,920 --> 00:31:57,200
So the reason is appropriate, I think, to actually think through these things is whether

414
00:31:57,200 --> 00:32:02,880
or not you care about these things from a kind of normative perspective or making sure

415
00:32:02,880 --> 00:32:05,200
that you obey the law.

416
00:32:05,200 --> 00:32:10,560
Companies actually now face genuinely significant financial penalty for failing to comply with

417
00:32:10,560 --> 00:32:11,560
the law.

418
00:32:11,560 --> 00:32:17,680
My sense is that the motivation for taking this serious, much more seriously anyway than

419
00:32:17,680 --> 00:32:22,160
it had been in the past is less that the law has changed dramatically.

420
00:32:22,160 --> 00:32:27,480
It's much more that the kind of consequences for failing to meet the law are now significantly

421
00:32:27,480 --> 00:32:28,960
more severe.

422
00:32:28,960 --> 00:32:34,120
So if you had been following the law already, which you should have been, this will not

423
00:32:34,120 --> 00:32:36,920
actually require a radical change.

424
00:32:36,920 --> 00:32:41,400
But the fact of the matter is that most people were not even really aware of the law, let

425
00:32:41,400 --> 00:32:43,320
alone following it.

426
00:32:43,320 --> 00:32:47,160
So to me, that really accounts for the difference.

427
00:32:47,160 --> 00:32:51,840
Having said all that, I still think that the main thing people at least were concerned

428
00:32:51,840 --> 00:32:58,360
about when it came to machine learning was this provision that required that you explain

429
00:32:58,360 --> 00:32:59,360
the decisions.

430
00:32:59,360 --> 00:33:06,080
So for certain types of decisions, automated decision making in general is forbidden

431
00:33:06,080 --> 00:33:08,960
unless people give consent.

432
00:33:08,960 --> 00:33:14,320
And even when they give consent, you still have to be able to explain the decision.

433
00:33:14,320 --> 00:33:20,520
And as I mentioned, there's a sense in which perhaps there is a trade-off between performance

434
00:33:20,520 --> 00:33:22,200
and explainability.

435
00:33:22,200 --> 00:33:28,480
So even if this wouldn't necessarily prohibit machine learning, it might be a constraint

436
00:33:28,480 --> 00:33:34,520
on the complexity of the model in order to make sure that it remains explainable.

437
00:33:34,520 --> 00:33:38,120
And that that might mean a degradation in performance.

438
00:33:38,120 --> 00:33:41,920
My sense is, though, that depending on how this law is interpreted, and this is also

439
00:33:41,920 --> 00:33:48,120
something we go into in the paper, you could potentially satisfy this requirement without

440
00:33:48,120 --> 00:33:54,000
necessarily building a model that is sufficiently simple that even a layperson could be able

441
00:33:54,000 --> 00:33:56,000
to look at it and understand it.

442
00:33:56,000 --> 00:34:00,840
There might be other ways of providing explanations, which still satisfy the law that don't require

443
00:34:00,840 --> 00:34:03,320
this potential trade-off.

444
00:34:03,320 --> 00:34:07,800
Can this bring us back to the point that you were about to make about looking at the details

445
00:34:07,800 --> 00:34:10,080
of complying with these various laws?

446
00:34:10,080 --> 00:34:12,120
Yeah, it's certainly in that direction.

447
00:34:12,120 --> 00:34:20,360
So I think the hope is that by explaining how decisions are made, you will know whether

448
00:34:20,360 --> 00:34:23,520
or not that is a good way of making decisions.

449
00:34:23,520 --> 00:34:29,240
So explanations are sort of a mechanism to check for other things you care about.

450
00:34:29,240 --> 00:34:35,760
Check to see that the decision making process, or in this case, the model, is taken to consideration

451
00:34:35,760 --> 00:34:39,080
things that you think of as being legitimate and relevant.

452
00:34:39,080 --> 00:34:41,080
So it should be considering these factors.

453
00:34:41,080 --> 00:34:43,560
So I can check, is it considering these factors?

454
00:34:43,560 --> 00:34:48,320
It could also be a way to check that it's not taken to consideration things that it shouldn't

455
00:34:48,320 --> 00:34:49,320
be.

456
00:34:49,320 --> 00:34:53,240
So it shouldn't be considering explicitly things like race, or it shouldn't be considering

457
00:34:53,240 --> 00:34:57,040
things that are arbitrary or clearly irrelevant.

458
00:34:57,040 --> 00:35:01,680
One of the challenges here is that sometimes you might be in a situation where even if

459
00:35:01,680 --> 00:35:08,720
you explain how the model makes the decisions, you as a human may not have any good intuitions

460
00:35:08,720 --> 00:35:13,640
for whether or not that is a reliable or sound basis for decision making.

461
00:35:13,640 --> 00:35:18,600
So one of the reasons people are interested in machine learning is that they can uncover

462
00:35:18,600 --> 00:35:22,760
patterns and data sets that would just escape humans' attention.

463
00:35:22,760 --> 00:35:27,000
And one would really be able to figure out that there are some kind of subtle signal across

464
00:35:27,000 --> 00:35:33,000
10,000 features that none of which on their face seem particularly relevant to the task

465
00:35:33,000 --> 00:35:34,920
at hand.

466
00:35:34,920 --> 00:35:39,880
So if it turns out, however, that the model has found such a signal, you could potentially

467
00:35:39,880 --> 00:35:44,000
try to explain the ones that are relevant to any particular decision, but even if you

468
00:35:44,000 --> 00:35:49,200
did, it would be potentially impossible for humans to know whether or not that's reasonable

469
00:35:49,200 --> 00:35:52,040
or really appropriate.

470
00:35:52,040 --> 00:35:56,080
So the example, I'll just come up with a kind of toy example here.

471
00:35:56,080 --> 00:36:02,160
If it turns out that the way you tie your shoes is predictive of some kind of performance

472
00:36:02,160 --> 00:36:05,680
on the job on its face that seems sort of laughable, right?

473
00:36:05,680 --> 00:36:09,400
Like why should that matter to my job performance?

474
00:36:09,400 --> 00:36:13,920
But let's just say for the sake of argument that it's a pretty robust finding that like

475
00:36:13,920 --> 00:36:18,320
the data really supported and wants you to play the model in practice, it actually shows

476
00:36:18,320 --> 00:36:20,920
that it does a pretty good job.

477
00:36:20,920 --> 00:36:25,000
On what basis do you actually say this is a good or bad model?

478
00:36:25,000 --> 00:36:29,640
It's good in the sense that it's potentially accurate, but it's not bad in the sense that

479
00:36:29,640 --> 00:36:34,400
it's choosing to pay attention to something irrelevant or obviously unfair.

480
00:36:34,400 --> 00:36:39,840
It's unsettling because it has discovered that there's something relevant in a feature

481
00:36:39,840 --> 00:36:43,680
that we as humans just cannot see is possibly relevant.

482
00:36:43,680 --> 00:36:46,040
And this is, I think, one of the real challenges here, right?

483
00:36:46,040 --> 00:36:51,000
So even if we use some of these awesome machine learning techniques to give explanations of models,

484
00:36:51,000 --> 00:36:55,440
they may not help us as humans to be able to assess whether those are even reasonable

485
00:36:55,440 --> 00:36:58,360
things to rely on to make important decisions.

486
00:36:58,360 --> 00:37:05,520
I think one of the interesting things here is some of the work that is going into kind

487
00:37:05,520 --> 00:37:12,040
of as opposed to trying to make your models, your fundamental fairness technique, being

488
00:37:12,040 --> 00:37:18,400
one of trying to make your models blind to factors like race or gender, actually taking

489
00:37:18,400 --> 00:37:26,600
part of this fairness challenge to build into your models, trying to predict these things

490
00:37:26,600 --> 00:37:33,280
like race and gender as part of the decision making and using whether the models can predict,

491
00:37:33,280 --> 00:37:40,840
race, for example, in the features that you kind of build into your model as an indicator

492
00:37:40,840 --> 00:37:46,760
that your model may be surreptitiously kind of making decisions based on race, right?

493
00:37:46,760 --> 00:37:50,040
It's there in the signal, it's just not obvious to you.

494
00:37:50,040 --> 00:37:53,680
Yeah, and so this is exactly the approach that some people are taking.

495
00:37:53,680 --> 00:37:55,440
It's an interesting problem, right?

496
00:37:55,440 --> 00:38:01,360
So you say, well, my model is non-discriminatory because it doesn't consider race or gender

497
00:38:01,360 --> 00:38:05,960
explicitly, but it turns out that other features are highly correlated.

498
00:38:05,960 --> 00:38:10,680
So maybe what you want to do is strip out those features that are highly correlated.

499
00:38:10,680 --> 00:38:15,160
This becomes a kind of impossible exercise at some point because with sufficiently

500
00:38:15,160 --> 00:38:21,720
rich data sets, it's almost certain that these kinds of details about you will be reflected

501
00:38:21,720 --> 00:38:24,480
in the other features that you have.

502
00:38:24,480 --> 00:38:28,440
And so it's not so obvious at what point you really are supposed to stop, right?

503
00:38:28,440 --> 00:38:33,400
Like how many things you'd remove from your model until you feel comfortable.

504
00:38:33,400 --> 00:38:37,800
And it's interesting because it actually relates to the other issue of explainability that

505
00:38:37,800 --> 00:38:39,520
we were just discussing.

506
00:38:39,520 --> 00:38:45,440
And the simple fact that some feature is correlated with race or gender on its own is not enough

507
00:38:45,440 --> 00:38:50,560
to say that it's illegitimate or illegal to actually consider that feature, right?

508
00:38:50,560 --> 00:38:54,480
And the reason for that is that there just happens to be inequality in society.

509
00:38:54,480 --> 00:39:00,200
Some people possess features at a certain value, at a higher or lower rate than others.

510
00:39:00,200 --> 00:39:02,480
That's a fact potentially of the world.

511
00:39:02,480 --> 00:39:08,000
And saying that this kind of difference in the value of this feature means that we shouldn't

512
00:39:08,000 --> 00:39:13,680
be actually considering that feature is not itself a sufficient argument, even when it

513
00:39:13,680 --> 00:39:17,680
comes to like cases around discrimination.

514
00:39:17,680 --> 00:39:18,920
And this is a complicated issue, right?

515
00:39:18,920 --> 00:39:23,760
Because it may well be the reason that some feature actually is correlated with race,

516
00:39:23,760 --> 00:39:28,120
for instance, is because of a long history of racial discrimination.

517
00:39:28,120 --> 00:39:32,840
So for instance, zip codes, right, they can be very informative when trying to predict

518
00:39:32,840 --> 00:39:38,800
the value of someone's home, but of course at the same time zip codes are highly correlated

519
00:39:38,800 --> 00:39:45,880
with race because of a long-term race-based discrimination in housing, right?

520
00:39:45,880 --> 00:39:47,840
So this is a tricky problem.

521
00:39:47,840 --> 00:39:52,320
In other situations, what happens is that people might understand if I'm talking to people

522
00:39:52,320 --> 00:39:57,200
in practice, is that they will want to find out which features are in fact correlated

523
00:39:57,200 --> 00:39:59,640
with race or gender in their model.

524
00:39:59,640 --> 00:40:03,040
And rather than just stripping them out because they happen to be highly correlated, they

525
00:40:03,040 --> 00:40:04,840
actually will just go look at them.

526
00:40:04,840 --> 00:40:10,080
They'll have a kind of rank-ordered list where the top is those features have been most

527
00:40:10,080 --> 00:40:14,240
correlated with the sensitive feature race or gender would have you.

528
00:40:14,240 --> 00:40:17,240
And then they look at it and they say, is it okay?

529
00:40:17,240 --> 00:40:23,600
Is it like reasonable to consider this feature, even though it is highly correlated with

530
00:40:23,600 --> 00:40:25,400
race and gender?

531
00:40:25,400 --> 00:40:28,320
And in some cases, like the zip code example, you might say no.

532
00:40:28,320 --> 00:40:32,000
You know, there's obvious historical reasons why this is not acceptable.

533
00:40:32,000 --> 00:40:35,440
But in some cases, you might say, yes, you might say, well, this doesn't seem to be the

534
00:40:35,440 --> 00:40:38,280
result of some kind of past injustice.

535
00:40:38,280 --> 00:40:43,320
It may just reflect some true difference in the distribution of this feature in the population.

536
00:40:43,320 --> 00:40:47,320
So we're going to stand by its relevance for the decision at hand.

537
00:40:47,320 --> 00:40:49,720
What's an example that falls into that category?

538
00:40:49,720 --> 00:40:52,680
A good example of this might be something like this.

539
00:40:52,680 --> 00:40:54,200
What university do you go to?

540
00:40:54,200 --> 00:41:00,400
I'm a person trying to figure out who to hire and my model, maybe unsurprisingly, assigns

541
00:41:00,400 --> 00:41:05,120
a lot of significance to people who graduated from, I don't know, the Ivy League, right?

542
00:41:05,120 --> 00:41:09,720
But it may well be that the actual people who graduate from the Ivy League tend to be

543
00:41:09,720 --> 00:41:12,680
disproportionately white, right?

544
00:41:12,680 --> 00:41:16,800
And you might say, well, let's actually not consider this because in a way, it's sort

545
00:41:16,800 --> 00:41:22,920
of like saying, if you're white, you have a better chance of succeeding on this job, right?

546
00:41:22,920 --> 00:41:27,960
If we wanted to tell some story yet, we could about like why it is that the population

547
00:41:27,960 --> 00:41:29,960
in these schools looks the way it does.

548
00:41:29,960 --> 00:41:33,160
And we could potentially get to a point where we feel like it's in fact reasonable to

549
00:41:33,160 --> 00:41:38,720
say, don't consider the university someone want to, but I think a lot of employers would

550
00:41:38,720 --> 00:41:46,000
make a reasonable and plausible case for the relevance of the university you graduated

551
00:41:46,000 --> 00:41:47,000
from, right?

552
00:41:47,000 --> 00:41:51,080
You would say, no, no, no, no, no, like these are actually like reasonable markers of

553
00:41:51,080 --> 00:41:55,040
some lines, performance, like the performance and potential.

554
00:41:55,040 --> 00:41:58,520
And so it's reasonable for us to consider it.

555
00:41:58,520 --> 00:42:04,440
And so, you know, with all that, like where, where does that leave us in terms of, you

556
00:42:04,440 --> 00:42:11,480
know, there's certainly a lot of kind of potential thick gray lines here in terms of the counterpoint

557
00:42:11,480 --> 00:42:17,080
to, I think that example was an example where there is some historical, you know, evidence

558
00:42:17,080 --> 00:42:22,840
of historical discrimination, and certainly that's the case in this university example

559
00:42:22,840 --> 00:42:23,840
that you gave.

560
00:42:23,840 --> 00:42:25,960
So it's not quite the opposite.

561
00:42:25,960 --> 00:42:31,280
Are there concrete examples of how folks have parsed through all of this with some kind

562
00:42:31,280 --> 00:42:36,320
of framework, or is it kind of everyone making a judgment call based on what they think

563
00:42:36,320 --> 00:42:37,320
is right?

564
00:42:37,320 --> 00:42:38,320
That's a great question.

565
00:42:38,320 --> 00:42:41,520
It's a really good way of putting, I think, the current state of affairs.

566
00:42:41,520 --> 00:42:50,480
So I think for some people, there should be some kind of bright line, and that the kind

567
00:42:50,480 --> 00:42:57,040
of university using, you know, your alma mater as a way to determine whether to hire you

568
00:42:57,040 --> 00:43:00,440
should be obviously reasonable, right?

569
00:43:00,440 --> 00:43:04,640
Then there are other people who I think take the view that we, you know, there's lots of

570
00:43:04,640 --> 00:43:07,960
reasons to be suspicious of the admissions policies of those places.

571
00:43:07,960 --> 00:43:12,680
There's lots of reasons to be even more suspicious of the quality of the high school education

572
00:43:12,680 --> 00:43:15,480
that people receive to prepare them to apply to college.

573
00:43:15,480 --> 00:43:16,920
And you can go back even further, right?

574
00:43:16,920 --> 00:43:21,440
The disadvantage you face as a person earlier in life that kind of sets you on this particular

575
00:43:21,440 --> 00:43:22,760
course.

576
00:43:22,760 --> 00:43:28,200
So you know, this may be frustrating, but I think ultimately there are going to be these

577
00:43:28,200 --> 00:43:33,600
ongoing debates around how to even parse this issue, right?

578
00:43:33,600 --> 00:43:37,800
For some people, it will be clear cut that there are certain factors that despite how

579
00:43:37,800 --> 00:43:43,360
correlated they are, that they are legitimate to consider when building these models.

580
00:43:43,360 --> 00:43:47,560
For other people, you know, they can make very strong arguments about the need to actually

581
00:43:47,560 --> 00:43:54,120
use the model development process to compensate for the unfair disadvantage that people had

582
00:43:54,120 --> 00:43:57,040
suffered earlier in their lives.

583
00:43:57,040 --> 00:44:00,680
And ultimately, this is not a machine learning debate, right?

584
00:44:00,680 --> 00:44:04,520
This is not something that is peculiar to building machine learning models.

585
00:44:04,520 --> 00:44:09,560
This ultimately is just the kind of longstanding debate that people have had in general about

586
00:44:09,560 --> 00:44:14,280
the fairness of decision making and certain settings about what is the appropriate role

587
00:44:14,280 --> 00:44:17,000
of discrimination law in general.

588
00:44:17,000 --> 00:44:22,120
So it's unsurprising ultimately that some of these things are not settled or are not

589
00:44:22,120 --> 00:44:26,160
going to be settled in part because people have been arguing about this for at least 50

590
00:44:26,160 --> 00:44:29,960
years when it comes to discrimination law and for millennia when it comes to questions

591
00:44:29,960 --> 00:44:30,960
around fairness.

592
00:44:30,960 --> 00:44:36,480
So did we cover all of the points that you wanted to cover with regards to your paper?

593
00:44:36,480 --> 00:44:42,560
Yeah, I mean, I think the final thing I would just say about is there's going to ultimately

594
00:44:42,560 --> 00:44:47,800
be, there's going to be situations in which the attempt to achieve fairness will require

595
00:44:47,800 --> 00:44:49,480
explanations, right?

596
00:44:49,480 --> 00:44:53,560
You actually know whether or not something is a reasonable thing to consider.

597
00:44:53,560 --> 00:44:58,520
You need to be able to explain what the model is doing and you need to let humans actually

598
00:44:58,520 --> 00:45:03,080
look at it and see if they can kind of weave some story that makes it feel like a reasonable

599
00:45:03,080 --> 00:45:05,760
basis for making these important decisions.

600
00:45:05,760 --> 00:45:13,040
In some cases, we might say the effort to get questions of fairness through explanations

601
00:45:13,040 --> 00:45:14,040
is misguided.

602
00:45:14,040 --> 00:45:18,640
Maybe what we should do instead is just abandon these requirements for explanations and

603
00:45:18,640 --> 00:45:22,240
focus on providing kind of formal fairness guarantees.

604
00:45:22,240 --> 00:45:27,040
Say, you know, we just want to ensure that whatever model we build, we can prove we'll

605
00:45:27,040 --> 00:45:30,160
not have certain problems, right?

606
00:45:30,160 --> 00:45:34,520
And we can do that more directly rather than relying on explanation.

607
00:45:34,520 --> 00:45:38,320
And this just sort of summarizes, I think, the point I was making a moment ago, which

608
00:45:38,320 --> 00:45:43,640
is that this, I think, just sort of ultimately depends on your perspective.

609
00:45:43,640 --> 00:45:47,400
For some people, it will feel inadequate to just provide guarantees.

610
00:45:47,400 --> 00:45:52,560
I can imagine myself actually feeling pretty dissatisfied with something that said, like,

611
00:45:52,560 --> 00:45:57,760
this system is certified fair, but we're never going to tell you how it makes its decisions,

612
00:45:57,760 --> 00:45:58,760
right?

613
00:45:58,760 --> 00:46:02,680
At the same time, I think there's a lot more you can achieve with these kind of formal

614
00:46:02,680 --> 00:46:07,480
approaches to fairness than what people expect they will get out of explanations.

615
00:46:07,480 --> 00:46:10,680
And so I just think there's a role here for both things and an opportunity to spend a

616
00:46:10,680 --> 00:46:14,000
lot more time figuring out when each of those is most appropriate.

617
00:46:14,000 --> 00:46:19,400
Well, Salon, thank you so much for taking the time to chat with me about this service.

618
00:46:19,400 --> 00:46:22,720
It's super interesting and super important as well.

619
00:46:22,720 --> 00:46:23,720
Great.

620
00:46:23,720 --> 00:46:24,720
Yeah, thank you.

621
00:46:24,720 --> 00:46:26,720
I really enjoyed it.

622
00:46:26,720 --> 00:46:28,520
All right, everyone.

623
00:46:28,520 --> 00:46:30,280
That's our show for today.

624
00:46:30,280 --> 00:46:36,560
For more information on Salon or any of the topics covered in this show, visit twimlai.com

625
00:46:36,560 --> 00:46:39,560
slash talk slash 219.

626
00:46:39,560 --> 00:46:57,800
As always, thanks so much for listening and catch you next time.

